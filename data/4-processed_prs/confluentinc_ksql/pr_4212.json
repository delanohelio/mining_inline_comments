{"pr_number": 4212, "pr_title": "docs: add embedded Connect tutorial", "pr_createdAt": "2020-01-02T23:13:42Z", "pr_url": "https://github.com/confluentinc/ksql/pull/4212", "timeline": [{"oid": "a229fd3c432283cef0da62bf5a37f26fce4bcc22", "url": "https://github.com/confluentinc/ksql/commit/a229fd3c432283cef0da62bf5a37f26fce4bcc22", "message": "docs: add embedded Connect tutorial", "committedDate": "2020-01-02T23:10:30Z", "type": "commit"}, {"oid": "2e30cba43817b326914bea67f3a94956e28ae4dc", "url": "https://github.com/confluentinc/ksql/commit/2e30cba43817b326914bea67f3a94956e28ae4dc", "message": "Remove unnecessary /Users/derek.nelson/projects/ksql reference in docker-compose.yml", "committedDate": "2020-01-02T23:37:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY3NTUyNA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362675524", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n          \n          \n            \n            This tutorial shows how to integrate ksqlDB with an external data source to power a simple ridesharing app. The external source is a PostgreSQL database containing relatively static data that describes each driver\u2019s vehicle. This tutorial combines the human-friendly static data with a continuous stream of computer-friendly driver and rider location events to derive an enriched output stream that the ridesharing app uses to set up a rendezvous in real time.", "author": "JimGalasyn", "createdAt": "2020-01-03T00:11:52Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY4MTQ2MQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362681461", "bodyText": "Can you please break these paras at column 80?", "author": "JimGalasyn", "createdAt": "2020-01-03T00:52:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY3NTUyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MTIxMA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362891210", "bodyText": "My bad, I will ensure all lines are broken at 80 chars \ud83d\udc4d", "author": "derekjn", "createdAt": "2020-01-03T17:24:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY3NTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgyOTczNA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362829734", "bodyText": "nit: inconsistent spacing before AS", "author": "colinhicks", "createdAt": "2020-01-03T14:35:56Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzMTU3MA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362831570", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n          \n          \n            \n            Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. Because ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:", "author": "colinhicks", "createdAt": "2020-01-03T14:40:27Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzMTkzMg==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362831932", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.\n          \n          \n            \n            As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional information describing the driver's vehicle.", "author": "colinhicks", "createdAt": "2020-01-03T14:41:22Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (2, 37.7925, -122.4148, 11.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (3, 37.4471, -122.1625, 14.7);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (3, 37.4442, -122.1658);\n+```\n+\n+As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzMzEwNg==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362833106", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This tutorial has demonstrated how to run ksqlDB in embedded Connect mode using Docker. We have used the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.\n          \n          \n            \n            This tutorial demonstrated how to run ksqlDB in embedded Connect mode using Docker. We used the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.", "author": "colinhicks", "createdAt": "2020-01-03T14:44:45Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (2, 37.7925, -122.4148, 11.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (3, 37.4471, -122.1625, 14.7);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (3, 37.4442, -122.1658);\n+```\n+\n+As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.\n+\n+Next steps\n+-------------\n+\n+This tutorial has demonstrated how to run ksqlDB in embedded Connect mode using Docker. We have used the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzNDE3MQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362834171", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n          \n          \n            \n            INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887, -122.4074);", "author": "colinhicks", "createdAt": "2020-01-03T14:47:41Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzNDI0OA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362834248", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);\n          \n          \n            \n            INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876, -122.4235);", "author": "colinhicks", "createdAt": "2020-01-03T14:47:54Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (2, 37.7925, -122.4148, 11.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzNjE4OA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362836188", "bodyText": "The docker-compose files to the right\n\nI suspect \"to the right\" may be vestigial from the quickstart text. On the docs site it will render below, no?", "author": "colinhicks", "createdAt": "2020-01-03T14:53:27Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NDM0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362884345", "bodyText": "Yes, code renders inline.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:03:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzNjE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MDU2Mg==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362890562", "bodyText": "Good catch @colinhicks, missed that one ;)", "author": "derekjn", "createdAt": "2020-01-03T17:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjgzNjE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NTU5Nw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362885597", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n          \n          \n            \n            Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }} installation that ksqlDB is configured to use. The following docker-compose files run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of {{ site.kconnect }} without having to manage a separate {{ site.kconnect }} cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL as an external datastore to integrate with ksqlDB.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:06:56Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NTkxMw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362885913", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n          \n          \n            \n            In an empty local working directory, copy and paste the following `docker-compose` content into a file named `docker-compose.yml`. You will create and add a number of other files to this directory during this tutorial.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:07:54Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NjU3Mw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362886573", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```bash\n          \n          \n            \n            ```yaml", "author": "JimGalasyn", "createdAt": "2020-01-03T17:09:41Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NzA0OQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362887049", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n          \n          \n            \n            In the directory containing the `docker-compose.yml` file you created in the first step, run the following command to start all services in the correct order.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:11:08Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4NzI2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362887266", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Establish an interactive session with PostgreSQL by running this command:\n          \n          \n            \n            Run the following command to establish an interactive session with PostgreSQL.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:11:46Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4ODQ3MA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362888470", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n          \n          \n            \n            In the PostgreSQL session, run the following SQL statements to set up the driver data. You will join this PostgreSQL data with event streams in ksqlDB.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:15:41Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4ODc1NA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362888754", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n          \n          \n            \n            Run the following command to connect to the ksqlDB server and start an interactive command-line interface (CLI) session.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:16:30Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4OTAxMA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362889010", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n          \n          \n            \n            Make your PostgreSQL data accessible to ksqlDB by creating a *source* connector. In the ksqlDB CLI, run the following command.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:17:24Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4OTUwMA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362889500", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n          \n          \n            \n            When the source connector is created, it imports any PostgreSQL tables matching the specified `table.whitelist`. Tables are imported via {{ site.ak }} topics, with one topic per imported table. Once these topics are created, you can interact with them just like any other {{ site.ak }} topic used by ksqlDB.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:18:55Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg4OTg2NA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362889864", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n          \n          \n            \n            In the ksqlDB CLI session, run the following command to verify that the `drivers` table has been imported. Because you specified `jdbc_` as the topic prefix, you should see a `jdbc_drivers` topic in the output.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:20:07Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MDM2NQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362890365", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n          \n          \n            \n            The driver data is now integrated as a {{ site.ak }} topic, but you need to create a ksqlDB table over this topic to begin referencing it from ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a {{ site.ak }} topic, breaking each message in the topic into strongly typed columns.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:21:31Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MDY2OQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362890669", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            11. Create driverLocations and riderLocations streams\n          \n          \n            \n            11. Create streams for driver locations and rider locations", "author": "JimGalasyn", "createdAt": "2020-01-03T17:22:31Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "17a476708d03bbf59b3862ee2f316f7897873390", "url": "https://github.com/confluentinc/ksql/commit/17a476708d03bbf59b3862ee2f316f7897873390", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:22:47Z", "type": "commit"}, {"oid": "75db735d4288443961068e1f4aa0a9d2334e08fe", "url": "https://github.com/confluentinc/ksql/commit/75db735d4288443961068e1f4aa0a9d2334e08fe", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:23:00Z", "type": "commit"}, {"oid": "e9afcd0c9b91cfd0574410299cf4a007b9977620", "url": "https://github.com/confluentinc/ksql/commit/e9afcd0c9b91cfd0574410299cf4a007b9977620", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:23:08Z", "type": "commit"}, {"oid": "7367405af997bf93ae5dd4a1544a635bd6610164", "url": "https://github.com/confluentinc/ksql/commit/7367405af997bf93ae5dd4a1544a635bd6610164", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:23:16Z", "type": "commit"}, {"oid": "8d3bdd17a4e0397938dc68bc09ccd9fd56eabc05", "url": "https://github.com/confluentinc/ksql/commit/8d3bdd17a4e0397938dc68bc09ccd9fd56eabc05", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:23:22Z", "type": "commit"}, {"oid": "c2a1795728509f772f9c6fa8260a065e7cc134b7", "url": "https://github.com/confluentinc/ksql/commit/c2a1795728509f772f9c6fa8260a065e7cc134b7", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:23:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MTE1OA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362891158", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n          \n          \n            \n            In this step, you create streams to encapsulate location pings that are sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, so they're suitable for a continuous stream of location updates.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:23:54Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e1a7d3cea1c0b61a4a74d4360a5a30d90d91f71", "url": "https://github.com/confluentinc/ksql/commit/0e1a7d3cea1c0b61a4a74d4360a5a30d90d91f71", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Colin Hicks <colin.hicks@gmail.com>", "committedDate": "2020-01-03T17:24:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MTI3MQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362891271", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            12. Enrich driverLocations stream by joining with PostgreSQL data\n          \n          \n            \n            12. Enrich the driver locations stream by joining with PostgreSQL data", "author": "JimGalasyn", "createdAt": "2020-01-03T17:24:14Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "44dae0b672271a0191437861153cd7cb7aa14e9e", "url": "https://github.com/confluentinc/ksql/commit/44dae0b672271a0191437861153cd7cb7aa14e9e", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Colin Hicks <colin.hicks@gmail.com>", "committedDate": "2020-01-03T17:25:17Z", "type": "commit"}, {"oid": "fe1416e1cc92341fe230e2eef0f2e9a6d82189b5", "url": "https://github.com/confluentinc/ksql/commit/fe1416e1cc92341fe230e2eef0f2e9a6d82189b5", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Colin Hicks <colin.hicks@gmail.com>", "committedDate": "2020-01-03T17:25:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MTY2OQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362891669", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n          \n          \n            \n            The `driverLocations` stream has a relatively compact schema, and it doesn\u2019t contain much data that a human would find particularly useful. You can *enrich* the stream of driver location events by joining them with the human-friendly vehicle information stored in the PostgreSQL database. This enriched data can be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:25:32Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "97277e20efd132de9c132035c323a587386ff8e4", "url": "https://github.com/confluentinc/ksql/commit/97277e20efd132de9c132035c323a587386ff8e4", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Colin Hicks <colin.hicks@gmail.com>", "committedDate": "2020-01-03T17:25:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MjA1Ng==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362892056", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n          \n          \n            \n            You can achieve this result easily by joining the `driverLocations` stream with the `drivers` table stored in PostgreSQL.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:26:48Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ce9b26bcab7709bb900b34d7a27d805296d5fef8", "url": "https://github.com/confluentinc/ksql/commit/ce9b26bcab7709bb900b34d7a27d805296d5fef8", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Colin Hicks <colin.hicks@gmail.com>", "committedDate": "2020-01-03T17:26:50Z", "type": "commit"}, {"oid": "4013eedfb1ca435cdc1699f83439325544db4a52", "url": "https://github.com/confluentinc/ksql/commit/4013eedfb1ca435cdc1699f83439325544db4a52", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:26:58Z", "type": "commit"}, {"oid": "69effcdb39d21fc82de368e95ae52ab0bb90a1e7", "url": "https://github.com/confluentinc/ksql/commit/69effcdb39d21fc82de368e95ae52ab0bb90a1e7", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:27:06Z", "type": "commit"}, {"oid": "f3019d3779e3877d43d1a9e220e5ef5373fe84d5", "url": "https://github.com/confluentinc/ksql/commit/f3019d3779e3877d43d1a9e220e5ef5373fe84d5", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:27:13Z", "type": "commit"}, {"oid": "80e302420eece195ec899b8046d3992b37e2e7ad", "url": "https://github.com/confluentinc/ksql/commit/80e302420eece195ec899b8046d3992b37e2e7ad", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:27:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MjQ2NQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362892465", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n          \n          \n            \n            To put all of this together, create a final stream that the ridesharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app can use to show the rider their driver\u2019s position as the rider waits to be picked up.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:28:06Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5MjUzMg==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362892532", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            13. Create rendezvous stream\n          \n          \n            \n            13. Create the rendezvous stream", "author": "JimGalasyn", "createdAt": "2020-01-03T17:28:19Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f7dfda192ae6c26df9dd45c59f36d6c785c70da3", "url": "https://github.com/confluentinc/ksql/commit/f7dfda192ae6c26df9dd45c59f36d6c785c70da3", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:28:35Z", "type": "commit"}, {"oid": "bafda0809f601404d20860e37c24868c5bb4ec0a", "url": "https://github.com/confluentinc/ksql/commit/bafda0809f601404d20860e37c24868c5bb4ec0a", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:28:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5Mjg1MQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362892851", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n          \n          \n            \n            The rendezvous stream includes human-friendly information describing the driver\u2019s vehicle for the rider. Also, the rendezvous stream computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:29:23Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6586c126e0b05ad23b68cb68ef82785b64acce88", "url": "https://github.com/confluentinc/ksql/commit/6586c126e0b05ad23b68cb68ef82785b64acce88", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:30:17Z", "type": "commit"}, {"oid": "2d68eac9d08371f7ed1bf04522a3001e5f510719", "url": "https://github.com/confluentinc/ksql/commit/2d68eac9d08371f7ed1bf04522a3001e5f510719", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:30:25Z", "type": "commit"}, {"oid": "eaa95c0c6fb31854b4917e48c63a54180fe68151", "url": "https://github.com/confluentinc/ksql/commit/eaa95c0c6fb31854b4917e48c63a54180fe68151", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:31:27Z", "type": "commit"}, {"oid": "8528a12f4aa4dd7deae392d09dce900a095d2e1f", "url": "https://github.com/confluentinc/ksql/commit/8528a12f4aa4dd7deae392d09dce900a095d2e1f", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:31:35Z", "type": "commit"}, {"oid": "8616654629df8710ab57c93db88c9d57f3388acc", "url": "https://github.com/confluentinc/ksql/commit/8616654629df8710ab57c93db88c9d57f3388acc", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:31:42Z", "type": "commit"}, {"oid": "a3f383147504c718ffd94632a3ebeca3ce35bde7", "url": "https://github.com/confluentinc/ksql/commit/a3f383147504c718ffd94632a3ebeca3ce35bde7", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:31:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5Mzg3NQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362893875", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n          \n          \n            \n            Run the following command twice to open two separate ksqlDB CLI sessions. If you still have a CLI session open from a previous step, you can reuse that session.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:32:31Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NDA5Nw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362894097", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We\u2019re now going to run a continuous query over the rendezvous stream.\n          \n          \n            \n            In this step, you run a continuous query over the rendezvous stream.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:33:07Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "779d5f64979b4f6ebd71a7d3bf6c35b297c6edf3", "url": "https://github.com/confluentinc/ksql/commit/779d5f64979b4f6ebd71a7d3bf6c35b297c6edf3", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:33:37Z", "type": "commit"}, {"oid": "92df1e02b6a23da0573bcb127f36cdfdc43a7b7c", "url": "https://github.com/confluentinc/ksql/commit/92df1e02b6a23da0573bcb127f36cdfdc43a7b7c", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:33:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NDU3NA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362894574", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n          \n          \n            \n            This may feel a bit unfamiliar, because the query never returns until you terminate it. The query perpetually pushes output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as events are written into ksqlDB.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:34:23Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NDk3OQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362894979", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n          \n          \n            \n            Your continuous query reads from the `rendezvous` stream, which takes its input from the `enrichedDriverLocations` and `riderLocations` streams. And `enrichedDriverLocations` takes its input from the `driverLocations` stream, so you need to write data into `driverLocations` and `riderLocations` before `rendezvous` produces the joined output that the continuous query reads.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:35:33Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NTYyMA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362895620", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.\n          \n          \n            \n            As soon as you start writing rows to the input streams, your continuous query from the previous step starts producing joined output. The rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, and additional information describing the driver's vehicle.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:37:20Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (2, 37.7925, -122.4148, 11.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (3, 37.4471, -122.1625, 14.7);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (3, 37.4442, -122.1658);\n+```\n+\n+As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NjMwMg==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362896302", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This tutorial has demonstrated how to run ksqlDB in embedded Connect mode using Docker. We have used the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.\n          \n          \n            \n            This tutorial shows how to run ksqlDB in embedded {{ site.kconnect }} mode using Docker. It uses the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:39:02Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,334 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data source to power a simple ride sharing app. Our external source will be a PostgreSQL database containing relatively static data describing each driver\u2019s vehicle. By combining this human-friendly static data with a continuous stream of computer-friendly driver and rider location events, we derive an enriched output stream that the ride sharing app may use to facilitate a rendezvous in real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on Apache Kafka\u00ae, you'll need to have a Kafka installation running that ksqlDB is configured to use. The docker-compose files to the right will run everything for you via Docker, including ksqlDB running [Kafka Connect](https://docs.confluent.io/current/connect/index.html) in embedded mode. Embedded Connect enables you to leverage the power of Connect without having to manage a separate Connect cluster--ksqlDB will manage one for you. Additionally, we will be using PostgreSQL as an external datastore to integrate with ksqlDB.\n+\n+Copy and paste the below ``docker-compose`` content into a file named ``docker-compose.yml`` in an empty local working directory. We will create and add a number of other files to this directory during this tutorial.\n+\n+```bash\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc) to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB to a separate Connect configuration file. In our docker-compose file, this is done via the ``KSQL_KSQL_CONNECT_WORKER_CONFIG`` environment variable. From within your local working directory, run this command to generate the Connect configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+From a directory containing the ``docker-compose.yml`` file created in the first step, run this command in order to start all services in the correct order:\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Establish an interactive session with PostgreSQL by running this command:\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+From within the session opened in the previous step, run these SQL statements to set up our driver data. We will ultimately join this PostgreSQL data with our event streams in ksqlDB:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run this command to connect to the ksqlDB server and enter an interactive command-line interface (CLI) session:\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Next, we are going to make our PostgreSQL data accessible to ksqlDB by creating a *source* connector. From within the ksqlDB session opened in the previous step, run this command:\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+Once the source connector is created, it will import any PostgreSQL tables matching the specified ``table.whitelist``. Tables are imported via Kafka topics, one topic per imported table. Once these topics are created, we may interact with them just like any other Kafka topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+From within your ksqlDB CLI session, run this command to verify that the drivers table has been imported. Since we\u2019ve specified ``jdbc_`` as our topic prefix, you should see a ``jdbc_drivers`` topic in the output of this command:\n+\n+```bash\n+SHOW TOPICS;\n+```\n+\n+10. Create drivers table in ksqlDB\n+----------------------------------\n+\n+While our driver data is now integrated as a Kafka topic, we\u2019ll want to create a ksqlDB table over this topic in order to begin referencing it from any ksqlDB queries. Streams and tables in ksqlDB essentially associate a schema with a Kafka topic, breaking each message in the topic into strongly typed columns:\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id INTEGER,\n+  make STRING,\n+  model STRING,\n+  year INTEGER,\n+  license_plate STRING,\n+  rating DOUBLE\n+)\n+WITH (kafka_topic='jdbc_drivers', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+Tables in ksqlDB support update semantics, where each message in the underlying topic represents a row in the table. For messages in the topic with the same key, the latest message associated with a given key represents the latest value for the corresponding row in the table.\n+\n+11. Create driverLocations and riderLocations streams\n+-----------------------------------------------------\n+\n+Next we\u2019ll create streams to encapsulate location pings sent every few seconds by drivers\u2019 and riders\u2019 phones. In contrast to tables, ksqlDB streams are append-only collections of events, and therefore suitable for a continuous stream of location updates.\n+\n+```sql\n+CREATE STREAM driverLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE,\n+  speed DOUBLE\n+)\n+WITH (kafka_topic='driver_locations', value_format='json', partitions=1, key='driver_id');\n+\n+CREATE STREAM riderLocations (\n+  driver_id INTEGER,\n+  latitude DOUBLE,\n+  longitude DOUBLE\n+)\n+WITH (kafka_topic='rider_locations', value_format='json', partitions=1, key='driver_id');\n+```\n+\n+12. Enrich driverLocations stream by joining with PostgreSQL data\n+-----------------------------------------------------------------\n+\n+The ``driverLocations`` stream has a relatively compact schema, and doesn\u2019t contain much data that a human would find particularly useful. We\u2019d therefore like to *enrich* our stream of driver location events by joining them with the human-friendly vehicle information stored in our PostgreSQL database. This enriched data may then be presented by the rider\u2019s mobile application, ultimately helping the rider to safely identify the driver\u2019s vehicle.\n+\n+We can easily achieve this result using ksqlDB by simply joining the ``driverLocations`` stream with the ``drivers`` table stored in PostgreSQL:\n+\n+```sql\n+CREATE STREAM enrichedDriverLocations AS\n+  SELECT\n+    dl.driver_id       AS driver_id,\n+    dl.latitude        AS latitude,\n+    dl.longitude       AS longitude,\n+    dl.speed           AS speed,\n+    jdbc.make          AS make,\n+    jdbc.model         AS model,\n+    jdbc.year          AS year,\n+    jdbc.license_plate AS license_plate,\n+    jdbc.rating AS rating\n+  FROM driverLocations dl JOIN drivers jdbc\n+    ON dl.driver_id = jdbc.driver_id\n+  EMIT CHANGES; \n+```\n+\n+13. Create rendezvous stream\n+----------------------------\n+\n+Putting all of this together, we will now create a final stream that the ride sharing app can use to facilitate a driver-rider rendezvous in real time. This stream is defined by a query that joins together rider and driver location updates, resulting in a contextualized output that the app may use to show the rider their driver\u2019s position as the rider waits to be picked up.\n+\n+Our rendezvous stream also includes human-friendly information describing the driver\u2019s vehicle for the rider, and even computes (albeit naively) the driver\u2019s estimated time of arrival (ETA) at the rider\u2019s location:\n+\n+```sql\n+CREATE STREAM rendezvous AS\n+  SELECT\n+    e.license_plate AS license_plate,\n+    e.make          AS make,\n+    e.model         AS model,\n+    e.year          AS year,\n+    e.latitude      AS vehicle_lat,\n+    e.longitude     AS vehicle_long,\n+    GEO_DISTANCE(e.latitude, e.longitude, r.latitude, r.longitude) / e.speed AS eta\n+  FROM enrichedDriverLocations e JOIN riderLocations r WITHIN 1 MINUTE\n+    ON e.driver_id = r.driver_id\n+  EMIT CHANGES;\n+```\n+\n+14. Start two ksqlDB CLI sessions\n+---------------------------------\n+\n+Run this command to twice to open two separate ksqlDB CLI sessions. Will we use both of these sessions in the steps to follow. Note that if you still have a CLI session open from a previous step, you may reuse that session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+15. Run a continuous query\n+--------------------------\n+\n+We\u2019re now going to run a continuous query over the rendezvous stream.\n+\n+This is the first thing that may feel a bit unfamiliar to you, because the query will never return until it's terminated. It will perpetually push output rows to the client as events are written to the rendezvous stream. Leave the query running in your CLI session for now. It will begin producing output as soon as we write events into ksqlDB:\n+\n+```sql\n+SELECT * FROM rendezvous EMIT CHANGES;\n+```\n+\n+16. Write data to input streams\n+-------------------------------\n+\n+Our continuous query is reading from the ``rendezvous`` stream, which takes its input from the ``enrichedDriverLocations`` and ``riderLocations`` streams. And ``enrichedDriverLocations`` takes its input from the ``driverLocations`` stream, so we'll need to write data into ``driverLocations`` and ``riderLocations`` before ``rendezvous`` will produce the joined output that our continuous query will read:\n+\n+```sql\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (0, 37.3965, -122.0818, 23.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (0, 37.3952, -122.0813);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (1, 37.7850, -122.40270, 12.0);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (1, 37.7887,-122.4074);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (2, 37.7925, -122.4148, 11.2);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (2, 37.7876,-122.4235);\n+\n+INSERT INTO driverLocations (driver_id, latitude, longitude, speed) VALUES (3, 37.4471, -122.1625, 14.7);\n+INSERT INTO riderLocations (driver_id, latitude, longitude) VALUES (3, 37.4442, -122.1658);\n+```\n+\n+As soon as you start writing rows to the input streams, your continuous query from the previous step will begin producing joined output: the rider's location pings are joined with their inbound driver's location pings in real time, providing the rider with driver ETA, rating, as well as additional describing the driver's vehicle.\n+\n+Next steps\n+-------------\n+\n+This tutorial has demonstrated how to run ksqlDB in embedded Connect mode using Docker. We have used the JDBC connector to integrate ksqlDB with PostgreSQL data, but this is just one of many connectors that are available to help you integrate ksqlDB with external systems. Check out [Confluent Hub](https://www.confluent.io/hub/) to learn more about all of the various connectors that enable integration with a wide variety of external systems.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9548cea68869037c1d60495ce638377540a3ee12", "url": "https://github.com/confluentinc/ksql/commit/9548cea68869037c1d60495ce638377540a3ee12", "message": "Update docs-md/tutorials/embedded-connect.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:39:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjg5NzUyOQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362897529", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ksqlDB has native integration with Kafka Connect. While ksqlDB can integrate with a separate [Kafka Connect](https://docs.confluent.io/current/connect/index.html) cluster, it can also run Connect embedded within the ksqlDB server, making it unnecessary to run a separate Connect cluster. The [embedded Connect tutorial](embedded-connect.md) provides an overview of how ksqlDB may be configured and used to run Connect in embedded mode.\n          \n          \n            \n            ksqlDB has native integration with {{ site.kconnect }}. While ksqlDB can integrate with a separate [Kafka Connect](https://docs.confluent.io/current/connect/index.html) cluster, it can also run {{ site.kconnect }} embedded within the ksqlDB server, making it unnecessary to run a separate {{ site.kconnect }} cluster. The [embedded Connect tutorial](embedded-connect.md) shows how you can configure ksqlDB to run {{ site.kconnect }} in embedded mode.", "author": "JimGalasyn", "createdAt": "2020-01-03T17:42:33Z", "path": "docs-md/tutorials/index.md", "diffHunk": "@@ -101,6 +102,11 @@ installs. Running the Clickstream demo locally without Docker requires\n that you have {{ site.cp }} installed locally, along with\n Elasticsearch and Grafana.\n \n+ksqlDB with Embedded Connect\n+-------------------------------\n+\n+ksqlDB has native integration with Kafka Connect. While ksqlDB can integrate with a separate [Kafka Connect](https://docs.confluent.io/current/connect/index.html) cluster, it can also run Connect embedded within the ksqlDB server, making it unnecessary to run a separate Connect cluster. The [embedded Connect tutorial](embedded-connect.md) provides an overview of how ksqlDB may be configured and used to run Connect in embedded mode.", "originalCommit": "2e30cba43817b326914bea67f3a94956e28ae4dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "145acf0fe0e15e802866903320b8fcd111e208a6", "url": "https://github.com/confluentinc/ksql/commit/145acf0fe0e15e802866903320b8fcd111e208a6", "message": "Update docs-md/tutorials/index.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-03T17:45:29Z", "type": "commit"}, {"oid": "93856378bd0352d8bc919cce91e0b72b0316a0ef", "url": "https://github.com/confluentinc/ksql/commit/93856378bd0352d8bc919cce91e0b72b0316a0ef", "message": "Copy edits from PR feedback", "committedDate": "2020-01-03T17:47:21Z", "type": "commit"}, {"oid": "32f545402c69a75fbd7e696080f2c1c603aa3033", "url": "https://github.com/confluentinc/ksql/commit/32f545402c69a75fbd7e696080f2c1c603aa3033", "message": "Address PR feedback", "committedDate": "2020-01-03T17:56:32Z", "type": "commit"}, {"oid": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "url": "https://github.com/confluentinc/ksql/commit/cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "message": "Wrap lines at 80 chars", "committedDate": "2020-01-03T18:05:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk3ODM1NA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362978354", "bodyText": "should we give some guidance about when it makes sense to use embedded connect vs a separate cluster?", "author": "rodesai", "createdAt": "2020-01-03T22:11:56Z", "path": "docs-md/tutorials/index.md", "diffHunk": "@@ -101,6 +102,11 @@ installs. Running the Clickstream demo locally without Docker requires\n that you have {{ site.cp }} installed locally, along with\n Elasticsearch and Grafana.\n \n+ksqlDB with Embedded Connect\n+-------------------------------\n+\n+ksqlDB has native integration with {{ site.kconnect }}. While ksqlDB can integrate with a separate [Kafka Connect](https://docs.confluent.io/current/connect/index.html) cluster, it can also run {{ site.kconnect }} embedded within the ksqlDB server, making it unnecessary to run a separate {{ site.kconnect }} cluster. The [embedded Connect tutorial](embedded-connect.md) shows how you can configure ksqlDB to run {{ site.kconnect }} in embedded mode.", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk3OTg0OA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362979848", "bodyText": "maybe specify that the drivers table has been imported as a kafka topic? At first I thought you meant I would see it as a KSQL table.", "author": "rodesai", "createdAt": "2020-01-03T22:18:05Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,409 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n+to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB\n+to a separate Connect configuration file. In our docker-compose file, this is\n+done via the `KSQL_KSQL_CONNECT_WORKER_CONFIG` environment variable. From\n+within your local working directory, run this command to generate the Connect\n+configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+In the directory containing the `docker-compose.yml` file you created in the\n+first step, run the following command to start all services in the correct\n+order.\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Run the following command to establish an interactive session with PostgreSQL.\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+In the PostgreSQL session, run the following SQL statements to set up the\n+driver data. You will join this PostgreSQL data with event streams in ksqlDB.\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run the following command to connect to the ksqlDB server and start an\n+interactive command-line interface (CLI) session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Make your PostgreSQL data accessible to ksqlDB by creating a *source*\n+connector. In the ksqlDB CLI, run the following command.\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+When the source connector is created, it imports any PostgreSQL tables matching\n+the specified `table.whitelist`. Tables are imported via {{ site.ak }} topics,\n+with one topic per imported table. Once these topics are created, you can\n+interact with them just like any other {{ site.ak }} topic used by ksqlDB.\n+\n+9. View imported topic\n+----------------------\n+\n+In the ksqlDB CLI session, run the following command to verify that the\n+`drivers` table has been imported. Because you specified `jdbc_` as the topic", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5MjQ5Mw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362992493", "bodyText": "Latest JDBC source connector is now confluentinc-kafka-connect-jdbc-5.3.2.zip", "author": "mikebin", "createdAt": "2020-01-03T23:21:31Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,409 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n+to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3ODA1OA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r365178058", "bodyText": "Updated", "author": "rmoff", "createdAt": "2020-01-10T10:52:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5MjQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5Mjc1Ng==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362992756", "bodyText": "5.3.2 is now the latest for all CP components (with 5.4.0 coming soon)", "author": "mikebin", "createdAt": "2020-01-03T23:23:17Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,409 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3ODIwNA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r365178204", "bodyText": "updated", "author": "rmoff", "createdAt": "2020-01-10T10:52:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5Mjc1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5MzQxNA==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362993414", "bodyText": "5.3.2 is the current JDBC connector download version on Confluent Hub", "author": "mikebin", "createdAt": "2020-01-03T23:27:28Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,409 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTE3ODE0Mw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r365178143", "bodyText": "Updated", "author": "rmoff", "createdAt": "2020-01-10T10:52:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5MzQxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mjk5NDMwMw==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r362994303", "bodyText": "Would it be worth mentioning the special key config property on the connector? This is unique to ksqlDB.", "author": "mikebin", "createdAt": "2020-01-03T23:33:25Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,409 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n+to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB\n+to a separate Connect configuration file. In our docker-compose file, this is\n+done via the `KSQL_KSQL_CONNECT_WORKER_CONFIG` environment variable. From\n+within your local working directory, run this command to generate the Connect\n+configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+In the directory containing the `docker-compose.yml` file you created in the\n+first step, run the following command to start all services in the correct\n+order.\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Run the following command to establish an interactive session with PostgreSQL.\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+In the PostgreSQL session, run the following SQL statements to set up the\n+driver data. You will join this PostgreSQL data with event streams in ksqlDB.\n+\n+```sql\n+CREATE TABLE drivers (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO drivers (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run the following command to connect to the ksqlDB server and start an\n+interactive command-line interface (CLI) session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Make your PostgreSQL data accessible to ksqlDB by creating a *source*\n+connector. In the ksqlDB CLI, run the following command.\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'drivers',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);\n+\n+```\n+\n+When the source connector is created, it imports any PostgreSQL tables matching", "originalCommit": "cbaa50bbeead67cf2c2327c4c67750e1f8ed7d24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d8efecdc76fc514b0357384503c49e328062ca25", "url": "https://github.com/confluentinc/ksql/commit/d8efecdc76fc514b0357384503c49e328062ca25", "message": "Address PR feedback", "committedDate": "2020-01-04T00:30:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzk1NDk0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r363954945", "bodyText": "Internal converters are not needed, they were deprecated in Apache Kafka 2.0 / KIP -174", "author": "rmoff", "createdAt": "2020-01-07T21:17:21Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,408 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n+to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB\n+to a separate Connect configuration file. In our docker-compose file, this is\n+done via the `KSQL_KSQL_CONNECT_WORKER_CONFIG` environment variable. From\n+within your local working directory, run this command to generate the Connect\n+configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false", "originalCommit": "d8efecdc76fc514b0357384503c49e328062ca25", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzk1NTMwOQ==", "url": "https://github.com/confluentinc/ksql/pull/4212#discussion_r363955309", "bodyText": "Not needed as you've already specified it in the worker config", "author": "rmoff", "createdAt": "2020-01-07T21:18:13Z", "path": "docs-md/tutorials/embedded-connect.md", "diffHunk": "@@ -0,0 +1,408 @@\n+---\n+layout: page\n+title: ksqlDB with Embedded Connect\n+tagline: Run Kafka Connect embedded within ksqlDB\n+description: Learn how to use ksqlDB with embedded Connect to integrate with external data sources and sinks\n+keywords: ksqlDB, connect, PostgreSQL, jdbc\n+---\n+\n+Overview\n+==============\n+\n+This tutorial will demonstrate how to integrate ksqlDB with an external data\n+source to power a simple ride sharing app. Our external source will be a\n+PostgreSQL database containing relatively static data describing each driver\u2019s\n+vehicle. By combining this human-friendly static data with a continuous stream\n+of computer-friendly driver and rider location events, we derive an enriched\n+output stream that the ride sharing app may use to facilitate a rendezvous in\n+real time.\n+\n+1. Get ksqlDB\n+--------------\n+\n+Since ksqlDB runs natively on {{ site.aktm }}, you need a running {{ site.ak }}\n+installation that ksqlDB is configured to use. The following docker-compose\n+files run everything for you via Docker, including ksqlDB running\n+[Kafka Connect](https://docs.confluent.io/current/connect/index.html) in\n+embedded mode. Embedded Connect enables you to leverage the power of\n+{{ site.kconnect }} without having to manage a separate {{ site.kconnect }}\n+cluster, because ksqlDB manages one for you. Also, this tutorial use PostgreSQL\n+as an external datastore to integrate with ksqlDB.\n+\n+In an empty local working directory, copy and paste the following\n+`docker-compose` content into a file named `docker-compose.yml`. You will\n+create and add a number of other files to this directory during this tutorial.\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.3.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.6.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: http://0.0.0.0:8088\n+      KSQL_BOOTSTRAP_SERVERS: broker:9092\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_CONNECT_WORKER_CONFIG: \"/connect/connect.properties\"\n+    volumes:\n+      - ./confluentinc-kafka-connect-jdbc-5.3.1:/usr/share/kafka/plugins/jdbc\n+      - ./connect.properties:/connect/connect.properties\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.6.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+  \n+  postgres:\n+    image: postgres:12\n+    hostname: postgres\n+    container_name: postgres\n+    ports:\n+      - \"5432:5432\"\n+```\n+\n+2. Get the JDBC connector\n+-------------------------\n+\n+[Download the JDBC connector](https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc)\n+to your local working directory. Next, unzip the downloaded archive:\n+\n+```bash\n+unzip confluentinc-kafka-connect-jdbc-5.3.1.zip\n+```\n+\n+3. Configure Connect\n+--------------------\n+\n+In order to tell ksqlDB to run Connect in embedded mode, we must point ksqlDB\n+to a separate Connect configuration file. In our docker-compose file, this is\n+done via the `KSQL_KSQL_CONNECT_WORKER_CONFIG` environment variable. From\n+within your local working directory, run this command to generate the Connect\n+configuration file:\n+\n+```bash\n+cat << EOF > ./connect.properties\n+bootstrap.servers=broker:9092\n+plugin.path=/usr/share/kafka/plugins\n+group.id=ksql-connect-cluster\n+key.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter=org.apache.kafka.connect.json.JsonConverter\n+value.converter.schemas.enable=false\n+internal.key.converter.schemas.enable=false\n+config.storage.topic=ksql-connect-configs\n+offset.storage.topic=ksql-connect-offsets\n+status.storage.topic=ksql-connect-statuses\n+config.storage.replication.factor=1\n+offset.storage.replication.factor=1\n+status.storage.replication.factor=1\n+EOF\n+```\n+\n+4. Start ksqlDB and PostgreSQL\n+------------------------------\n+\n+In the directory containing the `docker-compose.yml` file you created in the\n+first step, run the following command to start all services in the correct\n+order.\n+\n+```bash\n+docker-compose up\n+```\n+\n+5. Connect to PostgreSQL\n+------------------------\n+\n+Run the following command to establish an interactive session with PostgreSQL.\n+\n+```bash\n+docker exec -it postgres psql -U postgres\n+```\n+\n+6. Populate PostgreSQL with vehicle/driver data\n+-----------------------------------------------\n+\n+In the PostgreSQL session, run the following SQL statements to set up the\n+driver data. You will join this PostgreSQL data with event streams in ksqlDB.\n+\n+```sql\n+CREATE TABLE driver_profiles (\n+  driver_id integer PRIMARY KEY,\n+  make text,\n+  model text,\n+  year integer,\n+  license_plate text,\n+  rating float\n+);\n+\n+INSERT INTO driver_profiles (driver_id, make, model, year, license_plate, rating) VALUES\n+  (0, 'Toyota', 'Prius',   2019, 'KAFKA-1', 5.00),\n+  (1, 'Kia',    'Sorento', 2017, 'STREAMS', 4.89),\n+  (2, 'Tesla',  'Model S', 2019, 'CNFLNT',  4.92),\n+  (3, 'Toyota', 'Camry',   2018, 'ILVKSQL', 4.85);\n+```\n+\n+7. Start ksqlDB's interactive CLI\n+---------------------------------\n+\n+ksqlDB runs as a server which clients connect to in order to issue queries.\n+\n+Run the following command to connect to the ksqlDB server and start an\n+interactive command-line interface (CLI) session.\n+\n+```bash\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+8. Create source connector\n+--------------------------\n+\n+Make your PostgreSQL data accessible to ksqlDB by creating a *source*\n+connector. In the ksqlDB CLI, run the following command.\n+\n+```sql\n+CREATE SOURCE CONNECTOR jdbc_source WITH (\n+  'connector.class'          = 'io.confluent.connect.jdbc.JdbcSourceConnector',\n+  'connection.url'           = 'jdbc:postgresql://postgres:5432/postgres',\n+  'connection.user'          = 'postgres',\n+  'topic.prefix'             = 'jdbc_',\n+  'table.whitelist'          = 'driver_profiles',\n+  'mode'                     = 'incrementing',\n+  'numeric.mapping'          = 'best_fit',\n+  'incrementing.column.name' = 'driver_id',\n+  'key'                      = 'driver_id',\n+  'value.converter.schemas.enable' = false);", "originalCommit": "d8efecdc76fc514b0357384503c49e328062ca25", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f11251e3d246a0b834aca1cb2c849396dc232961", "url": "https://github.com/confluentinc/ksql/commit/f11251e3d246a0b834aca1cb2c849396dc232961", "message": "docs: tweaks to embedded connector tutorial (#4252)\n\n* Fix connector config nits\r\n\r\n* Bump to 5.3.2 since JDBC connector is now at 5.3.2 so references to 5.3.1 didn't work\r\n\r\n* Remove partitions from CT because the topic already exists\r\n\r\n* Clarify that new topic is being created\r\n(vs the driverProfiles where the topic already exists)\r\n\r\n* Add note re. Avro and JSON\r\n\r\n* Update docs-md/tutorials/embedded-connect.md\r\n\r\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>\r\n\r\n* Update docs-md/tutorials/embedded-connect.md\r\n\r\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>\r\n\r\n* Update docs-md/tutorials/embedded-connect.md\r\n\r\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>\r\n\r\n* Update docs-md/tutorials/embedded-connect.md\r\n\r\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>\r\n\r\n* Update docs-md/tutorials/embedded-connect.md\r\n\r\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>\r\n\r\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-01-09T19:50:35Z", "type": "commit"}, {"oid": "e9ade744b5698b505ac552198b09b0201509ea59", "url": "https://github.com/confluentinc/ksql/commit/e9ade744b5698b505ac552198b09b0201509ea59", "message": "Add guidance around when to use embedded versus external Connect", "committedDate": "2020-01-13T19:40:41Z", "type": "commit"}]}