{"pr_number": 5122, "pr_title": "Event-driven microservice tutorial", "pr_createdAt": "2020-04-20T23:17:50Z", "pr_url": "https://github.com/confluentinc/ksql/pull/5122", "timeline": [{"oid": "dfb2f7c56e72c26298c17666452bc7f0bf47b9d3", "url": "https://github.com/confluentinc/ksql/commit/dfb2f7c56e72c26298c17666452bc7f0bf47b9d3", "message": "docs: initial code for tutorial", "committedDate": "2020-04-20T23:15:56Z", "type": "commit"}, {"oid": "77dc6a3086efedab31dd8c0908e86005eb7fbf1b", "url": "https://github.com/confluentinc/ksql/commit/77dc6a3086efedab31dd8c0908e86005eb7fbf1b", "message": "docs: event driven microservice tutorial", "committedDate": "2020-04-22T22:25:58Z", "type": "commit"}, {"oid": "8885e77c8ce3ce549f6c09ab642cf9d08ffc26b4", "url": "https://github.com/confluentinc/ksql/commit/8885e77c8ce3ce549f6c09ab642cf9d08ffc26b4", "message": "docs: final copy edit", "committedDate": "2020-04-22T22:47:41Z", "type": "commit"}, {"oid": "00d9c7df988cca776f88ea288c706163d9cb06d0", "url": "https://github.com/confluentinc/ksql/commit/00d9c7df988cca776f88ea288c706163d9cb06d0", "message": "docs: factor out version tokens", "committedDate": "2020-04-22T23:13:50Z", "type": "commit"}, {"oid": "144c0821773ada7c67d3aea4d2b1c88bda885c71", "url": "https://github.com/confluentinc/ksql/commit/144c0821773ada7c67d3aea4d2b1c88bda885c71", "message": "Merge branch 'master' into mdrogalis-event-driven-tutorial", "committedDate": "2020-04-22T23:15:32Z", "type": "commit"}, {"oid": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "url": "https://github.com/confluentinc/ksql/commit/fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "message": "docs: move docs to new dir", "committedDate": "2020-04-22T23:17:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNTE4NA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413405184", "bodyText": "Nice use of site vars!", "author": "JimGalasyn", "createdAt": "2020-04-22T23:39:19Z", "path": "docs/tutorials/etl.md", "diffHunk": "@@ -108,7 +108,7 @@ services:\n       discovery.type: single-node\n \n   zookeeper:\n-    image: confluentinc/cp-zookeeper:5.4.0\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNTU3Mg==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413405572", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n          \n          \n            \n            A common way that you might implement this architecture is to feed event streams into {{ site.ak }}, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?", "author": "JimGalasyn", "createdAt": "2020-04-22T23:40:25Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNjA0NQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413406045", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n          \n          \n            \n            This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomalies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:41:28Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNjQ1Mg==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413406452", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n          \n          \n            \n            We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions`) does not exist yet, ksqlDB creates it on your behalf.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:42:37Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNjc4MA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413406780", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n          \n          \n            \n            Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it uses the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out-of-order events.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:43:25Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwNzAxOQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413407019", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n          \n          \n            \n            The stream is also configured to use the `Avro` format for the value part of the underlying {{ site.ak }} records that it generates. Because ksqlDB has been configured with {{ site.sr }} (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:44:02Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwODAzNQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413408035", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Let's break down what this does:\n          \n          \n            \n            Here's what this statement does:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:46:38Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwODI5Nw==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413408297", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n          \n          \n            \n            - The underlying {{ site.ak }} topic for this table is explicitly set to `possible_anomalies`.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:47:21Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwODQyMA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413408420", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n          \n          \n            \n            - The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. You'll use this later in the microservice.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:47:42Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwODgxMg==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413408812", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n          \n          \n            \n            Check what anomalies the table picked up. Run the following statement to select a stream of events emitted from the table:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:48:42Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwODkyMg==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413408922", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n          \n          \n            \n            This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30-second tumbling window:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:48:59Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwOTA4Nw==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413409087", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n          \n          \n            \n            You can also print out the contents of the underlying {{ site.ak }} topic for this table, which you will programmatically access in the microservice:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:49:24Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwOTU0NA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413409544", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n          \n          \n            \n            Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. You want to send an email each time an anomaly is found. To do that, you'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, just use a {{ site.ak }} consumer client.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:50:41Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQwOTgzMQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413409831", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n          \n          \n            \n            Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the `anomalies` {{ site.ak }} topic and sending an email for each event it receives. Dependencies are declared on {{ site.ak }}, Avro, SendGrid, and a few other things:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:51:18Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxMDUzMA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413410530", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:\n          \n          \n            \n            To ensure that your microservice logs output to the console, create the following file at `src/main/resources/log4j.properties`:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:53:26Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxMDgxMg==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413410812", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Before you can begin coding your microservice, you'll need access to the Avro schemas that the Kafka topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of Schema Registry to your local machine:\n          \n          \n            \n            Before you can begin coding your microservice, you'll need access to the Avro schemas that the {{ site.ak }} topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of {{ site.sr }} to your local machine:", "author": "JimGalasyn", "createdAt": "2020-04-22T23:54:09Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:\n+\n+```\n+# Root logger option\n+log4j.rootLogger=WARN, stdout\n+ \n+# Direct log messages to stdout\n+log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n+log4j.appender.stdout.Target=System.err\n+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n+log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n+```\n+\n+### Download and compile the Avro schemas\n+\n+Before you can begin coding your microservice, you'll need access to the Avro schemas that the Kafka topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of Schema Registry to your local machine:", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxMTAzMQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413411031", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the Kafka records of the `possible_anomalies` topic.\n          \n          \n            \n            You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the {{ site.ak }} records of the `possible_anomalies` topic.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:54:48Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:\n+\n+```\n+# Root logger option\n+log4j.rootLogger=WARN, stdout\n+ \n+# Direct log messages to stdout\n+log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n+log4j.appender.stdout.Target=System.err\n+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n+log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n+```\n+\n+### Download and compile the Avro schemas\n+\n+Before you can begin coding your microservice, you'll need access to the Avro schemas that the Kafka topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of Schema Registry to your local machine:\n+\n+```\n+mvn schema-registry:download\n+```\n+\n+You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the Kafka records of the `possible_anomalies` topic.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxMTMzOQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413411339", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now we can write the code that triggers side effects when anomalies are found. Add the following Java file at `src/main/java/io/ksqldb/tutorial/EmailSender.java`. This is a simple program that consumes events from Kafka and sends an email with SendGrid for each one it finds. There are a few constants to fill in, including a SendGrid API key. You can get one by signing up for SendGrid, if you wish.\n          \n          \n            \n            Now we can write the code that triggers side effects when anomalies are found. Add the following Java file at `src/main/java/io/ksqldb/tutorial/EmailSender.java`. This is a simple program that consumes events from {{ site.ak }} and sends an email with SendGrid for each one it finds. There are a few constants to fill in, including a SendGrid API key. You can get one by signing up for SendGrid.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:55:38Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:\n+\n+```\n+# Root logger option\n+log4j.rootLogger=WARN, stdout\n+ \n+# Direct log messages to stdout\n+log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n+log4j.appender.stdout.Target=System.err\n+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n+log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n+```\n+\n+### Download and compile the Avro schemas\n+\n+Before you can begin coding your microservice, you'll need access to the Avro schemas that the Kafka topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of Schema Registry to your local machine:\n+\n+```\n+mvn schema-registry:download\n+```\n+\n+You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the Kafka records of the `possible_anomalies` topic.\n+\n+Next, compile the Avro schema into a Java file. The [Avro Maven plugin](https://avro.apache.org/docs/current/gettingstartedjava.html) (already added to the `pom.xml` file, too) makes this simple:\n+\n+```\n+mvn generate-sources\n+```\n+\n+You should now have a file called `target/generated-sources/io/ksqldb/tutorial/PossibleAnomaly.java` containing the compiled Java code.\n+\n+### Write the Kafka consumer code\n+\n+Now we can write the code that triggers side effects when anomalies are found. Add the following Java file at `src/main/java/io/ksqldb/tutorial/EmailSender.java`. This is a simple program that consumes events from Kafka and sends an email with SendGrid for each one it finds. There are a few constants to fill in, including a SendGrid API key. You can get one by signing up for SendGrid, if you wish.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxMjE4Mw==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r413412183", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Finally, note that ksqlDB emits a new event every time a tumbling window changes. ksqlDB uses a model called refinements to continutally emit new changes to stateful aggregations. For example, if an anomaly was detected because 3 credit card transactions were found in a given interval, an event would be emitted from the table. If a 4th was detected in the same interval, another event will be emitted. Because SendGrid does not (at the time of writing) support idempotent email submission, you would need to have a small piece of logic in your program to prevent sending an email multiple times for the same period. We've ommitted this for brevity.\n          \n          \n            \n            Finally, note that ksqlDB emits a new event every time a tumbling window changes. ksqlDB uses a model called \"refinements\" to continually emit new changes to stateful aggregations. For example, if an anomaly was detected because three credit card transactions were found in a given interval, an event would be emitted from the table. If a fourth is detected in the same interval, another event is emitted. Because SendGrid does not (at the time of writing) support idempotent email submission, you would need to have a small piece of logic in your program to prevent sending an email multiple times for the same period. This is omitted for brevity.", "author": "JimGalasyn", "createdAt": "2020-04-22T23:57:40Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into Kafka, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomolies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions)` does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it will use the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out of order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying Kafka records that it generates. Becuse ksqlDB has been configured with Schema Registry (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Let's break down what this does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying Kafka topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. We'll use this later in the microservice.\n+\n+Let's see what anomalies the table picked up. Run the following to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30 second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying Kafka topic for this table, which we will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. We want to send an email each time an anomaly is found. To do that, we'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, we'll just use a Kafka consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the anomalies Kafka topic and sending an email for each event it receives. Dependencies are declared on Kafka, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that our microservice will log output to the console, create the following at `src/main/resources/log4j.properties`:\n+\n+```\n+# Root logger option\n+log4j.rootLogger=WARN, stdout\n+ \n+# Direct log messages to stdout\n+log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n+log4j.appender.stdout.Target=System.err\n+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n+log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n+```\n+\n+### Download and compile the Avro schemas\n+\n+Before you can begin coding your microservice, you'll need access to the Avro schemas that the Kafka topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of Schema Registry to your local machine:\n+\n+```\n+mvn schema-registry:download\n+```\n+\n+You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the Kafka records of the `possible_anomalies` topic.\n+\n+Next, compile the Avro schema into a Java file. The [Avro Maven plugin](https://avro.apache.org/docs/current/gettingstartedjava.html) (already added to the `pom.xml` file, too) makes this simple:\n+\n+```\n+mvn generate-sources\n+```\n+\n+You should now have a file called `target/generated-sources/io/ksqldb/tutorial/PossibleAnomaly.java` containing the compiled Java code.\n+\n+### Write the Kafka consumer code\n+\n+Now we can write the code that triggers side effects when anomalies are found. Add the following Java file at `src/main/java/io/ksqldb/tutorial/EmailSender.java`. This is a simple program that consumes events from Kafka and sends an email with SendGrid for each one it finds. There are a few constants to fill in, including a SendGrid API key. You can get one by signing up for SendGrid, if you wish.\n+\n+```java\n+package io.ksqldb.tutorial;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+\n+import com.sendgrid.SendGrid;\n+import com.sendgrid.Request;\n+import com.sendgrid.Response;\n+import com.sendgrid.Method;\n+import com.sendgrid.helpers.mail.Mail;\n+import com.sendgrid.helpers.mail.objects.Email;\n+import com.sendgrid.helpers.mail.objects.Content;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.FormatStyle;\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.Locale;\n+import java.io.IOException;\n+\n+public class EmailSender {\n+\n+    private final static String BOOTSTRAP_SERVERS = \"localhost:29092\";\n+    private final static String SCHEMA_REGISTRY_URL = \"http://localhost:8081\";\n+    private final static String TOPIC = \"possible_anomalies\";\n+    private final static String FROM_EMAIL = \"test@example.com\";\n+    private final static String SENDGRID_API_KEY = \"\";\n+    private final static SendGrid sg = new SendGrid(SENDGRID_API_KEY);\n+\n+    private final static DateTimeFormatter formatter =\n+            DateTimeFormatter.ofLocalizedDateTime(FormatStyle.SHORT)\n+                    .withLocale(Locale.US)\n+                    .withZone(ZoneId.systemDefault());\n+\n+    public static void main(final String[] args) throws IOException {\n+        final Properties props = new Properties();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"email-sender\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");\n+        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);\n+        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);\n+\n+        try (final KafkaConsumer<String, PossibleAnomaly> consumer = new KafkaConsumer<>(props)) {\n+            consumer.subscribe(Collections.singletonList(TOPIC));\n+\n+            while (true) {\n+                final ConsumerRecords<String, PossibleAnomaly> records = consumer.poll(Duration.ofMillis(100));\n+                for (final ConsumerRecord<String, PossibleAnomaly> record : records) {\n+                    final PossibleAnomaly value = record.value();\n+\n+                    if (value != null) {\n+                        sendEmail(value);\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static void sendEmail(PossibleAnomaly anomaly) throws IOException {\n+        Email from = new Email(FROM_EMAIL);\n+        Email to = new Email(anomaly.getEmailAddress().toString());\n+        String subject = makeSubject(anomaly);\n+        Content content = new Content(\"text/plain\", makeContent(anomaly));\n+        Mail mail = new Mail(from, subject, to, content);\n+\n+        Request request = new Request();\n+        try {\n+            request.setMethod(Method.POST);\n+            request.setEndpoint(\"mail/send\");\n+            request.setBody(mail.build());\n+            Response response = sg.api(request);\n+            System.out.println(\"Attempted to send email!\\n\");\n+            System.out.println(\"Status code: \" + response.getStatusCode());\n+            System.out.println(\"Body: \" + response.getBody());\n+            System.out.println(\"Headers: \" + response.getHeaders());\n+            System.out.println(\"======================\");\n+        } catch (IOException ex) {\n+            throw ex;\n+        }\n+    }\n+\n+    private static String makeSubject(PossibleAnomaly anomaly) {\n+        return \"Suspicious activity detected for card \" + anomaly.getCardNumber();\n+    }\n+\n+    private static String makeContent(PossibleAnomaly anomaly) {\n+        return String.format(\"Found suspicious activity for card number %s. %s transactions were made for a total of %s between %s and %s\",\n+                anomaly.getCardNumber(),\n+                anomaly.getNAttempts(),\n+                anomaly.getTotalAmount(),\n+                formatter.format(Instant.ofEpochMilli(anomaly.getStartBoundary())),\n+                formatter.format(Instant.ofEpochMilli(anomaly.getEndBoundary())));\n+    }\n+\n+}\n+```\n+\n+### Run the microservice\n+\n+Compile the program with:\n+\n+```\n+mvn compile\n+```\n+\n+And run it:\n+\n+```\n+mvn exec:java -Dexec.mainClass=\"io.ksqldb.tutorial.EmailSender\"\n+```\n+\n+If everything is configured correctly, emails will be sent whenever an anomaly is detected. There are a few things to note with this simple implementation.\n+\n+First, if you start more instances of this microservice, the partitions of the `possible_anomalies` topic will be load balanced across them. This takes advantage of the standard [Kafka consumer groups](https://kafka.apache.org/documentation/#intro_consumers) behavior.\n+\n+Second, this microservice is configured to checkpoint its progress every `100` milliseconds through the `ENABLE_AUTO_COMMIT_CONFIG` configuration. That means any successfully processed messages will not be reprocessed if the microservice is taken down and turned on again.\n+\n+Finally, note that ksqlDB emits a new event every time a tumbling window changes. ksqlDB uses a model called refinements to continutally emit new changes to stateful aggregations. For example, if an anomaly was detected because 3 credit card transactions were found in a given interval, an event would be emitted from the table. If a 4th was detected in the same interval, another event will be emitted. Because SendGrid does not (at the time of writing) support idempotent email submission, you would need to have a small piece of logic in your program to prevent sending an email multiple times for the same period. We've ommitted this for brevity.", "originalCommit": "fa4d2f9d15019e32b715d6725cee1d04820fb5c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f91aa042e3711caf46f0a74c5ee767570cd2e271", "url": "https://github.com/confluentinc/ksql/commit/f91aa042e3711caf46f0a74c5ee767570cd2e271", "message": "Apply suggestions from code review\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-04-23T15:24:04Z", "type": "commit"}, {"oid": "5454c6be3f226b093594865f8c3ae4f0a63c30eb", "url": "https://github.com/confluentinc/ksql/commit/5454c6be3f226b093594865f8c3ae4f0a63c30eb", "message": "Apply suggestions from code review\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-04-23T15:28:37Z", "type": "commit"}, {"oid": "738223d818697fd056bf908eacba24ca665d7afa", "url": "https://github.com/confluentinc/ksql/commit/738223d818697fd056bf908eacba24ca665d7afa", "message": "docs: fix topic name", "committedDate": "2020-04-23T15:29:05Z", "type": "commit"}, {"oid": "a6a29f36e262b38920e1bcfa8fe95a2fdf2b1be1", "url": "https://github.com/confluentinc/ksql/commit/a6a29f36e262b38920e1bcfa8fe95a2fdf2b1be1", "message": "docs: add to index", "committedDate": "2020-04-23T15:33:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAwOTAwNA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r414009004", "bodyText": "Should we include a short section at the end about tearing down the tutorial once done with it, by running docker-compose down and stopping the microservice?", "author": "vcrfxia", "createdAt": "2020-04-23T18:01:05Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into {{ site.ak }}, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomalies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up", "originalCommit": "a6a29f36e262b38920e1bcfa8fe95a2fdf2b1be1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDA0MDY1NQ==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r414040655", "bodyText": "This is a great idea. I've added it to each of the three tutorials.", "author": "MichaelDrogalis", "createdAt": "2020-04-23T18:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAwOTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAxMDM3Ng==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r414010376", "bodyText": "Maybe add comments to make it clear which of these fields need to be modified/populated by the tutorial user? \"There are a few constants to fill in, including a SendGrid API key\" is a bit vague otherwise.", "author": "vcrfxia", "createdAt": "2020-04-23T18:03:12Z", "path": "docs/tutorials/event-driven-microservice.md", "diffHunk": "@@ -0,0 +1,613 @@\n+What is it?\n+-----------\n+\n+An event-driven microservice is a pattern in which a piece of code only communicates with the outside world through messages called events. This technique can dramatically simplify an architecture because each microservice only receives and emits information from clearly defined communication channels. Because state is localized within each microservice, complexity is tightly contained.\n+\n+![hard](../img/microservice-hard.png){: class=\"centered-img\" style=\"width: 90%\"}\n+\n+A common way that you might implement this architecture is to feed event streams into {{ site.ak }}, read them with a stream processing framework, and trigger side-effects whenever something of interest happens \u2014 like sending an email with [Twilio SendGrid](https://sendgrid.com/). This works, but it's up to you to blend your stream processing, state, and side-effects logic in a maintainable way. Is there a better approach?\n+\n+Why ksqlDB?\n+-----------\n+\n+Scaling stateful services is challenging. Coupling a stateful service with the responsibility of triggering side-effects makes it even harder. It\u2019s up to you to manage both as if they were one, even though they might have completely different needs. If you want to change how side-effects behave, you also need to redeploy your stateful stream processor. ksqlDB helps simplify this by splitting things up: stateful stream processing is managed on a cluster of servers, while side-effects run inside your stateless microservice.\n+\n+![easy](../img/microservice-easy.png){: class=\"centered-img\" style=\"width: 70%\"}\n+\n+Using ksqlDB, you can isolate complex stateful operations within ksqlDB\u2019s runtime. Your app stays simple because it is stateless. It merely reads events from a Kafka topic and executes side-effects as needed.\n+\n+Implement it\n+------------\n+\n+Imagine that you work at a financial services company which clears many credit card transactions each day. You want to prevent malicious activity in your customer base. When a high number of transactions occurs in a narrow window of time, you want to notify the cardholder of suspicious activity.\n+\n+This tutorial shows how to create an event-driven microservice that identifies suspicious activity and notifies customers. It demonstrates finding anomalies with ksqlDB and sending alert emails using a simple Kafka consumer with SendGrid.\n+\n+### Start the stack\n+\n+To get started, create the following `docker-compose.yml` file. This specifies all the infrastructure that you'll need to run this tutorial:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.release }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.release }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+### Create the transactions stream\n+\n+Connect to ksqlDB's server by using its interactive CLI. Run the following command from your host:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Before you issue more commands, tell ksqlDB to start all queries from earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+We want to model a stream of credit card transactions from which we'll look for anomalous activity. To do that, create a ksqlDB stream to represent the transactions. Each transaction has a few key pieces of information, like the card number, amount, and email address that it's associated with. Because the specified topic (`transactions`) does not exist yet, ksqlDB creates it on your behalf.\n+\n+```sql\n+CREATE STREAM transactions (\n+    email_address VARCHAR,\n+    card_number VARCHAR,\n+    tx_id VARCHAR,\n+    timestamp VARCHAR,\n+    amount DECIMAL(12, 2)\n+) WITH (\n+    kafka_topic = 'transactions',\n+    key = 'tx_id',\n+    partitions = 8,\n+    value_format = 'avro',\n+    timestamp = 'timestamp',\n+    timestamp_format = 'yyyy-MM-dd''T''HH:mm:ss'\n+);\n+```\n+\n+Notice that this stream is configured with a custom `timestamp` to signal that [event-time](../../concepts/time-and-windows-in-ksqldb-queries/#event-time) should be used instead of [processing-time](../../concepts/time-and-windows-in-ksqldb-queries/#processing-time). What this means is that when ksqlDB does time-related operations over the stream, it uses the `timestamp` column to measure time, not the current time of the operating system. This makes it possible to handle out-of-order events.\n+\n+The stream is also configured to use the `Avro` format for the value part of the underlying {{ site.ak }} records that it generates. Because ksqlDB has been configured with {{ site.sr }} (as part of the Docker Compose file), the schemas of each stream and table are centrally tracked. We'll make use of this in our microservice later.\n+\n+### Seed some transaction events\n+\n+With the stream in place, seed it with some initial events. Run these statements at the CLI:\n+\n+```sql\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'f88c5ebb-699c-4a7b-b544-45b30681cc39',\n+    '2020-04-22T03:19:58',\n+    50.25\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '0cf100ca-993c-427f-9ea5-e892ef350363',\n+    '2020-04-25T12:50:30',\n+    18.97\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:45:15',\n+    12.50\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    '044530c0-b15d-4648-8f05-940acc321eb7',\n+    '2020-04-22T03:19:54',\n+    103.43\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    '5d916e65-1af3-4142-9fd3-302dd55c512f',\n+    '2020-04-25T12:50:25',\n+    3200.80\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'derek@example.com',\n+    '352642227248344',\n+    'd7d47fdb-75e9-46c0-93f6-d42ff1432eea',\n+    '2020-04-25T12:51:55',\n+    154.32\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'michael@example.com',\n+    '358579699410099',\n+    'c5719d20-8d4a-47d4-8cd3-52ed784c89dc',\n+    '2020-04-22T03:19:32',\n+    78.73\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    '2360d53e-3fad-4e9a-b306-b166b7ca4f64',\n+    '2020-04-19T09:45:35',\n+    234.65\n+);\n+\n+INSERT INTO transactions (\n+    email_address, card_number, tx_id, timestamp, amount\n+) VALUES (\n+    'colin@example.com',\n+    '373913272311617',\n+    'de9831c0-7cf1-4ebf-881d-0415edec0d6b',\n+    '2020-04-19T09:44:03',\n+    150.00\n+);\n+```\n+\n+### Create the anomalies table\n+\n+If a single credit card is transacted many times within a short duration, there's probably something suspicious going on. A table is an ideal choice to model this because you want to aggregate events over time and find activity that spans multiple events. Run the following statement:\n+\n+```sql\n+CREATE TABLE possible_anomalies WITH (\n+    kafka_topic = 'possible_anomalies',\n+    VALUE_AVRO_SCHEMA_FULL_NAME = 'io.ksqldb.tutorial.PossibleAnomaly'\n+)   AS\n+    SELECT latest_by_offset(email_address) AS `email_address`,\n+           card_number AS `card_number`,\n+           count(*) AS `n_attempts`,\n+           sum(amount) AS `total_amount`,\n+           collect_list(tx_id) AS `tx_ids`,\n+           WINDOWSTART as `start_boundary`,\n+           WINDOWEND as `end_boundary`\n+    FROM transactions\n+    WINDOW TUMBLING (SIZE 30 SECONDS)\n+    GROUP BY card_number\n+    HAVING count(*) >= 3\n+    EMIT CHANGES;\n+```\n+\n+Here's what this statement does:\n+\n+- For each credit card number, 30 second [tumbling windows](../../concepts/time-and-windows-in-ksqldb-queries/#tumbling-window) are created to group activity. A new row is inserted into the table when at least 3 transactions take place inside a given window.\n+- The individual transaction IDs and amounts that make up the window are collected as lists.\n+- The last transaction's email address is \"carried forward\" with `latest_by_offset`.\n+- Column aliases are surrounded by backticks, which tells ksqlDB to use exactly that case. ksqlDB uppercases identity names by default.\n+- The underlying {{ site.ak }} topic for this table is explicitly set to `possible_anomalies`.\n+- The Avro schema that ksqlDB generates for the value portion of its records is recorded under the namespace `io.ksqldb.tutorial.PossibleAnomaly`. You'll use this later in the microservice.\n+\n+Check what anomalies the table picked up. Run the following statement to select a stream of events emitted from the table:\n+\n+```sql\n+SELECT * FROM possible_anomalies EMIT CHANGES;\n+```\n+\n+This should yield a single row. Three transactions for card number `358579699410099` were recorded with timestamps within a single 30-second tumbling window:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWTIME           |ROWKEY            |WINDOWSTART       |WINDOWEND         |email_address     |card_number       |n_attempts        |total_amount      |tx_ids            |start_boundary    |end_boundary      |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|1587525598000     |358579699410099   |1587525570000     |1587525600000     |michael@example.co|358579699410099   |3                 |232.41            |[044530c0-b15d-464|1587525570000     |1587525600000     |\n+|                  |                  |                  |                  |m                 |                  |                  |                  |8-8f05-940acc321eb|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |7, f88c5ebb-699c-4|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |a7b-b544-45b30681c|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c39, c5719d20-8d4a|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |-47d4-8cd3-52ed784|                  |                  |\n+|                  |                  |                  |                  |                  |                  |                  |                  |c89dc]            |                  |                  |\n+```\n+\n+You can also print out the contents of the underlying {{ site.ak }} topic for this table, which you will programmatically access in the microservice:\n+\n+```sql\n+PRINT 'possible_anomalies' FROM BEGINNING;\n+```\n+\n+### Create a Kafka client project\n+\n+Notice that so far, all the heavy lifting happens inside of ksqlDB. ksqlDB takes care of the stateful stream processing. Triggering side-effects will be delegated to a light-weight service that consumes from a Kafka topic. You want to send an email each time an anomaly is found. To do that, you'll implement a simple, scalable microservice. In practice, you might use [Kafka Streams](https://kafka.apache.org/documentation/streams/) to handle this piece, but to keep things simple, just use a {{ site.ak }} consumer client.\n+\n+Start by creating a `pom.xml` file for your microservice. This simple microservice will run a loop, reading from the `possible_anomalies` {{ site.ak }} topic and sending an email for each event it receives. Dependencies are declared on {{ site.ak }}, Avro, SendGrid, and a few other things:\n+\n+```xml\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <groupId>io.ksqldb</groupId>\n+  <artifactId>email-sender</artifactId>\n+  <version>0.0.1</version>\n+\n+  <properties>\n+    <!-- Keep versions as properties to allow easy modification -->\n+    <java.version>8</java.version>\n+    <confluent.version>{{ site.cprelease }}</confluent.version>\n+    <kafka.version>2.5.0</kafka.version>\n+    <avro.version>1.9.1</avro.version>\n+    <slf4j.version>1.7.30</slf4j.version>\n+    <sendgrid.version>4.4.8</sendgrid.version>\n+    <!-- Maven properties for compilation -->\n+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n+  </properties>\n+\n+  <repositories>\n+    <repository>\n+      <id>confluent</id>\n+      <name>Confluent</name>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </repository>\n+  </repositories>\n+\n+  <pluginRepositories>\n+    <pluginRepository>\n+      <id>confluent</id>\n+      <url>https://packages.confluent.io/maven/</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n+  <dependencies>  \n+    <!-- Add the Kafka dependencies -->\n+    <dependency>\n+      <groupId>io.confluent</groupId>\n+      <artifactId>kafka-avro-serializer</artifactId>\n+      <version>${confluent.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+      <version>${kafka.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro</artifactId>\n+      <version>${avro.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+      <version>${slf4j.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.sendgrid</groupId>\n+      <artifactId>sendgrid-java</artifactId>\n+      <version>${sendgrid.version}</version>\n+    </dependency>\n+  </dependencies>\n+\n+  <build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <version>3.8.1</version>\n+        <configuration>\n+          <source>${java.version}</source>\n+          <target>${java.version}</target>\n+          <compilerArgs>\n+            <arg>-Xlint:all</arg>\n+          </compilerArgs>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.avro</groupId>\n+        <artifactId>avro-maven-plugin</artifactId>\n+        <version>${avro.version}</version>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>schema</goal>\n+            </goals>\n+            <configuration>\n+              <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>\n+              <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>\n+              <enableDecimalLogicalType>true</enableDecimalLogicalType>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-maven-plugin</artifactId>\n+        <version>${confluent.version}</version>\n+        <configuration>\n+          <schemaRegistryUrls>\n+            <param>http://localhost:8081</param>\n+          </schemaRegistryUrls>\n+          <outputDirectory>src/main/avro</outputDirectory>\n+          <subjectPatterns>\n+            <param>possible_anomalies-value</param>\n+          </subjectPatterns>\n+          <prettyPrintSchemas>true</prettyPrintSchemas>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+  </build>\n+</project>\n+```\n+\n+Create the directory structure for the rest of the project:\n+\n+```\n+mkdir -p src/main/java/io/ksqldb/tutorial src/main/resources src/main/avro\n+```\n+\n+To ensure that your microservice logs output to the console, create the following file at `src/main/resources/log4j.properties`:\n+\n+```\n+# Root logger option\n+log4j.rootLogger=WARN, stdout\n+ \n+# Direct log messages to stdout\n+log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n+log4j.appender.stdout.Target=System.err\n+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n+log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n+```\n+\n+### Download and compile the Avro schemas\n+\n+Before you can begin coding your microservice, you'll need access to the Avro schemas that the {{ site.ak }} topic is serialized with. Confluent has [a Maven plugin](https://docs.confluent.io/current/schema-registry/develop/maven-plugin.html) that makes this simple, which you might have already noticed is present in the `pom.xml` file. Run the following command, which downloads the required Avro schema out of {{ site.sr }} to your local machine:\n+\n+```\n+mvn schema-registry:download\n+```\n+\n+You should now have a file called `src/main/avro/possible_anomalies-value.avsc`. This is the Avro schema generated by ksqlDB for the value portion of the {{ site.ak }} records of the `possible_anomalies` topic.\n+\n+Next, compile the Avro schema into a Java file. The [Avro Maven plugin](https://avro.apache.org/docs/current/gettingstartedjava.html) (already added to the `pom.xml` file, too) makes this simple:\n+\n+```\n+mvn generate-sources\n+```\n+\n+You should now have a file called `target/generated-sources/io/ksqldb/tutorial/PossibleAnomaly.java` containing the compiled Java code.\n+\n+### Write the Kafka consumer code\n+\n+Now we can write the code that triggers side effects when anomalies are found. Add the following Java file at `src/main/java/io/ksqldb/tutorial/EmailSender.java`. This is a simple program that consumes events from {{ site.ak }} and sends an email with SendGrid for each one it finds. There are a few constants to fill in, including a SendGrid API key. You can get one by signing up for SendGrid.\n+\n+```java\n+package io.ksqldb.tutorial;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+\n+import com.sendgrid.SendGrid;\n+import com.sendgrid.Request;\n+import com.sendgrid.Response;\n+import com.sendgrid.Method;\n+import com.sendgrid.helpers.mail.Mail;\n+import com.sendgrid.helpers.mail.objects.Email;\n+import com.sendgrid.helpers.mail.objects.Content;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.FormatStyle;\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.Locale;\n+import java.io.IOException;\n+\n+public class EmailSender {\n+\n+    private final static String BOOTSTRAP_SERVERS = \"localhost:29092\";\n+    private final static String SCHEMA_REGISTRY_URL = \"http://localhost:8081\";\n+    private final static String TOPIC = \"possible_anomalies\";\n+    private final static String FROM_EMAIL = \"test@example.com\";\n+    private final static String SENDGRID_API_KEY = \"\";", "originalCommit": "a6a29f36e262b38920e1bcfa8fe95a2fdf2b1be1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDA0NDU4OA==", "url": "https://github.com/confluentinc/ksql/pull/5122#discussion_r414044588", "bodyText": "Good call - I added some more comments.", "author": "MichaelDrogalis", "createdAt": "2020-04-23T18:55:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAxMDM3Ng=="}], "type": "inlineReview"}, {"oid": "aca28611e31e07278a7c1ab25f0d6525cb5039e9", "url": "https://github.com/confluentinc/ksql/commit/aca28611e31e07278a7c1ab25f0d6525cb5039e9", "message": "docs: teardown instructions", "committedDate": "2020-04-23T18:49:02Z", "type": "commit"}, {"oid": "fb6a35f9178a7720d385a210e952e03a5a6ddd60", "url": "https://github.com/confluentinc/ksql/commit/fb6a35f9178a7720d385a210e952e03a5a6ddd60", "message": "docs: clearer comments in code", "committedDate": "2020-04-23T18:54:46Z", "type": "commit"}]}