{"pr_number": 4489, "pr_title": "fix: csas/ctas with timestamp column is used for output rowtime", "pr_createdAt": "2020-02-07T22:46:30Z", "pr_url": "https://github.com/confluentinc/ksql/pull/4489", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r376643236", "bodyText": "I think the logic here should write the System.currenttimeMillis() in case there is no timestamp column set, right? Otherwise, the output ROWTIME is derived from the source topics.", "author": "spena", "createdAt": "2020-02-07T22:47:47Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          if (timestampColumnIndex >= 0 && row.get(timestampColumnIndex) instanceof Long) {\n+            processorContext.forward(\n+                key,\n+                row,\n+                To.all().withTimestamp((long) row.get(timestampColumnIndex))\n+            );", "originalCommit": "a63d42f59e78d6b41f8b2046b1c697b1d6a93341", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NzU0OQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r376687549", "bodyText": "I don't think so. From my understanding the WITH ROWTIME clause in CTAS/CSAS statement overwrite the existing event-timestamp with something different. Hence, if no WITH ROWTIME column is specified the input key and value should be forwarded without a modified timestamp?\nIt seem atm, the transform() would drop the record if no WITH ROWTIME clause is present?", "author": "mjsax", "createdAt": "2020-02-08T04:53:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMTExNQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380821115", "bodyText": "Is this an out of date comment? It doesn't look to match the current impl...", "author": "big-andy-coates", "createdAt": "2020-02-18T17:24:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0NTIzOQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r376645239", "bodyText": "I added the WITH TIMESTAMP here to use the source timestamp as the output timestamp.", "author": "spena", "createdAt": "2020-02-07T22:54:37Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -12,7 +12,7 @@\n       \"name\": \"timestamp format\",\n       \"statements\": [\n         \"CREATE STREAM TEST (ID bigint, event_timestamp VARCHAR) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='event_timestamp', timestamp_format='yyyy-MM-dd''T''HH:mm:ssX');\",\n-        \"CREATE STREAM TS AS select id, stringtotimestamp(event_timestamp, 'yyyy-MM-dd''T''HH:mm:ssX') as ets from test;\"\n+        \"CREATE STREAM TS WITH (timestamp='ets') AS select id, stringtotimestamp(event_timestamp, 'yyyy-MM-dd''T''HH:mm:ssX') as ets from test;\"", "originalCommit": "a63d42f59e78d6b41f8b2046b1c697b1d6a93341", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "95d22bfcda7cfc462a40beddd2456c6b9ac14bc8", "url": "https://github.com/confluentinc/ksql/commit/95d22bfcda7cfc462a40beddd2456c6b9ac14bc8", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-10T22:34:22Z", "type": "forcePushed"}, {"oid": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "url": "https://github.com/confluentinc/ksql/commit/2a587f17b5c22837a4bee556d86a12b4bb742abe", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-11T14:01:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjA3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380802070", "bodyText": "why would an Optional field be required?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  @JsonProperty(value = \"timestampColumn\", required = true)\n          \n          \n            \n                  @JsonProperty(value = \"timestampColumn\")", "author": "big-andy-coates", "createdAt": "2020-02-18T16:52:41Z", "path": "ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java", "diffHunk": "@@ -16,26 +16,34 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.errorprone.annotations.Immutable;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+\n import java.util.Collections;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Optional;\n \n @Immutable\n public class StreamSink<K> implements ExecutionStep<KStreamHolder<K>> {\n   private final ExecutionStepPropertiesV1 properties;\n   private final ExecutionStep<KStreamHolder<K>>  source;\n   private final Formats formats;\n   private final String topicName;\n+  private final Optional<TimestampColumn> timestampColumn;\n \n   public StreamSink(\n       @JsonProperty(value = \"properties\", required = true) final ExecutionStepPropertiesV1 props,\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KStreamHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n-      @JsonProperty(value = \"topicName\", required = true) final String topicName) {\n+      @JsonProperty(value = \"topicName\", required = true) final String topicName,\n+      @JsonProperty(value = \"timestampColumn\", required = true)", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMjc1Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383412756", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjA3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjM3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380802370", "bodyText": "as above\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  @JsonProperty(value = \"timestampColumn\", required = true)\n          \n          \n            \n                  @JsonProperty(value = \"timestampColumn\")", "author": "big-andy-coates", "createdAt": "2020-02-18T16:53:14Z", "path": "ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java", "diffHunk": "@@ -17,27 +17,34 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.errorprone.annotations.Immutable;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+\n import java.util.Collections;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Optional;\n \n @Immutable\n public class TableSink<K> implements ExecutionStep<KTableHolder<K>> {\n   private final ExecutionStepPropertiesV1 properties;\n   private final ExecutionStep<KTableHolder<K>> source;\n   private final Formats formats;\n   private final String topicName;\n+  private final Optional<TimestampColumn> timestampColumn;\n \n   public TableSink(\n       @JsonProperty(value = \"properties\", required = true) final ExecutionStepPropertiesV1 props,\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KTableHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n-      @JsonProperty(value = \"topicName\", required = true) final String topicName\n+      @JsonProperty(value = \"topicName\", required = true) final String topicName,\n+      @JsonProperty(value = \"timestampColumn\", required = true)", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMjgwMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383412803", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:40:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjM3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMzQ4MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380803480", "bodyText": "We should not be pushing these new properties into the expected topologies of old version. Just make the minimal topology change, not the props", "author": "big-andy-coates", "createdAt": "2020-02-18T16:54:41Z", "path": "ksql-functional-tests/src/test/resources/expected_topology/0_6_0-pre/timestampformat_-_timestamp_format", "diffHunk": "@@ -8,6 +8,7 @@\n   \"ksql.schema.registry.url\" : \"\",\n   \"ksql.streams.default.deserialization.exception.handler\" : \"io.confluent.ksql.errors.LogMetricAndContinueExceptionHandler\",\n   \"ksql.output.topic.name.prefix\" : \"\",\n+  \"ksql.query.pull.enable.stale.reads\" : \"false\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMjk5MQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383412991", "bodyText": "Done. I now used TopologyFileRewriter to generate the new files.", "author": "spena", "createdAt": "2020-02-24T17:40:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMzQ4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwODA3Mg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380808072", "bodyText": "would be good to extend this set of tests to include some negative tests, e.g.\n\nwhere sink_ts doesn't exist in the schema (add separate test)\nwhere sink_ts includes invalid data, i.e. where ID is a negative number. (add to existing tests)\nwhere sink_ts is in string form, i.e. use timestampformat in the with of the CTAS/CSAS, (add separate tests)\nwhere the source has a timestamp column set in the WITH clause, and that column exists in the CSAS schema: the timestamp should NOT be updated in the CSAS, e.g.\n\n {\n      \"name\": \"timestamp column of source should not influent sink\",\n     \"statements\": [\n        \"CREATE STREAM INPUT (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",\n        \"CREATE STREAM OUTPUT AS SELECT id as EVENT_TS FROM INPUT;\"\n      ],\n      \"inputs\": [\n        {\"topic\": \"test_topic\", \"value\": {\"ID\": 1, \"EVENT_TS\":  1526075913000}},\n        {\"topic\": \"test_topic\", \"value\": {\"ID\": 2, \"EVENT_TS\":  1589234313000}}\n      ],\n      \"outputs\": [\n        {\"topic\": \"OUTPUT\", \"value\": {\"EVENT_TS\":1}, \"timestamp\": 1526075913000},\n        {\"topic\": \"OUTPUT\", \"value\": {\"EVENT_TS\":3}, \"timestamp\": 1589234313000}\n      ]\n    }", "author": "big-andy-coates", "createdAt": "2020-02-18T17:02:16Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -37,6 +37,38 @@\n       \"outputs\": [\n         {\"topic\": \"TS\", \"value\": {\"ETS\": 1566912669200}, \"timestamp\": 1566912669200}\n       ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CSAS\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzA2NA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413064", "bodyText": "Done. Added more tests.", "author": "spena", "createdAt": "2020-02-24T17:41:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwODA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwODk0MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380808940", "bodyText": "The test case isn't testing timestamp in the with clause of the CS statement, so remove it, as it may confuse people.  Which means you can remove EVENT_TS too.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"CREATE STREAM TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",\n          \n          \n            \n                    \"CREATE STREAM TEST (ID bigint) WITH (kafka_topic='test_topic', value_format='JSON');\",", "author": "big-andy-coates", "createdAt": "2020-02-18T17:03:43Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -37,6 +37,38 @@\n       \"outputs\": [\n         {\"topic\": \"TS\", \"value\": {\"ETS\": 1566912669200}, \"timestamp\": 1566912669200}\n       ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CSAS\",\n+      \"statements\": [\n+        \"CREATE STREAM TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzExMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413113", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:41:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwODk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwOTEzNg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380809136", "bodyText": "As above\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"CREATE TABLE TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",\n          \n          \n            \n                    \"CREATE TABLE TEST (ID bigint) WITH (kafka_topic='test_topic', value_format='JSON');\",", "author": "big-andy-coates", "createdAt": "2020-02-18T17:04:03Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -37,6 +37,38 @@\n       \"outputs\": [\n         {\"topic\": \"TS\", \"value\": {\"ETS\": 1566912669200}, \"timestamp\": 1566912669200}\n       ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CSAS\",\n+      \"statements\": [\n+        \"CREATE STREAM TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",\n+        \"CREATE STREAM TS WITH (timestamp='sink_ts') AS SELECT id as sink_ts, id, event_ts FROM test;\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 1, \"EVENT_TS\":  1526075913000}, \"timestamp\": 1526075913000},\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 2, \"EVENT_TS\":  -1}, \"timestamp\": -1},\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 3, \"EVENT_TS\":  1589234313000}, \"timestamp\": 1589234313000}\n+      ],\n+      \"outputs\": [\n+        {\"topic\": \"TS\", \"value\": {\"SINK_TS\":1, \"ID\": 1, \"EVENT_TS\":  1526075913000}, \"timestamp\": 1},\n+        {\"topic\": \"TS\", \"value\": {\"SINK_TS\":3, \"ID\": 3, \"EVENT_TS\":  1589234313000}, \"timestamp\": 3}\n+      ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CTAS\",\n+      \"statements\": [\n+        \"CREATE TABLE TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzE2MQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413161", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:41:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwOTEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMTgxNw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380811817", "bodyText": "Can we only add the transform step if there is a timestamp column specified?\n   final Optional<TransformTimestamp> tsTransformer = timestampColumn\n        .map(TimestampColumn::getColumn)\n        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n        .map(Column::index)\n        .map(TransformTimestamp::new);\n\n   final KStream<K, GenericRow> transformed = tsTransformer\n        .map(stream::transform)\n        .orElse(stream);\n\n   return stream.to(topicName, Produced.with(keySerde, valueSerde));", "author": "big-andy-coates", "createdAt": "2020-02-18T17:08:22Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzIxMA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413210", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMTgxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMjE2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380812166", "bodyText": "Do we need equals and hashCode for this class?", "author": "big-andy-coates", "createdAt": "2020-02-18T17:09:00Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzc3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413770", "bodyText": "Not needed anymore. I was using it for mocking tests when I used the assertThat methods to verify the same TransformTimestamp was called. I changed the approach and they're not used anymore.", "author": "spena", "createdAt": "2020-02-24T17:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMjE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMzkyNA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380813924", "bodyText": "nit: KSQL in name is superfluous!\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  \"name\": \"KSQL override output timestamp for CSAS\",\n          \n          \n            \n                  \"name\": \"override output timestamp for CSAS\",", "author": "big-andy-coates", "createdAt": "2020-02-18T17:11:58Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -37,6 +37,38 @@\n       \"outputs\": [\n         {\"topic\": \"TS\", \"value\": {\"ETS\": 1566912669200}, \"timestamp\": 1566912669200}\n       ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CSAS\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzc5OQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413799", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMzkyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxNDA4MQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380814081", "bodyText": "as above\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  \"name\": \"KSQL override output timestamp for CTAS\",\n          \n          \n            \n                  \"name\": \"override output timestamp for CTAS\",", "author": "big-andy-coates", "createdAt": "2020-02-18T17:12:10Z", "path": "ksql-functional-tests/src/test/resources/query-validation-tests/timestampformat.json", "diffHunk": "@@ -37,6 +37,38 @@\n       \"outputs\": [\n         {\"topic\": \"TS\", \"value\": {\"ETS\": 1566912669200}, \"timestamp\": 1566912669200}\n       ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CSAS\",\n+      \"statements\": [\n+        \"CREATE STREAM TEST (ID bigint, EVENT_TS bigint) WITH (kafka_topic='test_topic', value_format='JSON', timestamp='EVENT_TS');\",\n+        \"CREATE STREAM TS WITH (timestamp='sink_ts') AS SELECT id as sink_ts, id, event_ts FROM test;\"\n+      ],\n+      \"inputs\": [\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 1, \"EVENT_TS\":  1526075913000}, \"timestamp\": 1526075913000},\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 2, \"EVENT_TS\":  -1}, \"timestamp\": -1},\n+        {\"topic\": \"test_topic\", \"value\": {\"ID\": 3, \"EVENT_TS\":  1589234313000}, \"timestamp\": 1589234313000}\n+      ],\n+      \"outputs\": [\n+        {\"topic\": \"TS\", \"value\": {\"SINK_TS\":1, \"ID\": 1, \"EVENT_TS\":  1526075913000}, \"timestamp\": 1},\n+        {\"topic\": \"TS\", \"value\": {\"SINK_TS\":3, \"ID\": 3, \"EVENT_TS\":  1589234313000}, \"timestamp\": 3}\n+      ]\n+    },\n+    {\n+      \"name\": \"KSQL override output timestamp for CTAS\",", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzgzMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413833", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxNDA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxODY1NA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380818654", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state, i.e. throw if negative.", "author": "big-andy-coates", "createdAt": "2020-02-18T17:20:10Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzg2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413866", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxODY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDYzOQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380820639", "bodyText": "The timestampColumnIndex >= 0 bit can be removed if we're not introducing this step if not timestamp column is supplied.\nThe row.get(timestampColumnIndex) instanceof Long bit should be removed as prior code should already have checked the timestamp column exists and is of the right type.\nBTW, I believe CTAS/CSAS support WITH(TIMESTAMP_FORMAT) (the docs say they do), so the value may well be a string!", "author": "big-andy-coates", "createdAt": "2020-02-18T17:23:37Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          if (timestampColumnIndex >= 0 && row.get(timestampColumnIndex) instanceof Long) {", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzkxNA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413914", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDI2MQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380824261", "bodyText": "This test class and TableSinkBuilderTest are now both essentially just testing the new SinkBuilder class.\nMove the tests from these two test classes into SinkBuilderTest and remove them.", "author": "big-andy-coates", "createdAt": "2020-02-18T17:30:02Z", "path": "ksql-streams/src/test/java/io/confluent/ksql/execution/streams/StreamSinkBuilderTest.java", "diffHunk": "@@ -90,12 +92,14 @@ public void setup() {\n     when(keySerdeFactory.buildKeySerde(any(), any(), any())).thenReturn(keySerde);\n     when(queryBuilder.buildValueSerde(any(), any(), any())).thenReturn(valSerde);\n     when(source.build(any())).thenReturn(new KStreamHolder<>(kStream, SCHEMA, keySerdeFactory));\n+    doReturn(kStream).when(kStream).transform(any());", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNDAwNQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383414005", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDI2MQ=="}], "type": "inlineReview"}, {"oid": "ab711d526cf993b0b8102a7250ac7b96e978e706", "url": "https://github.com/confluentinc/ksql/commit/ab711d526cf993b0b8102a7250ac7b96e978e706", "message": "fix: address Andy's feedback (run TopologyFileWriter)", "committedDate": "2020-02-19T19:45:45Z", "type": "forcePushed"}, {"oid": "0e10dcb916ef02fa5eca439c1a0d69915cfccfcd", "url": "https://github.com/confluentinc/ksql/commit/0e10dcb916ef02fa5eca439c1a0d69915cfccfcd", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-24T16:40:16Z", "type": "forcePushed"}, {"oid": "d4989d2cc4bddc2174b304a11a661f212077a61f", "url": "https://github.com/confluentinc/ksql/commit/d4989d2cc4bddc2174b304a11a661f212077a61f", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-24T17:38:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzNDc3OA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383434778", "bodyText": "timestampColumn should be optional, right? Not required.\nI think with this the way it is the JSON would fail to validate if it had no timestamp column defined.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:25:02Z", "path": "ksql-rest-app/src/test/resources/ksql-plan-schema/schema.json", "diffHunk": "@@ -447,10 +447,13 @@\n         },\n         \"topicName\" : {\n           \"type\" : \"string\"\n+        },\n+        \"timestampColumn\" : {\n+          \"$ref\" : \"#/definitions/TimestampColumn\"\n         }\n       },\n       \"title\" : \"streamSinkV1\",\n-      \"required\" : [ \"@type\", \"properties\", \"source\", \"formats\", \"topicName\" ]\n+      \"required\" : [ \"@type\", \"properties\", \"source\", \"formats\", \"topicName\", \"timestampColumn\" ]", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1MTg0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383451845", "bodyText": "@rodesai is there a test somewhere that tests existing plans validate against the schema?", "author": "big-andy-coates", "createdAt": "2020-02-24T18:58:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzNDc3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU2NTA2NQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383565065", "bodyText": "our qtt tests should catch any existing plans that fail to deserialize. In this case I think the schema is just wrong. @spena how did you generate it?", "author": "rodesai", "createdAt": "2020-02-24T22:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzNDc3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzkwNTIxNQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383905215", "bodyText": "But do we deserialize using the schema?", "author": "big-andy-coates", "createdAt": "2020-02-25T14:17:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzNDc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzNDg5NA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383434894", "bodyText": "As above:\n\ntimestampColumn should be optional, right? Not required.\nI think with this the way it is the JSON would fail to validate if it had no timestamp column defined.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:25:13Z", "path": "ksql-rest-app/src/test/resources/ksql-plan-schema/schema.json", "diffHunk": "@@ -803,10 +806,13 @@\n         },\n         \"topicName\" : {\n           \"type\" : \"string\"\n+        },\n+        \"timestampColumn\" : {\n+          \"$ref\" : \"#/definitions/TimestampColumn\"\n         }\n       },\n       \"title\" : \"tableSinkV1\",\n-      \"required\" : [ \"@type\", \"properties\", \"source\", \"formats\", \"topicName\" ]\n+      \"required\" : [ \"@type\", \"properties\", \"source\", \"formats\", \"topicName\", \"timestampColumn\" ]", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzOTkxMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383439913", "bodyText": "Can we used the variant of transform that takes a Named please, so that we can give the step in the topology a more descriptive name than just KSTREAM-TRANSFORM-0003, e.g. name it Apply-timestamp or similar.\n(You'll need to re-write the topologies again once you've done this)", "author": "big-andy-coates", "createdAt": "2020-02-24T18:35:02Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ0NjgxNA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383446814", "bodyText": "Remove this filter (and cast), by passing making TimestampExtractionPolicy.create return KsqlTimestampExtraction, where KsqlTimestampExtraction extends TimestampExtraction.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:48:26Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  private static  <K> Optional<TransformTimestamp<K>> timestampTransformer(\n+      final KsqlConfig ksqlConfig,\n+      final LogicalSchema sourceSchema,\n+      final Optional<TimestampColumn> timestampColumn\n+  ) {\n+    if (!timestampColumn.isPresent()) {\n+      return Optional.empty();\n+    }\n+\n+    final TimestampExtractionPolicy timestampPolicy = TimestampExtractionPolicyFactory.create(\n+        ksqlConfig,\n+        sourceSchema,\n+        timestampColumn\n+    );\n+\n+    return timestampColumn\n+        .map(TimestampColumn::getColumn)\n+        .map(c -> sourceSchema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .map(timestampPolicy::create)\n+        .filter(te -> te instanceof AbstractColumnTimestampExtractor)", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ0ODQ5MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383448490", "bodyText": "I'd consider just making this an interface:\npublic interface KsqlTimestampExtractor implements TimestampExtractor {\n\n  long extract(GenericRow row);\n\n  default long extract(final ConsumerRecord<Object, Object> record, final long previousTimestamp) {\n       return extract((GenericRow) record.value());\n  }\n}\nThis is because the base class can't actually handle getting the column at index timetampColumnIndex out of the row for the subclasses, so the abstract class isn't adding much.\nNote:\n\nThis impl throws if the value is not a GenericRow - as it should always be a GenericRow and it should fail if its not.  Return a timestamp of 0 just doesn't make any sense.\nI've removed the common exception handling. Why? Because we'd want the same exception handling when calling extreact(row) directly, so it seems to me we still need the exception handling in the subclasses.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:51:46Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/timestamp/AbstractColumnTimestampExtractor.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams.timestamp;\n+\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.util.KsqlException;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+public abstract class AbstractColumnTimestampExtractor implements TimestampExtractor {\n+  protected final int timetampColumnIndex;\n+\n+  AbstractColumnTimestampExtractor(final int timestampColumnindex) {\n+    Preconditions.checkArgument(timestampColumnindex >= 0, \"timestampColumnindex must be >= 0\");\n+    this.timetampColumnIndex = timestampColumnindex;\n+  }\n+\n+  @Override\n+  public long extract(final ConsumerRecord<Object, Object> record, final long previousTimestamp) {\n+    if (record.value() instanceof GenericRow) {\n+      try {\n+        return extract((GenericRow) record.value());\n+      } catch (final Exception e) {\n+        throw new KsqlException(\"Unable to extract timestamp from record.\"\n+            + \" record=\" + record,\n+            e);\n+      }\n+    }\n+\n+    return 0;\n+  }\n+\n+  public abstract long extract(GenericRow row);\n+}", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1MDA5Mw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383450093", "bodyText": "If this throws.... does the exception get logged?  Does it kill the streams task?\n@rodesai what do you think should be the behaviour here?  I guess it should be inline with other parts of the processing. What happens with (de)serialization errors?", "author": "big-andy-coates", "createdAt": "2020-02-24T18:54:47Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  private static  <K> Optional<TransformTimestamp<K>> timestampTransformer(\n+      final KsqlConfig ksqlConfig,\n+      final LogicalSchema sourceSchema,\n+      final Optional<TimestampColumn> timestampColumn\n+  ) {\n+    if (!timestampColumn.isPresent()) {\n+      return Optional.empty();\n+    }\n+\n+    final TimestampExtractionPolicy timestampPolicy = TimestampExtractionPolicyFactory.create(\n+        ksqlConfig,\n+        sourceSchema,\n+        timestampColumn\n+    );\n+\n+    return timestampColumn\n+        .map(TimestampColumn::getColumn)\n+        .map(c -> sourceSchema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .map(timestampPolicy::create)\n+        .filter(te -> te instanceof AbstractColumnTimestampExtractor)\n+        .map(te -> new TransformTimestamp<>((AbstractColumnTimestampExtractor)te));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    final AbstractColumnTimestampExtractor timestampExtractor;\n+\n+    TransformTimestamp(final AbstractColumnTimestampExtractor timestampExtractor) {\n+      this.timestampExtractor = requireNonNull(timestampExtractor, \"timestampExtractor\");\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          processorContext.forward(\n+              key,\n+              row,\n+              To.all().withTimestamp(timestampExtractor.extract(row))", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1MDc0Mg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383450742", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public void shouldThrowIfNegativeProcessorContext() {\n          \n          \n            \n              public void shouldThrowOnNullProcessorContext() {", "author": "big-andy-coates", "createdAt": "2020-02-24T18:56:05Z", "path": "ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java", "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.serde.FormatInfo;\n+import io.confluent.ksql.serde.SerdeOption;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.InOrder;\n+import org.mockito.Mock;\n+import org.mockito.Mockito;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+import java.util.Optional;\n+\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyString;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class SinkBuilderTest {\n+  private static final String TOPIC = \"TOPIC\";\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"BLUE\"), SqlTypes.BIGINT)\n+      .valueColumn(ColumnName.of(\"GREEN\"), SqlTypes.STRING)\n+      .build();\n+\n+  private static final FormatInfo KEY_FORMAT = FormatInfo.of(FormatFactory.KAFKA.name());\n+  private static final FormatInfo VALUE_FORMAT = FormatInfo.of(FormatFactory.JSON.name());\n+  private static final PhysicalSchema PHYSICAL_SCHEMA =\n+      PhysicalSchema.from(SCHEMA.withoutMetaAndKeyColsInValue(), SerdeOption.none());\n+\n+  @Mock\n+  private KsqlQueryBuilder queryBuilder;\n+  @Mock\n+  private KeySerdeFactory<Struct> keySerdeFactory;\n+  @Mock\n+  private KStream<Struct, GenericRow> kStream;\n+  @Mock\n+  private Serde<Struct> keySerde;\n+  @Mock\n+  private Serde<GenericRow> valSerde;\n+  @Mock\n+  private QueryContext queryContext;\n+  @Mock\n+  private GenericRow row;\n+  @Captor\n+  private ArgumentCaptor<To> toCaptor;\n+\n+  @Before\n+  public void setup() {\n+    when(keySerdeFactory.buildKeySerde(any(), any(), any())).thenReturn(keySerde);\n+    when(queryBuilder.buildValueSerde(any(), any(), any())).thenReturn(valSerde);\n+    doReturn(kStream).when(kStream).transform(any());\n+  }\n+\n+  @Test\n+  public void shouldBuildKeySerdeCorrectly() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then:\n+    verify(keySerdeFactory).buildKeySerde(KEY_FORMAT, PHYSICAL_SCHEMA, queryContext);\n+  }\n+\n+  @Test\n+  public void shouldBuildValueSerdeCorrectly() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then:\n+    verify(queryBuilder).buildValueSerde(\n+        VALUE_FORMAT,\n+        PHYSICAL_SCHEMA,\n+        queryContext\n+    );\n+  }\n+\n+  @Test\n+  public void shouldWriteOutStreamWithCorrectSerdes() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    verify(kStream).to(anyString(), eq(Produced.with(keySerde, valSerde)));\n+  }\n+\n+  @Test\n+  public void shouldWriteOutStreamToCorrectTopic() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    verify(kStream).to(eq(TOPIC), any());\n+  }\n+\n+  @Test\n+  public void shouldBuildStreamUsingTransformTimestampWhenTimestampIsSpecified() {\n+    // Given/When\n+    SinkBuilder.build(\n+        SCHEMA,\n+        Formats.of(KEY_FORMAT, VALUE_FORMAT, SerdeOption.none()),\n+        Optional.of(new TimestampColumn(ColumnName.of(\"BLUE\"), Optional.empty())),\n+        TOPIC,\n+        kStream,\n+        keySerdeFactory,\n+        queryContext,\n+        queryBuilder\n+    );\n+\n+    // Then\n+    final InOrder inOrder = Mockito.inOrder(kStream);\n+    inOrder.verify(kStream).transform(any());\n+    inOrder.verify(kStream).to(anyString(), any());\n+    inOrder.verifyNoMoreInteractions();\n+  }\n+\n+  @Test\n+  public void shouldBuildStreamWithoutTransformTimestampWhenNoTimestampIsSpecified() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    final InOrder inOrder = Mockito.inOrder(kStream);\n+    inOrder.verify(kStream).to(anyString(), any());\n+    inOrder.verifyNoMoreInteractions();\n+  }\n+\n+  @Test\n+  public void shouldTransformTimestampRow() {\n+    // Given\n+    final long timestampColumnValue = 10001;\n+    final ProcessorContext context = mock(ProcessorContext.class);\n+    final AbstractColumnTimestampExtractor timestampExtractor\n+        = mock(AbstractColumnTimestampExtractor.class);\n+    when(timestampExtractor.extract(any())).thenReturn(timestampColumnValue);\n+\n+    // When\n+    final Transformer<String, GenericRow, KeyValue<String, GenericRow>> transformer =\n+        new SinkBuilder.TransformTimestamp<String>(timestampExtractor).get();\n+    transformer.init(context);\n+    final KeyValue<String, GenericRow> kv = transformer.transform(\"key\", row);\n+\n+    // Then\n+    assertNull(kv);\n+    verify(timestampExtractor).extract(row);\n+    verify(context, Mockito.times(1))\n+        .forward(eq(\"key\"), eq(row), toCaptor.capture());\n+    assertTrue(toCaptor.getValue().equals(To.all().withTimestamp(timestampColumnValue)));\n+  }\n+\n+  @Test(expected = NullPointerException.class)\n+  public void shouldThrowIfNegativeProcessorContext() {", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "url": "https://github.com/confluentinc/ksql/commit/6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T15:06:45Z", "type": "forcePushed"}, {"oid": "5df1126f2a99a6e13c8b2fc22e0dea483233eaba", "url": "https://github.com/confluentinc/ksql/commit/5df1126f2a99a6e13c8b2fc22e0dea483233eaba", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T15:31:37Z", "type": "forcePushed"}, {"oid": "2eece4260a94dc68940ed5b0bd6277c5fc261630", "url": "https://github.com/confluentinc/ksql/commit/2eece4260a94dc68940ed5b0bd6277c5fc261630", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T23:27:34Z", "type": "forcePushed"}, {"oid": "4ca53c138161b23be67a82c0aa59818933657460", "url": "https://github.com/confluentinc/ksql/commit/4ca53c138161b23be67a82c0aa59818933657460", "message": "feat: csas/ctas with timestamp column is used for output rowtime", "committedDate": "2020-02-26T16:10:43Z", "type": "commit"}, {"oid": "89aecd6c389feeb8642c493e438079a35af23d38", "url": "https://github.com/confluentinc/ksql/commit/89aecd6c389feeb8642c493e438079a35af23d38", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-26T16:10:44Z", "type": "commit"}, {"oid": "6b26d669e8643022055bab41f3a945bbe0199bb4", "url": "https://github.com/confluentinc/ksql/commit/6b26d669e8643022055bab41f3a945bbe0199bb4", "message": "fix: address Andy's feedback", "committedDate": "2020-02-26T16:10:46Z", "type": "commit"}, {"oid": "66f027ce347c3ec681d8fc4be4726a46921638ed", "url": "https://github.com/confluentinc/ksql/commit/66f027ce347c3ec681d8fc4be4726a46921638ed", "message": "fix: address Andy's feedback (run TopologyFileWriter)", "committedDate": "2020-02-26T16:10:47Z", "type": "commit"}, {"oid": "965dace44db6ae359e933071d73149b9ff9c8a5d", "url": "https://github.com/confluentinc/ksql/commit/965dace44db6ae359e933071d73149b9ff9c8a5d", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-26T16:10:48Z", "type": "commit"}, {"oid": "489cc33084014a9b8f9220683de7b069c82be108", "url": "https://github.com/confluentinc/ksql/commit/489cc33084014a9b8f9220683de7b069c82be108", "message": "fix: send transform forward errors to processing log", "committedDate": "2020-02-26T16:10:49Z", "type": "commit"}, {"oid": "5b4058c52804163cc756141d241ebc3db2164927", "url": "https://github.com/confluentinc/ksql/commit/5b4058c52804163cc756141d241ebc3db2164927", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-26T16:10:50Z", "type": "commit"}, {"oid": "aa40d7bca748268f45ea2769adeffb6ba4d8028e", "url": "https://github.com/confluentinc/ksql/commit/aa40d7bca748268f45ea2769adeffb6ba4d8028e", "message": "fix: regenerate query plans for join-with-custom-timestamp", "committedDate": "2020-02-26T16:27:57Z", "type": "commit"}, {"oid": "aa40d7bca748268f45ea2769adeffb6ba4d8028e", "url": "https://github.com/confluentinc/ksql/commit/aa40d7bca748268f45ea2769adeffb6ba4d8028e", "message": "fix: regenerate query plans for join-with-custom-timestamp", "committedDate": "2020-02-26T16:27:57Z", "type": "forcePushed"}]}