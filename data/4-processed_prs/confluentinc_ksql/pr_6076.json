{"pr_number": 6076, "pr_title": "chore: support filters for stream aggregations", "pr_createdAt": "2020-08-21T18:20:29Z", "pr_url": "https://github.com/confluentinc/ksql/pull/6076", "timeline": [{"oid": "969f74b265c501b3a9a6e97e72776a063c184f99", "url": "https://github.com/confluentinc/ksql/commit/969f74b265c501b3a9a6e97e72776a063c184f99", "message": "chore: support filters for stream aggregations", "committedDate": "2020-08-21T21:11:11Z", "type": "commit"}, {"oid": "969f74b265c501b3a9a6e97e72776a063c184f99", "url": "https://github.com/confluentinc/ksql/commit/969f74b265c501b3a9a6e97e72776a063c184f99", "message": "chore: support filters for stream aggregations", "committedDate": "2020-08-21T21:11:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg0MTE1OA==", "url": "https://github.com/confluentinc/ksql/pull/6076#discussion_r475841158", "bodyText": "this is kind of a confusing error message since the columns are part of the aggregate, and the user likely doesn't know about the implicit substitution of wildcard for ROWTIME. I think its fair to say fixing this is probably out of scope for this PR, but maybe file an issue?", "author": "rodesai", "createdAt": "2020-08-24T19:22:20Z", "path": "ksqldb-functional-tests/src/test/resources/sql-tests/query-upgrades/filters.sql", "diffHunk": "@@ -131,16 +125,133 @@ CREATE STREAM j AS SELECT s.id, s.foo, t.bar FROM s JOIN t ON s.id = t.id;\n CREATE OR REPLACE STREAM j AS SELECT s.id, s.foo, t.bar FROM s JOIN t ON s.id = t.id WHERE s.foo > 0;\n \n ----------------------------------------------------------------------------------------------------\n---@test: add filter to StreamAggregate\n+--@test: change filter in StreamAggregate (StreamGroupByKey)\n+----------------------------------------------------------------------------------------------------\n+SET 'ksql.create.or.replace.enabled' = 'true';\n+\n+CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT id, COUNT(*) as count FROM foo WHERE col1 >= 0 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, 0);\n+\n+ASSERT VALUES bar (id, count) VALUES (1, 1);\n+ASSERT VALUES bar (id, count) VALUES (2, 1);\n+\n+CREATE OR REPLACE TABLE bar AS SELECT id, COUNT(*) as count FROM foo WHERE col1 > 0 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, 1);\n+\n+ASSERT VALUES bar (id, count) VALUES (2, 2);\n+\n+----------------------------------------------------------------------------------------------------\n+--@test: change filter in StreamAggregate (StreamGroupBy)\n+----------------------------------------------------------------------------------------------------\n+SET 'ksql.create.or.replace.enabled' = 'true';\n+\n+CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT col1, COUNT(*) as count FROM foo WHERE col1 >= 0 GROUP BY col1;\n+\n+INSERT INTO foo (col1, id) VALUES (1, 0);\n+INSERT INTO foo (col1, id) VALUES (2, 0);\n+\n+ASSERT VALUES bar (col1, count) VALUES (1, 1);\n+ASSERT VALUES bar (col1, count) VALUES (2, 1);\n+\n+CREATE OR REPLACE TABLE bar AS SELECT col1, COUNT(*) as count FROM foo WHERE col1 > 1 GROUP BY col1;\n+\n+INSERT INTO foo (col1, id) VALUES (1, 0);\n+INSERT INTO foo (col1, id) VALUES (2, 1);\n+\n+ASSERT VALUES bar (col1, count) VALUES (2, 2);\n+\n+----------------------------------------------------------------------------------------------------\n+--@test: add filter in StreamAggregate where columns are already in input schema\n+----------------------------------------------------------------------------------------------------\n+SET 'ksql.create.or.replace.enabled' = 'true';\n+\n+CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, 0);\n+\n+ASSERT VALUES bar (id, count) VALUES (1, 1);\n+ASSERT VALUES bar (id, count) VALUES (2, 1);\n+\n+CREATE OR REPLACE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo WHERE col1 > 0 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, 1);\n+\n+ASSERT VALUES bar (id, count) VALUES (2, 2);\n+\n+----------------------------------------------------------------------------------------------------\n+--@test: remove filter in StreamAggregate where columns are already in input schema\n+----------------------------------------------------------------------------------------------------\n+SET 'ksql.create.or.replace.enabled' = 'true';\n+\n+CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo WHERE col1 > 0 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 1);\n+\n+ASSERT VALUES bar (id, count) VALUES (1, 1);\n+\n+CREATE OR REPLACE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+\n+ASSERT VALUES bar (id, count) VALUES (1, 2);\n+\n+----------------------------------------------------------------------------------------------------\n+--@test: change filter in StreamAggregate to another column that already exists in input\n+----------------------------------------------------------------------------------------------------\n+SET 'ksql.create.or.replace.enabled' = 'true';\n+\n+CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo WHERE col1 >= 0 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, 0);\n+\n+ASSERT VALUES bar (id, count) VALUES (1, 1);\n+ASSERT VALUES bar (id, count) VALUES (2, 1);\n+\n+CREATE OR REPLACE TABLE bar AS SELECT id, COUNT(col1) as count FROM foo WHERE id > 1 GROUP BY id;\n+\n+INSERT INTO foo (id, col1) VALUES (1, 0);\n+INSERT INTO foo (id, col1) VALUES (2, -1);\n+\n+ASSERT VALUES bar (id, count) VALUES (2, 2);\n+\n+----------------------------------------------------------------------------------------------------\n+--@test: remove filter in StreamAggregate where columns are not already in input schema\n+--@test: add filter in StreamAggregate where columns are not in input schema\n --@expected.error: io.confluent.ksql.util.KsqlException\n---@expected.message: Upgrades not yet supported for StreamAggregate\n+--@expected.message: StreamAggregate must have matching nonAggregateColumns. Values differ: [`ID`, `ROWTIME`, `COL1`] vs. [`ID`, `ROWTIME`]\n ----------------------------------------------------------------------------------------------------\n SET 'ksql.create.or.replace.enabled' = 'true';\n \n CREATE STREAM foo (id INT KEY, col1 INT) WITH (kafka_topic='foo', value_format='JSON');\n+CREATE TABLE bar AS SELECT id, COUNT(*) as count FROM foo WHERE col1 > 0 GROUP BY id;\n+\n+CREATE OR REPLACE TABLE bar AS SELECT id, COUNT(*) as count FROM foo GROUP BY id;\n \n+----------------------------------------------------------------------------------------------------\n+-- until we think this through a little bit more, don't allow changing non-aggregate columns\n+-- to StreamAggregate nodes, though this should technically be OK\n+\n+--@test: add filter in StreamAggregate where columns are not in input schema\n+--@expected.error: io.confluent.ksql.util.KsqlException\n+--@expected.message: StreamAggregate must have matching nonAggregateColumns. Values differ: [`ID`, `ROWTIME`] vs. [`ID`, `COL1`]", "originalCommit": "969f74b265c501b3a9a6e97e72776a063c184f99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg1NjM4MA==", "url": "https://github.com/confluentinc/ksql/pull/6076#discussion_r475856380", "bodyText": "yeah, I need to improve the error messages across the board for query upgrades when physical plan checks fail. I'm not entirely sure what a good strategy for that is but I'll file a ticket", "author": "agavra", "createdAt": "2020-08-24T19:52:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg0MTE1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg1ODM3Nw==", "url": "https://github.com/confluentinc/ksql/pull/6076#discussion_r475858377", "bodyText": "#6087", "author": "agavra", "createdAt": "2020-08-24T19:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg0MTE1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNjY3Mw==", "url": "https://github.com/confluentinc/ksql/pull/6076#discussion_r475936673", "bodyText": "and specifically for fixing the rowtime issue: #3732", "author": "agavra", "createdAt": "2020-08-24T22:49:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg0MTE1OA=="}], "type": "inlineReview"}]}