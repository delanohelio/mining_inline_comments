{"pr_number": 5355, "pr_title": "How-to guides: first set", "pr_createdAt": "2020-05-13T22:09:53Z", "pr_url": "https://github.com/confluentinc/ksql/pull/5355", "timeline": [{"oid": "ce4fbbc14c044422c72e784236e94eb17c44e55e", "url": "https://github.com/confluentinc/ksql/commit/ce4fbbc14c044422c72e784236e94eb17c44e55e", "message": "docs: finish first cut", "committedDate": "2020-05-13T21:52:13Z", "type": "commit"}, {"oid": "d6c985e7a478ff294fdff583bde82595e53c255c", "url": "https://github.com/confluentinc/ksql/commit/d6c985e7a478ff294fdff583bde82595e53c255c", "message": "docs: select tweaks", "committedDate": "2020-05-13T22:29:59Z", "type": "commit"}, {"oid": "881492a92303fe16938aa8ced9058901645c18b2", "url": "https://github.com/confluentinc/ksql/commit/881492a92303fe16938aa8ced9058901645c18b2", "message": "docs: draft of stream/table conversion", "committedDate": "2020-05-14T23:33:39Z", "type": "commit"}, {"oid": "345cdf83bd5af6b774fb8d599b39b4f7ab6a8aeb", "url": "https://github.com/confluentinc/ksql/commit/345cdf83bd5af6b774fb8d599b39b4f7ab6a8aeb", "message": "docs: custom timestamp guide", "committedDate": "2020-05-15T20:02:48Z", "type": "commit"}, {"oid": "761f31f1326e50f6aafc45a84934ed21a7404352", "url": "https://github.com/confluentinc/ksql/commit/761f31f1326e50f6aafc45a84934ed21a7404352", "message": "docs: guide tweaks", "committedDate": "2020-05-15T20:12:23Z", "type": "commit"}, {"oid": "59fc78e89699d087667600fd80257cb2e56f0610", "url": "https://github.com/confluentinc/ksql/commit/59fc78e89699d087667600fd80257cb2e56f0610", "message": "docs: wip connector management", "committedDate": "2020-05-15T23:18:24Z", "type": "commit"}, {"oid": "02983b538d79b7aaa3d3823397723035eb8c941c", "url": "https://github.com/confluentinc/ksql/commit/02983b538d79b7aaa3d3823397723035eb8c941c", "message": "docs: more connect guide", "committedDate": "2020-05-18T17:21:50Z", "type": "commit"}, {"oid": "2b5d8b3b35eb5bbcf6fd167bd933f4ee047d0366", "url": "https://github.com/confluentinc/ksql/commit/2b5d8b3b35eb5bbcf6fd167bd933f4ee047d0366", "message": "docs: fix some terminology", "committedDate": "2020-05-18T17:29:18Z", "type": "commit"}, {"oid": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "url": "https://github.com/confluentinc/ksql/commit/0e30358203e77cd0aeb24705c0159d3f9da2dd85", "message": "Merge branch 'master' into mdrogalis-how-to-guides", "committedDate": "2020-05-18T17:31:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0NjcyNA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426846724", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To nested values, use the destructuring syntax from each data type. Notice how you can chain them together:\n          \n          \n            \n            To access nested values, use the destructuring syntax from each data type. Notice how you can chain them together:", "author": "JimGalasyn", "createdAt": "2020-05-18T19:27:26Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s3` with two columns: `a` and `b`. `b` is a map with `VARCHAR` keys and `INT` values.\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR,\n+    b MAP<VARCHAR, INT>\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s3`. You can represent a MAP literal by using the `MAP` constructor, which takes a variable number of key/value arguments. `c` and `d` are used consistently in this example, but the key names can be heterogeneous in practice.\n+\n+```sql\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k1', MAP('c' := 2, 'd' := 4)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k2', MAP('c' := 4, 'd' := 8)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k3', MAP('c' := 8, 'd' := 16)\n+);\n+```\n+\n+To access a map in a query, start with the name of a column and add `[]` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b['c'] AS C,\n+       b['d'] AS D\n+FROM s3\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The last two column names have been aliased. If you elect not to give them a name, ksqlDB will generate names like `KSQL_COL_0` for each.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{c=2, d=4}                    |2                             |4                             |\n+|k2                            |{c=4, d=8}                    |4                             |8                             |\n+|k3                            |{c=8, d=16}                   |8                             |16                            |\n+```\n+\n+### Arrays\n+\n+Arrays are a collection data type that contain a sequence of values of a single type. Destructure arrays using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. `b` is an array with `INT` elements.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b ARRAY<INT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. You can represent an array literal by using the `ARRAY` constructor, which takes a variable number of elements.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', ARRAY[1]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2', ARRAY[2, 3]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3', ARRAY[4, 5, 6]\n+);\n+```\n+\n+To access an array in a query, start with the name of a column and add `[]` each index you want to drill into. This query selects column `a`, `b`, the first element of `b`, the second element of `b`, the third element of `b`, and the last element of `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b[1] AS b_1,\n+       b[2] AS b_2,\n+       b[3] AS b_3, b[-1] AS b_minus_1\n+FROM s4\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. Notice that index `1` represents the first element of each array. By constrast to many programming languages which represent the first element of an array as `0`, most databases, like ksqlDB, represent it as `1`. If an element is absent, the result is `null`. You can use negative indices to navigate backwards through the array. In this example, `-1` retrieves the last element of each array regardless of its length.\n+\n+```\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|A                  |B                  |B_1                |B_2                |B_3                |B_MINUS_1          |\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|k1                 |[1]                |1                  |null               |null               |1                  |\n+|k2                 |[2, 3]             |2                  |3                  |null               |3                  |\n+|k3                 |[4, 5, 6]          |4                  |5                  |6                  |6                  |\n+```\n+\n+## Deeply nested data\n+\n+You may have structured data types that are nested within one another. Each data type's destructuring syntax composes irrespective of how it is nested.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. Here is how `b` breaks down:\n+\n+- `b` is a struct with `VARCHAR` keys `c` and `d`.\n+- `c` is an array of `INT` elements.\n+- `d` is a map of `VARCHAR` keys and struct values.\n+- That struct has keys `e` and `f`, with values of type `VARCHAR` and `BOOLEAN` respectively.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c ARRAY<INT>,\n+        d MAP<\n+            VARCHAR,\n+            STRUCT<\n+                e VARCHAR,\n+                f BOOLEAN\n+            >\n+        >\n+    >\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. Notice how the constructors for each data type readily compose.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1',\n+    STRUCT(\n+        c := ARRAY[5, 10, 15],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v1', f := true),\n+            'y' := STRUCT(e := 'v2', f := false)\n+        )\n+    )\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2',\n+    STRUCT(\n+        c := ARRAY[3, 6, 9],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v3', f := false),\n+            'y' := STRUCT(e := 'v4', f := false)\n+        )\n+    )\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3',\n+    STRUCT(\n+        c := ARRAY[2, 4, 8],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v5', f := true),\n+            'y' := STRUCT(e := 'v6', f := true)\n+        )\n+    )\n+);\n+```\n+\n+To nested values, use the destructuring syntax from each data type. Notice how you can chain them together:", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0Njk5MQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426846991", "bodyText": "Usually we say, \"Your output should resemble:\"", "author": "JimGalasyn", "createdAt": "2020-05-18T19:27:57Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s3` with two columns: `a` and `b`. `b` is a map with `VARCHAR` keys and `INT` values.\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR,\n+    b MAP<VARCHAR, INT>\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s3`. You can represent a MAP literal by using the `MAP` constructor, which takes a variable number of key/value arguments. `c` and `d` are used consistently in this example, but the key names can be heterogeneous in practice.\n+\n+```sql\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k1', MAP('c' := 2, 'd' := 4)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k2', MAP('c' := 4, 'd' := 8)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k3', MAP('c' := 8, 'd' := 16)\n+);\n+```\n+\n+To access a map in a query, start with the name of a column and add `[]` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b['c'] AS C,\n+       b['d'] AS D\n+FROM s3\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The last two column names have been aliased. If you elect not to give them a name, ksqlDB will generate names like `KSQL_COL_0` for each.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{c=2, d=4}                    |2                             |4                             |\n+|k2                            |{c=4, d=8}                    |4                             |8                             |\n+|k3                            |{c=8, d=16}                   |8                             |16                            |\n+```\n+\n+### Arrays\n+\n+Arrays are a collection data type that contain a sequence of values of a single type. Destructure arrays using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. `b` is an array with `INT` elements.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b ARRAY<INT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. You can represent an array literal by using the `ARRAY` constructor, which takes a variable number of elements.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', ARRAY[1]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2', ARRAY[2, 3]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3', ARRAY[4, 5, 6]\n+);\n+```\n+\n+To access an array in a query, start with the name of a column and add `[]` each index you want to drill into. This query selects column `a`, `b`, the first element of `b`, the second element of `b`, the third element of `b`, and the last element of `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b[1] AS b_1,\n+       b[2] AS b_2,\n+       b[3] AS b_3, b[-1] AS b_minus_1\n+FROM s4\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. Notice that index `1` represents the first element of each array. By constrast to many programming languages which represent the first element of an array as `0`, most databases, like ksqlDB, represent it as `1`. If an element is absent, the result is `null`. You can use negative indices to navigate backwards through the array. In this example, `-1` retrieves the last element of each array regardless of its length.\n+\n+```\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|A                  |B                  |B_1                |B_2                |B_3                |B_MINUS_1          |\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|k1                 |[1]                |1                  |null               |null               |1                  |\n+|k2                 |[2, 3]             |2                  |3                  |null               |3                  |\n+|k3                 |[4, 5, 6]          |4                  |5                  |6                  |6                  |\n+```\n+\n+## Deeply nested data\n+\n+You may have structured data types that are nested within one another. Each data type's destructuring syntax composes irrespective of how it is nested.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. Here is how `b` breaks down:\n+\n+- `b` is a struct with `VARCHAR` keys `c` and `d`.\n+- `c` is an array of `INT` elements.\n+- `d` is a map of `VARCHAR` keys and struct values.\n+- That struct has keys `e` and `f`, with values of type `VARCHAR` and `BOOLEAN` respectively.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c ARRAY<INT>,\n+        d MAP<\n+            VARCHAR,\n+            STRUCT<\n+                e VARCHAR,\n+                f BOOLEAN\n+            >\n+        >\n+    >\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. Notice how the constructors for each data type readily compose.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1',\n+    STRUCT(\n+        c := ARRAY[5, 10, 15],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v1', f := true),\n+            'y' := STRUCT(e := 'v2', f := false)\n+        )\n+    )\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2',\n+    STRUCT(\n+        c := ARRAY[3, 6, 9],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v3', f := false),\n+            'y' := STRUCT(e := 'v4', f := false)\n+        )\n+    )\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3',\n+    STRUCT(\n+        c := ARRAY[2, 4, 8],\n+        d := MAP(\n+            'x' := STRUCT(e := 'v5', f := true),\n+            'y' := STRUCT(e := 'v6', f := true)\n+        )\n+    )\n+);\n+```\n+\n+To nested values, use the destructuring syntax from each data type. Notice how you can chain them together:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c[2] AS c_2,\n+       b->d['x']->f,\n+       b->d['y']->e\n+FROM s4\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The rules for how each column name are generated based on the data type that is at the tail of each selected element.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0NzUyOA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426847528", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n          \n          \n            \n            In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a {{ site.kconnectlong }} server in distributed mode.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:29:11Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0ODA5Mw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426848093", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n          \n          \n            \n            The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed by Confluent.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:30:19Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0ODMxMQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426848311", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n          \n          \n            \n            Run the following command to get the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:", "author": "JimGalasyn", "createdAt": "2020-05-18T19:30:46Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MDEwMQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426850101", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n          \n          \n            \n            When you have all the connectors that you need, configure ksqlDB to find them.\n          \n          \n            \n            \n          \n          \n            \n            !!!important\n          \n          \n            \n               You must restart all of the ksqlDB servers to finish installing the new connectors.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:34:38Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MTk2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426851966", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n          \n          \n            \n            You control whether ksqlDB uses embedded mode by supplying server configuration properties to ksqlDB. If any {{ site.kconnect }}-related properties are present, which are properties prefixed with `ksql.connect.*`, ksqlDB uses these and applies them to the embedded {{ site.kconnect }} server. Although embedded mode eases the operational burden of running a full {{ site.kconnectlong }} cluster, it doesn't dilute {{ site.kconnect }}'s power. Any property that you can configure for a regular {{ site.kconnectlong }} cluster can also be configured for embedded mode.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:38:24Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MzEyMA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426853120", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n          \n          \n            \n            There are a number of properties that you must set to have a valid {{ site.kconnect }} setup. Refer to the [Kafka Connect documentation](https://docs.confluent.io/current/connect/index.html) to learn about the right properties to set. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:40:57Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MzU5OQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426853599", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n          \n          \n            \n            To get started, here is a Docker Compose example with a server configured for embedded mode. All `KSQL_` environment variables are converted automatically to server configuration properties. Any connectors installed on your host at `confluent-hub-components` are loaded. Save this in a file named `docker-compose.yml`:", "author": "JimGalasyn", "createdAt": "2020-05-18T19:41:59Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MzgxMQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426853811", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                image: confluentinc/cp-zookeeper:5.5.0\n          \n          \n            \n                image: confluentinc/cp-zookeeper:{{ site.cprelease }}", "author": "JimGalasyn", "createdAt": "2020-05-18T19:42:27Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NDgyNg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426854826", "bodyText": "And similarly for other image tags.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1MzgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NDQxNg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426854416", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                image: confluentinc/ksqldb-cli:0.9.0\n          \n          \n            \n                image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}", "author": "JimGalasyn", "createdAt": "2020-05-18T19:43:46Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTA1MA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426855050", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-05-18T19:45:01Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTE3Nw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426855177", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-05-18T19:45:14Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTQ1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426855456", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n          \n          \n            \n            Starting a connector is as simple as giving it a name and properties. In this example, you launch the Voluble connector to source random events into a {{ site.ak }} topic. Run the following SQL statement:", "author": "JimGalasyn", "createdAt": "2020-05-18T19:45:49Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTk3MA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426855970", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n          \n          \n            \n            - ksqlDB interacts with {{ site.kconnectlong }} to create a new source connector named `s`.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:46:50Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NjI4Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426856286", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n          \n          \n            \n            - {{ site.kconnectlong }} infers that `s` is a Voluble connector because of the value of `connector.class`. {{ site.kconnectlong }} searches its plugin path to find a connector that matches the specified class.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:47:29Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NjUxNA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426856514", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n          \n          \n            \n            - Voluble publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:47:53Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NjgwNg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426856806", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n          \n          \n            \n            The properties are the same that you would pass to a connector if it was running in a dedicated {{ site.kconnect }} cluster. You can pass it any properties that the connector or {{ site.kconnectlong }} respects, like `max.tasks` to scale the number of instances of the connector.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:48:29Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NzMzNw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426857337", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.\n          \n          \n            \n            Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded {{ site.kconnectlong }} server. First, notice that ksqlDB is really just wrapping a regular {{ site.kconnectlong }} server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other {{ site.kconnect }} server.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:49:36Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n+\n+Check that the connector working is by printing the contents of the `people` topic, which connector `s` created.\n+\n+```sql\n+PRINT 'people' FROM BEGINNING;\n+```\n+\n+Because the data is random, your output should look roughly like the following:\n+\n+```\n+Key format: HOPPING(KAFKA_STRING) or TUMBLING(KAFKA_STRING) or KAFKA_STRING\n+Value format: AVRO or KAFKA_STRING\n+rowtime: 2020/05/18 17:03:38.020 Z, key: [8a9f5f18-f389-480e-9022-4fa0@7162241151841559604/-], value: {\"name\": \"Robert Macejkovic\", \"creditCardNumber\": \"4753792478828\"}\n+rowtime: 2020/05/18 17:03:38.023 Z, key: [96e3c6ff-60e2-4985-b962-4278@7365413101558183730/-], value: {\"name\": \"Evelyne Schroeder\", \"creditCardNumber\": \"3689-911575-9931\"}\n+rowtime: 2020/05/18 17:03:38.524 Z, key: [c865dd33-f854-4ad6-a95f-a9ee@7147828756964729958/-], value: {\"name\": \"Barbar Roberts\", \"creditCardNumber\": \"6565-5340-0407-5224\"}\n+rowtime: 2020/05/18 17:03:39.023 Z, key: [d29bb1e9-a8b0-4bdd-a76b-6fc1@7004895543925224502/-], value: {\"name\": \"Rosetta Swift\", \"creditCardNumber\": \"5019-5129-1138-1079\"}\n+rowtime: 2020/05/18 17:03:39.524 Z, key: [c7d74a03-ff21-4dd3-a60c-566d@7089291673502049328/-], value: {\"name\": \"Amado Leuschke\", \"creditCardNumber\": \"6771-8942-4365-4019\"}\n+```\n+\n+When you're done, you can drop the connector by running:\n+\n+```sql\n+DROP CONNECTOR s;\n+```\n+\n+You can confirm that the connector is no longer running by looking at the output of `SHOW CONNECTORS;`.\n+\n+### Introspecting embedded mode\n+\n+Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NzQyMQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426857421", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-05-18T19:49:49Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n+\n+Check that the connector working is by printing the contents of the `people` topic, which connector `s` created.\n+\n+```sql\n+PRINT 'people' FROM BEGINNING;\n+```\n+\n+Because the data is random, your output should look roughly like the following:\n+\n+```\n+Key format: HOPPING(KAFKA_STRING) or TUMBLING(KAFKA_STRING) or KAFKA_STRING\n+Value format: AVRO or KAFKA_STRING\n+rowtime: 2020/05/18 17:03:38.020 Z, key: [8a9f5f18-f389-480e-9022-4fa0@7162241151841559604/-], value: {\"name\": \"Robert Macejkovic\", \"creditCardNumber\": \"4753792478828\"}\n+rowtime: 2020/05/18 17:03:38.023 Z, key: [96e3c6ff-60e2-4985-b962-4278@7365413101558183730/-], value: {\"name\": \"Evelyne Schroeder\", \"creditCardNumber\": \"3689-911575-9931\"}\n+rowtime: 2020/05/18 17:03:38.524 Z, key: [c865dd33-f854-4ad6-a95f-a9ee@7147828756964729958/-], value: {\"name\": \"Barbar Roberts\", \"creditCardNumber\": \"6565-5340-0407-5224\"}\n+rowtime: 2020/05/18 17:03:39.023 Z, key: [d29bb1e9-a8b0-4bdd-a76b-6fc1@7004895543925224502/-], value: {\"name\": \"Rosetta Swift\", \"creditCardNumber\": \"5019-5129-1138-1079\"}\n+rowtime: 2020/05/18 17:03:39.524 Z, key: [c7d74a03-ff21-4dd3-a60c-566d@7089291673502049328/-], value: {\"name\": \"Amado Leuschke\", \"creditCardNumber\": \"6771-8942-4365-4019\"}\n+```\n+\n+When you're done, you can drop the connector by running:\n+\n+```sql\n+DROP CONNECTOR s;\n+```\n+\n+You can confirm that the connector is no longer running by looking at the output of `SHOW CONNECTORS;`.\n+\n+### Introspecting embedded mode\n+\n+Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.\n+\n+```", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1Nzc2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426857766", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, embedded Kafka Connect will log messages inline with ksqlDB's server's log messages. You can view them by running:\n          \n          \n            \n            By default, embedded {{ site.kconnectlong }} logs messages inline with ksqlDB's server's log messages. View them by running the following command:", "author": "JimGalasyn", "createdAt": "2020-05-18T19:50:28Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n+\n+Check that the connector working is by printing the contents of the `people` topic, which connector `s` created.\n+\n+```sql\n+PRINT 'people' FROM BEGINNING;\n+```\n+\n+Because the data is random, your output should look roughly like the following:\n+\n+```\n+Key format: HOPPING(KAFKA_STRING) or TUMBLING(KAFKA_STRING) or KAFKA_STRING\n+Value format: AVRO or KAFKA_STRING\n+rowtime: 2020/05/18 17:03:38.020 Z, key: [8a9f5f18-f389-480e-9022-4fa0@7162241151841559604/-], value: {\"name\": \"Robert Macejkovic\", \"creditCardNumber\": \"4753792478828\"}\n+rowtime: 2020/05/18 17:03:38.023 Z, key: [96e3c6ff-60e2-4985-b962-4278@7365413101558183730/-], value: {\"name\": \"Evelyne Schroeder\", \"creditCardNumber\": \"3689-911575-9931\"}\n+rowtime: 2020/05/18 17:03:38.524 Z, key: [c865dd33-f854-4ad6-a95f-a9ee@7147828756964729958/-], value: {\"name\": \"Barbar Roberts\", \"creditCardNumber\": \"6565-5340-0407-5224\"}\n+rowtime: 2020/05/18 17:03:39.023 Z, key: [d29bb1e9-a8b0-4bdd-a76b-6fc1@7004895543925224502/-], value: {\"name\": \"Rosetta Swift\", \"creditCardNumber\": \"5019-5129-1138-1079\"}\n+rowtime: 2020/05/18 17:03:39.524 Z, key: [c7d74a03-ff21-4dd3-a60c-566d@7089291673502049328/-], value: {\"name\": \"Amado Leuschke\", \"creditCardNumber\": \"6771-8942-4365-4019\"}\n+```\n+\n+When you're done, you can drop the connector by running:\n+\n+```sql\n+DROP CONNECTOR s;\n+```\n+\n+You can confirm that the connector is no longer running by looking at the output of `SHOW CONNECTORS;`.\n+\n+### Introspecting embedded mode\n+\n+Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.\n+\n+```\n+docker exec -it ksqldb-server curl http://localhost:8083/\n+```\n+\n+Your output should look something like:\n+\n+```json\n+{\"version\":\"5.5.0-ccs\",\"commit\":\"785a156634af5f7e\",\"kafka_cluster_id\":\"bfz7rsyJRtOx5fs-2l4W4A\"}\n+```\n+\n+This can be really useful if you're having trouble getting a connector to load or need more insight into how connector tasks are behaving.\n+\n+### Logging\n+\n+By default, embedded Kafka Connect will log messages inline with ksqlDB's server's log messages. You can view them by running:", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1Nzg1Mg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426857852", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-05-18T19:50:38Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n+\n+Check that the connector working is by printing the contents of the `people` topic, which connector `s` created.\n+\n+```sql\n+PRINT 'people' FROM BEGINNING;\n+```\n+\n+Because the data is random, your output should look roughly like the following:\n+\n+```\n+Key format: HOPPING(KAFKA_STRING) or TUMBLING(KAFKA_STRING) or KAFKA_STRING\n+Value format: AVRO or KAFKA_STRING\n+rowtime: 2020/05/18 17:03:38.020 Z, key: [8a9f5f18-f389-480e-9022-4fa0@7162241151841559604/-], value: {\"name\": \"Robert Macejkovic\", \"creditCardNumber\": \"4753792478828\"}\n+rowtime: 2020/05/18 17:03:38.023 Z, key: [96e3c6ff-60e2-4985-b962-4278@7365413101558183730/-], value: {\"name\": \"Evelyne Schroeder\", \"creditCardNumber\": \"3689-911575-9931\"}\n+rowtime: 2020/05/18 17:03:38.524 Z, key: [c865dd33-f854-4ad6-a95f-a9ee@7147828756964729958/-], value: {\"name\": \"Barbar Roberts\", \"creditCardNumber\": \"6565-5340-0407-5224\"}\n+rowtime: 2020/05/18 17:03:39.023 Z, key: [d29bb1e9-a8b0-4bdd-a76b-6fc1@7004895543925224502/-], value: {\"name\": \"Rosetta Swift\", \"creditCardNumber\": \"5019-5129-1138-1079\"}\n+rowtime: 2020/05/18 17:03:39.524 Z, key: [c7d74a03-ff21-4dd3-a60c-566d@7089291673502049328/-], value: {\"name\": \"Amado Leuschke\", \"creditCardNumber\": \"6771-8942-4365-4019\"}\n+```\n+\n+When you're done, you can drop the connector by running:\n+\n+```sql\n+DROP CONNECTOR s;\n+```\n+\n+You can confirm that the connector is no longer running by looking at the output of `SHOW CONNECTORS;`.\n+\n+### Introspecting embedded mode\n+\n+Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.\n+\n+```\n+docker exec -it ksqldb-server curl http://localhost:8083/\n+```\n+\n+Your output should look something like:\n+\n+```json\n+{\"version\":\"5.5.0-ccs\",\"commit\":\"785a156634af5f7e\",\"kafka_cluster_id\":\"bfz7rsyJRtOx5fs-2l4W4A\"}\n+```\n+\n+This can be really useful if you're having trouble getting a connector to load or need more insight into how connector tasks are behaving.\n+\n+### Logging\n+\n+By default, embedded Kafka Connect will log messages inline with ksqlDB's server's log messages. You can view them by running:\n+\n+```", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1Nzk5NA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426857994", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In external mode, ksqlDB communicates with an external Kafka Connect cluster. It's able to create and destroy connectors as needed. Use external mode when you have high volumes of input and output.\n          \n          \n            \n            In external mode, ksqlDB communicates with an external {{ site.kconnectlong }} cluster. It's able to create and destroy connectors as needed. Use external mode when you have high volumes of input and output.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:50:56Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a Kafka Connect server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+And run the following to obtain the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you would like to use, you can configure ksqlDB to find them, as seen in the next step. ksqlDB cannot add new connectors without restarting its servers.\n+\n+### Configuring ksqlDB\n+\n+You control whether or not ksqlDB uses embedded mode by the server configuration properties that you supply to ksqlDB. If any Connect related properties are present (properties prefixed with `ksql.connect.`), ksqlDB will use those and apply them to the embedded Connect server. Although embedded mode eases the operational burden of running a full Kafka Connector cluster, it doesn't dilute Connect's power. Any property that can be configured for a regular Kafka Connect cluster can also be configured for embedded mode.\n+\n+There are a number of properties that you must set to have a valid Connect setup. Refer to Kafka Connect's documentation to learn what the right properties to set are. One critical property is `ksql.connect.plugin.path`, which specifies the path to find the connector jars. If you're using Docker, use a volume to mount your connector jars from your host into the container.\n+\n+To get started, here is a Docker Compose example a server configured for embedded mode. All `KSQL_` environment variables are automatically converted to server configuration properties. Any connectors installed on your host at `confluent-hub-components` will be loaded. Place this in a file named `docker-compose.yml`:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.5.0\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.5.0\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:0.9.0\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./confluent-hub-components/debezium-debezium-connector-mysql:/usr/share/kafka/plugins/debezium-mysql\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration to embed Kafka Connect support.\n+      KSQL_CONNECT_GROUP_ID: \"ksql-connect-cluster\"\n+      KSQL_CONNECT_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_CONNECT_KEY_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\"\n+      KSQL_CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\"\n+      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: \"ksql-connect-configs\"\n+      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: \"ksql-connect-offsets\"\n+      KSQL_CONNECT_STATUS_STORAGE_TOPIC: \"ksql-connect-statuses\"\n+      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n+      KSQL_CONNECT_PLUGIN_PATH: \"/usr/share/kafka/plugins\"\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:0.9.0\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Bring up the stack with:\n+\n+```\n+docker-compose up\n+```\n+\n+### Launching a connector\n+\n+Now that ksqlDB has a connector and is configured to run it in embedded mode, you can launch it. Start by running ksqlDB's CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Starting a connector is as simple as giving it a name and properties. In this example, you will launch the Voluble connector to source random events into a Kafka topic. Run the following:\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+Here is what this ksqlDB statement does:\n+\n+- ksqlDB interacts with Kafka Connect to create a new source connector named `s`.\n+- Kafka Connect infers that `s` is a Voluble connector because of the value of `connector.class`. Kafka Connect searches its plugin path to find a connector that matches the given class.\n+- ksqlDB passes the remaining properties directly to the Voluble connector so that it can configure itself.\n+- Voluble creates publishes a new event to topic `people` every `500` milliseconds with a UUID key and a map value of two keys, `name` and `creditCardNumber`.\n+\n+The properties are the same that you would pass to a connector if it was running in a dedicated Connect cluster. You can pass it any properties that the connector (or Kafka Connect) respects, such as `max.tasks` to scale the number of instances of the connector.\n+\n+Check that the connector working is by printing the contents of the `people` topic, which connector `s` created.\n+\n+```sql\n+PRINT 'people' FROM BEGINNING;\n+```\n+\n+Because the data is random, your output should look roughly like the following:\n+\n+```\n+Key format: HOPPING(KAFKA_STRING) or TUMBLING(KAFKA_STRING) or KAFKA_STRING\n+Value format: AVRO or KAFKA_STRING\n+rowtime: 2020/05/18 17:03:38.020 Z, key: [8a9f5f18-f389-480e-9022-4fa0@7162241151841559604/-], value: {\"name\": \"Robert Macejkovic\", \"creditCardNumber\": \"4753792478828\"}\n+rowtime: 2020/05/18 17:03:38.023 Z, key: [96e3c6ff-60e2-4985-b962-4278@7365413101558183730/-], value: {\"name\": \"Evelyne Schroeder\", \"creditCardNumber\": \"3689-911575-9931\"}\n+rowtime: 2020/05/18 17:03:38.524 Z, key: [c865dd33-f854-4ad6-a95f-a9ee@7147828756964729958/-], value: {\"name\": \"Barbar Roberts\", \"creditCardNumber\": \"6565-5340-0407-5224\"}\n+rowtime: 2020/05/18 17:03:39.023 Z, key: [d29bb1e9-a8b0-4bdd-a76b-6fc1@7004895543925224502/-], value: {\"name\": \"Rosetta Swift\", \"creditCardNumber\": \"5019-5129-1138-1079\"}\n+rowtime: 2020/05/18 17:03:39.524 Z, key: [c7d74a03-ff21-4dd3-a60c-566d@7089291673502049328/-], value: {\"name\": \"Amado Leuschke\", \"creditCardNumber\": \"6771-8942-4365-4019\"}\n+```\n+\n+When you're done, you can drop the connector by running:\n+\n+```sql\n+DROP CONNECTOR s;\n+```\n+\n+You can confirm that the connector is no longer running by looking at the output of `SHOW CONNECTORS;`.\n+\n+### Introspecting embedded mode\n+\n+Sometimes you might need a little more power to introspect how your connectors are behaving by interacting directly with the embedded Kafka Connect server. First, notice that ksqlDB is really just wrapping a regular Kafka Connect server. You can curl it and interact with its [REST API](https://docs.confluent.io/current/connect/references/restapi.html) just like any other Connect server.\n+\n+```\n+docker exec -it ksqldb-server curl http://localhost:8083/\n+```\n+\n+Your output should look something like:\n+\n+```json\n+{\"version\":\"5.5.0-ccs\",\"commit\":\"785a156634af5f7e\",\"kafka_cluster_id\":\"bfz7rsyJRtOx5fs-2l4W4A\"}\n+```\n+\n+This can be really useful if you're having trouble getting a connector to load or need more insight into how connector tasks are behaving.\n+\n+### Logging\n+\n+By default, embedded Kafka Connect will log messages inline with ksqlDB's server's log messages. You can view them by running:\n+\n+```\n+docker logs -f ksqldb-server\n+```\n+\n+... << TODO: show how to redirect the logs >> ...\n+\n+## External mode\n+\n+In external mode, ksqlDB communicates with an external Kafka Connect cluster. It's able to create and destroy connectors as needed. Use external mode when you have high volumes of input and output.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1ODU2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426858566", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called \"materializing\" a table.\n          \n          \n            \n            You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called *materializing* a table.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:52:11Z", "path": "docs/how-to-guides/convert-changelog-to-table.md", "diffHunk": "@@ -0,0 +1,115 @@\n+# How to convert a changelog to a table\n+\n+## Context\n+\n+You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called \"materializing\" a table.", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1OTE1NQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426859155", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as \"processing-time\" (using the wall clock) and \"event-time\" (using timestamps within events).\n          \n          \n            \n            You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as \"processing-time\" (using the wall clock) and \"event-time\" (using timestamps within events).\n          \n      \n    \n    \n  \n\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as \"processing-time\" (using the wall clock) and \"event-time\" (using timestamps within events).\n          \n          \n            \n            You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as *processing-time* (using the wall clock) and *event-time* (using timestamps within events).", "author": "JimGalasyn", "createdAt": "2020-05-18T19:53:19Z", "path": "docs/how-to-guides/use-a-custom-timestamp-column.md", "diffHunk": "@@ -0,0 +1,213 @@\n+# How to use a custom timestamp column\n+\n+## Context\n+\n+You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as \"processing-time\" (using the wall clock) and \"event-time\" (using timestamps within events).", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1OTY2NA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r426859664", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".\n          \n          \n            \n            You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of {{ site.kconnectlong }} connectors through its SQL syntax. This functionality is called *connector management*.", "author": "JimGalasyn", "createdAt": "2020-05-18T19:54:21Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,244 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the vast ecosystem of Kafka Connect connectors through its SQL syntax. This functionality is called \"connector management\".", "originalCommit": "0e30358203e77cd0aeb24705c0159d3f9da2dd85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3ced25d7f6188f3d7d43e79a5dcfd1b4991d27b4", "url": "https://github.com/confluentinc/ksql/commit/3ced25d7f6188f3d7d43e79a5dcfd1b4991d27b4", "message": "Apply suggestions from code review\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-05-18T20:07:33Z", "type": "commit"}, {"oid": "09bc1e7076ca31949cc81df7572e076ad6119e6c", "url": "https://github.com/confluentinc/ksql/commit/09bc1e7076ca31949cc81df7572e076ad6119e6c", "message": "docs: use version tokens", "committedDate": "2020-05-18T20:11:20Z", "type": "commit"}, {"oid": "46263e19e1b64dcb3ce5b5f6de8b991bee9b5dce", "url": "https://github.com/confluentinc/ksql/commit/46263e19e1b64dcb3ce5b5f6de8b991bee9b5dce", "message": "Update docs/how-to-guides/use-connector-management.md\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-05-18T20:14:44Z", "type": "commit"}, {"oid": "87b20324681ba4c5c6a339f2d59838bc846122ba", "url": "https://github.com/confluentinc/ksql/commit/87b20324681ba4c5c6a339f2d59838bc846122ba", "message": "docs: phrasing", "committedDate": "2020-05-18T20:15:32Z", "type": "commit"}, {"oid": "203ad64fe30501b82bf2c508ba0b08eefbca1251", "url": "https://github.com/confluentinc/ksql/commit/203ad64fe30501b82bf2c508ba0b08eefbca1251", "message": "Merge branch 'mdrogalis-how-to-guides' of github.com:confluentinc/ksql into mdrogalis-how-to-guides", "committedDate": "2020-05-18T20:15:44Z", "type": "commit"}, {"oid": "e682e76c68739d4be2889ab6bd44f12403cbfd79", "url": "https://github.com/confluentinc/ksql/commit/e682e76c68739d4be2889ab6bd44f12403cbfd79", "message": "Update docs/how-to-guides/use-connector-management.md\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-05-18T20:17:28Z", "type": "commit"}, {"oid": "08c5c9cc84f2cc5910b5ffeb8f7db0d23e9773b4", "url": "https://github.com/confluentinc/ksql/commit/08c5c9cc84f2cc5910b5ffeb8f7db0d23e9773b4", "message": "docs: fix admonition", "committedDate": "2020-05-18T20:19:48Z", "type": "commit"}, {"oid": "a98885ce0c8868cb2e7458d1deb916c4744f77e3", "url": "https://github.com/confluentinc/ksql/commit/a98885ce0c8868cb2e7458d1deb916c4744f77e3", "message": "docs: finish out external connectors", "committedDate": "2020-05-18T23:20:08Z", "type": "commit"}, {"oid": "5f9984cc182a63ea496436c19aa4c89af2e67a70", "url": "https://github.com/confluentinc/ksql/commit/5f9984cc182a63ea496436c19aa4c89af2e67a70", "message": "docs: synopsis", "committedDate": "2020-05-19T15:51:19Z", "type": "commit"}, {"oid": "e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "url": "https://github.com/confluentinc/ksql/commit/e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "message": "docs: strip redundant note", "committedDate": "2020-05-19T15:55:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5Nzc4NA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427497784", "bodyText": "Because ksqlDB defaults to using wall clock time\n\nThe way that I interpreted this isn't true - we use the default timestamp column in the message, not the time we processed the message. The reason that INSERT INTO makes it look like wall clock time is because it sets the timestamp field to be the wall clock time if not specified. The timestamp column allows you to use something other than the default timestamp in the kafka message", "author": "agavra", "createdAt": "2020-05-19T18:03:53Z", "path": "docs/how-to-guides/use-a-custom-timestamp-column.md", "diffHunk": "@@ -0,0 +1,213 @@\n+# How to use a custom timestamp column\n+\n+## Context\n+\n+You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as *processing-time* (using the wall clock) and *event-time* (using timestamps within events).", "originalCommit": "e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxNTY5MA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427615690", "bodyText": "Ohh, nice catch. I've updated the text. Let me know if that reads clearly.", "author": "MichaelDrogalis", "createdAt": "2020-05-19T21:38:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5Nzc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUwMzE3OA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427503178", "bodyText": "we might want to note that this won't change the data in the stream, it just instructs all downstream queries to use the timestamp field instead of the rowtime", "author": "agavra", "createdAt": "2020-05-19T18:12:53Z", "path": "docs/how-to-guides/use-a-custom-timestamp-column.md", "diffHunk": "@@ -0,0 +1,213 @@\n+# How to use a custom timestamp column\n+\n+## Context\n+\n+You have events that have a timestamp attribute. You want to do time-related processing over them and want ksqlDB to use those timestamps for processing. Because ksqlDB defaults to using wall clock time, you need to tell ksqlDB where to find the timestamps within the events. This is known as *processing-time* (using the wall clock) and *event-time* (using timestamps within events).\n+\n+## In action\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR,\n+    ts VARCHAR,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'k',\n+    timestamp = 'ts',                        -- the column to use as a timestamp\n+    timestamp_format = 'yyyy-MM-dd HH:mm:ss' -- the format to parse the timestamp\n+);\n+```\n+\n+## Using event-time\n+\n+Using event-time allows ksqlDB to handle out-of-order events during time-related processing. Set the `timestamp` property when creating a stream or table to denote which column to use as the timestamp. If the timestamp column is a string, also set the `timestamp_format` property to tell ksqlDB how to parse it.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Create a stream `s1` that has a timestamp column, `ts`. Notice that the `timestamp` property hasn't been set yet. This will make it easier to see how the functionality behaves later in this guide.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR,\n+    ts VARCHAR,\n+    v1 INT,\n+    v2 VARCHAR\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'k'\n+);\n+```\n+\n+Insert some rows into `s1`, setting the `ts` column to dates that are not \"now\".\n+\n+```sql\n+INSERT INTO s1 (\n+    k, ts, v1, v2\n+) VALUES (\n+    'k1', '2020-05-04 01:00:00', 0, 'a'\n+);\n+\n+INSERT INTO s1 (\n+    k, ts, v1, v2\n+) VALUES (\n+    'k2', '2020-05-04 02:00:00', 1, 'b'\n+);\n+```\n+\n+Query the stream for its columns, including `ROWTIME`. `ROWTIME` is a system-column that ksqlDB reserves to track the timestamp of the event.\n+\n+```sql\n+SELECT k,\n+       ROWTIME,\n+       TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS rowtime_formatted,\n+       ts,\n+       v1,\n+       v2\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+Your results should look similiar to what is below with the exception of `ROWTIME` and `ROWTIME_FORMATTED`, which will mirror your wall clock. Because you didn't yet instruct ksqlDB to use event-time, `ROWTIME` is inherited from the underlying Kafka record. Kafka's default is to set the timestamp at which the record was produced to the topic.\n+\n+```\n++------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+\n+|K                                   |ROWTIME                             |ROWTIME_FORMATTED                   |TS                                  |V1                                  |V2                                  |\n++------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+\n+|k1                                  |1589564380616                       |2020-05-15 17:39:40.616             |2020-05-04 01:00:00                 |0                                   |a                                   |\n+|k2                                  |1589564380731                       |2020-05-15 17:39:40.731             |2020-05-04 02:00:00                 |1                                   |b                                   |\n+```\n+\n+Derive a new stream, `s2`, from `s1` and tell ksqlDB to use event-time. Set the `timestamp` property to the `ts` column.\n+\n+```sql\n+CREATE STREAM S2 WITH (\n+    timestamp = 'ts',\n+    timestamp_format = 'yyyy-MM-dd HH:mm:ss'\n+)   AS\n+    SELECT *\n+    FROM s1\n+    EMIT CHANGES;\n+```\n+\n+Now compare the timestamps again. This time, notice that `ROWTIME` has been set to the same value as `ts`. `s2` is now using event-time.\n+\n+```sql\n+SELECT k,\n+       ROWTIME,\n+       TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS rowtime_formatted,\n+       ts,\n+       v1,\n+       v2\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+The query should return the following results.\n+\n+```\n++------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+\n+|K                                   |ROWTIME                             |ROWTIME_FORMATTED                   |TS                                  |V1                                  |V2                                  |\n++------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+\n+|k1                                  |1588554000000                       |2020-05-04 01:00:00.000             |2020-05-04 01:00:00                 |0                                   |a                                   |\n+|k2                                  |1588557600000                       |2020-05-04 02:00:00.000             |2020-05-04 02:00:00                 |1                                   |b                                   |\n+```\n+\n+Any new streams or tables derived from `s2` will continue to have their timestamp set to `ts` unless an operation instructs otherwise.\n+\n+\n+## Timestamps on base streams/tables\n+\n+Not only can you change the timestamp to use as you derive new streams and tables, you can also set it on base ones, too. Simply set the `timestamp` and `timestamp_format` properties on the `WITH` clause.", "originalCommit": "e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxNjQ2OA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427616468", "bodyText": "When you say that it doesn't change the data in the stream, what you mean is that the metadata for the underlying Kafka records are not changed, right?\nMy understanding is that from a users point of view, as soon as you interact with the stream in any way (select whatever), rowtime is set to the indicated timestamp.", "author": "MichaelDrogalis", "createdAt": "2020-05-19T21:40:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUwMzE3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUwODI2OA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427508268", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n          \n          \n            \n            Before you can use an embedded connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.", "author": "agavra", "createdAt": "2020-05-19T18:20:57Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,246 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the [vast ecosystem](https://www.confluent.io/hub/) of [{{ site.kconnectlong }}](https://kafka.apache.org/documentation/#connect) connectors through its SQL syntax. This functionality is called *connector management*.\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a {{ site.kconnectlong }} server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.", "originalCommit": "e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUwOTM3OQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427509379", "bodyText": "This isn't exactly true - the docker image has a special configuration script that will look for any ksql.connect.* properties. If you're not using docker, you explicitly need to specify the ksql.connect.worker.config", "author": "agavra", "createdAt": "2020-05-19T18:22:52Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -0,0 +1,246 @@\n+# How to use connector management\n+\n+## Context\n+\n+You have external data stores that you want to read from and write to with ksqlDB, but you don\u2019t want to write custom glue code to do it. ksqlDB is capable of using the [vast ecosystem](https://www.confluent.io/hub/) of [{{ site.kconnectlong }}](https://kafka.apache.org/documentation/#connect) connectors through its SQL syntax. This functionality is called *connector management*.\n+\n+## In action\n+\n+```sql\n+CREATE SOURCE CONNECTOR s WITH (\n+  'connector.class' = 'io.mdrogalis.voluble.VolubleSourceConnector',\n+\n+  'genkp.people.with' = '#{Internet.uuid}',\n+  'genv.people.name.with' = '#{Name.full_name}',\n+  'genv.people.creditCardNumber.with' = '#{Finance.credit_card}',\n+\n+  'global.throttle.ms' = '500'\n+);\n+```\n+\n+## Modes\n+\n+Before you can use connector management, you need to decide what mode you want to run connectors in. ksqlDB can run connectors in two different modes: **embedded** or **external**. This controls how and where the connectors are executed. The way in which you configure ksqlDB's server determines which mode it will use. All nodes in a single ksqlDB cluster must use the same mode.\n+\n+Regardless of which mode you use, the syntax to create and use connectors is the same.\n+\n+## Embedded mode\n+\n+In embedded mode, ksqlDB runs connectors directly on its servers. This is convenient because it reduces the number of moving parts that you need to manage in your infrastructure. Embedded mode is highly useful for development, testing, and production workloads that have light/moderate data volumes. Use this mode when you don't need to scale your ingest/egress capacity independently from your processing capacity. When you use embedded mode, ksqlDB server is actually running a {{ site.kconnectlong }} server in distributed mode.\n+\n+### Dowloading connectors\n+\n+Before you can use a connector, you need to download it prior to starting ksqlDB. A downloaded connector package is essentially a set of jars that contain the code for interacting with the target data store.\n+\n+The easiest way to download a connector is to use [`confluent-hub`](https://docs.confluent.io/current/connect/managing/confluent-hub/client.html), a utility program distributed by Confluent.\n+\n+Create a directory for your connectors:\n+\n+```\n+mkdir confluent-hub-components\n+```\n+\n+Run the following command to get the [Voluble](https://github.com/MichaelDrogalis/voluble) data generator connector:\n+\n+```\n+confluent-hub install --component-dir confluent-hub-components --no-prompt mdrogalis/voluble:0.3.0\n+```\n+\n+After running this command, `confluent-hub-components` should contain the Voluble jars. If you are running in clustered mode, you must install the connector on every server.\n+\n+When you have all the connectors that you need, configure ksqlDB to find them.\n+\n+!!! important\n+    You must restart all of the ksqlDB servers to finish installing the new connectors.\n+\n+### Configuring ksqlDB\n+\n+You control whether ksqlDB uses embedded mode by supplying server configuration properties to ksqlDB. If any {{ site.kconnect }}-related properties are present, which are properties prefixed with `ksql.connect.*`, ksqlDB uses these and applies them to the embedded {{ site.kconnect }} server. Although embedded mode eases the operational burden of running a full {{ site.kconnectlong }} cluster, it doesn't dilute {{ site.kconnect }}'s power. Any property that you can configure for a regular {{ site.kconnectlong }} cluster can also be configured for embedded mode.", "originalCommit": "e32b5e5c0f8bcec904aeb01c4fe707cc253a30ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYzMzY1MQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427633651", "bodyText": "Thanks for clarifying! I reworded. What do you think?", "author": "MichaelDrogalis", "createdAt": "2020-05-19T22:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUwOTM3OQ=="}], "type": "inlineReview"}, {"oid": "8f9f35fb4b1840822f6a4453aa59709cd6ef7241", "url": "https://github.com/confluentinc/ksql/commit/8f9f35fb4b1840822f6a4453aa59709cd6ef7241", "message": "docs: remove processing time", "committedDate": "2020-05-19T21:38:12Z", "type": "commit"}, {"oid": "e5131a1177719785780b3f23b00558903c3cc728", "url": "https://github.com/confluentinc/ksql/commit/e5131a1177719785780b3f23b00558903c3cc728", "message": "Update docs/how-to-guides/use-connector-management.md\n\nCo-authored-by: Almog Gavra <almog@confluent.io>", "committedDate": "2020-05-19T21:40:55Z", "type": "commit"}, {"oid": "25d273e7ea2746957860d257015954ee2ce4b2a3", "url": "https://github.com/confluentinc/ksql/commit/25d273e7ea2746957860d257015954ee2ce4b2a3", "message": "docs: clarify connector management", "committedDate": "2020-05-19T22:20:18Z", "type": "commit"}, {"oid": "a400d3a5f401fa94fdd24cd9dc4eb7e264b9dc89", "url": "https://github.com/confluentinc/ksql/commit/a400d3a5f401fa94fdd24cd9dc4eb7e264b9dc89", "message": "Merge branch 'mdrogalis-how-to-guides' of github.com:confluentinc/ksql into mdrogalis-how-to-guides", "committedDate": "2020-05-19T22:20:44Z", "type": "commit"}, {"oid": "d1f91c0ce268cf81c141f5c9b052abe8a1f3c9c9", "url": "https://github.com/confluentinc/ksql/commit/d1f91c0ce268cf81c141f5c9b052abe8a1f3c9c9", "message": "docs: clarify base timestamps", "committedDate": "2020-05-19T22:25:57Z", "type": "commit"}, {"oid": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "url": "https://github.com/confluentinc/ksql/commit/bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "message": "docs: reorder note", "committedDate": "2020-05-19T22:28:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDYwMw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427644603", "bodyText": "Should we instead say \"This is broadly called materializing a changelog stream into a table\"? The inconsistency between \"materializing a table\" here and \"Materializing a changelog stream\" below (next subheader) is a little jarring.", "author": "vcrfxia", "createdAt": "2020-05-19T22:52:05Z", "path": "docs/how-to-guides/convert-changelog-to-table.md", "diffHunk": "@@ -0,0 +1,115 @@\n+# How to convert a changelog to a table\n+\n+## Context\n+\n+You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called *materializing* a table.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0ODYyMA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427648620", "bodyText": "Good call. I've updated it. \ud83d\udc4d", "author": "MichaelDrogalis", "createdAt": "2020-05-19T23:03:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDYyNg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427644626", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Begin by telling ksqlDB to start all queries from earliest point in each topic.\n          \n          \n            \n            Begin by telling ksqlDB to start all queries from the earliest point in each topic.", "author": "vcrfxia", "createdAt": "2020-05-19T22:52:09Z", "path": "docs/how-to-guides/convert-changelog-to-table.md", "diffHunk": "@@ -0,0 +1,115 @@\n+# How to convert a changelog to a table\n+\n+## Context\n+\n+You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called *materializing* a table.\n+\n+## In action\n+\n+```sql\n+CREATE TABLE t1 AS\n+    SELECT k,\n+           LATEST_BY_OFFSET(v1) AS v1,\n+           LATEST_BY_OFFSET(v2) AS v2,\n+           LATEST_BY_OFFSET(v3) AS v3\n+    FROM s1\n+    GROUP BY k\n+    EMIT CHANGES;\n+```\n+\n+## Materializing a changelog stream\n+\n+In ksqlDB, you derive new tables by aggregating other streams and tables. To create a table that reflects the latest values for each key, use the `LATEST_BY_OFFSET` aggregation.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDgyNg==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427644826", "bodyText": "What's the reasoning behind using Avro in the examples rather than JSON? Avro requires Schema Registry so it's more pieces to get off the ground.", "author": "vcrfxia", "createdAt": "2020-05-19T22:52:44Z", "path": "docs/how-to-guides/convert-changelog-to-table.md", "diffHunk": "@@ -0,0 +1,115 @@\n+# How to convert a changelog to a table\n+\n+## Context\n+\n+You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called *materializing* a table.\n+\n+## In action\n+\n+```sql\n+CREATE TABLE t1 AS\n+    SELECT k,\n+           LATEST_BY_OFFSET(v1) AS v1,\n+           LATEST_BY_OFFSET(v2) AS v2,\n+           LATEST_BY_OFFSET(v3) AS v3\n+    FROM s1\n+    GROUP BY k\n+    EMIT CHANGES;\n+```\n+\n+## Materializing a changelog stream\n+\n+In ksqlDB, you derive new tables by aggregating other streams and tables. To create a table that reflects the latest values for each key, use the `LATEST_BY_OFFSET` aggregation.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s1` with four columns. `k` represents the key of the table. Rows with the same key represent information about the same entity. `v1`, `v2`, and `v3` are various value columns.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR,\n+    v1 INT,\n+    v2 VARCHAR,\n+    v3 BOOLEAN\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro',", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NzM4OQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427647389", "bodyText": "We're encouraging using Avro everywhere just so people can get the benefits of Schema Registry. Im all in favor for fewer moving parts, but I think this one is important.", "author": "MichaelDrogalis", "createdAt": "2020-05-19T23:00:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDgyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDk3Ng==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427644976", "bodyText": "This syntax is outdated on master. I assume the reason we're using the old syntax is so the how-to guides can go live sooner? We should remember to update this on master after merging.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:09Z", "path": "docs/how-to-guides/convert-changelog-to-table.md", "diffHunk": "@@ -0,0 +1,115 @@\n+# How to convert a changelog to a table\n+\n+## Context\n+\n+You have a stream of events that represent a series of changes, known as a changelog. You want a view of the data that reflects only the last change for each key. Because ksqlDB represents change over time using tables, you need a way to convert your changelog stream into a table. This is broadly called *materializing* a table.\n+\n+## In action\n+\n+```sql\n+CREATE TABLE t1 AS\n+    SELECT k,\n+           LATEST_BY_OFFSET(v1) AS v1,\n+           LATEST_BY_OFFSET(v2) AS v2,\n+           LATEST_BY_OFFSET(v3) AS v3\n+    FROM s1\n+    GROUP BY k\n+    EMIT CHANGES;\n+```\n+\n+## Materializing a changelog stream\n+\n+In ksqlDB, you derive new tables by aggregating other streams and tables. To create a table that reflects the latest values for each key, use the `LATEST_BY_OFFSET` aggregation.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s1` with four columns. `k` represents the key of the table. Rows with the same key represent information about the same entity. `v1`, `v2`, and `v3` are various value columns.\n+\n+```sql\n+CREATE STREAM s1 (\n+    k VARCHAR,\n+    v1 INT,\n+    v2 VARCHAR,\n+    v3 BOOLEAN\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'k'", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NzY2OA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427647668", "bodyText": "I actually forgot about that. I'll volunteer to update these for the next release. It'll let me learn about the new syntax. :) cc @big-andy-coates Bug me when it's time.", "author": "MichaelDrogalis", "createdAt": "2020-05-19T23:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NDk3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTA5NA==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645094", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Begin by telling ksqlDB to start all queries from earliest point in each topic.\n          \n          \n            \n            Begin by telling ksqlDB to start all queries from the earliest point in each topic.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:32Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTExMQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645111", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n          \n          \n            \n            Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, whose value data types are `VARCHAR` and `INT` respectively.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:35Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTE1Nw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645157", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Begin by telling ksqlDB to start all queries from earliest point in each topic.\n          \n          \n            \n            Begin by telling ksqlDB to start all queries from the earliest point in each topic.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:42Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+Your output should resemble the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTE3NQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645175", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Begin by telling ksqlDB to start all queries from earliest point in each topic.\n          \n          \n            \n            Begin by telling ksqlDB to start all queries from the earliest point in each topic.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:46Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+Your output should resemble the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s3` with two columns: `a` and `b`. `b` is a map with `VARCHAR` keys and `INT` values.\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR,\n+    b MAP<VARCHAR, INT>\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s3`. You can represent a MAP literal by using the `MAP` constructor, which takes a variable number of key/value arguments. `c` and `d` are used consistently in this example, but the key names can be heterogeneous in practice.\n+\n+```sql\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k1', MAP('c' := 2, 'd' := 4)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k2', MAP('c' := 4, 'd' := 8)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k3', MAP('c' := 8, 'd' := 16)\n+);\n+```\n+\n+To access a map in a query, start with the name of a column and add `[]` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b['c'] AS C,\n+       b['d'] AS D\n+FROM s3\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The last two column names have been aliased. If you elect not to give them a name, ksqlDB will generate names like `KSQL_COL_0` for each.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{c=2, d=4}                    |2                             |4                             |\n+|k2                            |{c=4, d=8}                    |4                             |8                             |\n+|k3                            |{c=8, d=16}                   |8                             |16                            |\n+```\n+\n+### Arrays\n+\n+Arrays are a collection data type that contain a sequence of values of a single type. Destructure arrays using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTE5Mw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645193", "bodyText": "nit: newline", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:49Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+Your output should resemble the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s3` with two columns: `a` and `b`. `b` is a map with `VARCHAR` keys and `INT` values.\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR,\n+    b MAP<VARCHAR, INT>\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s3`. You can represent a MAP literal by using the `MAP` constructor, which takes a variable number of key/value arguments. `c` and `d` are used consistently in this example, but the key names can be heterogeneous in practice.\n+\n+```sql\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k1', MAP('c' := 2, 'd' := 4)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k2', MAP('c' := 4, 'd' := 8)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k3', MAP('c' := 8, 'd' := 16)\n+);\n+```\n+\n+To access a map in a query, start with the name of a column and add `[]` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b['c'] AS C,\n+       b['d'] AS D\n+FROM s3\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The last two column names have been aliased. If you elect not to give them a name, ksqlDB will generate names like `KSQL_COL_0` for each.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{c=2, d=4}                    |2                             |4                             |\n+|k2                            |{c=4, d=8}                    |4                             |8                             |\n+|k3                            |{c=8, d=16}                   |8                             |16                            |\n+```\n+\n+### Arrays\n+\n+Arrays are a collection data type that contain a sequence of values of a single type. Destructure arrays using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. `b` is an array with `INT` elements.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b ARRAY<INT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. You can represent an array literal by using the `ARRAY` constructor, which takes a variable number of elements.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', ARRAY[1]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2', ARRAY[2, 3]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3', ARRAY[4, 5, 6]\n+);\n+```\n+\n+To access an array in a query, start with the name of a column and add `[]` each index you want to drill into. This query selects column `a`, `b`, the first element of `b`, the second element of `b`, the third element of `b`, and the last element of `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b[1] AS b_1,\n+       b[2] AS b_2,\n+       b[3] AS b_3, b[-1] AS b_minus_1", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0ODg0Mw==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427648843", "bodyText": "Fixed! Thanks!", "author": "MichaelDrogalis", "createdAt": "2020-05-19T23:04:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTE5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY0NTIxNQ==", "url": "https://github.com/confluentinc/ksql/pull/5355#discussion_r427645215", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Begin by telling ksqlDB to start all queries from earliest point in each topic.\n          \n          \n            \n            Begin by telling ksqlDB to start all queries from the earliest point in each topic.", "author": "vcrfxia", "createdAt": "2020-05-19T22:53:54Z", "path": "docs/how-to-guides/query-structured-data.md", "diffHunk": "@@ -0,0 +1,333 @@\n+# How to query structured data\n+\n+## Context\n+\n+You have events that contain structured data types like structs, maps, and arrays. You want to write them to ksqlDB and read their inner contents with queries. Because ksqlDB represents each event as a row with a flat series of columns, you need a bit of syntax to work with these data types. This is sometimes called \"destructuring\".\n+\n+## In action\n+\n+```sql\n+SELECT a->d    AS d,   -- destructure a struct\n+       b[1]    AS b_1  -- destructure an array\n+       c['k1'] AS k1   -- destructure a map\n+FROM s1\n+EMIT CHANGES;\n+```\n+\n+## Data types\n+\n+### Structs\n+\n+Structs are an associative data type that map `VARCHAR` keys to values of any type. Destructure structs by using arrow syntax (`->`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s2` with two columns: `a` and `b`. `b` is a struct with `VARCHAR` keys `c` and `d`, who's value data types are `VARCHAR` and `INT` respectively.\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR,\n+    b STRUCT<\n+        c VARCHAR,\n+        d INT\n+    >\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s2`. You can represent a struct literal by using the `STRUCT` constructor, which takes a variable number of key/value arguments.\n+\n+```sql\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 'v1', d := 5)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k2', STRUCT(c := 'v2', d := 6)\n+);\n+\n+INSERT INTO s2 (\n+    a, b\n+) VALUES (\n+    'k3', STRUCT(c := 'v3', d := 7)\n+);\n+```\n+\n+To access a struct in a query, start with the name of a column and add `->` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b->c,\n+       b->d\n+FROM s2\n+EMIT CHANGES;\n+```\n+\n+Your output should resemble the following results. Notice that the column names for the last two columns and `C` and `D` respectively. By default, ksqlDB will give the column the name of the last identifier in the arrow chain. You can override this by aliasing, such as `b->c AS x`. If you drill into nested values that finish with the same identifier name, ksqlDB will force you to provide an alias to avoid ambiguity.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{C=v1, D=5}                   |v1                            |5                             |\n+|k2                            |{C=v2, D=6}                   |v2                            |6                             |\n+|k3                            |{C=v3, D=7}                   |v3                            |7                             |\n+```\n+\n+\n+### Maps\n+\n+Maps are an associative data type that map keys of any type to values of any type. The types across all keys must be the same. The same rule holds for values. Destructure maps using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s3` with two columns: `a` and `b`. `b` is a map with `VARCHAR` keys and `INT` values.\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR,\n+    b MAP<VARCHAR, INT>\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s3`. You can represent a MAP literal by using the `MAP` constructor, which takes a variable number of key/value arguments. `c` and `d` are used consistently in this example, but the key names can be heterogeneous in practice.\n+\n+```sql\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k1', MAP('c' := 2, 'd' := 4)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k2', MAP('c' := 4, 'd' := 8)\n+);\n+\n+INSERT INTO s3 (\n+    a, b\n+) VALUES (\n+    'k3', MAP('c' := 8, 'd' := 16)\n+);\n+```\n+\n+To access a map in a query, start with the name of a column and add `[]` each time you want to drill into a key. This query selects column `a`, `b`, the key `c` within `b`, and the key `d` within `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b['c'] AS C,\n+       b['d'] AS D\n+FROM s3\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. The last two column names have been aliased. If you elect not to give them a name, ksqlDB will generate names like `KSQL_COL_0` for each.\n+\n+```\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|A                             |B                             |C                             |D                             |\n++------------------------------+------------------------------+------------------------------+------------------------------+\n+|k1                            |{c=2, d=4}                    |2                             |4                             |\n+|k2                            |{c=4, d=8}                    |4                             |8                             |\n+|k3                            |{c=8, d=16}                   |8                             |16                            |\n+```\n+\n+### Arrays\n+\n+Arrays are a collection data type that contain a sequence of values of a single type. Destructure arrays using bracket syntax (`[]`).\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Make a stream `s4` with two columns: `a` and `b`. `b` is an array with `INT` elements.\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR,\n+    b ARRAY<INT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro',\n+    key = 'a'\n+);\n+```\n+\n+Insert some rows into `s4`. You can represent an array literal by using the `ARRAY` constructor, which takes a variable number of elements.\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', ARRAY[1]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k2', ARRAY[2, 3]\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k3', ARRAY[4, 5, 6]\n+);\n+```\n+\n+To access an array in a query, start with the name of a column and add `[]` each index you want to drill into. This query selects column `a`, `b`, the first element of `b`, the second element of `b`, the third element of `b`, and the last element of `b`:\n+\n+```sql\n+SELECT a,\n+       b,\n+       b[1] AS b_1,\n+       b[2] AS b_2,\n+       b[3] AS b_3, b[-1] AS b_minus_1\n+FROM s4\n+EMIT CHANGES;\n+```\n+\n+This query should return the following results. Notice that index `1` represents the first element of each array. By constrast to many programming languages which represent the first element of an array as `0`, most databases, like ksqlDB, represent it as `1`. If an element is absent, the result is `null`. You can use negative indices to navigate backwards through the array. In this example, `-1` retrieves the last element of each array regardless of its length.\n+\n+```\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|A                  |B                  |B_1                |B_2                |B_3                |B_MINUS_1          |\n++-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n+|k1                 |[1]                |1                  |null               |null               |1                  |\n+|k2                 |[2, 3]             |2                  |3                  |null               |3                  |\n+|k3                 |[4, 5, 6]          |4                  |5                  |6                  |6                  |\n+```\n+\n+## Deeply nested data\n+\n+You may have structured data types that are nested within one another. Each data type's destructuring syntax composes irrespective of how it is nested.\n+\n+Begin by telling ksqlDB to start all queries from earliest point in each topic.", "originalCommit": "bad991e6a3a1dac011bdd6f0f6ea0f6e58b9656f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f04e7a60994f6bbc9e0e86a817dcb94e05097703", "url": "https://github.com/confluentinc/ksql/commit/f04e7a60994f6bbc9e0e86a817dcb94e05097703", "message": "Update docs/how-to-guides/query-structured-data.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T22:57:27Z", "type": "commit"}, {"oid": "ca3aef32e8712925cbb92af74e10e98e4d656f26", "url": "https://github.com/confluentinc/ksql/commit/ca3aef32e8712925cbb92af74e10e98e4d656f26", "message": "Update docs/how-to-guides/query-structured-data.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T22:57:40Z", "type": "commit"}, {"oid": "d3dfa552ad00003da59166f4adea0207ce19ce4e", "url": "https://github.com/confluentinc/ksql/commit/d3dfa552ad00003da59166f4adea0207ce19ce4e", "message": "Update docs/how-to-guides/query-structured-data.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T22:57:50Z", "type": "commit"}, {"oid": "89e905740fdcb73d308d0c19feafbd19aa25a3b2", "url": "https://github.com/confluentinc/ksql/commit/89e905740fdcb73d308d0c19feafbd19aa25a3b2", "message": "Update docs/how-to-guides/query-structured-data.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T22:58:02Z", "type": "commit"}, {"oid": "54713f5493974933f4b13a456275beebf3a9ead4", "url": "https://github.com/confluentinc/ksql/commit/54713f5493974933f4b13a456275beebf3a9ead4", "message": "Update docs/how-to-guides/convert-changelog-to-table.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T22:58:18Z", "type": "commit"}, {"oid": "29fc083247565c1e64f60f9db4cdb67148eddfe8", "url": "https://github.com/confluentinc/ksql/commit/29fc083247565c1e64f60f9db4cdb67148eddfe8", "message": "Update docs/how-to-guides/query-structured-data.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-05-19T23:01:05Z", "type": "commit"}, {"oid": "cac151e15067cf25cee402060096ac70fd456605", "url": "https://github.com/confluentinc/ksql/commit/cac151e15067cf25cee402060096ac70fd456605", "message": "docs: fix typo", "committedDate": "2020-05-19T23:02:25Z", "type": "commit"}, {"oid": "1ff9280b2a049c2a64e629afb4d30d28f69a15f2", "url": "https://github.com/confluentinc/ksql/commit/1ff9280b2a049c2a64e629afb4d30d28f69a15f2", "message": "docs: fix formatting", "committedDate": "2020-05-19T23:03:02Z", "type": "commit"}, {"oid": "dd29fdffe6b0922f107e7be3ca2cd486b245dae3", "url": "https://github.com/confluentinc/ksql/commit/dd29fdffe6b0922f107e7be3ca2cd486b245dae3", "message": "docs: fix phrasing", "committedDate": "2020-05-19T23:03:44Z", "type": "commit"}]}