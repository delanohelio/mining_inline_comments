{"pr_number": 6017, "pr_title": "docs: klip-33 key format", "pr_createdAt": "2020-08-13T18:02:18Z", "pr_url": "https://github.com/confluentinc/ksql/pull/6017", "timeline": [{"oid": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "url": "https://github.com/confluentinc/ksql/commit/5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "message": "docs: klip-33 key format", "committedDate": "2020-08-13T17:58:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NTkxMw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470185913", "bodyText": "I suspect KAFKA will be supported as well?", "author": "agavra", "createdAt": "2020-08-13T19:10:43Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwMTk1NQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470501955", "bodyText": "It's already supported. So... adding support is not in scope.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:01:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NTkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjY1Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471062653", "bodyText": "Well, but KEY_FORMAT='KAFKA' should be a valid option thus it seem it should be listed? It's not about adding support for the format, but about the language change to explicitly set the format.", "author": "mjsax", "createdAt": "2020-08-16T04:13:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NTkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NjI4MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470186280", "bodyText": "out of interest, is this actually any different than KAFKA?", "author": "agavra", "createdAt": "2020-08-13T19:11:25Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwMjkzNw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470502937", "bodyText": "Yes, it is different. JSON is serialized as a JSON payload in a string. So, for example where as a KAFKA INT would be serialized as 4 bytes, a JSON INT would be essentially serialized as the val.toString().getBytes(UTF8);, i.e. the same as a KAFKA STRING containing the number.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4NjI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4ODYyMA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470188620", "bodyText": "I'm worried this might change partitioning in an unexpected way - are there any complications here that we need to think about, or is it always OK because it's a new query anyway?", "author": "agavra", "createdAt": "2020-08-13T19:15:20Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwMzQwNA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470503404", "bodyText": "I can't think of any issues... but that's why we put these designs up for review. If you can... then let me know :D", "author": "big-andy-coates", "createdAt": "2020-08-14T09:04:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4ODYyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4OTk0Ng==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470189946", "bodyText": "I'm wondering if we should default the default to KAFKA - it's not unlikely that most users will have KAFKA formatted keys and this will make it a better OOTB experience", "author": "agavra", "createdAt": "2020-08-13T19:17:24Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. ", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwMzU4OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470503588", "bodyText": "Already suggested in the KLIP.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:04:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE4OTk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE5MTE2Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470191163", "bodyText": "I suppose we still need partitions to indicate that the topic should be created, unless we want to re-intorudce a default there", "author": "agavra", "createdAt": "2020-08-13T19:19:41Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the ", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNDM0Nw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470504347", "bodyText": "I had thought the same.  Yes, we could include a default partition count.  I don't think adding one would cause any problems with the current 'use existing' vs 'create new' logic.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE5MTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzkzNzUzMA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473937530", "bodyText": "I'll add this to the KLIP I'm raising for making the WITH clause optional", "author": "big-andy-coates", "createdAt": "2020-08-20T12:38:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE5MTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDEwODcxNw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474108717", "bodyText": "KLIP for making WITH clause optional: #6065", "author": "big-andy-coates", "createdAt": "2020-08-20T16:20:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE5MTE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNDA5NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470204094", "bodyText": "+1", "author": "agavra", "createdAt": "2020-08-13T19:42:50Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * making KAFKA_TOPIC property optional\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key and value formats.\n+    * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+      the server configuration provides a default.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of these key formats for all supported SQL syntax  \n+1. **Auto-repartitioning on key format mismatch**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Avro support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Delimited support**: Adds support for the `DELIMITED` key format.\n+1. **Optional KAFKA_TOPIC property**: makes the `KAFKA_TOPIC` property optional everywhere and add\n+   configuration to control case and case sensitivity of topic names.\n+1. **Blog post**: write a blog post about the new features. (Potentially more than once if \n+  work span multiple releases).\n+   \n+LOE TBD once scope and design confirmed.\n+\n+## Documentation Updates\n+\n+New server config and new `CREATE` properties will be added to main docs site.\n+\n+There are no incompatible changes within the proposal, so no demos and examples _must_ change.\n+However, it probably pays to update some to highlight the new features. We propose updating the \n+Kafka micro site examples to leverage the new functionality, as these have automated testing.  \n+It may be worth changing the ksqlDB quickstart too - TBD, as this will require extending DataGen \n+to support other key formats. Something we may want in scope anyway - or should be end-of-life \n+DataGen in favour of the datagen connector?", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDEzOTY1NQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474139655", "bodyText": "Please don't eol the datagen tool!\nIt's far too useful for anyone who wants to test or demo something quickly. And before someone says \"you can just run an embedded datagen connector though so what's the difference?\" :-),  allow me to opine that this requires a steeper learning curve and advanced troubleshooting skills to get working right, which I think should be avoided where possible. Just an opinion of course ;)\nI'd also say that the connector should be refactored (as i recall suggesting loudly when it was originally forked off from here) so that it embeds some re-usable portion of the datagen tool (which likely requires a small refactor of datagen itself too, to facilitate, to be fair) rather than being a copy/paste that now proceeds on its own life journey and inevitable divergence.", "author": "blueedgenick", "createdAt": "2020-08-20T17:00:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNDA5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQzNDEwMQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r484434101", "bodyText": "@blueedgenick are you putting a case forward that DataGen should be enhanced to support Avro / Json keys as part of this work, i.e. it should be in-scaope?", "author": "big-andy-coates", "createdAt": "2020-09-07T13:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNDA5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNDQ4MQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470204481", "bodyText": "+1 as above, why not have this be the in-code default?", "author": "agavra", "createdAt": "2020-08-13T19:43:36Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * making KAFKA_TOPIC property optional\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key and value formats.\n+    * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+      the server configuration provides a default.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of these key formats for all supported SQL syntax  \n+1. **Auto-repartitioning on key format mismatch**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Avro support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Delimited support**: Adds support for the `DELIMITED` key format.\n+1. **Optional KAFKA_TOPIC property**: makes the `KAFKA_TOPIC` property optional everywhere and add\n+   configuration to control case and case sensitivity of topic names.\n+1. **Blog post**: write a blog post about the new features. (Potentially more than once if \n+  work span multiple releases).\n+   \n+LOE TBD once scope and design confirmed.\n+\n+## Documentation Updates\n+\n+New server config and new `CREATE` properties will be added to main docs site.\n+\n+There are no incompatible changes within the proposal, so no demos and examples _must_ change.\n+However, it probably pays to update some to highlight the new features. We propose updating the \n+Kafka micro site examples to leverage the new functionality, as these have automated testing.  \n+It may be worth changing the ksqlDB quickstart too - TBD, as this will require extending DataGen \n+to support other key formats. Something we may want in scope anyway - or should be end-of-life \n+DataGen in favour of the datagen connector?\n+\n+## Compatibility Implications\n+\n+As mentioned above, existing query plays already include key formats for all topics. So existing\n+queries will continue to work.\n+\n+Without `ksql.persistence.default.format.key` set to `KAFKA` existing queries in the form:\n+\n+```sql\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    VALUE_FORMAT='JSON'\n+  );\n+```\n+\n+...will start failing, as they do not specify the `KEY_FORMAT`. We therefore propose the server \n+config shipped with ccloud and on-prem releases has `ksql.persistence.default.format.key` set to \n+`KAFKA`.", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNTc1OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470505758", "bodyText": "I see two main ways of implementing this:\n\ndefault in the KsqlConfig\ndefaults in the server properties files.\n\nThe former has the pro of handling both on-prem and ccloud, but the con that it might not be possible to unset the property, i.e. to add config to say 'I don't want a default'.  Not sure that's an issue though.  Anyway, that's a bit low level for a KLIP.,", "author": "big-andy-coates", "createdAt": "2020-08-14T09:08:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNDQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNTg5OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470205898", "bodyText": "Clarify here that this includes JSON_SR: I see it's below but wasn't sure for a while.", "author": "vcrfxia", "createdAt": "2020-08-13T19:46:23Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNjM5NQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470506395", "bodyText": "Good catch. Added.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:10:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwNTg5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwODQwOQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470208409", "bodyText": "These new configs are orthogonal to the primitive key work, right? I'm a bit confused why they're in this KLIP.\nI guess they're nice-to-have's if we make the WITH clause optional, but that also seems a little orthogonal to introducing KEY_FORMAT and FORMAT.\n(To clarify -- this proposal SGTM. I'm just surprised it's part of this KLIP since someone looking for changes here probably wouldn't think to check this KLIP.)", "author": "vcrfxia", "createdAt": "2020-08-13T19:51:23Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNjg1Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470506853", "bodyText": "Originally, this was a tiny little additional piece of work. But its growing, so will pull into a separate KLIP.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:11:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwODQwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDEwODgyMQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474108821", "bodyText": "KLIP for making WITH clause optional: #6065", "author": "big-andy-coates", "createdAt": "2020-08-20T16:20:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIwODQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMjQyMg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470212422", "bodyText": "For on-prem, are you proposing we have a code default of KAFKA or only that the example config file sets the config value to KAFKA? I think the former makes sense, as we then have full backwards compatibility.\nWe can introduce another option for ksql.persistence.default.format.key to represent no default, e.g., NONE, which users can set if they want the server to fail statements without key format provided (rather than defaulting to KAFKA).", "author": "vcrfxia", "createdAt": "2020-08-13T19:58:40Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * making KAFKA_TOPIC property optional\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key and value formats.\n+    * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+      the server configuration provides a default.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of these key formats for all supported SQL syntax  \n+1. **Auto-repartitioning on key format mismatch**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Avro support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Delimited support**: Adds support for the `DELIMITED` key format.\n+1. **Optional KAFKA_TOPIC property**: makes the `KAFKA_TOPIC` property optional everywhere and add\n+   configuration to control case and case sensitivity of topic names.\n+1. **Blog post**: write a blog post about the new features. (Potentially more than once if \n+  work span multiple releases).\n+   \n+LOE TBD once scope and design confirmed.\n+\n+## Documentation Updates\n+\n+New server config and new `CREATE` properties will be added to main docs site.\n+\n+There are no incompatible changes within the proposal, so no demos and examples _must_ change.\n+However, it probably pays to update some to highlight the new features. We propose updating the \n+Kafka micro site examples to leverage the new functionality, as these have automated testing.  \n+It may be worth changing the ksqlDB quickstart too - TBD, as this will require extending DataGen \n+to support other key formats. Something we may want in scope anyway - or should be end-of-life \n+DataGen in favour of the datagen connector?\n+\n+## Compatibility Implications\n+\n+As mentioned above, existing query plays already include key formats for all topics. So existing\n+queries will continue to work.\n+\n+Without `ksql.persistence.default.format.key` set to `KAFKA` existing queries in the form:\n+\n+```sql\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    VALUE_FORMAT='JSON'\n+  );\n+```\n+\n+...will start failing, as they do not specify the `KEY_FORMAT`. We therefore propose the server \n+config shipped with ccloud and on-prem releases has `ksql.persistence.default.format.key` set to ", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNzUyNw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470507527", "bodyText": "The former.\nIf the KSqlConfig doesn't define a default, i.e. the default is in the server config file we ship, then we don't need NONE.  If it does, then NONE may be needed.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:12:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMjQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMzU5NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470213594", "bodyText": "Which quickstart does this refer to? Neither the on-prem nor cloud quickstart uses ksql-datagen.", "author": "vcrfxia", "createdAt": "2020-08-13T20:00:48Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * making KAFKA_TOPIC property optional\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key and value formats.\n+    * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+      the server configuration provides a default.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of these key formats for all supported SQL syntax  \n+1. **Auto-repartitioning on key format mismatch**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Avro support** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Delimited support**: Adds support for the `DELIMITED` key format.\n+1. **Optional KAFKA_TOPIC property**: makes the `KAFKA_TOPIC` property optional everywhere and add\n+   configuration to control case and case sensitivity of topic names.\n+1. **Blog post**: write a blog post about the new features. (Potentially more than once if \n+  work span multiple releases).\n+   \n+LOE TBD once scope and design confirmed.\n+\n+## Documentation Updates\n+\n+New server config and new `CREATE` properties will be added to main docs site.\n+\n+There are no incompatible changes within the proposal, so no demos and examples _must_ change.\n+However, it probably pays to update some to highlight the new features. We propose updating the \n+Kafka micro site examples to leverage the new functionality, as these have automated testing.  \n+It may be worth changing the ksqlDB quickstart too - TBD, as this will require extending DataGen ", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUwNzkzMQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470507931", "bodyText": "Hummm... I thought clickstream was our quick start.  Good to know.", "author": "big-andy-coates", "createdAt": "2020-08-14T09:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMzU5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk0MDY0NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473940644", "bodyText": "Updated.", "author": "big-andy-coates", "createdAt": "2020-08-20T12:44:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMzU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIzNjAyNw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r470236027", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.\n          \n          \n            \n            1. **JSON_SR support** Adds support for the `JSON_SR` key format, inc. schema registry integration.", "author": "vcrfxia", "createdAt": "2020-08-13T20:42:31Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * making KAFKA_TOPIC property optional\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key and value formats.\n+    * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+      the server configuration provides a default.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of these key formats for all supported SQL syntax  \n+1. **Auto-repartitioning on key format mismatch**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support** Adds support for the `AVRO` key format, inc. schema registry integration.", "originalCommit": "5d07d4501ae52a6357f0654e03da1bb4a0ada9c3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9a47d319050da52c009d7b3f6946d86b15cbb751", "url": "https://github.com/confluentinc/ksql/commit/9a47d319050da52c009d7b3f6946d86b15cbb751", "message": "chore: feedback", "committedDate": "2020-08-14T09:14:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471062730", "bodyText": "Why do we have two values? And what is JSON_SR? (I guess this is just a question for my own education).", "author": "mjsax", "createdAt": "2020-08-16T04:14:23Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk1ODQzOQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473958439", "bodyText": "... it's not nice :(\nJSON format can read a schema from the SR, but doesn't write to the SR.\nJSON_SR format both reads and writes schemas to SR.\nAnd internally they use different code, so have different behaviour, e.g. when serializing maps (#6049).\nI'm not sure of the why. @agavra added it, so I'm sure there was good reason.  He can likely comment more.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:13:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDA5NDI0NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474094244", "bodyText": "yeah it was a product decision how we wanted to handle it - the difference is that they are actually different serialization formats and are not byte compatible (one adds the magic byte and the schema header, the other is vanilla json). We could've hacked around this and magically removed the header if it exists, but we decided it would be more confusing to the user than just having two formats\ndrives me bananas too \ud83c\udf4c", "author": "agavra", "createdAt": "2020-08-20T15:58:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMzOTUwMQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474339501", "bodyText": "Not sure if I understand. For JSON if the schema is received from the SR, the must still be some \"header\" encoding the schema ID to fetch the schema? So how can one for both formats not have a \"header\"?", "author": "mjsax", "createdAt": "2020-08-21T00:17:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMzOTg4OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474339888", "bodyText": "JSON does not support schema registry integration", "author": "agavra", "createdAt": "2020-08-21T00:18:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM0MDA3MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474340070", "bodyText": "So how can one for both formats not have a \"header\"?\n\nthe idea was that we would check the data - if it had a header we'd use the schema from schema registry, otherwise we'd use vanilla json schema-less deserialization", "author": "agavra", "createdAt": "2020-08-21T00:19:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwNzI2Mg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474607262", "bodyText": "@agavra\n\nJSON does not support schema registry integration\n\nI thought it supported reading, but not writing schemas? i.e. if it found the magic byte it handled it.  Or does it just ignore it?", "author": "big-andy-coates", "createdAt": "2020-08-21T10:15:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MjczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mjg3MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471062870", "bodyText": "If we have structured keys in the future, how can a user set a non-primitive single-column AVRO key format? -- Would it be better to reserve AVRO for this case? and use PRIMITIVE_AVRO (or similar) for the case you describe? (Similar for JSON actually).", "author": "mjsax", "createdAt": "2020-08-16T04:16:14Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk1NzA4OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473957088", "bodyText": "We already support WRAP_SINGLE_VALUE in the WITH clause to handle switching between anonymous and wrapped serialized forms.   The future work to support wrapped keys will introduce WRAP_SINGLE_KEY.\nI've added details to the KLIP.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:11:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mjg3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDA4ODI2Mg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474088262", "bodyText": "See https://github.com/confluentinc/ksql/blob/dcd28ff1b26705625a477db4e3e81a1891b184aa/design-proposals/klip-33-key-format.md#future-multi-column-key-work", "author": "big-andy-coates", "createdAt": "2020-08-20T15:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mjg3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMzOTg5OQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474339899", "bodyText": "Thanks. Seems I missed the memo. WRAP_SINGLE_VALUE sounds a little awkward to me personally...", "author": "mjsax", "createdAt": "2020-08-21T00:18:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mjg3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzEzMg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471063132", "bodyText": "Do we want to allow to mix KEY_FORMAT, VALUE_FORMAT and FORMAT or not? Ie, we can either say: (1) use only KEY_FORMAT + VALUE_FORMAT or only FORMAT (but don't mix both), or (2) if you mix FORMAT with KEY_FORMAT (or VALUE_FORMAT) the KEY_FORMAT overwrites whatever FORMAT specifies.", "author": "mjsax", "createdAt": "2020-08-16T04:20:06Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk1OTM0MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473959340", "bodyText": "As the KLIP states:\n\nProviding FORMAT along with either KEY_FORMAT or VALUE_FORMAT will result in an error.\n\n:)", "author": "big-andy-coates", "createdAt": "2020-08-20T13:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzEzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzE5MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471063190", "bodyText": "Should there also be ksql.persistence.default.format ?", "author": "mjsax", "createdAt": "2020-08-16T04:20:42Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk1OTg5OQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473959899", "bodyText": "I don't think so.  If you're setting config, easy enough to just set both.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzI4NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471063284", "bodyText": "Ah. Here we go -- this should be added above already :)", "author": "mjsax", "createdAt": "2020-08-16T04:22:18Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzcxNg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471063716", "bodyText": "When does the format change happen? Or this this unspecified by definition?\nFor example, the KS program could be jsonStreamBids.selectKey(/*change the object from JSON to AVRO*) (on read) or it could be selectKey(/*change the object from JSON to AVRO*).to(\"AVRP_BIDS\") (on write).\nFor the simple SELECT * query from the example there is no difference, but assume you do anything stateful the data format of the state stores might differ -- also if you do joins, it might differ (and might require repartitioning).\nOverall, it might be good to give the optimizer some freedom when to do the format change and thus it might be good to only guarantee that the output stream will have the format but no guarantee about intermediate formats.", "author": "mjsax", "createdAt": "2020-08-16T04:29:20Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk3MjUxNA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473972514", "bodyText": "Internal topics are... internal :).\nI'll add a section to the compatibility section to cover this.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDA5MDA4Mg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474090082", "bodyText": "Add https://github.com/confluentinc/ksql/blob/dcd28ff1b26705625a477db4e3e81a1891b184aa/design-proposals/klip-33-key-format.md#internal-topics\nIn general, I'm saying we don't need to make any guarantees from release to release about the internal formats of topics. We're free to change them as we want.\nOf course, we do maintain the internal formats for existing queries. But that's handled already by capturing the internal format in the query plan.", "author": "big-andy-coates", "createdAt": "2020-08-20T15:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mzg1Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471063853", "bodyText": "This is also an interesting point with regard to when the format changes happens -- internal topics might register different stuff in the SR -- if we let the optimizer decide, and we change the optimization rules in the future, it raises schema compatibility questions.", "author": "mjsax", "createdAt": "2020-08-16T04:31:27Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk3MzIxMg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473973212", "bodyText": "Nope, no concerns at all.   See the compatibility section.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:29:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mzg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDA5MDQxNg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474090416", "bodyText": "Again, see https://github.com/confluentinc/ksql/blob/dcd28ff1b26705625a477db4e3e81a1891b184aa/design-proposals/klip-33-key-format.md#internal-topics", "author": "big-andy-coates", "createdAt": "2020-08-20T15:52:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mzg1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDA3OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471064078", "bodyText": "Why the right side?", "author": "mjsax", "createdAt": "2020-08-16T04:34:54Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk4MzE5OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473983198", "bodyText": "Good question! Because it's better than the wrong side, obviously!\nUnlike a traditional system, we can't easily know which side is the cheapest to repartition.  When queries are started the topics in question could be empty.  Even if they aren't empty, we can't really read anything into the size of the topic. A topic may be huge, while the other side tiny, but that's because the huge topic contains a pretty static table, and the other side is about to have a fire hose turned on. We can't even assume a large change log means a large table, it could just be bad/no compaction.\nWithout an optimiser to choose which side to repartition, I figured we just have to pick one.  And it felt more likely that the right would be the cheaper, on average, to repartition, mainly because of stream-table joins, where the table doesn't trigger joins and will often be updated less frequently than the right.\nThe user is always free to explicitly repartition themselves if they don't like our choice.\nOf course, this is just a gut feeling. So feel free to plow in and brake all my assumptions ;)", "author": "big-andy-coates", "createdAt": "2020-08-20T13:38:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDA3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDA5MTA3NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474091074", "bodyText": "I've extended the detail about how we will pick the formats during joins, though the actual algo is still not defined: https://github.com/confluentinc/ksql/blob/dcd28ff1b26705625a477db4e3e81a1891b184aa/design-proposals/klip-33-key-format.md#implementation", "author": "big-andy-coates", "createdAt": "2020-08-20T15:53:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDA3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r471064219", "bodyText": "It may still introduce out-of-order data across different keys and thus time-tracking (\"stream-time\"), and grace-period and retention-time might be affected. (But it might be the same issue if we pick the left-input for repartitioning?).\nHowever, for a stream-table join, it might be simpler to repartition the stream-side? Most likely, the join won't happen on the original key and the stream must be repartioned anyway (and we can just change the format on-the-fly)?", "author": "mjsax", "createdAt": "2020-08-16T04:37:01Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,299 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: _link to the design discussion PR_\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key and value formats.\n+ * Removal of requirement for `VALUE_FORMAT` property in the `WITH` clause of CT/CS statements, where\n+   the server configuration provides a default.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+The existing `VALUE_FORMAT` property will no longer be required, if the server config provides a \n+default. (See below).\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+* `ksql.persistence.default.format.value`: the default value format.\n+\n+## Design\n+\n+### New CREATE properties\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product.\n+\n+### New server config\n+\n+In addition to new `CREATE` properties, the user will also be able to set default key and value \n+formats using system configuration. \n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set. \n+`VALUE_FORMAT` is currently a required property, but will no longer be required _if_ \n+`ksql.persistence.default.format.value` is set.\n+\n+Somewhat unrelated, but small, we propose also changing the `KAFKA_TOPIC` property from _required_ to \n+_optional_ in `CREATE STREAM` and `CREATE TABLE` statements. If not supplied, it will default to the \n+uppercase name of the stream or table.  This matches the behaviour of the property in \n+`CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements, where it is already optional.\n+\n+To additional configurations will be added to control topic names:\n+\n+* `ksql.persistence.topic.name.case`: if `lower`, defaults ksql to using lowercase topics names\n+  where topic names are not explicitly provided. This includes matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic. Default: `upper`.\n+* `ksql.persistence.topic.name.case.insensitive`: if `true`, matching a `CREATE TABLE` or \n+  `CREATE STREAM` statement to an existing topic will be case insensitive. Multiple matches will \n+  result in an error. Default: `false`.\n+\n+With these changes there will no longer be any required `CREATE` properties. This means the `WITH` \n+clause for `CREATE` statements for streams and tables would be optional. We propose this makes the \n+syntax more intuitive for those already familiar with SQL. This would make a `CREATE TABLE` \n+statement ANSI standard.\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='Avro';\n+SET 'ksql.persistence.default.format.value'='Avro';\n+\n+CREATE TABLE USERS (\n+  ID BIGINT PRIMARY KEY,\n+  NAME STRING\n+);\n+\n+CREATE STREAM BIDS (\n+  ITEM_ID BIGINT KEY,\n+  USER_ID BIGINT,\n+  AMMOUNT INT\n+);\n+```\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected is joins.\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition the right side of a join to match the key\n+format of the left side.  Such repartitioning is possible and safe, even for tables, because the \n+logical key of the data will not have changed, only the serialization format. This ensures the \n+ordering of updates to a specific key are maintained across the repartition.", "originalCommit": "9a47d319050da52c009d7b3f6946d86b15cbb751", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk4ODI2NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r473988264", "bodyText": "It may still introduce out-of-order data across different keys and thus time-tracking (\"stream-time\"), and grace-period and retention-time might be affected. (But it might be the same issue if we pick the left-input for repartitioning?).\n\nSure, just like any other implicit repartitioning we do.  Not a lot we can do about it, other than call it out in the docs. (Noted in the PR now).\n\nHowever, for a stream-table join, it might be simpler to repartition the stream-side? Most likely, the join won't happen on the original key and the stream must be repartitioned anyway (and we can just change the format on-the-fly)?\n\nGood point about leveraging any existing repartition to also change the format. (I'll add this to the KLIP) If there is no existing repartition, which side to choose is fairly arbitrary, however, I've outlined why I think it should be the right in my comment about.", "author": "big-andy-coates", "createdAt": "2020-08-20T13:43:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDEzNjU1NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474136554", "bodyText": "i think the idea that, in a stream-table join, it's the stream that gets repartitioned (assuming any repartitioning needed at all of course) makes intuitive sense. If you think about it, you should only join to the table on it's key anyway - unless you really understand the implications at least.\nObviously table-table joins that require a re-partition should be delegated to the kstreams FK-join feature to handle. (I know that's not implemented into ksqlDB yet but it hopefully won't be too far in the future - seems sub-optimal to build some strange short-term workaround for it?)\nThat leaves stream-stream joins, for which I agree that some arbitrary rule is required, hopefully one that's easily remembered like \"left side always gets repartitioned\" :-)", "author": "blueedgenick", "createdAt": "2020-08-20T16:55:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYxMjQ3Nw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474612477", "bodyText": "Thanks for chiming in Nick - you're views are always appreciated! Now I'm going to disagree with you :D\n\ni think the idea that, in a stream-table join, it's the stream that gets repartitioned (assuming any repartitioning needed at all of course) makes intuitive sense.\n\nWhy is that more intuitive?  And, even if it is, do we need to do the more intuitive thing if we this is an internal thing and we think it will hurt performance compared to the other option?\n\nIf you think about it, you should only join to the table on it's key anyway - unless you really understand the implications at least.\n\nThe repartitioning of the table in this scenario isn't changing the logical key or the ordering of updates to the key.  Though I guess it will often cause out-of-order data in the repartitioned changelog.  That's out-of-order in terms of stream-time, not out-of-order in terms of updates to a specific key.\nIs it this out-of-ordering that you're worried about?  You may be right to worry. I've not had time to fully think about the implications of this, but it feels like this could increase the likelihood of strange join behaviour.  @mjsax is probably the best to comment as his brain seems to hold all this without needing deep thought and time ;).  If this is a valid concern, then I'd agree we should repartition stream side over table.\n\nObviously table-table joins that require a re-partition should be delegated to the kstreams FK-join feature to handle.\n\nWhy would we need FK joins?? It's not joining on a FK, it's joining on the primary key. It's just that the binary primary key is in the wrong format.\n\nThat leaves stream-stream joins, for which I agree that some arbitrary rule is required, hopefully one that's easily remembered like \"left side always gets repartitioned\" :-)\n\nI think we can be smarter than that. (Yes, I know I put right side always gets repartitioned in the KLIP :p). I've updated the KLIP to propose we look at the whole join, (remember it could be an N-way join), and look at what combination and permutation results in the smallest number of repartitions.", "author": "big-andy-coates", "createdAt": "2020-08-21T10:25:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgxODM3NA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474818374", "bodyText": "Thanks for chiming in Nick - you're views are always appreciated! Now I'm going to disagree with you :D\n\nHa, thanks - wish I had more time to read some of these more thoroughly. And I expect nothing less! :D\nYes, I was worried about the re-ordering implications. As you mention, picking the prodigious brain of @mjsax is often the best resource for the final word here.\nI confess I hadn't really grokked that you had intended this to apply only in cases where the key remains the same, in which case some of my points here are irrelevant (FK joins)- thanks for clarifying, that's my bad for not reading more carefully.\n\nI think we can be smarter than that. (Yes, I know I put right side always gets repartitioned in the KLIP :p). I've updated the KLIP to propose we look at the whole join, (remember it could be an N-way join), and look at what combination and permutation results in the smallest number of repartitions.\n\nThis requires care! As you mentioned elsewhere in the KIP, the science for optimizing continuous joins isn't fully fleshed out yet. I'm not at all convinced that simply optimizing for a target of \"least number of repartitions\" is going to give an optimal outcome given the probable imbalance in the message rate across incoming topics. Given this, I still favor something easy to remember like \"left-to-right ordering of required repartitions\" or similar. Not only is it easy to remember, and therefore predict/understand what's going to happen, it also gives the user a chance to influence the outcome simply by re-ordering the joins within her query.", "author": "blueedgenick", "createdAt": "2020-08-21T17:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg0NDMzMw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474844333", "bodyText": "As mentioned in my original comment, there will be some impact on ordering, but as the key itself does not change, but only the key-format the order per-key is preserved -- to this end, I don't have strong correctness concerns (of course, we might need proper garcePeriod/retentionTimes in case we introduce such parameters for stream-table join at some point) to take potentially higher degree of unorder into account -- for example, a changelog topic chould be ordered originally if populated accordingly from an upstream app and we might \"destroy\" this (cross-key) order. This could impact the stream-table join result (as long as we are not able to handle out-of-order data more gracefully). Repartitioning the stream-side of a stream-table join might have the same ordering issues though, so I don't think it would buy us much from this POV. -- However, as stream-side repartitioning happens ofter for non-key steam-table joins anyway, we should try to piggyback here to avoid two repartitioning steps.\nFor table-table joins, the impact should be limited to the order of the output record, however, the result records should be exactly the same (as the per-key order is preserved, what is strong enough to not alter the result).\nFor stream-steam (inner) joins, we handle out-of-order data perfectly fine and thus there is no concern.\nThe \"higher throughput\" argument from Andy is a good one though, but it seems only to apply to stream-table joins (it seems a reasonable assumption that the stream has higher throughput than the table; but how often the join happens on the original stream-key is questionable anyway...). For stream-stream, and table-table joins (that are symmetric) is hard (impossible?) to know which side should be repartitioned from a perf point of view... Maybe taking the number of partitions into account (if ksqlDB could fetch the corresponding metadata upfront before compiling down to a Kafka Streams program) could be data point though -- we might want to pick the topic with fewer partitions to do the repartitioning and scale it out to the anticipated higher-throughput input.\nBut those optimizations might be beyond the scope of this KIP...\nThus, overall I am ok with picking the right input for all cases (as it fits the stream-table case, and does not impact the stream-stream or table-table case), but would highly recommend to piggyback the format change for stream-table joins to the stream-side repartitioning if possible.", "author": "mjsax", "createdAt": "2020-08-21T17:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NDIxOQ=="}], "type": "inlineReview"}, {"oid": "dcd28ff1b26705625a477db4e3e81a1891b184aa", "url": "https://github.com/confluentinc/ksql/commit/dcd28ff1b26705625a477db4e3e81a1891b184aa", "message": "chore: suggestions", "committedDate": "2020-08-20T15:47:49Z", "type": "commit"}, {"oid": "4ca0788edb5e72537890bdb1c89628e039680320", "url": "https://github.com/confluentinc/ksql/commit/4ca0788edb5e72537890bdb1c89628e039680320", "message": "chore: remove outstanding", "committedDate": "2020-08-20T15:58:48Z", "type": "commit"}, {"oid": "fb32216b9d6088432aed7810a102a585a5d092b9", "url": "https://github.com/confluentinc/ksql/commit/fb32216b9d6088432aed7810a102a585a5d092b9", "message": "docs: NULL key format and scheme evolution", "committedDate": "2020-08-21T11:01:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDcyMDgwNg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474720806", "bodyText": "Is this a typo? This is the same statement as after the text \"It will still be possible to define a key-less stream by not providing any key-column for key formats don't support schema inference:\"\nThis statement looks like it should be fine.", "author": "vcrfxia", "createdAt": "2020-08-21T14:07:53Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -363,41 +381,78 @@ CREATE STREAM FOO (\n -- key-less stream with value columns using schema inference:\n CREATE STREAM FOO WITH (\n     KAFKA_TOPIC='foo',\n-    VALUE_FORMAT='DELIMITED'\n+    VALUE_FORMAT='AVRO'\n );\n ```\n \n-If the `ksql.persistence.default.format.key` system configuration is providing a default key format,\n-and that format supports schema inference, then it may not be possible to differentiation a key-less\n-stream from a stream where the key schema is missing. \n+But what happens once users can supply the key format? Key format currently defaults to `KAFKA`, but\n+it doesn't make sense to force users to set `KEY_FORMAT` to `KAFKA` if there is no key!\n \n-It is likely the schema registry does not support registering an 'empty' schema. Even if this is\n-possible today, will all future formats support such a concept?\n+```sql\n+-- BAD!", "originalCommit": "fb32216b9d6088432aed7810a102a585a5d092b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc4ODAwNA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474788004", "bodyText": "Not a typo, it's referring to forcing users to define a key format when they don't want a key.\nThe other statement is about maintaining backwards compatibility - but thinking more about it now, we don't need this - we should fail if the user supplies a key_format but no key!", "author": "big-andy-coates", "createdAt": "2020-08-21T16:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDcyMDgwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDcyMTQzNA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r474721434", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            SET 'ksql.persistence.default.format.key''='AVRO';' \n          \n          \n            \n            SET 'ksql.persistence.default.format.key'='AVRO';", "author": "vcrfxia", "createdAt": "2020-08-21T14:08:56Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -363,41 +381,78 @@ CREATE STREAM FOO (\n -- key-less stream with value columns using schema inference:\n CREATE STREAM FOO WITH (\n     KAFKA_TOPIC='foo',\n-    VALUE_FORMAT='DELIMITED'\n+    VALUE_FORMAT='AVRO'\n );\n ```\n \n-If the `ksql.persistence.default.format.key` system configuration is providing a default key format,\n-and that format supports schema inference, then it may not be possible to differentiation a key-less\n-stream from a stream where the key schema is missing. \n+But what happens once users can supply the key format? Key format currently defaults to `KAFKA`, but\n+it doesn't make sense to force users to set `KEY_FORMAT` to `KAFKA` if there is no key!\n \n-It is likely the schema registry does not support registering an 'empty' schema. Even if this is\n-possible today, will all future formats support such a concept?\n+```sql\n+-- BAD!\n+CREATE STREAM FOO WITH (\n+    KAFKA_TOPIC='foo',\n+    KEY_FORMAT='KAFKA',\n+    VALUE_FORMAT='AVRO'\n+);\n+```\n \n-We may want to introduce some kind of syntax to represent 'schema inference' in the statement. For \n-example, we could use an ellipse to represent key and value inference and a dash to represent no\n-schema. For example, (`WITH` clause removed for clarity):\n+The user may also have set a default key format, via the `ksql.persistence.default.format.key` \n+system configuration, that supports schema inference. How then does a user declare a key-less \n+stream as opposed to a stream where the key schema is loaded from the Schema Registry? \n \n ```sql\n--- table created with keys and values using schema inference:\n-CREATE TABLE USERS (..., ...);\n+SET 'ksql.persistence.default.format.key''='AVRO';' ", "originalCommit": "fb32216b9d6088432aed7810a102a585a5d092b9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMjY1Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r478622653", "bodyText": "Sorry I am late to the party.\nWhat happens in this scenario: CREATE TABLE test as SELECT * from test1 join test2 where the test1 join key has format f1 and test2 join key has format f2 . The output key format is inherited from the left-most source hence will be f1? If the algorithm chooses that the cheapest partitioning strategy is to partition by f2, does this mean that we need an extra partitioning at the end for the sink topic to be in f1 format?\nIf yes, then this should be part of the decision choosing which side to repartition, no?\nSo the heuristics for the algorithm should take into account:\n\nWhat is the output format?\nWhat keys are the sources partitioned by?\n\nAnd the definition of cheapest should take into account the cost of partitioning at the end to match the final output format as well as the cost of re-partitioning the inputs.", "author": "vpapavas", "createdAt": "2020-08-27T18:43:45Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,461 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NULL`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which it can \n+now handle.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRP_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default.\n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins, single\n+key wrapping and key-less streams.\n+\n+#### Joins", "originalCommit": "fb32216b9d6088432aed7810a102a585a5d092b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMzgxNw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r478623817", "bodyText": "In addition to this, will this be supported\nCREATE TABLE TEST \n  WITH (\n    KEY_FORMAT='F3'\n  ) AS\n    SELECT * FROM TEST1 JOIN TEST2;\n\nIf yes, then the output format can differ completely from the input formats which will definitely necessitate a final repartition for the sink topic.", "author": "vpapavas", "createdAt": "2020-08-27T18:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMjY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1ODQxNg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r478758416", "bodyText": "which will definitely necessitate a final repartition for the sink topic.\n\nWell, yes, but this is not really an issue as the \"repartitioning\" is for free: we write the data into the sink topic anyway and the costs is the same with or without changing the partitioning.", "author": "mjsax", "createdAt": "2020-08-27T23:59:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMjY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3OTUzMw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r478779533", "bodyText": "How is the cost the same? If the partitioning changes, isn't there network overhead and data shuffling involved?", "author": "vpapavas", "createdAt": "2020-08-28T01:21:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMjY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODg0MTUwMg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r478841502", "bodyText": "There is not more network overhead involved. Every message is written once.\nNote, in general when we talk about repartitioning overhead, the issue is that we need to write the data and read it back to shuffle records. (And with no repartitioning, we don't neither write nor read to a topic, ie, the \"overhead/cost\" of repartitioning is one write and one read operation per record.) -- However, when we write to a sink anyway, we don't have more write operations if we change the partitioning.\nMaybe, the only \"overhead\" we might have is that the producer may need to open more network connections as it may need to write data into different partitions that might be hosted on different brokers. However, this seems negligible.", "author": "mjsax", "createdAt": "2020-08-28T05:40:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyMjY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjAzMzc2MA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r482033760", "bodyText": "todo: Enhancing QTT, testing too ... and new SQL testing tool?", "author": "big-andy-coates", "createdAt": "2020-09-02T12:36:58Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,461 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but are\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is already often used as the glue between \n+disparate systems, even though it is limited to changing the _values_ format and structure.\n+Supporting other key formats opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NULL`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry. ", "originalCommit": "fb32216b9d6088432aed7810a102a585a5d092b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjk1OA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r482202958", "bodyText": "If I've been coding it as I intended, YATT should get this KLIP for free \ud83d\ude02", "author": "agavra", "createdAt": "2020-09-02T16:27:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjAzMzc2MA=="}], "type": "inlineReview"}, {"oid": "fb32216b9d6088432aed7810a102a585a5d092b9", "url": "https://github.com/confluentinc/ksql/commit/fb32216b9d6088432aed7810a102a585a5d092b9", "message": "docs: NULL key format and scheme evolution", "committedDate": "2020-08-21T11:01:46Z", "type": "forcePushed"}, {"oid": "04c5fb58aa14ef33df494282162b78097d217ee1", "url": "https://github.com/confluentinc/ksql/commit/04c5fb58aa14ef33df494282162b78097d217ee1", "message": "chore: updates\n\n- Switch NULL -> NONE format.\n- Switch JOINs to repartition the right source\n- Add more details to NONE format", "committedDate": "2020-09-07T15:06:56Z", "type": "commit"}, {"oid": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "url": "https://github.com/confluentinc/ksql/commit/1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "message": "Add LOE", "committedDate": "2020-09-10T14:39:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQxNzUzOA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486417538", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The limitation is less damaging for streams. However, it is still the case that the user looses the\n          \n          \n            \n            The limitation is less damaging for streams. However, it is still the case that the user loses the", "author": "colinhicks", "createdAt": "2020-09-10T15:05:06Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyMTE3MQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486421171", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## Public APIS\n          \n          \n            \n            ## Public APIs", "author": "colinhicks", "createdAt": "2020-09-10T15:09:59Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyMTU3Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486421573", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * In `CRREATE STREAM` and `CREATE TABLE` statements.\n          \n          \n            \n             * In `CREATE STREAM` and `CREATE TABLE` statements.", "author": "colinhicks", "createdAt": "2020-09-10T15:10:29Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyMTgwNg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486421806", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            in the with clause.\n          \n          \n            \n            in the `WITH` clause.", "author": "colinhicks", "createdAt": "2020-09-10T15:10:47Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyMzEwOA==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486423108", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n          \n          \n            \n            The new `KEY_FORMAT` or `FORMAT` property will be supported wherever the current `VALUE_FORMAT` is", "author": "colinhicks", "createdAt": "2020-09-10T15:12:33Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyNDI4Mw==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486424283", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Where one side must be repartitioned to correct the key format, choosing which side to reparation \n          \n          \n            \n            Where one side must be repartitioned to correct the key format, choosing which side to repartition", "author": "colinhicks", "createdAt": "2020-09-10T15:14:11Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation ", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyNTIyOQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486425229", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            even if the source partitions where correctly ordered by time, the re-partitioned partitions would \n          \n          \n            \n            even if the source partitions were correctly ordered by time, the re-partitioned partitions would", "author": "colinhicks", "createdAt": "2020-09-10T15:15:24Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation \n+can not be driven by the size of the data, as in a traditional database system, as the size of \n+the data is unknown, likely infinite. Ideally, for a streaming system it is the rate of change of \n+the data, i.e. the throughput, that would drive the choice. Unfortunately, this too can not be \n+known upfront.  For this reason, we propose repartitioning based on the order of sources within \n+the query, with the source on the _right_ being repartitioned.\n+\n+A benefit of making the choice order-based is that, once the rule is learned, users can predicate \n+and control which side is re-partitioned in some situations, i.e. stream-stream and table-table joins.\n+\n+Note: allowing users to freely switch left and right sources to control which side is repartitioned\n+will work for all but left-outer joins. To support switching left-outer joins ksqlDB would need to\n+support a right-outer join. The addition of this is deemed out of scope.\n+\n+Repartitioning the right side was chosen over the left, as it will mean stream-table joins will \n+repartition the table, which we propose will _generally_ see a lower throughput of updates to the \n+stream side. \n+\n+Such repartitioning is possible and safe... ish, even for tables, because the logical key of the \n+data will not have changed, only the serialization format. This ensures the ordering of updates to \n+a specific key are maintained across the repartition. Of course, the repartitioning would introduce \n+cross-key out-of-order data, as the records are shuffled across partitions. That is to say that \n+even if the source partitions where correctly ordered by time, the re-partitioned partitions would ", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQyODM4MQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486428381", "bodyText": "\ud83d\udc4d", "author": "colinhicks", "createdAt": "2020-09-10T15:19:25Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation \n+can not be driven by the size of the data, as in a traditional database system, as the size of \n+the data is unknown, likely infinite. Ideally, for a streaming system it is the rate of change of \n+the data, i.e. the throughput, that would drive the choice. Unfortunately, this too can not be \n+known upfront.  For this reason, we propose repartitioning based on the order of sources within \n+the query, with the source on the _right_ being repartitioned.\n+\n+A benefit of making the choice order-based is that, once the rule is learned, users can predicate \n+and control which side is re-partitioned in some situations, i.e. stream-stream and table-table joins.\n+\n+Note: allowing users to freely switch left and right sources to control which side is repartitioned\n+will work for all but left-outer joins. To support switching left-outer joins ksqlDB would need to\n+support a right-outer join. The addition of this is deemed out of scope.\n+\n+Repartitioning the right side was chosen over the left, as it will mean stream-table joins will \n+repartition the table, which we propose will _generally_ see a lower throughput of updates to the \n+stream side. \n+\n+Such repartitioning is possible and safe... ish, even for tables, because the logical key of the \n+data will not have changed, only the serialization format. This ensures the ordering of updates to \n+a specific key are maintained across the repartition. Of course, the repartitioning would introduce \n+cross-key out-of-order data, as the records are shuffled across partitions. That is to say that \n+even if the source partitions where correctly ordered by time, the re-partitioned partitions would \n+see out-of-order records, though per-key ordering would be maintained. Thus time-tracking \n+(\"stream-time\"), grace-period and retention-time might be affected. However, this  phenomenon \n+already exists, and is deemed acceptable, for other implicit re-partitions.\n+\n+#### Single key wrapping   \n+\n+To ensure query plans written after this work are forward compatible with future enhancements to \n+support single key columns wrapped in JSON object, Avro records, etc, and ultimately multiple key \n+columns, a new `UNWRAP_SINGLE_KEY` value will be added to `SerdeOption` and explicitly set on all\n+source, sink and internal topics. See [Future multi-column key work](#future-multi-column-key-work) \n+below for more info / background.\n+\n+#### Key-less streams\n+\n+A new `NONE` format will be introduced to allow users to provide a `KEY_FORMAT` that informs ksqlDB\n+to ignore the key. This format will be rejected as a `VALUE_FORMAT` for now, as ksqlDB does not yet\n+support value-less streams and tables. See [Schema Inference](#schema-inference) below for more \n+info / background.\n+\n+This format is predominately being added to allow users to declare key-less streams when the new \n+`ksql.persistence.default.format.key` system configuration is set to a format that supports schema\n+inference, i.e. loading the schema from the schema registry. If a user were not to explicitly set\n+the key format to `NONE` and attempt to create a stream, ksqlDB would attempt to read the key schema\n+from the schema registry, and report an error if the schema did not exist. The `NONE` format will \n+allow users to override the default key format and explicitly inform ksqlDB to ignore the key:\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='AVRO';\n+\n+-- Only the value columns of CLICKS will be loaded from the schema registry.\n+CREATE STREAM CLICKS \n+ WITH (\n+   key_format='NONE',  -- Informs kdqlDB to ignore the key\n+   value_format='AVRO',\n+   ...\n+);\n+```\n+\n+Declaring a table with key format `NONE` will result in an error.\n+\n+Defining key columns IN `CREATE TABLE` or `CREATE STREAM` statements where the key format is `NONE` \n+will result in an error:\n+\n+```sql\n+CREATE TABLE USER (\n+   ID INT PRIMARY KEY, \n+   NAME STRING\n+ ) WITH (\n+   key_format='NONE'  -- Error! Can't define key columns with this format\n+   ...\n+)  \n+``` \n+\n+`CREATE AS` statements that set the key, i.e. those containing `GROUP BY`, `PARTITION BY` and \n+`JOIN`, where the source has a `NONE` key format, and which do not explicitly define a key format, \n+will pick up their key format from the new `ksql.persistence.default.format.key` system \n+configuration. If this setting is not set, the statement will generate an error.\n+\n+```sql\n+CREATE STREAM KEY_LESS (\n+   NAME STRING\n+ ) WITH (\n+   key_format='NONE',\n+   ...\n+);\n+\n+-- Table T will get key format from the 'ksql.persistence.default.format.key' system config. \n+-- If the config is not set, an error will be generated. \n+CREATE TABLE T AS \n+  SELECT \n+    NAME, \n+    COUNT()\n+  FROM KEY_LESS\n+  GROUP BY NAME;\n+```\n+\n+`CREATE AS` statements that create key-less streams will now implicitly set the key format to \n+`NONE`.\n+\n+## Test plan\n+\n+Aside from the usual unit tests etc, the QTT suit of tests will be extended to cover the different\n+key formats. Tests will be added to cover the new syntax and configuration combinations and \n+permutations. Existing tests covering aggregations, re-partitions and joins will be extended to \n+include variants with different key formats. \n+\n+## LOEs and Delivery Milestones\n+\n+The KLIP will be broken down into the following deliverables:\n+\n+1. **Basic JSON support (5 weeks)**: Support for the `JSON` key format, without:\n+    * schema registry integration\n+    * Automatic repartitioning of streams and tables for joins where key formats do not match: such\n+      joins will result in an error initially.\n+  \n+    Included in this milestone:\n+    \n+    * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+    * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+    * Addition of new server configuration to provide defaults for key format.\n+    * Support for additional key column data types, as JSON supports them:\n+        * `DECIMAL`\n+        * `BOOLEAN`\n+    * Full support of the key format for all supported SQL syntax.  \n+    * Enhancements to QTT and the ksqlDB testing tool\n+    * Rest and HTTP2 server endpoints and Java client to work with new key format.\n+1. **NONE format (1 week)**: Supported on keys only. Needed to support key-less streams once we have SR integration.\n+1. **Schema Registry support**: Adds support for reading and writing schemas to and from the schema\n+   registry.\n+1. **JSON_SR support (1 week)** Adds support for the `JSON_SR` key format, inc. schema registry integration.\n+1. **Avro support (1 week)** Adds support for the `AVRO` key format, inc. schema registry integration.\n+1. **Delimited support (1 week)**: Adds support for the `DELIMITED` key format.\n+1. **Auto-repartitioning on key format mismatch (1.5 weeks)**. Adds support for automatic repartitioning of \n+   streams and tables for joins where key formats do not match.\n+1. **Blog post (1 week)**: write a blog post about the new features. Likely one post for everything _but_ \n+   auto-repartitioning, and a second to cover this.\n+   \n+## Documentation Updates\n+\n+New server config and new `CREATE` properties will be added to main docs site.\n+\n+There are no incompatible changes within the proposal, so no demos and examples _must_ change.\n+However, it probably pays to update some to highlight the new features. We propose updating the \n+Kafka micro site examples to leverage the new functionality, as these have automated testing.  \n+It may be worth changing the ksqlDB quickstart too. ", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0MTAyNQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486541025", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A benefit of making the choice order-based is that, once the rule is learned, users can predicate \n          \n          \n            \n            A benefit of making the choice order-based is that, once the rule is learned, users can predict", "author": "vcrfxia", "createdAt": "2020-09-10T18:15:28Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation \n+can not be driven by the size of the data, as in a traditional database system, as the size of \n+the data is unknown, likely infinite. Ideally, for a streaming system it is the rate of change of \n+the data, i.e. the throughput, that would drive the choice. Unfortunately, this too can not be \n+known upfront.  For this reason, we propose repartitioning based on the order of sources within \n+the query, with the source on the _right_ being repartitioned.\n+\n+A benefit of making the choice order-based is that, once the rule is learned, users can predicate ", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0OTAxOQ==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486549019", "bodyText": "Add note clarifying that users can always repartition topics themselves before the join, in order to have full control over which sources are repartitioned (and that choosing the same sources ksqlDB would repartition and performing the repartitions upfront is equivalent from a resource-usage standpoint)? Or if not here, at least in the docs section so we don't forget to add the note later.", "author": "vcrfxia", "createdAt": "2020-09-10T18:26:10Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation \n+can not be driven by the size of the data, as in a traditional database system, as the size of \n+the data is unknown, likely infinite. Ideally, for a streaming system it is the rate of change of \n+the data, i.e. the throughput, that would drive the choice. Unfortunately, this too can not be \n+known upfront.  For this reason, we propose repartitioning based on the order of sources within \n+the query, with the source on the _right_ being repartitioned.\n+\n+A benefit of making the choice order-based is that, once the rule is learned, users can predicate \n+and control which side is re-partitioned in some situations, i.e. stream-stream and table-table joins.\n+\n+Note: allowing users to freely switch left and right sources to control which side is repartitioned\n+will work for all but left-outer joins. To support switching left-outer joins ksqlDB would need to\n+support a right-outer join. The addition of this is deemed out of scope.\n+\n+Repartitioning the right side was chosen over the left, as it will mean stream-table joins will \n+repartition the table, which we propose will _generally_ see a lower throughput of updates to the \n+stream side. \n+\n+Such repartitioning is possible and safe... ish, even for tables, because the logical key of the \n+data will not have changed, only the serialization format. This ensures the ordering of updates to \n+a specific key are maintained across the repartition. Of course, the repartitioning would introduce \n+cross-key out-of-order data, as the records are shuffled across partitions. That is to say that \n+even if the source partitions where correctly ordered by time, the re-partitioned partitions would \n+see out-of-order records, though per-key ordering would be maintained. Thus time-tracking \n+(\"stream-time\"), grace-period and retention-time might be affected. However, this  phenomenon \n+already exists, and is deemed acceptable, for other implicit re-partitions.\n+", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MTQyMg==", "url": "https://github.com/confluentinc/ksql/pull/6017#discussion_r486551422", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               key_format='NONE',  -- Informs kdqlDB to ignore the key\n          \n          \n            \n               key_format='NONE',  -- Informs ksqlDB to ignore the key", "author": "vcrfxia", "createdAt": "2020-09-10T18:29:27Z", "path": "design-proposals/klip-33-key-format.md", "diffHunk": "@@ -0,0 +1,561 @@\n+# KLIP 33 - Key format\n+\n+**Author**: @big-andy-coates | \n+**Release Target**: 0.13 | \n+**Status**: In Discussion | \n+**Discussion**: https://github.com/confluentinc/ksql/pull/6017\n+\n+**tl;dr:** ksqlDB currently only supports keys compatible with the `KAFKA` format. This limits the\n+           data ksqlDB can work with. Extending the set of key formats ksqlDB supports immediately\n+           opens up the use of ksqlDB with previously incompatible datasets.\n+           \n+## Motivation and background\n+\n+Data stored in Kafka has a key and a value. These can be serialized using different formats, but\n+generally use a common format. ksqlDB supports multiple _value_ [data formats][1], but requires the \n+key data format to be the `KAFKA`.  \n+\n+This limitation is particularly problematic for tables. ksqlDB is unable to access changelog topics\n+in Kafka that have non-`KAFKA` formatted keys. As the key of the Kafka record is the `PRIMARY KEY` \n+of the table, it is essential that the key can be read if the changelog is to be materialised into a \n+table. When changelog topics have non-`KAFKA` key formats, the limitation precludes ksqlDB as a solution.\n+\n+The limitation is less damaging for streams. However, it is still the case that the user looses the\n+ability to access the data in the Kafka record's key. If this data is not duplicated in the record's\n+value, which generally seems to be the case, then the data is not accessible at all. If the data is\n+required, then the limitation precludes ksqlDB as a solution.\n+\n+As well as unsupported _input_ key formats, ksqlDB is equally precluded should a solution require the \n+_output_ to have a non-`KAFKA` key format. ksqlDB is often used as the glue between disparate systems, \n+even though it is limited to changing the _values_ format and structure. Supporting other key formats \n+opens this up to also transforming the key into a different format. \n+\n+In some cases users are able to work around this limitation. This may involve changing upstream code,\n+or introducing pre-processing, or, in the case of Connect, using SMTs to convert the key format. All\n+such solutions tend to increase the complexity of the system, and generally hurt performance.\n+\n+To open ksqlDB up to new problems spaces and to drive adoption, ksqlDB should support other key formats. \n+\n+## What is in scope\n+\n+ * Addition of a new optional `KEY_FORMAT` property in the `WITH` clause, to set the key format.\n+ * Addition of a new optional `FORMAT` property in the `WITH` clause, to set both the key & value formats.\n+ * Addition of new server configuration to provide defaults for key format.\n+ * Support for additional key column data types, where the key format supports it:\n+    * `DECIMAL`\n+    * `BOOLEAN`\n+ * Support of the following key formats:\n+    * `KAFKA`: the current key format.\n+    * `DELIMITED`: single key columns as a single string value.\n+    * `JSON` / `JSON_SR`: single key column as an anonymous value, i.e. not within a JSON object.\n+    * `AVRO`: single key column as an anonymous value, i.e. not within an Avro record.\n+    * `NONE`: special format indicating no data, or ignored data, e.g. a key-less stream.\n+  * Storing and retrieving key schemas from the Schema Registry for formats that support the integration.\n+  * Full support of these key formats for all supported SQL syntax.\n+  * Automatic repartitioning of streams and tables for joins where key formats do not match.\n+  * Support for reading & writing key schemas to & from the schema registry.\n+  * Enhancements to QTT and the ksqlDB testing tool to allow for keys with formats beyond KAFKA. \n+\n+## What is not in scope\n+\n+ * Support for multiple key columns: this will come later.\n+ * Support for single key columns _wrapped_ in an envelope of some kind: this will come later.\n+ * Support for complex key column data types, i.e. array, struct and map: this will come later. \n+ * Support for `PROTOBUF` keys, as this requires support for wrapped keys: this will come later.\n+ * Enhancing DataGen to support non-KAFKA keys.\n+ * Key schema evolution. (See [key schema evolution](#key-schema-evolution)) in the compatibility \n+   section.\n+ * Support for right-outer joins. This may be covered in a future KLIP.\n+\n+## Value/Return\n+\n+We know from customers and community members that there are a lot of people that have data with \n+non-`KAFKA` formatted keys. This is the first step to unlocking that data and use-cases.\n+\n+With support for `AVRO` and `JSON` key formats there are a lot of existing use-cases that suddenly\n+no longer require pre-processing, or tricky Connect SMTs configured, and there are new use-cases,\n+which ksqlDB was previously unsuitable for, as documented in the motivation section, which can \n+now be handled.\n+\n+## Public APIS\n+\n+### CREATE properties\n+\n+The following new properties will be accepted in the `WITH` clause of `CREATE` statements for streams\n+and tables.\n+\n+* `KEY_FORMAT`: sets the key format, works long the same lines as the existing `VALUE_FORMAT`.\n+* `FORMAT`: sets both the key and value format with a single property.\n+\n+`KEY_FORMAT` will not be a required property _if_ `ksql.persistence.default.format.key` is set.\n+\n+Providing `FORMAT` will set both the key and value formats. Providing `FORMAT` along with either \n+`KEY_FORMAT` or `VALUE_FORMAT` will result in an error.\n+\n+### Server configs\n+\n+The following new configuration options will be added. These configurations can be set globally, \n+within the application property file, or locally, via the `SET` command.\n+\n+* `ksql.persistence.default.format.key`: the default key format.\n+\n+## Design\n+\n+The new `KEY_FORMAT` or `FORMAT` property will be supported where ever the current `VALUE_FORMAT` is\n+supported. Namely:\n+\n+ * In `CRREATE STREAM` and `CREATE TABLE` statements.\n+ * In `CREATE STREAM AS SELECT` and `CREATE TABLE AS SELECT` statements.\n+ \n+The key format will follow the same inheritance rules as the current value format. Namely: any \n+derived stream will inherit the format of its leftmost source, unless the format is explicitly set\n+in the with clause.\n+\n+For example:\n+\n+```sql\n+-- Creates a table over a changelog topic with AVRO key and JSON value:\n+CREATE TABLE USERS (\n+    ID BIGINT PRIMARY KEY,\n+    NAME STRING\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    KEY_FORMAT='AVRO',\n+    VALUE_FORMAT='JSON'\n+  );\n+\n+-- Creates a stream over a topic with JSON key and value:\n+CREATE STREAM BIDS (\n+    ITEM_ID BIGINT KEY,\n+    USER_ID BIGINT,\n+    AMMOUNT INT\n+  ) WITH (\n+    KAFKA_TOPIC='USERS',\n+    FORMAT='JSON'\n+  );\n+\n+-- Change the key format of a stream:\n+CREATE STREAM AVRO_BIDS \n+  WITH WITH (\n+    KEY_FORMAT='AVRO'\n+  ) AS\n+    SELECT * FROM BIDS;\n+\n+-- Creates an enriched stream. The key format is inherited from the leftmost source, i.e. JSON:\n+CREATE STREAM ENRICHED_BIDS AS \n+  SELECT *\n+  FROM BIDS \n+   JOIN USERS ON BIDS.USER_ID = USERS.ID;\n+```\n+\n+For formats that support integration with the schema registry, the key schema will be read and \n+registered with the Schema Registry as needed, following the same pattern as the value schema in \n+the current product. \n+\n+In addition, where possible, key schemas will be marked as `READONLY` to avoid unintentional \n+changes to the key schema id, which would break compatibility. If the Schema Registry is not\n+configured to allow schema mutability to be set, then the statement will still succeed, only \n+a warning will be logged, with link to Schema Registry config that needs changing.\n+\n+If a `CREATE TABLE` or `CREATE STREAM` statement does not include a `KEY_FORMAT` property, the \n+key format is picked up from the `ksql.persistence.default.format.key` system configuration. If this\n+is not set, then an error is returned.  Note: The server config will have this set to `KAFKA` to \n+maintain backwards compatibility with current system by default. \n+\n+### Implementation\n+\n+The system already serializes the key format of source, intermediate and sink topics as part of the\n+query plan, meaning it should be fairly easily to plug in new formats. \n+\n+Validation will be added to ensure only supported key formats are set, and that key column data types\n+are supported by key formats.\n+\n+Most existing functionality should _just work_, as the key format only comes into play during \n+(de)serialization, (obviously). The only area where additional work is expected are joins and key-less \n+streams.\n+\n+#### Joins\n+\n+Joins require the binary key of both sides of the join to match and both sides to be delivered to \n+the same ksqlDB node.  The former normally ensuring the latter, unless a custom partitioning \n+strategy has been used.\n+\n+The introduction of additional key formats means that while the deserialized key from both sides of \n+a join may match, the serialized binary data may differ if the key serialization format is different.\n+To accommodate this, ksqlDB will automagically repartition one side of a join to match the key\n+format of the other.\n+\n+Many joins require an implicit repartition of one or both sides to facilitate the join. In such \n+situations the change of key format can be performed in the same repartitioned step, avoiding any\n+additional re-partitions. This means that joining sources with different key formats will only \n+require an implicit repartition to converge the key formats _if_ neither side is already being \n+repartitioned.\n+\n+Where one side must be repartitioned to correct the key format, choosing which side to reparation \n+can not be driven by the size of the data, as in a traditional database system, as the size of \n+the data is unknown, likely infinite. Ideally, for a streaming system it is the rate of change of \n+the data, i.e. the throughput, that would drive the choice. Unfortunately, this too can not be \n+known upfront.  For this reason, we propose repartitioning based on the order of sources within \n+the query, with the source on the _right_ being repartitioned.\n+\n+A benefit of making the choice order-based is that, once the rule is learned, users can predicate \n+and control which side is re-partitioned in some situations, i.e. stream-stream and table-table joins.\n+\n+Note: allowing users to freely switch left and right sources to control which side is repartitioned\n+will work for all but left-outer joins. To support switching left-outer joins ksqlDB would need to\n+support a right-outer join. The addition of this is deemed out of scope.\n+\n+Repartitioning the right side was chosen over the left, as it will mean stream-table joins will \n+repartition the table, which we propose will _generally_ see a lower throughput of updates to the \n+stream side. \n+\n+Such repartitioning is possible and safe... ish, even for tables, because the logical key of the \n+data will not have changed, only the serialization format. This ensures the ordering of updates to \n+a specific key are maintained across the repartition. Of course, the repartitioning would introduce \n+cross-key out-of-order data, as the records are shuffled across partitions. That is to say that \n+even if the source partitions where correctly ordered by time, the re-partitioned partitions would \n+see out-of-order records, though per-key ordering would be maintained. Thus time-tracking \n+(\"stream-time\"), grace-period and retention-time might be affected. However, this  phenomenon \n+already exists, and is deemed acceptable, for other implicit re-partitions.\n+\n+#### Single key wrapping   \n+\n+To ensure query plans written after this work are forward compatible with future enhancements to \n+support single key columns wrapped in JSON object, Avro records, etc, and ultimately multiple key \n+columns, a new `UNWRAP_SINGLE_KEY` value will be added to `SerdeOption` and explicitly set on all\n+source, sink and internal topics. See [Future multi-column key work](#future-multi-column-key-work) \n+below for more info / background.\n+\n+#### Key-less streams\n+\n+A new `NONE` format will be introduced to allow users to provide a `KEY_FORMAT` that informs ksqlDB\n+to ignore the key. This format will be rejected as a `VALUE_FORMAT` for now, as ksqlDB does not yet\n+support value-less streams and tables. See [Schema Inference](#schema-inference) below for more \n+info / background.\n+\n+This format is predominately being added to allow users to declare key-less streams when the new \n+`ksql.persistence.default.format.key` system configuration is set to a format that supports schema\n+inference, i.e. loading the schema from the schema registry. If a user were not to explicitly set\n+the key format to `NONE` and attempt to create a stream, ksqlDB would attempt to read the key schema\n+from the schema registry, and report an error if the schema did not exist. The `NONE` format will \n+allow users to override the default key format and explicitly inform ksqlDB to ignore the key:\n+\n+```sql\n+SET 'ksql.persistence.default.format.key'='AVRO';\n+\n+-- Only the value columns of CLICKS will be loaded from the schema registry.\n+CREATE STREAM CLICKS \n+ WITH (\n+   key_format='NONE',  -- Informs kdqlDB to ignore the key", "originalCommit": "1e036bffcdf0d37a826b1e4e037e7319d6cc39c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1b3cbac208c1abd945353328c09f3f57868e927d", "url": "https://github.com/confluentinc/ksql/commit/1b3cbac208c1abd945353328c09f3f57868e927d", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-09-10T19:37:12Z", "type": "commit"}, {"oid": "2cb5af11645bedcd3aebe512dba39c9a9ca3d9e6", "url": "https://github.com/confluentinc/ksql/commit/2cb5af11645bedcd3aebe512dba39c9a9ca3d9e6", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:37:32Z", "type": "commit"}, {"oid": "34e879009e4c24e07503b5b370a5d5397123322c", "url": "https://github.com/confluentinc/ksql/commit/34e879009e4c24e07503b5b370a5d5397123322c", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:37:43Z", "type": "commit"}, {"oid": "558cd3c015e3aaf10572cc0bc43b5eb37a8e0c41", "url": "https://github.com/confluentinc/ksql/commit/558cd3c015e3aaf10572cc0bc43b5eb37a8e0c41", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:37:52Z", "type": "commit"}, {"oid": "c5f6e8f2641f262bcb818d8d6da99b7af1947a7e", "url": "https://github.com/confluentinc/ksql/commit/c5f6e8f2641f262bcb818d8d6da99b7af1947a7e", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:38:01Z", "type": "commit"}, {"oid": "9f5524fefe187b42ab1f74039460b446b8b0f247", "url": "https://github.com/confluentinc/ksql/commit/9f5524fefe187b42ab1f74039460b446b8b0f247", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:38:11Z", "type": "commit"}, {"oid": "1eaa91390a05679ff435052fa307a4e3a2d48118", "url": "https://github.com/confluentinc/ksql/commit/1eaa91390a05679ff435052fa307a4e3a2d48118", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:38:26Z", "type": "commit"}, {"oid": "591985a063d3a7b6909ca9e192a5c5d18ad2a5dd", "url": "https://github.com/confluentinc/ksql/commit/591985a063d3a7b6909ca9e192a5c5d18ad2a5dd", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-09-10T19:38:45Z", "type": "commit"}, {"oid": "8d733f39fcacce3067d2a8b4e19059cd0e7dad4b", "url": "https://github.com/confluentinc/ksql/commit/8d733f39fcacce3067d2a8b4e19059cd0e7dad4b", "message": "Update design-proposals/klip-33-key-format.md\n\nCo-authored-by: Victoria Xia <victoria.f.xia281@gmail.com>", "committedDate": "2020-09-10T19:38:59Z", "type": "commit"}, {"oid": "862f1fd948414419383ea88e9db73103625620b6", "url": "https://github.com/confluentinc/ksql/commit/862f1fd948414419383ea88e9db73103625620b6", "message": "Update klip-33-key-format.md", "committedDate": "2020-09-11T10:55:42Z", "type": "commit"}]}