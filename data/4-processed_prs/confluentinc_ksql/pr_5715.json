{"pr_number": 5715, "pr_title": "New how-to guide: create a UDF", "pr_createdAt": "2020-06-26T23:02:41Z", "pr_url": "https://github.com/confluentinc/ksql/pull/5715", "timeline": [{"oid": "6fb0775482c3988cc3659d42ab9bbf9fee201594", "url": "https://github.com/confluentinc/ksql/commit/6fb0775482c3988cc3659d42ab9bbf9fee201594", "message": "docs: stubs for udf how-to", "committedDate": "2020-06-08T16:48:40Z", "type": "commit"}, {"oid": "d8d722fcac2cfa7a9477fdaaac73125341cb4fe4", "url": "https://github.com/confluentinc/ksql/commit/d8d722fcac2cfa7a9477fdaaac73125341cb4fe4", "message": "Merge branch 'master' into mdrogalis-how-to-fn", "committedDate": "2020-06-10T15:50:58Z", "type": "commit"}, {"oid": "51d9718220f543876a7144ce45a41eca2a47421a", "url": "https://github.com/confluentinc/ksql/commit/51d9718220f543876a7144ce45a41eca2a47421a", "message": "docs: how-to fn WIP", "committedDate": "2020-06-11T20:41:05Z", "type": "commit"}, {"oid": "48eaed47f2b497c1955e77fe6a320b491178cbf0", "url": "https://github.com/confluentinc/ksql/commit/48eaed47f2b497c1955e77fe6a320b491178cbf0", "message": "docs: wip", "committedDate": "2020-06-25T15:45:08Z", "type": "commit"}, {"oid": "8f73cff28d5484a2b42d20b1c594bf5705fc0cac", "url": "https://github.com/confluentinc/ksql/commit/8f73cff28d5484a2b42d20b1c594bf5705fc0cac", "message": "Merge branch 'master' into mdrogalis-how-to-fn", "committedDate": "2020-06-25T15:45:50Z", "type": "commit"}, {"oid": "e1ef93ac7e4f20c2c4accce9b9f394997459c2c1", "url": "https://github.com/confluentinc/ksql/commit/e1ef93ac7e4f20c2c4accce9b9f394997459c2c1", "message": "docs: wip, more on the how to guide", "committedDate": "2020-06-25T22:35:17Z", "type": "commit"}, {"oid": "1d6c0000902200785443bc28e2320c5ce41f7eb5", "url": "https://github.com/confluentinc/ksql/commit/1d6c0000902200785443bc28e2320c5ce41f7eb5", "message": "docs: wip, more on the how to guide", "committedDate": "2020-06-25T23:15:52Z", "type": "commit"}, {"oid": "eb82ff78aebb8f46388f8852714a7d874ade9d8a", "url": "https://github.com/confluentinc/ksql/commit/eb82ff78aebb8f46388f8852714a7d874ade9d8a", "message": "docs: wip. more prose", "committedDate": "2020-06-25T23:55:30Z", "type": "commit"}, {"oid": "28e9149a4050a21b72f1ff6f5c772db066d0401f", "url": "https://github.com/confluentinc/ksql/commit/28e9149a4050a21b72f1ff6f5c772db066d0401f", "message": "docs: vars for releases", "committedDate": "2020-06-26T00:00:05Z", "type": "commit"}, {"oid": "fba49430c720d7d5647828521e8b369bc531170e", "url": "https://github.com/confluentinc/ksql/commit/fba49430c720d7d5647828521e8b369bc531170e", "message": "docs: prose of udf/udtf", "committedDate": "2020-06-26T22:34:33Z", "type": "commit"}, {"oid": "d03aa8374bca3751091d55d715ef49561d85d290", "url": "https://github.com/confluentinc/ksql/commit/d03aa8374bca3751091d55d715ef49561d85d290", "message": "docs: wip udafs", "committedDate": "2020-06-26T22:54:23Z", "type": "commit"}, {"oid": "744b91d369dddb23dd04430e634b62896297b13f", "url": "https://github.com/confluentinc/ksql/commit/744b91d369dddb23dd04430e634b62896297b13f", "message": "docs: touch up", "committedDate": "2020-06-26T23:01:49Z", "type": "commit"}, {"oid": "fac692e9a48dea9eb1274986a5e4140cc49d1ede", "url": "https://github.com/confluentinc/ksql/commit/fac692e9a48dea9eb1274986a5e4140cc49d1ede", "message": "docs: redirect from old guide", "committedDate": "2020-06-29T17:29:23Z", "type": "commit"}, {"oid": "79dfea8930e340521b7fbd4f94c1c9b8b8b4a781", "url": "https://github.com/confluentinc/ksql/commit/79dfea8930e340521b7fbd4f94c1c9b8b8b4a781", "message": "docs: fix typos", "committedDate": "2020-06-29T17:57:48Z", "type": "commit"}, {"oid": "a5aca3810aaf933c4cd601e3d288f995e99904df", "url": "https://github.com/confluentinc/ksql/commit/a5aca3810aaf933c4cd601e3d288f995e99904df", "message": "docs: fix project name", "committedDate": "2020-06-29T19:48:58Z", "type": "commit"}, {"oid": "cb28784dbf694d3fa2e73e9351aaae618b3200d3", "url": "https://github.com/confluentinc/ksql/commit/cb28784dbf694d3fa2e73e9351aaae618b3200d3", "message": "docs: struct example", "committedDate": "2020-06-30T00:13:38Z", "type": "commit"}, {"oid": "754d715394a59322dbe8a62d019f534b0a057121", "url": "https://github.com/confluentinc/ksql/commit/754d715394a59322dbe8a62d019f534b0a057121", "message": "docs: first draft done", "committedDate": "2020-06-30T00:24:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNzE4Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447337183", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n          \n          \n            \n            To implement a user-defined function, start by creating a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you use to signal that the classes you're implementing are UDFs specifically. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. What matters is that you can put an uberjar in ksqlDB's extension directory.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:31:33Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNzQzNg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447337436", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n          \n          \n            \n            Dependencies are also declared on `kafka` and `connect-api`. You need both of these dependencies to make your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:32:31Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNzY4NA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447337684", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n          \n          \n            \n            There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features. You can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)).\n          \n          \n            \n            \n          \n          \n            \n            Start by creating a directory for the class files:", "author": "JimGalasyn", "createdAt": "2020-06-30T00:33:28Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODIyMA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447338220", "bodyText": "In general, we try not to make promises about future features in docs.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:35:16Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODMwMQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447338301", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                External parameters do not yet work for tabular or aggregation functions.\n          \n          \n            \n                External parameters aren't supported for tabular or aggregation functions.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:35:33Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODM0OQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447338349", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                Support for these function types will be added soon.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:35:46Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODgyMg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447338822", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n          \n          \n            \n            - This UDTF uses Java generics, which enable operating over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method, since you can have multiple signatures, each with a different generic type parameter.", "author": "JimGalasyn", "createdAt": "2020-06-30T00:37:15Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgxNDkwNQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447814905", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n          \n          \n            \n            - Aggregation functions are designated by a static method with the `@UdafFactory` annotation, which differs from scalar and tabular functions. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:24:10Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgxNjc5Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447816792", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n          \n          \n            \n            - The static factory method must either return `Udaf` or `TableUdaf` in package `io.confluent.ksql.function.udaf`.\n          \n          \n            \n               - Use the `Udaf` return value, as shown in this example, to aggregate streams into tables.\n          \n          \n            \n               - Use the `TableUdaf` return value, which derives from `Udaf`, to aggregate tables into other tables.\n          \n          \n            \n                 Also, you must implement the `undo()` method.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:26:47Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMDE4Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447820183", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n          \n          \n            \n            - ksqlDB decouples the internal representation of an aggregate from its use in an operation. This is useful because aggregations can maintain complex state and expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:31:03Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMTY4Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447821686", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n          \n          \n            \n            - The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you're using session windows, consider what good merge semantics are for your aggregation.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:33:01Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMjgwNA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447822804", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-06-30T16:34:45Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMjkzNw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447822937", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-06-30T16:34:57Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMzA2Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447823062", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-06-30T16:35:07Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNDE0Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447824142", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n          \n          \n            \n                UDFs are loaded only once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you must create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:36:22Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNDgzMg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447824832", "bodyText": "It's probably not great to have UDF version mismatches across the cluster, should we give guidance about this?", "author": "JimGalasyn", "createdAt": "2020-06-30T16:37:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNDE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNTc2NQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447825765", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:38:48Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNzUzNQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447827535", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            The DESCRIBE FUNCTION statement shows all of the type signatures that the UDF implements. For the `formula` function, there are two type signatures.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:41:33Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyODQ1NQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447828455", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n          \n          \n            \n            Execute a push query. The `formula` function multiplies two integers and adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:42:53Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyODc0OQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447828749", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should see:\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:43:17Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyOTU5Nw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447829597", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n          \n          \n            \n            Try the other variant of the `formula` function, which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:44:37Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyOTgxMg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447829812", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should see:\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:44:57Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMDE1Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447830153", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:45:26Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMDk1OA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447830958", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            \n          \n          \n            \n            The DESCRIBE FUNCTION statement shows that `index_seq`  is a generic function with the type parameter `E`, which means that this UDTF can take a parameter that is an array of any type.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:46:44Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMjIwNA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447832204", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n          \n          \n            \n            Execute a push query. The `index_seq` function creates one row for each element in an array, concatenated with the element's index position.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:48:44Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMzI5MA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447833290", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You should see the following:\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:50:28Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMzUwNQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447833505", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It should output the following:\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:50:47Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNDAyOQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447834029", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n          \n          \n            \n            Execute a push query. The `rolling_sum` function aggregates the previous three elements together, sums them, and emits their output.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:51:35Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNDY4Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447834682", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:52:36Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNDk2NA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447834964", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            \n          \n          \n            \n            `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:53:01Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNTQwMw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447835403", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Insert some more rows and shift older elements out of the aggregate:\n          \n          \n            \n            Insert more rows, to shift older elements out of the aggregate:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:53:42Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNTY5NA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447835694", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T16:54:09Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNjMzNA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447836334", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            \n          \n          \n            \n             The output from the `rolling_sum` function has changed. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are now `2`, `1`, and `6`.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:55:07Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNzIzMw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447837233", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n          \n          \n            \n            Using structs in UDFs requires a more specific type contract with ksqlDB. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:56:27Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzNzg1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447837856", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n          \n          \n            \n            For example, create a simple function that maintains simple statistics. This example uses a UDAF, but the concepts are applicable for UDFs and UDTFs. Although the example is a bit contrived, it is useful because it demonstrates using a struct in all possible positions.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:57:26Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzOTA3MQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447839071", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n          \n          \n            \n            - Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. These structs hold different data, so they each need their own schema.", "author": "JimGalasyn", "createdAt": "2020-06-30T16:59:15Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzOTU3Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447839572", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.\n          \n          \n            \n            - Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand.", "author": "JimGalasyn", "createdAt": "2020-06-30T17:00:03Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n+\n+- Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0MDQ4MQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447840481", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The schemas, and all the fields within, are declared as optional. ksqlDB does not yet have null constraints, meaning that today, any value can be null. To cope with this, all schemas and field values must be marked as optional.\n          \n          \n            \n            - The schemas, and all of the contained fields, are declared as optional. ksqlDB doesn't have null constraints, meaning that any value can be null. To handle this, all schemas and field values must be marked as optional.", "author": "JimGalasyn", "createdAt": "2020-06-30T17:01:24Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n+\n+- Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.\n+\n+- The schemas, and all the fields within, are declared as optional. ksqlDB does not yet have null constraints, meaning that today, any value can be null. To cope with this, all schemas and field values must be marked as optional.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0MTE5NA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447841194", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It should output the following:\n          \n          \n            \n            Your output should resemble:", "author": "JimGalasyn", "createdAt": "2020-06-30T17:02:26Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n+\n+- Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.\n+\n+- The schemas, and all the fields within, are declared as optional. ksqlDB does not yet have null constraints, meaning that today, any value can be null. To cope with this, all schemas and field values must be marked as optional.\n+\n+- Explictly declaring schemas is *only* needed when using structs. But because this example makes use of structs in all possible places (input, intermediate, and output values), schemas are declared for all of them.\n+\n+!!! info\n+    If you're using a struct with a UDF or UDTF, you can set the schema using the `Udf`, `Udtf`, and `UdfParameter` annotations. Each provides the option to supply a schema for various positions.\n+\n+Create an uberjar in the same manner. If you already have ksqlDB running, be sure to restart it and remount the jar so that it picks up the new code. When you restart the server, keep an eye on the log files. If any of the type schemas are missing or incoherant, ksqlDB will log an error, such as:\n+\n+```\n+[2020-06-29 21:10:23,889] WARN Failed to create UDAF name=struct_example, method=createUdaf, class=class my.example.StructExample, path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar (io.confluent.ksql.function.UdafLoader:87)\n+io.confluent.ksql.util.KsqlException: Must specify 'aggregateSchema' for STRUCT parameter in @UdafFactory.\n+```\n+\n+### Invoke the function\n+\n+Try using the UDAF. Create the stream `s4`:\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR KEY,\n+    b STRUCT<c BIGINT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+And insert some rows into it:\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 5)\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 3)\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 9)\n+);\n+```\n+\n+Remember to have ksqlDB start all queries from the earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Execute the following push query:\n+\n+```sql\n+SELECT a, stats(b) AS stats FROM s4 GROUP BY a EMIT CHANGES;\n+```\n+\n+It should output the following:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0MTM1Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447841353", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-06-30T17:02:45Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n+\n+- Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.\n+\n+- The schemas, and all the fields within, are declared as optional. ksqlDB does not yet have null constraints, meaning that today, any value can be null. To cope with this, all schemas and field values must be marked as optional.\n+\n+- Explictly declaring schemas is *only* needed when using structs. But because this example makes use of structs in all possible places (input, intermediate, and output values), schemas are declared for all of them.\n+\n+!!! info\n+    If you're using a struct with a UDF or UDTF, you can set the schema using the `Udf`, `Udtf`, and `UdfParameter` annotations. Each provides the option to supply a schema for various positions.\n+\n+Create an uberjar in the same manner. If you already have ksqlDB running, be sure to restart it and remount the jar so that it picks up the new code. When you restart the server, keep an eye on the log files. If any of the type schemas are missing or incoherant, ksqlDB will log an error, such as:\n+\n+```\n+[2020-06-29 21:10:23,889] WARN Failed to create UDAF name=struct_example, method=createUdaf, class=class my.example.StructExample, path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar (io.confluent.ksql.function.UdafLoader:87)\n+io.confluent.ksql.util.KsqlException: Must specify 'aggregateSchema' for STRUCT parameter in @UdafFactory.\n+```\n+\n+### Invoke the function\n+\n+Try using the UDAF. Create the stream `s4`:\n+\n+```sql\n+CREATE STREAM s4 (\n+    a VARCHAR KEY,\n+    b STRUCT<c BIGINT>\n+) WITH (\n+    kafka_topic = 's4',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+And insert some rows into it:\n+\n+```sql\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 5)\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 3)\n+);\n+\n+INSERT INTO s4 (\n+    a, b\n+) VALUES (\n+    'k1', STRUCT(c := 9)\n+);\n+```\n+\n+Remember to have ksqlDB start all queries from the earliest point in each topic:\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+Execute the following push query:\n+\n+```sql\n+SELECT a, stats(b) AS stats FROM s4 GROUP BY a EMIT CHANGES;\n+```\n+\n+It should output the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STATS                                                         |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |{MIN=3, MAX=9, COUNT=3, DIFFERENTIAL=6}                       |\n+```\n+\n+If you like, you can destructure the output into individual columns. Try following the [query structured data guide](query-structured-data.md).\n+\n+## Tear down the stack\n+\n+When you're done, tear down the stack by running:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0MTUzMw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447841533", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```\n          \n          \n            \n            ```bash", "author": "JimGalasyn", "createdAt": "2020-06-30T17:03:02Z", "path": "docs/how-to-guides/use-connector-management.md", "diffHunk": "@@ -247,3 +247,11 @@ In external mode, ksqlDB communicates with an external {{ site.kconnectlong }} c\n External mode essentially works the same way as embedded mode, except connectors run outside of ksqlDB's servers. All that is needed is to configure ksqlDB server with the `ksql.connect.url` property, indicating the address of the {{ site.kconnect }} server. Beyond that, you can manage connectors exactly as you would in embedded mode. No other configuration is needed.\n \n This guide omits an example of setting up an external {{ site.kconnectlong }} cluster. Many great examples, [like Apache Kafka's](https://kafka.apache.org/documentation/#connect), have already been published.\n+\n+## Tear down the stack\n+\n+When you're done, tear down the stack by running:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0MTc3Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447841773", "bodyText": "Nice!", "author": "JimGalasyn", "createdAt": "2020-06-30T17:03:29Z", "path": "mkdocs.yml", "diffHunk": "@@ -164,6 +164,9 @@ plugins:\n     - search\n     - git-revision-date # Must come before the macros plugin\n     - macros\n+    - redirects:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg0Mjg4Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447842883", "bodyText": "Cool, I'll add to the docs readme.", "author": "JimGalasyn", "createdAt": "2020-06-30T17:05:05Z", "path": "docs/requirements.txt", "diffHunk": "@@ -5,4 +5,5 @@ mkdocs-git-revision-date-plugin==0.3\n pymdown-extensions==7.1\n Pygments==2.4.2\n mkdocs-material==5.1.3\n-python-dateutil==2.8.1\n\\ No newline at end of file\n+python-dateutil==2.8.1\n+mkdocs-redirects==1.0.1", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg1NjA5Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447856096", "bodyText": "do we want the top-level example to include custom configurations? I get the feeling that UDF configurations are rather fringe requirements (it's only useful if you use the same udf in multiple clusters) and it can confuse people (make them think it's required) to have the first example be Configurable", "author": "agavra", "createdAt": "2020-06-30T17:25:37Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAxNzkzNA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r448017934", "bodyText": "Yeah, you're probably right. I wanted each example to show a bunch of different features, but I also just copied the first example to the top. I'll strip it down more.", "author": "MichaelDrogalis", "createdAt": "2020-06-30T22:38:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg1NjA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MDI4Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447860286", "bodyText": "It could help to have a section overview for all of the different types of functions:\n| type | input | output |\n|------|-------|--------|\n| UDF | one row | one row |\n| UDTF | one row | many rows |\n| UDAF | many rows | one row* |\n\n* UDAFs will output multiple rows if `EMIT CHANGES` is specified", "author": "agavra", "createdAt": "2020-06-30T17:31:37Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTIwNjk1Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r451206952", "bodyText": "Will save that for the reference section.", "author": "MichaelDrogalis", "createdAt": "2020-07-07T23:54:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MDI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MjI4Mw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447862283", "bodyText": "three nits here:\n\nif they compile with -parameters they don't need value=\"a\"\nif they insist to specify the name, they can use @UdfParameter(\"a\")\nit probably makes sense to name the parameter the same thing as the annotation (e.g. @UdfParameter(\"a\") int a or @UdfParameter(\"v1\") int v1) to avoid confusion", "author": "agavra", "createdAt": "2020-06-30T17:34:52Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTY3MTU3OQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r451671579", "bodyText": "Today I learned. :) Updated.", "author": "MichaelDrogalis", "createdAt": "2020-07-08T16:26:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MjI4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MzA1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447863056", "bodyText": "we have a maven archetype, why not use that instead of adding this gradle snippet?", "author": "agavra", "createdAt": "2020-06-30T17:35:44Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTIwODYyNQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r451208625", "bodyText": "Updated to give it a mention. I'd like to be more explicit with how this is all set up to demystify it.", "author": "MichaelDrogalis", "createdAt": "2020-07-08T00:00:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MzA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2NDkwOA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447864908", "bodyText": "does the version value here actually do anything? not sure how valuable it is to include it in the guide or if itll just add confusion", "author": "agavra", "createdAt": "2020-06-30T17:38:47Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMDIwOA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r448020208", "bodyText": "It seems like it might be useful in practice so I figured I'd add it in. I'll call it out the first time it's used so that it doesn't look arbitrary or magical.", "author": "MichaelDrogalis", "createdAt": "2020-06-30T22:45:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2NDkwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg3MjA4Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447872086", "bodyText": "this is a nice way to introduce these concepts by example :D", "author": "agavra", "createdAt": "2020-06-30T17:51:06Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg3MzYxOA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447873618", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Iterator<Integer> it = intermediate.iterator();\n          \n          \n            \n                        int k = 0;\n          \n          \n            \n            \n          \n          \n            \n                        while(it.hasNext()) {\n          \n          \n            \n                            k += it.next();\n          \n          \n            \n                        }\n          \n          \n            \n            \n          \n          \n            \n                        return k;\n          \n          \n            \n                        return intermediate.stream().reduce(Integer::sum);\n          \n      \n    \n    \n  \n\njust to save some vertical space", "author": "agavra", "createdAt": "2020-06-30T17:53:43Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg3NDQ4MA==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447874480", "bodyText": "shouldn't this return aggTwo? in a session window, if we're merging we probably want the second aggregate not the first to keep in line with \"the latest 3 values\"", "author": "agavra", "createdAt": "2020-06-30T17:55:05Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMTAwMw==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r448031003", "bodyText": "Ah, nice catch. I'll do that.", "author": "MichaelDrogalis", "createdAt": "2020-06-30T23:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg3NDQ4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4NDExMg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447884112", "bodyText": "and decimals", "author": "agavra", "createdAt": "2020-06-30T18:11:45Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4NDY4Mg==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447884682", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Explictly declaring schemas is *only* needed when using structs. But because this example makes use of structs in all possible places (input, intermediate, and output values), schemas are declared for all of them.\n          \n          \n            \n            - Explicitly declaring schemas is *only* needed when using structs and decimals. But because this example makes use of structs in all possible places (input, intermediate, and output values), schemas are declared for all of them.", "author": "agavra", "createdAt": "2020-06-30T18:12:49Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs\n+\n+Using structs in UDFs requires somewhat more ceremony and deserves special attention. Structs are different from the other Java types that ksqlDB interfaces with because their typing is more dynamic. Fields can be added and removed, and their types are inferred on the fly. Because of this dynamism, UDFs need to be more explicit in their type contract with ksqlDB.\n+\n+To demonstrate, create a simple function that maintains simple statistics. This example will use a UDAF, though the concepts are applicable for both UDFs and UDTFs, too. Although the example is a bit fabricated, it is useful because it demonstrates using a struct in all possible positions.\n+\n+### Implement the class\n+\n+Create a file at `src/main/java/com/example/StatsUdaf.java` and populate it with the following code. This UDAF maintains the minimum, maximum, count, and difference between the min and max for a series of numbers.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Schema;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"stats\",\n+                 author = \"example user\",\n+                 version = \"1.3.5\",\n+                 description = \"Maintains statistical values.\")\n+public class StatsUdaf {\n+\n+    public static final Schema PARAM_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"C\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String PARAM_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"C BIGINT\" +\n+        \">\";\n+\n+    public static final Schema AGGREGATE_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String AGGREGATE_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT\" +\n+        \">\";\n+\n+    public static final Schema RETURN_SCHEMA = SchemaBuilder.struct().optional()\n+        .field(\"MIN\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"MAX\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"COUNT\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .field(\"DIFFERENTIAL\", Schema.OPTIONAL_INT64_SCHEMA)\n+        .build();\n+\n+    public static final String RETURN_SCHEMA_DESCRIPTOR = \"STRUCT<\" +\n+        \"MIN BIGINT,\" +\n+        \"MAX BIGINT,\" +\n+        \"COUNT BIGINT,\" +\n+        \"DIFFERENTIAL BIGINT\" +\n+        \">\";\n+\n+    private StatsUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Computes the min, max, count, and difference between min/max.\",\n+                 paramSchema = PARAM_SCHEMA_DESCRIPTOR,\n+                 aggregateSchema = AGGREGATE_SCHEMA_DESCRIPTOR,\n+                 returnSchema = RETURN_SCHEMA_DESCRIPTOR)\n+    public static Udaf<Struct, Struct, Struct> createUdaf() {\n+        return new StatsUdafImpl();\n+    }\n+\n+    private static class StatsUdafImpl implements Udaf<Struct, Struct, Struct> {\n+\n+        @Override\n+        public Struct initialize() {\n+            return new Struct(AGGREGATE_SCHEMA);\n+        }\n+\n+        @Override\n+        public Struct aggregate(Struct newValue, Struct aggregateValue) {\n+            long c = newValue.getInt64(\"C\");\n+            \n+            long min = Math.min(c, getMin(aggregateValue));\n+            long max = Math.max(c, getMax(aggregateValue));\n+            long count = (getCount(aggregateValue) + 1);\n+\n+            aggregateValue.put(\"MIN\", min);\n+            aggregateValue.put(\"MAX\", max);\n+            aggregateValue.put(\"COUNT\", count);\n+            \n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Struct map(Struct intermediate) {\n+            Struct result = new Struct(RETURN_SCHEMA);\n+\n+            long min = intermediate.getInt64(\"MIN\");\n+            long max = intermediate.getInt64(\"MAX\");\n+\n+            result.put(\"MIN\", min);\n+            result.put(\"MAX\", max);\n+            result.put(\"COUNT\", intermediate.getInt64(\"COUNT\"));\n+            result.put(\"DIFFERENTIAL\", max - min);\n+\n+            return result;\n+        }\n+\n+        @Override\n+        public Struct merge(Struct aggOne, Struct aggTwo) {\n+            return aggOne;\n+        }\n+\n+        private Long getMin(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MIN\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MAX_VALUE;\n+            }\n+        }\n+\n+        private Long getMax(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"MAX\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return Long.MIN_VALUE;\n+            }\n+        }\n+\n+        private Long getCount(Struct aggregateValue) {\n+            Long result = aggregateValue.getInt64(\"COUNT\");\n+\n+            if (result != null) {\n+                return result;\n+            } else {\n+                return 0L;\n+            }\n+        }\n+    }\n+}\n+```\n+\n+There are a few important things to call out in this class:\n+\n+- Schemas are declared for the input struct parameter, intermediate aggregation struct value, and output struct value. Becasue each of these three structs are different, they need their own schemas.\n+\n+- Descriptor strings are created for each schema, too. This communicates the underlying types to ksqlDB is a way that its type system can understand. In the future, this may become automated, but it must be maintained today.\n+\n+- The schemas, and all the fields within, are declared as optional. ksqlDB does not yet have null constraints, meaning that today, any value can be null. To cope with this, all schemas and field values must be marked as optional.\n+\n+- Explictly declaring schemas is *only* needed when using structs. But because this example makes use of structs in all possible places (input, intermediate, and output values), schemas are declared for all of them.", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4NjE0MQ==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r447886141", "bodyText": "we can probably omit this for now, or just add a placeholder, but this guide might want to consider covering @UdfSchemaProvider as well (and it can just point to examples in the existing code base, like Abs", "author": "agavra", "createdAt": "2020-06-30T18:14:58Z", "path": "docs/how-to-guides/create-a-user-defined-function.md", "diffHunk": "@@ -0,0 +1,913 @@\n+# How to create a user-defined function\n+\n+## Context\n+\n+You have a piece of logic for transforming or aggregating events that ksqlDB can't currently express. You want to extend ksqlDB to apply that logic in your queries. To do that, ksqlDB exposes hooks through Java programs. This functionality is broadly called *user-defined functions*, or UDFs for short.\n+\n+## In action\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+## Set up a Java project\n+\n+To implement a user-defined function, the first thing that you need to do is create a Java project with a dependency on ksqlDB's UDF library. This library contains the annotations you'll use to signal that the classes you're implementing aren't just any old classes, they're UDFs. You can manage your Java project with any build tool, but this guide demonstrates how it works with Gradle. In the end, all that matters is that you're able to put an uberjar in ksqlDB's extension directory.\n+\n+In a fresh directory, create the following `build.gradle` file to set up the Java project:\n+\n+```\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.github.johnrengelman.shadow\" version \"6.0.0\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    mavenCentral()\n+    jcenter()\n+\n+    maven {\n+        url \"http://packages.confluent.io/maven\"\n+    }\n+}\n+\n+dependencies {\n+    compile \"io.confluent.ksql:ksqldb-udf:{{ site.cprelease }}\"\n+    compile \"org.apache.kafka:kafka_2.13:2.5.0\"\n+    compile \"org.apache.kafka:connect-api:2.5.0\"\n+}\n+\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+apply plugin: \"java\"\n+\n+shadowJar {\n+    archiveBaseName = \"example-udfs\"\n+    archiveClassifier = \"\"\n+    destinationDir = file(\"extensions\")\n+}\n+```\n+\n+Dependencies are also declared on `kafka` and `connect-api`. You'll want both of these dependencies if you plan on making your UDFs capable of being externally configured or able to handle structs. This guide does both of those things.\n+\n+## Implement the classes\n+\n+There are three kinds of UDFs which manipulate rows in different ways: scalar functions, tabular functions, and aggregation functions. Each is demonstrated below with simple examples using a variety of features (you can learn about more sophisticated usage in the [concepts section](../concepts/functions.md)). Start by creating a directory to house the class files:\n+\n+```\n+mkdir -p src/main/java/com/example\n+```\n+\n+### Scalar functions\n+\n+A scalar function (UDF for short) consumes one row as input and produces one row as output. Use this when you simply want to transform a value.\n+\n+Create a file at `src/main/java/com/example/FormulaUdf.java` and populate it with the following code. This UDF takes two parameters and executes a simple formula.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udf.Udf;\n+import io.confluent.ksql.function.udf.UdfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import org.apache.kafka.common.Configurable;\n+\n+import java.util.Map;\n+\n+@UdfDescription(name = \"formula\",\n+                author = \"example user\",\n+                version = \"1.0.2\",\n+                description = \"A custom formula for important business logic.\")\n+public class FormulaUdf implements Configurable {\n+\n+    private int baseValue;\n+\n+    @Override\n+    public void configure(final Map<String, ?> map) {\n+        String s = (String) map.get(\"ksql.functions.formula.base.value\");\n+        baseValue = Integer.parseInt(s);\n+    }\n+\n+    @Udf(description = \"The standard version of the formula with integer parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") int v1, @UdfParameter(value = \"b\") int v2) {\n+        return (v1 * v2) + baseValue;\n+    }\n+\n+    @Udf(description = \"A special variant of the formula, handling double parameters.\")\n+    public long formula(@UdfParameter(value = \"a\") double v1, @UdfParameter(value = \"b\") double v2) {\n+        return ((int) (Math.ceil(v1) * Math.ceil(v2))) + baseValue;\n+    }\n+\n+}\n+```\n+\n+Some important points to notice:\n+\n+\n+- The `@UdfDescription` annotation marks the class as a scalar UDF. The `name` parameter gives the function a name so you can refer to it in SQL.\n+\n+- The `@Udf` annotation marks a method as a body of code to invoke when the function is called. Because ksqlDB is strongly typed, you need to supply multiple signatures if you want your function to work with different column types. This UDF has two signatures: one that takes integer parameters and another that takes doubles.\n+\n+- The `@UdfParameter` annotation lets you give the function parameters names. These are rendered when using the `DESCRIBE` statement on a function.\n+\n+- This UDF uses an external parameter, `ksql.functions.formula.base.value`. When a UDF implements the `Configurable` interface, it will be invoked once as the server starts up. `configure()` supplies a map of the parameters that ksqlDB server was started with. You will see how this value is populated later in the guide.\n+\n+!!! warning\n+    External parameters do not yet work for tabular or aggregation functions.\n+    Support for these function types will be added soon.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Tabular functions\n+\n+A tabular function (UDTF for short) takes one row as input and produces zero or more rows as output. This is sometimes called \"flat map\" or \"mapcat\" in different programming languages. Use this when a value represents many smaller values and needs to be \"exploded\" into its individual parts to be useful.\n+\n+Create a file at `src/main/java/com/example/IndexSequenceUdtf.java` and populate it with the following code. This UDTF takes one parameter as input, an array of any type, and returns a sequence of rows, where each element is the element in the array concatenated with its index position as a string.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udtf.Udtf;\n+import io.confluent.ksql.function.udtf.UdtfDescription;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+@UdtfDescription(name = \"index_seq\",\n+                 author = \"example user\",\n+                 version = \"1.5.0\",\n+                 description = \"Disassembles a sequence and produces new elements concatenated with indices.\")\n+public class IndexSequenceUdtf {\n+\n+    private final String DELIMITER = \"-\";\n+\n+    @Udtf(description = \"Takes an array of any type and returns rows with each element paired to its index.\")\n+    public <E> List<String> indexSequence(@UdfParameter(value = \"s\") List<E> x) {\n+        List<String> result = new ArrayList<>();\n+\n+        for(int i = 0; i < x.size(); i++) { \n+            result.add(x.get(i) + DELIMITER + i);\n+        }\n+\n+        return result;\n+    }\n+\n+}\n+```\n+\n+Notice how:\n+\n+- The UDTF returns a Java `List`. This is the collection type that ksqlDB expects all tabular functions to return.\n+\n+- This UDTF uses Java generics which allow it operate over any [ksqlDB supported types](../../concepts/functions/#supported-types). Use this if you want to express logic that operates uniformly over many different column types. The generic parameter must be declared at the head of the method since you can have multiple signatures, each with a different generic type parameter.\n+\n+\n+!!! info\n+    Java arrays and `List`s are not interchangable in user-defined functions,\n+    which is especially important to remember when working with UDTFs. ksqlDB\n+    arrays correspond to the Java `List` type, not native Java arrays.\n+\n+Either continue following this guide by implementing more functions or skip ahead to [compiling the classes](#add-the-uberjar-to-the-classpath) so you can use the functions in ksqlDB.\n+\n+### Aggregation functions\n+\n+An aggregation function (UDAF for short) consumes one row at a time and maintains a stateful representation of all historical data. Use this when you want to compound data from multiple rows together.\n+\n+Create a file at `src/main/java/com/example/RollingSumUdaf.java` and populate it with the following code. This UDAF maintains a rolling sum of the last `3` integers in a stream, discarding the oldest values as new ones arrive.\n+\n+```java\n+package my.example;\n+\n+import io.confluent.ksql.function.udaf.Udaf;\n+import io.confluent.ksql.function.udaf.UdafDescription;\n+import io.confluent.ksql.function.udaf.UdafFactory;\n+import io.confluent.ksql.function.udf.UdfParameter;\n+\n+import java.util.List;\n+import java.util.LinkedList;\n+import java.util.Iterator;\n+\n+@UdafDescription(name = \"rolling_sum\",\n+                 author = \"example user\",\n+                 version = \"2.0.0\",\n+                 description = \"Maintains a rolling sum of the last 3 integers of a stream.\")\n+public class RollingSumUdaf {\n+\n+    private RollingSumUdaf() {\n+    }\n+\n+    @UdafFactory(description = \"Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\")\n+    public static Udaf<Integer, List<Integer>, Integer> createUdaf() {\n+        return new RollingSumUdafImpl();\n+    }\n+\n+    private static class RollingSumUdafImpl implements Udaf<Integer, List<Integer>, Integer> {\n+\n+        private final int CAPACITY = 3;\n+\n+        @Override\n+        public List<Integer> initialize() {\n+            return new LinkedList<Integer>();\n+        }\n+\n+        @Override\n+        public List<Integer> aggregate(Integer newValue, List<Integer> aggregateValue) {\n+            aggregateValue.add(newValue);\n+\n+            if (aggregateValue.size() > CAPACITY) {\n+                aggregateValue = aggregateValue.subList(1, CAPACITY + 1);\n+            }\n+\n+            return aggregateValue;\n+        }\n+\n+        @Override\n+        public Integer map(List<Integer> intermediate) {\n+            Iterator<Integer> it = intermediate.iterator();\n+            int k = 0;\n+\n+            while(it.hasNext()) {\n+                k += it.next();\n+            }\n+\n+            return k;\n+        }\n+\n+        @Override\n+        public List<Integer> merge(List<Integer> aggOne, List<Integer> aggTwo) {\n+            return aggOne;\n+        }\n+    }\n+}\n+```\n+\n+There are many things to observe in this class:\n+\n+- By contrast to scalar and tabular functions, aggregation functions are designated by a static method with the `@UdafFactory` annotation. Because aggregations must implement multiple methods, this helps ksqlDB differentiate aggregations when multiple type signatures are used.\n+\n+- The static factory method needs to either return `Udaf` or `TableUdaf` (in package `io.confluent.ksql.function.udaf`). The former, which is used in this example, can only be used to aggregate streams into tables, and cannot aggregate tables into other tables. To achieve the latter, use the `TableUdaf`, which derives from `Udaf`, and implement the `undo()` method, too.\n+\n+- ksqlDB decouples the internal representation of an aggregate from how it is used in an operation. This is very useful because aggregations can maintain complex state, but expose it in a simpler way in a query. In this example, the internal representation is a `LinkedList`, as indicated by the `initialize()` method. But when ksqlDB interacts with the aggregation value, `map()` is called, which sums the values in the list. The `List` is needed to keep a running history of values, but the summed value is needed for the query itself.\n+\n+- UDAFs must be parameterized with three generic types. In this example, they are `<Integer, List<Integer>, Integer>`. The first parameter represents the type of the column to aggregate over. The second column represents the internal representation of the aggregation, which is established in `initialize()`. The third parameter represents the type that the query interacts with, which is converted by `map()`.\n+\n+- All types, including inputs, intermediate representations, and final representations, must be [types that ksqlDB supports](../../concepts/functions/#supported-types).\n+\n+- The `merge` method controls how two [session windows](../../concepts/time-and-windows-in-ksqldb-queries/#session-window) fuse together when one extends and overlaps another. In this example, the content of the \"earlier\" aggregate is simply taken. If you are using session windows, you'll want to think through what good merge semantics are for your aggregation.\n+\n+## Add the uberjar to ksqlDB server\n+\n+In order for ksqlDB to be able to load your UDFs, they need to be compiled from classes into an uberjar. Run the following command to build an uberjar:\n+\n+```\n+gradle shadowJar\n+```\n+\n+You should now have a directory, `extensions`, with a file named `example-udfs-0.0.1.jar` in it.\n+\n+In order to use the uberjar, you need to make it available to ksqlDB server. Create the following `docker-compose.yml` file:\n+\n+```yaml\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:{{ site.cprelease }}\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:{{ site.cprelease }}\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n+      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:{{ site.cprelease }}\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+\n+  ksqldb-server:\n+    image: confluentinc/ksqldb-server:{{ site.ksqldbversion }}\n+    hostname: ksqldb-server\n+    container_name: ksqldb-server\n+    depends_on:\n+      - broker\n+      - schema-registry\n+    ports:\n+      - \"8088:8088\"\n+    volumes:\n+      - \"./extensions/:/opt/ksqldb-udfs\"\n+    environment:\n+      KSQL_LISTENERS: \"http://0.0.0.0:8088\"\n+      KSQL_BOOTSTRAP_SERVERS: \"broker:9092\"\n+      KSQL_KSQL_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n+      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: \"true\"\n+      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: \"true\"\n+      # Configuration for UDFs\n+      KSQL_KSQL_EXTENSION_DIR: \"/opt/ksqldb-udfs\"\n+      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5\n+\n+  ksqldb-cli:\n+    image: confluentinc/ksqldb-cli:{{ site.ksqldbversion }}\n+    container_name: ksqldb-cli\n+    depends_on:\n+      - broker\n+      - ksqldb-server\n+    entrypoint: /bin/sh\n+    tty: true\n+```\n+\n+Notice that:\n+\n+- A volume is mounted from the local `extensions` directory (containing your uberjar) to the container `/opt/ksqldb-udfs` directory. The latter can be any directory that you like. This command effectively puts the uberjar on ksqlDB server's file system.\n+\n+- The environment variable `KSQL_KSQL_EXTENSION_DIR` is configured to the same path that was set for the container in the volume mount. This is the path that ksqlDB will look for UDFs in.\n+\n+- The environment variable `KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE` is set to `5`. Recall that in the UDF example, the function loads an external parameter named` ksql.functions.formula.base.value`. All `KSQL_` environment variables are converted automatically to server configuration properties, which is where UDF parameters are looked up.\n+\n+!!! info\n+    Although this is a single node setup, remember that every node in your ksqlDB cluster needs to have event variable parameters configured since any node can handle any query at any time.\n+\n+## Invoke the functions\n+\n+Bring up your stack by running:\n+\n+```\n+docker-compose up\n+```\n+\n+And connect to ksqlDB's server by using its interactive CLI:\n+\n+```\n+docker exec -it ksqldb-cli ksql http://ksqldb-server:8088\n+```\n+\n+Verify that your functions have been loaded by running the following ksqlDB command:\n+\n+```sql\n+SHOW FUNCTIONS;\n+```\n+\n+You should see a long list of built-in functions, including your own `FORMULA`, `INDEX_SEQ`, and `ROLLING_SUM` (which are listed as `SCALAR`, `TABLE`, and `AGGREGATE` respectivly). If they aren't there, check that your uberjar was correctly mounted into the container. Be sure to check the log files of ksqlDB server, too, using `docker logs -f ksqldb-server`. You should see log lines similar to:\n+\n+```\n+[2020-06-24 23:38:10,942] INFO Adding UDAF name=rolling_sum from path=/opt/ksqldb-udfs/example-udfs-0.0.1.jar class=class my.example.RollingSumUdaf (io.confluent.ksql.function.UdafLoader:71)\n+```\n+\n+!!! info\n+    UDFs are only loaded once as ksqlDB server starts up. ksqlDB does not support hot-reloading UDFs. If you want to change the code of a UDF, you need to create a new uberjar, replace the one that is available to ksqlDB, and restart the server. Keep in mind that in a multi-node setup, different nodes may be running different versions of a UDF at the same time.\n+\n+Before you run any queries, be sure to have ksqlDB start all queries from the earliest point in each topic.\n+\n+```sql\n+SET 'auto.offset.reset' = 'earliest';\n+```\n+\n+### Invoke the scalar function\n+\n+Inspect the `formula` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION formula;\n+```\n+\n+It should output the following. ksqlDB shows all the type signatures that the UDF implements, which in two in this case.\n+\n+```\n+Name        : FORMULA\n+Author      : example user\n+Version     : 1.0.2\n+Overview    : A custom formula for important business logic.\n+Type        : SCALAR\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : FORMULA(a DOUBLE, b DOUBLE)\n+\tReturns     : BIGINT\n+\tDescription : A special variant of the formula, handling double parameters.\n+\n+\tVariation   : FORMULA(a INT, b INT)\n+\tReturns     : BIGINT\n+\tDescription : The standard version of the formula with integer parameters.\n+```\n+\n+Create a stream named `s1`:\n+\n+```sql\n+CREATE STREAM s1 (\n+    a VARCHAR KEY,\n+    b INT,\n+    c INT\n+) WITH (\n+    kafka_topic = 's1',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s1 (a, b, c) VALUES ('k1', 2, 3);\n+INSERT INTO s1 (a, b, c) VALUES ('k2', 4, 6);\n+INSERT INTO s1 (a, b, c) VALUES ('k3', 6, 9);\n+```\n+\n+Execute a push query. Recall what `formula` does. When given two integers, it multiples the together, then adds the value of the parameter `ksql.functions.formula.base.value`, which is set to `5` in your Docker Compose file:\n+\n+```sql\n+SELECT a, formula(b, c) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |11                                                            |\n+|k2                                                            |29                                                            |\n+|k3                                                            |59                                                            |\n+```\n+\n+Try the other variant which takes two doubles. This implementation takes the ceiling of `a` and `b` before multiplying. Notice how you can use constants instead of column names as arguments to the function:\n+\n+```sql\n+SELECT a, formula(CAST(b AS DOUBLE), 7.3) AS result FROM s1 EMIT CHANGES;\n+```\n+\n+You should see:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |RESULT                                                        |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |37                                                            |\n+|k3                                                            |53                                                            |\n+\n+```\n+\n+### Invoke the tabular function\n+\n+Inspect the `index_seq` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION index_seq;\n+```\n+\n+It should output the following. Notice how ksqlDB shows that this is a generic function with the type parameter `E`. This means that this UDTF can take a parameter that is an array of any type.\n+\n+```\n+Name        : INDEX_SEQ\n+Author      : example user\n+Version     : 1.5.0\n+Overview    : Disassembles a sequence and produces new elements concatenated with indices.\n+Type        : TABLE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : INDEX_SEQ(s ARRAY<E>)\n+\tReturns     : VARCHAR\n+\tDescription : Disassembles a sequence and produces new elements concatenated with indices.\n+```\n+\n+Create a stream named `s2`:\n+\n+```sql\n+CREATE STREAM s2 (\n+    a VARCHAR KEY,\n+    b ARRAY<VARCHAR>\n+) WITH (\n+    kafka_topic = 's2',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s2 (a, b) VALUES ('k1', ARRAY['a', 'b', 'c']);\n+INSERT INTO s2 (a, b) VALUES ('k2', ARRAY['d', 'e']);\n+INSERT INTO s2 (a, b) VALUES ('k3', ARRAY['f']);\n+```\n+\n+Execute a push query. Recall what `index_seq` does. It creates a row per element in an array concatenated with its index position.\n+\n+```sql\n+SELECT a, index_seq(b) AS str FROM s2 EMIT CHANGES;\n+```\n+\n+You should see the following:\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |STR                                                           |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |a-0                                                           |\n+|k1                                                            |b-1                                                           |\n+|k1                                                            |c-2                                                           |\n+|k2                                                            |d-0                                                           |\n+|k2                                                            |e-1                                                           |\n+|k3                                                            |f-0                                                           |\n+```\n+\n+### Invoke the aggregation function\n+\n+Inspect the `rolling_sum` function by running:\n+\n+```sql\n+DESCRIBE FUNCTION rolling_sum;\n+```\n+\n+It should output the following:\n+\n+```\n+Name        : ROLLING_SUM\n+Author      : example user\n+Version     : 2.0.0\n+Overview    : Maintains a rolling sum of the last 3 integers of a stream.\n+Type        : AGGREGATE\n+Jar         : /opt/ksqldb-udfs/example-udfs-0.0.1.jar\n+Variations  : \n+\n+\tVariation   : ROLLING_SUM(val INT)\n+\tReturns     : INT\n+\tDescription : Sums the previous 3 integers of a stream, discarding the oldest elements as new ones arrive.\n+```\n+\n+Create a stream named `s3`:\n+\n+```sql\n+CREATE STREAM s3 (\n+    a VARCHAR KEY,\n+    b INT\n+) WITH (\n+    kafka_topic = 's3',\n+    partitions = 1,\n+    value_format = 'avro'\n+);\n+```\n+\n+Insert some rows into the stream:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 3);\n+INSERT INTO s3 (a, b) VALUES ('k1', 5);\n+INSERT INTO s3 (a, b) VALUES ('k1', 7);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+INSERT INTO s3 (a, b) VALUES ('k2', 2);\n+```\n+\n+Execute a push query. Recall what `rolling_sum` does. It aggregates the previous three elements together, sums them up, and emits their output.\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+Your output should look like the following. `k1` sums `3`, `5`, and `7` together to get a result of `15`. `k2` sums `6` and `2` together to get a result of `8`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |15                                                            |\n+|k2                                                            |8                                                             |\n+```\n+\n+Insert some more rows and shift older elements out of the aggregate:\n+\n+```sql\n+INSERT INTO s3 (a, b) VALUES ('k1', 9);\n+INSERT INTO s3 (a, b) VALUES ('k2', 1);\n+INSERT INTO s3 (a, b) VALUES ('k2', 6);\n+```\n+\n+Run the query again:\n+\n+```sql\n+SELECT a, rolling_sum(b) AS MOVING_SUM FROM s3 GROUP BY a EMIT CHANGES;\n+```\n+\n+And you should now see these results. In `k1`, the previous three values are now `5`, `7`, and `9`. In `k2`, the elements are `2`, `1`, and `6`.\n+\n+```\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|A                                                             |MOVING_SUM                                                    |\n++--------------------------------------------------------------+--------------------------------------------------------------+\n+|k1                                                            |21                                                            |\n+|k2                                                            |9                                                             |\n+```\n+\n+## Working with structs", "originalCommit": "754d715394a59322dbe8a62d019f534b0a057121", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTYzNzY3Ng==", "url": "https://github.com/confluentinc/ksql/pull/5715#discussion_r451637676", "bodyText": "I punked out and just ended up mentioning it. I feel a little bad since it's an important topic. Can you think of a graceful way to introduce it without making the tutorial much more lengthy?", "author": "MichaelDrogalis", "createdAt": "2020-07-08T15:34:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4NjE0MQ=="}], "type": "inlineReview"}, {"oid": "9deab8f2fdd4dfb8807e644ae7229e718b6da6aa", "url": "https://github.com/confluentinc/ksql/commit/9deab8f2fdd4dfb8807e644ae7229e718b6da6aa", "message": "Apply suggestions from code review\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-06-30T20:13:12Z", "type": "commit"}, {"oid": "2f427a7a6f36a77cf3bbf37cfcb6b67aa548061f", "url": "https://github.com/confluentinc/ksql/commit/2f427a7a6f36a77cf3bbf37cfcb6b67aa548061f", "message": "Apply suggestions from code review\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-06-30T20:17:31Z", "type": "commit"}, {"oid": "c6f30f6558cbc3305720d6b89f440e7eb39cd781", "url": "https://github.com/confluentinc/ksql/commit/c6f30f6558cbc3305720d6b89f440e7eb39cd781", "message": "docs: remove config from in-action", "committedDate": "2020-06-30T22:41:09Z", "type": "commit"}, {"oid": "271409fedd715b7647b4e5d64dc018eea39fe8a0", "url": "https://github.com/confluentinc/ksql/commit/271409fedd715b7647b4e5d64dc018eea39fe8a0", "message": "docs: back off future speculation", "committedDate": "2020-06-30T22:43:40Z", "type": "commit"}, {"oid": "9dcea60b5e6daa6183c9370bf60f5174504f486e", "url": "https://github.com/confluentinc/ksql/commit/9dcea60b5e6daa6183c9370bf60f5174504f486e", "message": "docs: mention what version is for", "committedDate": "2020-06-30T22:47:18Z", "type": "commit"}, {"oid": "a9f41c0009e737f3155b52a1addf563f816e538c", "url": "https://github.com/confluentinc/ksql/commit/a9f41c0009e737f3155b52a1addf563f816e538c", "message": "Update docs/how-to-guides/create-a-user-defined-function.md\n\nCo-authored-by: Almog Gavra <almog@confluent.io>", "committedDate": "2020-06-30T22:49:07Z", "type": "commit"}, {"oid": "08097735af395148ca979a43d319612f98f36ed3", "url": "https://github.com/confluentinc/ksql/commit/08097735af395148ca979a43d319612f98f36ed3", "message": "docs: fix code", "committedDate": "2020-06-30T23:17:08Z", "type": "commit"}, {"oid": "21e971a5972e37e9a72521c57330077b7c0efac1", "url": "https://github.com/confluentinc/ksql/commit/21e971a5972e37e9a72521c57330077b7c0efac1", "message": "docs: update session merge", "committedDate": "2020-06-30T23:19:53Z", "type": "commit"}, {"oid": "3699a726bdbb6ad77fead250c5e9651016a69abb", "url": "https://github.com/confluentinc/ksql/commit/3699a726bdbb6ad77fead250c5e9651016a69abb", "message": "Update docs/how-to-guides/create-a-user-defined-function.md\n\nCo-authored-by: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-06-30T23:24:17Z", "type": "commit"}, {"oid": "6c93e641cef88bfaec5fb4510e6a06e414e211ff", "url": "https://github.com/confluentinc/ksql/commit/6c93e641cef88bfaec5fb4510e6a06e414e211ff", "message": "docs: fix markdown list", "committedDate": "2020-06-30T23:51:47Z", "type": "commit"}, {"oid": "b421749f4b4b14afaeaea341d919866e92c5f339", "url": "https://github.com/confluentinc/ksql/commit/b421749f4b4b14afaeaea341d919866e92c5f339", "message": "docs: note about casing", "committedDate": "2020-06-30T23:55:50Z", "type": "commit"}, {"oid": "684c17ab67357073a02336138ac2af99aad9f1ff", "url": "https://github.com/confluentinc/ksql/commit/684c17ab67357073a02336138ac2af99aad9f1ff", "message": "Merge branch 'master' into mdrogalis-how-to-fn", "committedDate": "2020-07-07T22:43:16Z", "type": "commit"}, {"oid": "2d5e4b3abcea84381d826c8521cf5c19ab272a30", "url": "https://github.com/confluentinc/ksql/commit/2d5e4b3abcea84381d826c8521cf5c19ab272a30", "message": "docs: fix dead link", "committedDate": "2020-07-07T22:56:33Z", "type": "commit"}, {"oid": "7f84701ef46248296a52dc46d5058f88cd9327b4", "url": "https://github.com/confluentinc/ksql/commit/7f84701ef46248296a52dc46d5058f88cd9327b4", "message": "docs: mention the archetype", "committedDate": "2020-07-07T23:59:48Z", "type": "commit"}, {"oid": "2f3ad60c40886ba8cd0f1e48bb91dd4e07a97433", "url": "https://github.com/confluentinc/ksql/commit/2f3ad60c40886ba8cd0f1e48bb91dd4e07a97433", "message": "docs: mention schema provider", "committedDate": "2020-07-08T15:32:04Z", "type": "commit"}, {"oid": "661bec2dab49b33b7294227e7ab2313119b1d7a1", "url": "https://github.com/confluentinc/ksql/commit/661bec2dab49b33b7294227e7ab2313119b1d7a1", "message": "docs: mention decimals", "committedDate": "2020-07-08T15:35:57Z", "type": "commit"}, {"oid": "c21261b1f0c2f575f8168dc7b29703c849bddded", "url": "https://github.com/confluentinc/ksql/commit/c21261b1f0c2f575f8168dc7b29703c849bddded", "message": "docs: use parameters flag", "committedDate": "2020-07-08T16:16:32Z", "type": "commit"}, {"oid": "2fa684e62a468d1015aaa2eb6672aafe7cfe33f7", "url": "https://github.com/confluentinc/ksql/commit/2fa684e62a468d1015aaa2eb6672aafe7cfe33f7", "message": "docs: wrap lines at 80", "committedDate": "2020-07-08T21:05:54Z", "type": "commit"}]}