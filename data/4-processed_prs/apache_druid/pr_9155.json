{"pr_number": 9155, "pr_title": "Tutorials use new ingestion spec where possible", "pr_createdAt": "2020-01-09T00:25:39Z", "pr_url": "https://github.com/apache/druid/pull/9155", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "url": "https://github.com/apache/druid/commit/6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "message": "Tutorials use new ingestion spec where possible\n\nThere are 2 main changes\n  * Use task type index_parallel instead of index\n  * Remove the use of parser + firehose in favor of inputFormat + inputSource\n\nindex_parallel is the preferred method starting in 0.17. Setting the job to\nindex_parallel with the default maxNumConcurrentSubTasks(1) is the equivalent\nof an index task\n\nInstead of using a parserSpec, dimensionSpec and timestampSpec have been\npromoted to the dataSchema. The format is described in the ioConfig as the\ninputFormat.\n\nThere are a few cases where the new format is not supported\n * Hadoop must use firehoses instead of the inputSource and inputFormat\n * There is no equivalent of a combining firehose as an inputSource\n * A Combining firehose does not support index_parallel", "committedDate": "2020-01-15T17:14:30Z", "type": "commit"}, {"oid": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "url": "https://github.com/apache/druid/commit/6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "message": "Tutorials use new ingestion spec where possible\n\nThere are 2 main changes\n  * Use task type index_parallel instead of index\n  * Remove the use of parser + firehose in favor of inputFormat + inputSource\n\nindex_parallel is the preferred method starting in 0.17. Setting the job to\nindex_parallel with the default maxNumConcurrentSubTasks(1) is the equivalent\nof an index task\n\nInstead of using a parserSpec, dimensionSpec and timestampSpec have been\npromoted to the dataSchema. The format is described in the ioConfig as the\ninputFormat.\n\nThere are a few cases where the new format is not supported\n * Hadoop must use firehoses instead of the inputSource and inputFormat\n * There is no equivalent of a combining firehose as an inputSource\n * A Combining firehose does not support index_parallel", "committedDate": "2020-01-15T17:14:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NjEyMQ==", "url": "https://github.com/apache/druid/pull/9155#discussion_r367076121", "bodyText": "typo: The the", "author": "jihoonson", "createdAt": "2020-01-15T19:54:55Z", "path": "docs/tutorials/tutorial-ingestion-spec.md", "diffHunk": "@@ -490,44 +421,57 @@ The `dataSchema` is shared across all task types, but each task type has its own\n \n ## Define the input source\n \n-Now let's define our input source, which is specified in an `ioConfig` object. Each task type has its own type of `ioConfig`. The native batch task uses \"firehoses\" to read input data, so let's configure a \"local\" firehose to read the example netflow data we saved earlier:\n+Now let's define our input source, which is specified in an `ioConfig` object. Each task type has its own type of `ioConfig`. To read input data, we need to specify an `inputSource`. The the example netflow data we saved earlier needs to be read from a local file, which is configured below:", "originalCommit": "6d17d7a3d4111ad1685f2f1ff168a3c3101bdfce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3ODU0NQ==", "url": "https://github.com/apache/druid/pull/9155#discussion_r367078545", "bodyText": "Done", "author": "suneet-s", "createdAt": "2020-01-15T20:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NjEyMQ=="}], "type": "inlineReview"}, {"oid": "0988dec2d6acec75ec6cea891ac67c06d978c909", "url": "https://github.com/apache/druid/commit/0988dec2d6acec75ec6cea891ac67c06d978c909", "message": "fix typo", "committedDate": "2020-01-15T20:00:00Z", "type": "commit"}]}