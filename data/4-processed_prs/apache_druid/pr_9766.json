{"pr_number": 9766, "pr_title": "Druid Quickstart refactor and update", "pr_createdAt": "2020-04-24T16:45:44Z", "pr_url": "https://github.com/apache/druid/pull/9766", "timeline": [{"oid": "14c3c73f9d43849ee341cf0ceead7c105fceaf74", "url": "https://github.com/apache/druid/commit/14c3c73f9d43849ee341cf0ceead7c105fceaf74", "message": "Update data-formats.md\n\nPer Suneet, \"Since you're editing this file can you also fix the json on line 177 please - it's missing a comma after the }\"", "committedDate": "2020-01-22T19:58:49Z", "type": "commit"}, {"oid": "41c73a40c0dab293d7b588482fc2a83ba497f96f", "url": "https://github.com/apache/druid/commit/41c73a40c0dab293d7b588482fc2a83ba497f96f", "message": "Merge branch 'master' of github.com:apache/druid", "committedDate": "2020-04-21T18:23:46Z", "type": "commit"}, {"oid": "47e33bffa8fc0af55025fa37df266638c62ab94b", "url": "https://github.com/apache/druid/commit/47e33bffa8fc0af55025fa37df266638c62ab94b", "message": "Light text cleanup\n\n* Removing discussion of sample data, since it's repeated in the data loading tutorial, and not immediately relevant here.", "committedDate": "2020-04-21T18:58:12Z", "type": "commit"}, {"oid": "8d29c26719e03331a678026bef8c1e46970e573a", "url": "https://github.com/apache/druid/commit/8d29c26719e03331a678026bef8c1e46970e573a", "message": "Update index.md", "committedDate": "2020-04-21T22:56:23Z", "type": "commit"}, {"oid": "bc2d4a9ccfb3ac9928dca0e70e79fcddfda6e0e0", "url": "https://github.com/apache/druid/commit/bc2d4a9ccfb3ac9928dca0e70e79fcddfda6e0e0", "message": "original quickstart full first pass", "committedDate": "2020-04-22T19:30:01Z", "type": "commit"}, {"oid": "5738174501b11d4f70fb1fc404a1b2a7a9345b28", "url": "https://github.com/apache/druid/commit/5738174501b11d4f70fb1fc404a1b2a7a9345b28", "message": "original quickstart full first pass", "committedDate": "2020-04-22T19:30:21Z", "type": "commit"}, {"oid": "427885f1fe118ce1b93566e798ecc660e554adda", "url": "https://github.com/apache/druid/commit/427885f1fe118ce1b93566e798ecc660e554adda", "message": "original quickstart full first pass", "committedDate": "2020-04-22T19:36:17Z", "type": "commit"}, {"oid": "373675056c3a8fb7c7a548094dba0a6998e7b82f", "url": "https://github.com/apache/druid/commit/373675056c3a8fb7c7a548094dba0a6998e7b82f", "message": "first pass all the way through", "committedDate": "2020-04-22T23:55:58Z", "type": "commit"}, {"oid": "ec621605313e6ff675cc7a1faa202e9311d62147", "url": "https://github.com/apache/druid/commit/ec621605313e6ff675cc7a1faa202e9311d62147", "message": "straggler", "committedDate": "2020-04-22T23:57:34Z", "type": "commit"}, {"oid": "019f1d222bf51be59dc9baa84952249c932dca80", "url": "https://github.com/apache/druid/commit/019f1d222bf51be59dc9baa84952249c932dca80", "message": "image touchups and finished old tutorial", "committedDate": "2020-04-23T23:33:41Z", "type": "commit"}, {"oid": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "url": "https://github.com/apache/druid/commit/e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "message": "a bit of finishing up", "committedDate": "2020-04-24T16:39:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2MTY3MQ==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414961671", "bodyText": "Maybe \"you'll need a larger machine and configuration profile\"?  This paragraph talks about configuration profiles, and the original description mentioned \"we recommend a larger configuration than micro-quickstart\".  The focus on the configuration profile seems to have lost here.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:01:18Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -23,71 +23,63 @@ title: \"Quickstart\"\n   -->\n \n \n-In this quickstart, we will download Druid and set it up on a single machine. The cluster will be ready to load data\n-after completing this initial setup.\n+This quickstart gets you started with Apache Druid and introduces you to some of its basic features. \n+Following these steps, you will install Druid and load sample \n+data using its native batch ingestion feature. \n \n-Before beginning the quickstart, it is helpful to read the [general Druid overview](../design/index.md) and the\n-[ingestion overview](../ingestion/index.md), as the tutorials will refer to concepts discussed on those pages.\n+Before starting, you may want to read the [general Druid overview](../design/index.md) and\n+[ingestion overview](../ingestion/index.md), as the tutorials refer to concepts discussed on those pages.\n \n-## Prerequisites\n+## Requirements\n \n-### Software\n+You can follow these steps on a relatively small machine, such as a laptop with around 4 CPU and 16 GB of RAM. \n \n-You will need:\n+Druid comes with several startup configuration profiles for a range of machine sizes. \n+The `micro-quickstart`configuration profile shown here is suitable for early evaluation scenarios. To explore \n+Druid's performance or scaling capabilities, you'll need a larger machine.", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjAzODgyNA==", "url": "https://github.com/apache/druid/pull/9766#discussion_r416038824", "bodyText": "Done.", "author": "sthetland", "createdAt": "2020-04-27T18:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2MTY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2MjU3NA==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414962574", "bodyText": "\"X-Large\", instead of \"X-large\" perhaps, to be consistent with \"Nano-Quickstart\".  In single-server.md, it uses \"nano-quickstart\", \"xlarge\" at the beginning, and \"Nano-Quickstart\", \"X-Large\" at the end.  If \"Nano-Quickstart\" is used here, I think \"X-Large\" should go with it.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:06:49Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -23,71 +23,63 @@ title: \"Quickstart\"\n   -->\n \n \n-In this quickstart, we will download Druid and set it up on a single machine. The cluster will be ready to load data\n-after completing this initial setup.\n+This quickstart gets you started with Apache Druid and introduces you to some of its basic features. \n+Following these steps, you will install Druid and load sample \n+data using its native batch ingestion feature. \n \n-Before beginning the quickstart, it is helpful to read the [general Druid overview](../design/index.md) and the\n-[ingestion overview](../ingestion/index.md), as the tutorials will refer to concepts discussed on those pages.\n+Before starting, you may want to read the [general Druid overview](../design/index.md) and\n+[ingestion overview](../ingestion/index.md), as the tutorials refer to concepts discussed on those pages.\n \n-## Prerequisites\n+## Requirements\n \n-### Software\n+You can follow these steps on a relatively small machine, such as a laptop with around 4 CPU and 16 GB of RAM. \n \n-You will need:\n+Druid comes with several startup configuration profiles for a range of machine sizes. \n+The `micro-quickstart`configuration profile shown here is suitable for early evaluation scenarios. To explore \n+Druid's performance or scaling capabilities, you'll need a larger machine.\n \n-* **Java 8 (8u92+) or later**\n-* Linux, Mac OS X, or other Unix-like OS (Windows is not supported)\n-\n-> **Warning:** Druid only officially supports Java 8. Any Java version later than 8 is still experimental.\n->\n-> If needed, you can specify where to find Java using the environment variables `DRUID_JAVA_HOME` or `JAVA_HOME`. For more details run the verify-java script.\n+The configuration profiles included with Druid range from the even smaller _Nano-Quickstart_ configuration (1 CPU, 4GB RAM) \n+to the _X-large_ configuration (64 CPU, 512GB RAM). For more information, see ", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2MzAwNw==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414963007", "bodyText": "In my personal opinion, I find this part useful when I was reading the original version.  It gives the reader a sense of what's included in the package.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:09:49Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -23,71 +23,63 @@ title: \"Quickstart\"\n   -->\n \n \n-In this quickstart, we will download Druid and set it up on a single machine. The cluster will be ready to load data\n-after completing this initial setup.\n+This quickstart gets you started with Apache Druid and introduces you to some of its basic features. \n+Following these steps, you will install Druid and load sample \n+data using its native batch ingestion feature. \n \n-Before beginning the quickstart, it is helpful to read the [general Druid overview](../design/index.md) and the\n-[ingestion overview](../ingestion/index.md), as the tutorials will refer to concepts discussed on those pages.\n+Before starting, you may want to read the [general Druid overview](../design/index.md) and\n+[ingestion overview](../ingestion/index.md), as the tutorials refer to concepts discussed on those pages.\n \n-## Prerequisites\n+## Requirements\n \n-### Software\n+You can follow these steps on a relatively small machine, such as a laptop with around 4 CPU and 16 GB of RAM. \n \n-You will need:\n+Druid comes with several startup configuration profiles for a range of machine sizes. \n+The `micro-quickstart`configuration profile shown here is suitable for early evaluation scenarios. To explore \n+Druid's performance or scaling capabilities, you'll need a larger machine.\n \n-* **Java 8 (8u92+) or later**\n-* Linux, Mac OS X, or other Unix-like OS (Windows is not supported)\n-\n-> **Warning:** Druid only officially supports Java 8. Any Java version later than 8 is still experimental.\n->\n-> If needed, you can specify where to find Java using the environment variables `DRUID_JAVA_HOME` or `JAVA_HOME`. For more details run the verify-java script.\n+The configuration profiles included with Druid range from the even smaller _Nano-Quickstart_ configuration (1 CPU, 4GB RAM) \n+to the _X-large_ configuration (64 CPU, 512GB RAM). For more information, see \n+[Single server deployment](operations/single-server). Alternatively, see [Clustered deployment](tutorials/cluster) for \n+information on deploying Druid services across clustered machines. \n \n-### Hardware\n+The software requirements for the installation machine are:\n \n-Druid includes several example [single-server configurations](../operations/single-server.md), along with scripts to\n-start the Druid processes using these configurations.\n+* Linux, Mac OS X, or other Unix-like OS (Windows is not supported)\n+* Java 8, Update 92 or later (8u92+)\n \n-If you're running on a small machine such as a laptop for a quick evaluation, the `micro-quickstart` configuration is\n-a good choice, sized for a 4CPU/16GB RAM environment.\n+> Druid officially supports Java 8 only. Support for later major versions of Java is currently in experimental status.\n \n-If you plan to use the single-machine deployment for further evaluation beyond the tutorials, we recommend a larger\n-configuration than `micro-quickstart`.\n+> Druid relies on the environment variables `JAVA_HOME` or `DRUID_JAVA_HOME` to find Java on the machine. You can set \n+`DRUID_JAVA_HOME` if there is more than one instance of Java. To verify Java requirements for your environment, run the \n+`bin/verify-java` script.\n \n-## Getting started\n \n-[Download](https://www.apache.org/dyn/closer.cgi?path=/druid/{{DRUIDVERSION}}/apache-druid-{{DRUIDVERSION}}-bin.tar.gz)\n-the {{DRUIDVERSION}} release.\n+## Step 1. Install Druid\n \n-Extract Druid by running the following commands in your terminal:\n+After confirming the [requirements](#requirements), follow these steps: \n \n-```bash\n-tar -xzf apache-druid-{{DRUIDVERSION}}-bin.tar.gz\n-cd apache-druid-{{DRUIDVERSION}}\n-```\n+1. Download\n+the [{{DRUIDVERSION}} release](https://www.apache.org/dyn/closer.cgi?path=/druid/{{DRUIDVERSION}}/apache-druid-{{DRUIDVERSION}}-bin.tar.gz).\n+2. In your terminal, extract Druid and change directories to the distribution directory:\n \n-In the package, you should find:\n+   ```bash\n+   tar -xzf apache-druid-{{DRUIDVERSION}}-bin.tar.gz\n+   cd apache-druid-{{DRUIDVERSION}}\n+   ```\n+In the directory, you'll find `LICENSE` and `NOTICE` files and subdirectories for executable files, configuration files, sample data and more.\n \n-* `LICENSE` and `NOTICE` files\n-* `bin/*` - scripts useful for this quickstart\n-* `conf/*` - example configurations for single-server and clustered setup\n-* `extensions/*` - core Druid extensions\n-* `hadoop-dependencies/*` - Druid Hadoop dependencies\n-* `lib/*` - libraries and dependencies for core Druid", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjA1OTI4Mw==", "url": "https://github.com/apache/druid/pull/9766#discussion_r416059283", "bodyText": "Thanks, I'll keep a note of this and see how this change goes over... I removed the list based on previous feedback (plus a directory and file were missing, so the content was out of date).\nI'm open to restoring though, especially if installation isn't covered elsewhere..I'd say it's a lot of detail for a quickstart, especially one that's gotten much longer, but the right level for an installation guide.", "author": "sthetland", "createdAt": "2020-04-27T18:42:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2MzAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2NDQyNw==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414964427", "bodyText": "I think it's worthwhile to mention the data file that we will be loading here.  The original version has this part \"This sample data is located at quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz from the Druid package root.\".  That makes it a bit more clear before diving into base directory and file filer.  I agree that the part talking about columns can go.  I felt it was a bit too much even when I first read it.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:18:40Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -99,96 +91,173 @@ $ ./bin/start-micro-quickstart\n [Fri May  3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart\n ```\n \n-All persistent state such as the cluster metadata store and segments for the services will be kept in the `var` directory under the apache-druid-{{DRUIDVERSION}} package root. Logs for the services are located at `var/sv`.\n+All persistent state, such as the cluster metadata store and segments for the services, are kept in the `var` directory under \n+the Druid root directory, apache-druid-{{DRUIDVERSION}}. Each service writes to a log file under `var/sv`, as noted in the startup script output above.\n+\n+At any time, you can revert Druid to its original, post-installation state by deleting the entire `var` directory. You may\n+want to do this, for example, between Druid tutorials or after experimentation, to start with a fresh instance. \n+\n+To stop Druid at any time, use CTRL-C in the terminal. This exits the `bin/start-micro-quickstart` script and \n+terminates all Druid processes. \n+\n \n-Later on, if you'd like to stop the services, CTRL-C to exit the `bin/start-micro-quickstart` script, which will terminate the Druid processes.\n+## Step 3. Open the Druid console \n \n-Once the cluster has started, you can navigate to [http://localhost:8888](http://localhost:8888).\n-The [Druid router process](../design/router.md), which serves the [Druid console](../operations/druid-console.md), resides at this address.\n+After the Druid services finish startup, open the [Druid console](../operations/druid-console.md) at [http://localhost:8888](http://localhost:8888). \n \n ![Druid console](../assets/tutorial-quickstart-01.png \"Druid console\")\n \n-It takes a few seconds for all the Druid processes to fully start up. If you open the console immediately after starting the services, you may see some errors that you can safely ignore.\n-\n-\n-## Loading data\n-\n-### Tutorial dataset\n-\n-For the following data loading tutorials, we have included a sample data file containing Wikipedia page edit events that occurred on 2015-09-12.\n-\n-This sample data is located at `quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz` from the Druid package root.\n-The page edit events are stored as JSON objects in a text file.\n-\n-The sample data has the following columns, and an example event is shown below:\n-\n-  * added\n-  * channel\n-  * cityName\n-  * comment\n-  * countryIsoCode\n-  * countryName\n-  * deleted\n-  * delta\n-  * isAnonymous\n-  * isMinor\n-  * isNew\n-  * isRobot\n-  * isUnpatrolled\n-  * metroCode\n-  * namespace\n-  * page\n-  * regionIsoCode\n-  * regionName\n-  * user\n-\n-```json\n-{\n-  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n-  \"channel\":\"#en.wikipedia\",\n-  \"namespace\":\"Main\",\n-  \"page\":\"Spider-Man's powers and equipment\",\n-  \"user\":\"foobar\",\n-  \"comment\":\"/* Artificial web-shooters */\",\n-  \"cityName\":\"New York\",\n-  \"regionName\":\"New York\",\n-  \"regionIsoCode\":\"NY\",\n-  \"countryName\":\"United States\",\n-  \"countryIsoCode\":\"US\",\n-  \"isAnonymous\":false,\n-  \"isNew\":false,\n-  \"isMinor\":false,\n-  \"isRobot\":false,\n-  \"isUnpatrolled\":false,\n-  \"added\":99,\n-  \"delta\":99,\n-  \"deleted\":0,\n-}\n-```\n+It may take a few seconds for all Druid services to finish starting, including the [Druid router](../design/router.md), which serves the console. If you attempt to open the Druid console before startup is complete, you may see errors in the browser. Wait a few moments and try again. \n \n \n-### Data loading tutorials\n+## Step 4. Load data\n \n-The following tutorials demonstrate various methods of loading data into Druid, including both batch and streaming use cases.\n-All tutorials assume that you are using the `micro-quickstart` single-machine configuration mentioned above.\n \n-- [Loading a file](./tutorial-batch.md) - this tutorial demonstrates how to perform a batch file load, using Druid's native batch ingestion.\n-- [Loading stream data from Apache Kafka](./tutorial-kafka.md) - this tutorial demonstrates how to load streaming data from a Kafka topic.\n-- [Loading a file using Apache Hadoop](./tutorial-batch-hadoop.md) - this tutorial demonstrates how to perform a batch file load, using a remote Hadoop cluster.\n-- [Writing your own ingestion spec](./tutorial-ingestion-spec.md) - this tutorial demonstrates how to write a new ingestion spec and use it to load data.\n+Ingestion specs define the schema of the data Druid reads and stores. You can write ingestion specs by hand or using the _data loader_, \n+as we will do here. \n \n-### Resetting cluster state\n+For this tutorial, we'll load sample data bundled with Druid that represents Wikipedia page edits on a given day. ", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2NTk4OA==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414965988", "bodyText": "We might want to mention that the tutorial here is to do the batch file load using Druid's native batch ingestion.  The original page separates the data loading into 4 different links \"Loading a file\", \"Loading stream data from Apache Kafka\", \"Loading a file using Apache Hadoop\" and \"Writing your own ingestion spec\" with explanations after each link.  That makes it clear about what the user is reading when a link is clicked.  But once we move part of content from \"Loading a file\" here, it becomes less clear that we will be doing batch file load with native batch ingestion here.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:28:30Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -99,96 +91,173 @@ $ ./bin/start-micro-quickstart\n [Fri May  3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart\n ```\n \n-All persistent state such as the cluster metadata store and segments for the services will be kept in the `var` directory under the apache-druid-{{DRUIDVERSION}} package root. Logs for the services are located at `var/sv`.\n+All persistent state, such as the cluster metadata store and segments for the services, are kept in the `var` directory under \n+the Druid root directory, apache-druid-{{DRUIDVERSION}}. Each service writes to a log file under `var/sv`, as noted in the startup script output above.\n+\n+At any time, you can revert Druid to its original, post-installation state by deleting the entire `var` directory. You may\n+want to do this, for example, between Druid tutorials or after experimentation, to start with a fresh instance. \n+\n+To stop Druid at any time, use CTRL-C in the terminal. This exits the `bin/start-micro-quickstart` script and \n+terminates all Druid processes. \n+\n \n-Later on, if you'd like to stop the services, CTRL-C to exit the `bin/start-micro-quickstart` script, which will terminate the Druid processes.\n+## Step 3. Open the Druid console \n \n-Once the cluster has started, you can navigate to [http://localhost:8888](http://localhost:8888).\n-The [Druid router process](../design/router.md), which serves the [Druid console](../operations/druid-console.md), resides at this address.\n+After the Druid services finish startup, open the [Druid console](../operations/druid-console.md) at [http://localhost:8888](http://localhost:8888). \n \n ![Druid console](../assets/tutorial-quickstart-01.png \"Druid console\")\n \n-It takes a few seconds for all the Druid processes to fully start up. If you open the console immediately after starting the services, you may see some errors that you can safely ignore.\n-\n-\n-## Loading data\n-\n-### Tutorial dataset\n-\n-For the following data loading tutorials, we have included a sample data file containing Wikipedia page edit events that occurred on 2015-09-12.\n-\n-This sample data is located at `quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz` from the Druid package root.\n-The page edit events are stored as JSON objects in a text file.\n-\n-The sample data has the following columns, and an example event is shown below:\n-\n-  * added\n-  * channel\n-  * cityName\n-  * comment\n-  * countryIsoCode\n-  * countryName\n-  * deleted\n-  * delta\n-  * isAnonymous\n-  * isMinor\n-  * isNew\n-  * isRobot\n-  * isUnpatrolled\n-  * metroCode\n-  * namespace\n-  * page\n-  * regionIsoCode\n-  * regionName\n-  * user\n-\n-```json\n-{\n-  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n-  \"channel\":\"#en.wikipedia\",\n-  \"namespace\":\"Main\",\n-  \"page\":\"Spider-Man's powers and equipment\",\n-  \"user\":\"foobar\",\n-  \"comment\":\"/* Artificial web-shooters */\",\n-  \"cityName\":\"New York\",\n-  \"regionName\":\"New York\",\n-  \"regionIsoCode\":\"NY\",\n-  \"countryName\":\"United States\",\n-  \"countryIsoCode\":\"US\",\n-  \"isAnonymous\":false,\n-  \"isNew\":false,\n-  \"isMinor\":false,\n-  \"isRobot\":false,\n-  \"isUnpatrolled\":false,\n-  \"added\":99,\n-  \"delta\":99,\n-  \"deleted\":0,\n-}\n-```\n+It may take a few seconds for all Druid services to finish starting, including the [Druid router](../design/router.md), which serves the console. If you attempt to open the Druid console before startup is complete, you may see errors in the browser. Wait a few moments and try again. \n \n \n-### Data loading tutorials\n+## Step 4. Load data\n \n-The following tutorials demonstrate various methods of loading data into Druid, including both batch and streaming use cases.\n-All tutorials assume that you are using the `micro-quickstart` single-machine configuration mentioned above.\n \n-- [Loading a file](./tutorial-batch.md) - this tutorial demonstrates how to perform a batch file load, using Druid's native batch ingestion.\n-- [Loading stream data from Apache Kafka](./tutorial-kafka.md) - this tutorial demonstrates how to load streaming data from a Kafka topic.\n-- [Loading a file using Apache Hadoop](./tutorial-batch-hadoop.md) - this tutorial demonstrates how to perform a batch file load, using a remote Hadoop cluster.\n-- [Writing your own ingestion spec](./tutorial-ingestion-spec.md) - this tutorial demonstrates how to write a new ingestion spec and use it to load data.\n+Ingestion specs define the schema of the data Druid reads and stores. You can write ingestion specs by hand or using the _data loader_, ", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2NjI0Ng==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414966246", "bodyText": "\"Segment Granularity\" should be \"Segment granularity\"", "author": "weishiuntsai", "createdAt": "2020-04-25T03:30:12Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -99,96 +91,173 @@ $ ./bin/start-micro-quickstart\n [Fri May  3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart\n ```\n \n-All persistent state such as the cluster metadata store and segments for the services will be kept in the `var` directory under the apache-druid-{{DRUIDVERSION}} package root. Logs for the services are located at `var/sv`.\n+All persistent state, such as the cluster metadata store and segments for the services, are kept in the `var` directory under \n+the Druid root directory, apache-druid-{{DRUIDVERSION}}. Each service writes to a log file under `var/sv`, as noted in the startup script output above.\n+\n+At any time, you can revert Druid to its original, post-installation state by deleting the entire `var` directory. You may\n+want to do this, for example, between Druid tutorials or after experimentation, to start with a fresh instance. \n+\n+To stop Druid at any time, use CTRL-C in the terminal. This exits the `bin/start-micro-quickstart` script and \n+terminates all Druid processes. \n+\n \n-Later on, if you'd like to stop the services, CTRL-C to exit the `bin/start-micro-quickstart` script, which will terminate the Druid processes.\n+## Step 3. Open the Druid console \n \n-Once the cluster has started, you can navigate to [http://localhost:8888](http://localhost:8888).\n-The [Druid router process](../design/router.md), which serves the [Druid console](../operations/druid-console.md), resides at this address.\n+After the Druid services finish startup, open the [Druid console](../operations/druid-console.md) at [http://localhost:8888](http://localhost:8888). \n \n ![Druid console](../assets/tutorial-quickstart-01.png \"Druid console\")\n \n-It takes a few seconds for all the Druid processes to fully start up. If you open the console immediately after starting the services, you may see some errors that you can safely ignore.\n-\n-\n-## Loading data\n-\n-### Tutorial dataset\n-\n-For the following data loading tutorials, we have included a sample data file containing Wikipedia page edit events that occurred on 2015-09-12.\n-\n-This sample data is located at `quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz` from the Druid package root.\n-The page edit events are stored as JSON objects in a text file.\n-\n-The sample data has the following columns, and an example event is shown below:\n-\n-  * added\n-  * channel\n-  * cityName\n-  * comment\n-  * countryIsoCode\n-  * countryName\n-  * deleted\n-  * delta\n-  * isAnonymous\n-  * isMinor\n-  * isNew\n-  * isRobot\n-  * isUnpatrolled\n-  * metroCode\n-  * namespace\n-  * page\n-  * regionIsoCode\n-  * regionName\n-  * user\n-\n-```json\n-{\n-  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n-  \"channel\":\"#en.wikipedia\",\n-  \"namespace\":\"Main\",\n-  \"page\":\"Spider-Man's powers and equipment\",\n-  \"user\":\"foobar\",\n-  \"comment\":\"/* Artificial web-shooters */\",\n-  \"cityName\":\"New York\",\n-  \"regionName\":\"New York\",\n-  \"regionIsoCode\":\"NY\",\n-  \"countryName\":\"United States\",\n-  \"countryIsoCode\":\"US\",\n-  \"isAnonymous\":false,\n-  \"isNew\":false,\n-  \"isMinor\":false,\n-  \"isRobot\":false,\n-  \"isUnpatrolled\":false,\n-  \"added\":99,\n-  \"delta\":99,\n-  \"deleted\":0,\n-}\n-```\n+It may take a few seconds for all Druid services to finish starting, including the [Druid router](../design/router.md), which serves the console. If you attempt to open the Druid console before startup is complete, you may see errors in the browser. Wait a few moments and try again. \n \n \n-### Data loading tutorials\n+## Step 4. Load data\n \n-The following tutorials demonstrate various methods of loading data into Druid, including both batch and streaming use cases.\n-All tutorials assume that you are using the `micro-quickstart` single-machine configuration mentioned above.\n \n-- [Loading a file](./tutorial-batch.md) - this tutorial demonstrates how to perform a batch file load, using Druid's native batch ingestion.\n-- [Loading stream data from Apache Kafka](./tutorial-kafka.md) - this tutorial demonstrates how to load streaming data from a Kafka topic.\n-- [Loading a file using Apache Hadoop](./tutorial-batch-hadoop.md) - this tutorial demonstrates how to perform a batch file load, using a remote Hadoop cluster.\n-- [Writing your own ingestion spec](./tutorial-ingestion-spec.md) - this tutorial demonstrates how to write a new ingestion spec and use it to load data.\n+Ingestion specs define the schema of the data Druid reads and stores. You can write ingestion specs by hand or using the _data loader_, \n+as we will do here. \n \n-### Resetting cluster state\n+For this tutorial, we'll load sample data bundled with Druid that represents Wikipedia page edits on a given day. \n \n-If you want a clean start after stopping the services, delete the `var` directory and run the `bin/start-micro-quickstart` script again.\n+1. Click **Load data** from the Druid console header (![Load data](../assets/tutorial-batch-data-loader-00.png)).\n \n-Once every service has started, you are now ready to load data.\n+2. Select the **Local disk** tile and then click **Connect data**.\n \n-#### Resetting Kafka\n+   ![Data loader init](../assets/tutorial-batch-data-loader-01.png \"Data loader init\")\n+\n+3. Enter the following values: \n+\n+   - **Base directory**: `quickstart/tutorial/`\n+\n+   - **File filter**: `wikiticker-2015-09-12-sampled.json.gz` \n+\n+   ![Data location](../assets/tutorial-batch-data-loader-015.png \"Data location\")\n+\n+   Entering the base directory and [wildcard file filter](https://commons.apache.org/proper/commons-io/apidocs/org/apache/commons/io/filefilter/WildcardFileFilter.html) separately, as afforded by the UI, allows you to specify multiple files for ingestion at once.\n+\n+4. Click **Apply**. \n+\n+   The data loader displays the raw data, giving you a chance to verify that the data \n+   appears as expected. \n+\n+   ![Data loader sample](../assets/tutorial-batch-data-loader-02.png \"Data loader sample\")\n+\n+   Notice that your position in the sequence of steps to load data, **Connect** in our case, appears at the top of the console, as shown below. \n+   You can click other steps to move forward or backward in the sequence at any time.\n+   \n+   ![Load data](../assets/tutorial-batch-data-loader-12.png)  \n+   \n+\n+5. Click **Next: Parse data**. \n+\n+   The data loader tries to determine the parser appropriate for the data format automatically. In this case \n+   it identifies the data format as `json`, as shown in the **Input format** field at the bottom right.\n+\n+   ![Data loader parse data](../assets/tutorial-batch-data-loader-03.png \"Data loader parse data\")\n+\n+   Feel free to select other **Input format** options to get a sense of their configuration settings \n+   and how Druid parses other types of data.  \n+\n+6. With the JSON parser selected, click **Next: Parse time**. The **Parse time** settings are where you view and adjust the \n+   primary timestamp column for the data.\n+\n+   ![Data loader parse time](../assets/tutorial-batch-data-loader-04.png \"Data loader parse time\")\n+\n+   Druid requires data to have a primary timestamp column (internally stored in a column called `__time`).\n+   If you do not have a timestamp in your data, select `Constant value`. In our example, the data loader \n+   determines that the `time` column is the only candidate that can be used as the primary time column.\n+\n+7. Click **Next: Transform**, **Next: Filter**, and then **Next: Configure schema**, skipping a few steps.\n+\n+   You do not need to adjust transformation or filtering settings, as applying ingestion time transforms and \n+   filters are out of scope for this tutorial.\n+\n+8. The Configure schema settings are where you configure what [dimensions](../ingestion/index.md#dimensions) \n+   and [metrics](../ingestion/index.md#metrics) are ingested. The outcome of this configuration represents exactly how the \n+   data will appear in Druid after ingestion. \n+\n+   Since our dataset is very small, you can turn off [rollup](../ingestion/index.md#rollup) \n+   by unsetting the **Rollup** switch and confirming the change when prompted.\n+\n+   ![Data loader schema](../assets/tutorial-batch-data-loader-05.png \"Data loader schema\")\n+\n+\n+10. Click **Next: Partition** to configure how the data will be split into segments. In this case, choose `DAY` as \n+    the **Segment Granularity**. ", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2NzY2OA==", "url": "https://github.com/apache/druid/pull/9766#discussion_r414967668", "bodyText": "Perhaps incorporate the original sentence \"Let's name this datasource wikipedia\" here with \"and change the default from wikiticker-2015-09-12-sampled\" ?  The original sentence makes it more clear that the user has a choice here.", "author": "weishiuntsai", "createdAt": "2020-04-25T03:38:58Z", "path": "docs/tutorials/index.md", "diffHunk": "@@ -99,96 +91,173 @@ $ ./bin/start-micro-quickstart\n [Fri May  3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart\n ```\n \n-All persistent state such as the cluster metadata store and segments for the services will be kept in the `var` directory under the apache-druid-{{DRUIDVERSION}} package root. Logs for the services are located at `var/sv`.\n+All persistent state, such as the cluster metadata store and segments for the services, are kept in the `var` directory under \n+the Druid root directory, apache-druid-{{DRUIDVERSION}}. Each service writes to a log file under `var/sv`, as noted in the startup script output above.\n+\n+At any time, you can revert Druid to its original, post-installation state by deleting the entire `var` directory. You may\n+want to do this, for example, between Druid tutorials or after experimentation, to start with a fresh instance. \n+\n+To stop Druid at any time, use CTRL-C in the terminal. This exits the `bin/start-micro-quickstart` script and \n+terminates all Druid processes. \n+\n \n-Later on, if you'd like to stop the services, CTRL-C to exit the `bin/start-micro-quickstart` script, which will terminate the Druid processes.\n+## Step 3. Open the Druid console \n \n-Once the cluster has started, you can navigate to [http://localhost:8888](http://localhost:8888).\n-The [Druid router process](../design/router.md), which serves the [Druid console](../operations/druid-console.md), resides at this address.\n+After the Druid services finish startup, open the [Druid console](../operations/druid-console.md) at [http://localhost:8888](http://localhost:8888). \n \n ![Druid console](../assets/tutorial-quickstart-01.png \"Druid console\")\n \n-It takes a few seconds for all the Druid processes to fully start up. If you open the console immediately after starting the services, you may see some errors that you can safely ignore.\n-\n-\n-## Loading data\n-\n-### Tutorial dataset\n-\n-For the following data loading tutorials, we have included a sample data file containing Wikipedia page edit events that occurred on 2015-09-12.\n-\n-This sample data is located at `quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz` from the Druid package root.\n-The page edit events are stored as JSON objects in a text file.\n-\n-The sample data has the following columns, and an example event is shown below:\n-\n-  * added\n-  * channel\n-  * cityName\n-  * comment\n-  * countryIsoCode\n-  * countryName\n-  * deleted\n-  * delta\n-  * isAnonymous\n-  * isMinor\n-  * isNew\n-  * isRobot\n-  * isUnpatrolled\n-  * metroCode\n-  * namespace\n-  * page\n-  * regionIsoCode\n-  * regionName\n-  * user\n-\n-```json\n-{\n-  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n-  \"channel\":\"#en.wikipedia\",\n-  \"namespace\":\"Main\",\n-  \"page\":\"Spider-Man's powers and equipment\",\n-  \"user\":\"foobar\",\n-  \"comment\":\"/* Artificial web-shooters */\",\n-  \"cityName\":\"New York\",\n-  \"regionName\":\"New York\",\n-  \"regionIsoCode\":\"NY\",\n-  \"countryName\":\"United States\",\n-  \"countryIsoCode\":\"US\",\n-  \"isAnonymous\":false,\n-  \"isNew\":false,\n-  \"isMinor\":false,\n-  \"isRobot\":false,\n-  \"isUnpatrolled\":false,\n-  \"added\":99,\n-  \"delta\":99,\n-  \"deleted\":0,\n-}\n-```\n+It may take a few seconds for all Druid services to finish starting, including the [Druid router](../design/router.md), which serves the console. If you attempt to open the Druid console before startup is complete, you may see errors in the browser. Wait a few moments and try again. \n \n \n-### Data loading tutorials\n+## Step 4. Load data\n \n-The following tutorials demonstrate various methods of loading data into Druid, including both batch and streaming use cases.\n-All tutorials assume that you are using the `micro-quickstart` single-machine configuration mentioned above.\n \n-- [Loading a file](./tutorial-batch.md) - this tutorial demonstrates how to perform a batch file load, using Druid's native batch ingestion.\n-- [Loading stream data from Apache Kafka](./tutorial-kafka.md) - this tutorial demonstrates how to load streaming data from a Kafka topic.\n-- [Loading a file using Apache Hadoop](./tutorial-batch-hadoop.md) - this tutorial demonstrates how to perform a batch file load, using a remote Hadoop cluster.\n-- [Writing your own ingestion spec](./tutorial-ingestion-spec.md) - this tutorial demonstrates how to write a new ingestion spec and use it to load data.\n+Ingestion specs define the schema of the data Druid reads and stores. You can write ingestion specs by hand or using the _data loader_, \n+as we will do here. \n \n-### Resetting cluster state\n+For this tutorial, we'll load sample data bundled with Druid that represents Wikipedia page edits on a given day. \n \n-If you want a clean start after stopping the services, delete the `var` directory and run the `bin/start-micro-quickstart` script again.\n+1. Click **Load data** from the Druid console header (![Load data](../assets/tutorial-batch-data-loader-00.png)).\n \n-Once every service has started, you are now ready to load data.\n+2. Select the **Local disk** tile and then click **Connect data**.\n \n-#### Resetting Kafka\n+   ![Data loader init](../assets/tutorial-batch-data-loader-01.png \"Data loader init\")\n+\n+3. Enter the following values: \n+\n+   - **Base directory**: `quickstart/tutorial/`\n+\n+   - **File filter**: `wikiticker-2015-09-12-sampled.json.gz` \n+\n+   ![Data location](../assets/tutorial-batch-data-loader-015.png \"Data location\")\n+\n+   Entering the base directory and [wildcard file filter](https://commons.apache.org/proper/commons-io/apidocs/org/apache/commons/io/filefilter/WildcardFileFilter.html) separately, as afforded by the UI, allows you to specify multiple files for ingestion at once.\n+\n+4. Click **Apply**. \n+\n+   The data loader displays the raw data, giving you a chance to verify that the data \n+   appears as expected. \n+\n+   ![Data loader sample](../assets/tutorial-batch-data-loader-02.png \"Data loader sample\")\n+\n+   Notice that your position in the sequence of steps to load data, **Connect** in our case, appears at the top of the console, as shown below. \n+   You can click other steps to move forward or backward in the sequence at any time.\n+   \n+   ![Load data](../assets/tutorial-batch-data-loader-12.png)  \n+   \n+\n+5. Click **Next: Parse data**. \n+\n+   The data loader tries to determine the parser appropriate for the data format automatically. In this case \n+   it identifies the data format as `json`, as shown in the **Input format** field at the bottom right.\n+\n+   ![Data loader parse data](../assets/tutorial-batch-data-loader-03.png \"Data loader parse data\")\n+\n+   Feel free to select other **Input format** options to get a sense of their configuration settings \n+   and how Druid parses other types of data.  \n+\n+6. With the JSON parser selected, click **Next: Parse time**. The **Parse time** settings are where you view and adjust the \n+   primary timestamp column for the data.\n+\n+   ![Data loader parse time](../assets/tutorial-batch-data-loader-04.png \"Data loader parse time\")\n+\n+   Druid requires data to have a primary timestamp column (internally stored in a column called `__time`).\n+   If you do not have a timestamp in your data, select `Constant value`. In our example, the data loader \n+   determines that the `time` column is the only candidate that can be used as the primary time column.\n+\n+7. Click **Next: Transform**, **Next: Filter**, and then **Next: Configure schema**, skipping a few steps.\n+\n+   You do not need to adjust transformation or filtering settings, as applying ingestion time transforms and \n+   filters are out of scope for this tutorial.\n+\n+8. The Configure schema settings are where you configure what [dimensions](../ingestion/index.md#dimensions) \n+   and [metrics](../ingestion/index.md#metrics) are ingested. The outcome of this configuration represents exactly how the \n+   data will appear in Druid after ingestion. \n+\n+   Since our dataset is very small, you can turn off [rollup](../ingestion/index.md#rollup) \n+   by unsetting the **Rollup** switch and confirming the change when prompted.\n+\n+   ![Data loader schema](../assets/tutorial-batch-data-loader-05.png \"Data loader schema\")\n+\n+\n+10. Click **Next: Partition** to configure how the data will be split into segments. In this case, choose `DAY` as \n+    the **Segment Granularity**. \n+\n+    ![Data loader partition](../assets/tutorial-batch-data-loader-06.png \"Data loader partition\")\n+\n+    Since this is a small dataset, we can have just a single segment, which is what selecting `DAY` as the \n+    segment granularity gives us. \n+\n+11. Click **Next: Tune** and **Next: Publish**.\n+\n+12. The Publish settings are where you can specify the datasource name in Druid. Change the default from `wikiticker-2015-09-12-sampled` ", "originalCommit": "e6e7bc68b4ea8392f94d2bcec74951ad8668c645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjA1Mzk5NQ==", "url": "https://github.com/apache/druid/pull/9766#discussion_r416053995", "bodyText": "Done. Alternatively, I wondered if we should call it something unique, like wikipedia-batchfile, or something, to allow the tutorial datasources to live together without name collisions. Perhaps for later...", "author": "sthetland", "createdAt": "2020-04-27T18:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk2NzY2OA=="}], "type": "inlineReview"}, {"oid": "43efd6ad9b82bef5c199d0bbc94be11aacd52189", "url": "https://github.com/apache/druid/commit/43efd6ad9b82bef5c199d0bbc94be11aacd52189", "message": "Review comments", "committedDate": "2020-04-27T18:43:32Z", "type": "commit"}, {"oid": "e16f22b05c28f3fa21a98f9defc40b6f8a158832", "url": "https://github.com/apache/druid/commit/e16f22b05c28f3fa21a98f9defc40b6f8a158832", "message": "Merge branch 'master' of github.com:apache/druid into tutorial-screen-and-light-text-updates", "committedDate": "2020-04-27T22:00:03Z", "type": "commit"}, {"oid": "c10cd6b2f1f66e167f44dec47c24e12e4a78868e", "url": "https://github.com/apache/druid/commit/c10cd6b2f1f66e167f44dec47c24e12e4a78868e", "message": "fixing links", "committedDate": "2020-04-27T23:32:04Z", "type": "commit"}, {"oid": "397cf215bb70c2a13f47997cc9c3c3a554e7d0a9", "url": "https://github.com/apache/druid/commit/397cf215bb70c2a13f47997cc9c3c3a554e7d0a9", "message": "spell checking gymnastics", "committedDate": "2020-04-28T00:56:50Z", "type": "commit"}, {"oid": "71049889c511f23c8b2b68975237fd85c5701348", "url": "https://github.com/apache/druid/commit/71049889c511f23c8b2b68975237fd85c5701348", "message": "resolving merge conflicts", "committedDate": "2020-04-30T15:49:07Z", "type": "commit"}]}