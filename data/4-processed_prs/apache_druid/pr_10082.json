{"pr_number": 10082, "pr_title": "Fix RetryQueryRunner to actually do the job", "pr_createdAt": "2020-06-26T05:35:36Z", "pr_url": "https://github.com/apache/druid/pull/10082", "timeline": [{"oid": "93164665f131f6ca3151b7b25e49ebcd2f931e37", "url": "https://github.com/apache/druid/commit/93164665f131f6ca3151b7b25e49ebcd2f931e37", "message": "Fix RetryQueryRunner to actually do the job", "committedDate": "2020-06-26T05:17:28Z", "type": "commit"}, {"oid": "324ec1c6404f2a5407ab1ba611f086b711936401", "url": "https://github.com/apache/druid/commit/324ec1c6404f2a5407ab1ba611f086b711936401", "message": "more javadoc", "committedDate": "2020-06-26T05:31:01Z", "type": "commit"}, {"oid": "c7c3fb65092a225409809014caca0beda25c3787", "url": "https://github.com/apache/druid/commit/c7c3fb65092a225409809014caca0beda25c3787", "message": "fix test and checkstyle", "committedDate": "2020-06-26T06:33:44Z", "type": "commit"}, {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "url": "https://github.com/apache/druid/commit/a4326807fecbf4e64ed28318a7c4e26a1a152c71", "message": "don't combine for testing", "committedDate": "2020-06-26T16:48:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMxNDc0MQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446314741", "bodyText": "This should be added to DirectDruidClient.removeMagicResponseContextFields or something similar, so it doesn't end up in the response to the user.", "author": "gianm", "createdAt": "2020-06-26T17:25:10Z", "path": "processing/src/main/java/org/apache/druid/query/context/ResponseContext.java", "diffHunk": "@@ -112,6 +112,16 @@\n         \"uncoveredIntervalsOverflowed\",\n             (oldValue, newValue) -> (boolean) oldValue || (boolean) newValue\n     ),\n+    /**\n+     * Expected remaining number of responses from query nodes.\n+     * The value is initialized in {@code CachingClusteredClient} when it initializes the connection to the query nodes,\n+     * and is updated whenever they respond (@code DirectDruidClient). {@code RetryQueryRunner} uses this value to\n+     * check if the {@link #MISSING_SEGMENTS} is valid.\n+     */\n+    REMAINING_RESPONSES_FROM_QUERY_NODES(", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MDg0NQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446450845", "bodyText": "\ud83d\udc4d added.", "author": "jihoonson", "createdAt": "2020-06-26T23:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMxNDc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyMzI2Ng==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446323266", "bodyText": "I'm not sure this will work properly if there are multiple CCC queries associated with a given Druid query. This can happen with union queries and with subqueries. I believe they share response contexts, and this put would potentially cause things to get clobbered. Could you please look into this?\nBtw, the reason I believe they share contexts is that I only currently see concurrent contexts getting created in QueryLifecycle (which can wrap multiple CCC queries).\nIf it is a real issue, we might be able to address it by splitting out the number of servers remaining by subquery ID. However, in that case, we need to make sure subquery ID is set for union datasources. I don't think it is currently.", "author": "gianm", "createdAt": "2020-06-26T17:43:05Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -618,6 +619,7 @@ private void addSequencesFromServer(\n         final SortedMap<DruidServer, List<SegmentDescriptor>> segmentsByServer\n     )\n     {\n+      responseContext.put(Key.REMAINING_RESPONSES_FROM_QUERY_NODES, segmentsByServer.size());", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MDg2Mw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446450863", "bodyText": "I think the response contexts are shared between subqueries, but it won't be a problem because subqueries are not executed in parallel for now. Same for UnionDataSource. However, I agree that it should be per subquery for the future. I made needed changes to set subqueryId for union and store number of servers remaining per subquery or query.", "author": "jihoonson", "createdAt": "2020-06-26T23:29:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyMzI2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ4MDM1OQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446480359", "bodyText": "Oh, I actually found the response context is not shared between subqueries when inlining them. It is shared in UnionQueryRunner though. I think it makes sense to not share the context when inlining subqueries.", "author": "jihoonson", "createdAt": "2020-06-27T03:51:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyMzI2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyNTAxOA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446325018", "bodyText": "Is this change related?", "author": "gianm", "createdAt": "2020-06-26T17:46:42Z", "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -766,7 +766,6 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n                    .withDruidCluster(cluster)\n                    .withLoadManagementPeons(loadManagementPeons)\n                    .withSegmentReplicantLookup(segmentReplicantLookup)\n-                   .withBalancerReferenceTimestamp(DateTimes.nowUtc())", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTAzOA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451038", "bodyText": "No, it's not related. I happened to find it not in use. I know it's better to not fix unrelated stuffs, but seems pretty trivial.", "author": "jihoonson", "createdAt": "2020-06-26T23:30:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyNTAxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE2NTgxMQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447165811", "bodyText": "True, and I think it's fine if you want to leave this fix here, but next time it would be cleaner to do two separate patches.", "author": "gianm", "createdAt": "2020-06-29T18:23:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyNTAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNDM4NQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446334385", "bodyText": "Is there a guarantee that there will be 1 retry? Why won't the the correct, new server for the segment be selected the first time?", "author": "gianm", "createdAt": "2020-06-26T18:06:43Z", "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTIzNw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451237", "bodyText": "Before the last commit, baseRunner.run() was executed when RetryQueryRunner.run() is called. Since baseRunner is the query runner created by CachingClusteredClient, RetryQueryRunner.run() created the query distribution tree immediately. As a result, I could make the tests more deterministic by emulating moving segments between sequence = RetryQueryRunner.run() and sequence.toList().\nIn the last commit, I changed the code to defer calling baseRunner.run() until it has to get executed. This will minimize the chance of segments to move between the time to create query distribution tree and the time to actually send the query to servers. To keep the tests deterministic, I added a new constructor parameter, runnableAfterFirstAttempt, which is executed after creating query distribution true for the first attempt. This new parameter must be no-op in production code, but will be used only for testing. I think this parameter will be also useful for integration tests; I can add a new coordinator API to trigger segment balancing and call it after the first attempt using runnableAfterFirstAttempt.", "author": "jihoonson", "createdAt": "2020-06-26T23:31:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNDM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE2NzkyNA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447167924", "bodyText": "Oh, I see. It's because you called run before dropSegmentFromServerAndAddNewServerForSegment.\nIs deferring baseRunner.run() going to have any unintended consequences? For example, is it going to meaningfully delay initiation of connections to historicals? I'm not sure, but in general this area of the system is sensitive to when exactly things get called, so it would be good to look into this when changing it.", "author": "gianm", "createdAt": "2020-06-29T18:26:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNDM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjg5Nw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447336897", "bodyText": "Is deferring baseRunner.run() going to have any unintended consequences? For example, is it going to meaningfully delay initiation of connections to historicals? I'm not sure, but in general this area of the system is sensitive to when exactly things get called, so it would be good to look into this when changing it.\n\nGood question. It does defer initiating connections to query servers if the broker hasn't initiated them yet. AFAIT, there don't seem obvious unintended side effects. All these query runners running on top of RetryQueryRunner are not aware of that the query will be run remotely.", "author": "jihoonson", "createdAt": "2020-06-30T00:30:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNDM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTQ2Mg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446335462", "bodyText": "Should be ++retryCount, right? (You would want the first retry to say \"Retry attempt [1]\".)\nHowever, prefix and postfix incrementing in log messages can make things harder to read, so IMO it'd be better to split this up into two separate lines.", "author": "gianm", "createdAt": "2020-06-26T18:09:08Z", "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  private int numTotalRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @VisibleForTesting\n+  int getNumTotalRetries()\n+  {\n+    return numTotalRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    final Sequence<T> baseSequence = baseRunner.run(queryPlus, context);\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseSequence);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                numTotalRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        (Integer) context.get(Key.REMAINING_RESPONSES_FROM_QUERY_NODES),\n+        \"%s in responseContext\",\n+        Key.REMAINING_RESPONSES_FROM_QUERY_NODES.getName()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);\n+    }\n+\n+    final Object maybeMissingSegments = context.get(ResponseContext.Key.MISSING_SEGMENTS);\n+    if (maybeMissingSegments == null) {\n+      return Collections.emptyList();\n+    }\n+\n+    return jsonMapper.convertValue(\n+        maybeMissingSegments,\n+        new TypeReference<List<SegmentDescriptor>>()\n+        {\n+        }\n+    );\n+  }\n+\n+  /**\n+   * A lazy iterator populating {@link Sequence} by retrying the query. The first returned sequence is always the base\n+   * sequence given in the constructor. Subsequent sequences are created dynamically whenever it retries the query. All\n+   * the sequences populated by this iterator will be merged (not combined) with the base sequence.\n+   *\n+   * The design of this iterator depends on how {@link MergeSequence} works; the MergeSequence pops an item from\n+   * each underlying sequence and pushes them to a {@link java.util.PriorityQueue}. Whenever it pops from the queue,\n+   * it pushes a new item from the sequence where the returned item was originally from. Since the first returned\n+   * sequence from this iterator is always the base sequence, the MergeSequence will call {@link Sequence#toYielder}\n+   * on the base sequence first which in turn initializing query distribution tree. Once this tree is built, the query\n+   * nodes (historicals and realtime tasks) will lock all segments to read and report missing segments to the broker.\n+   * If there are missing segments reported, this iterator will rewrite the query with those reported segments and\n+   * reissue the rewritten query.\n+   *\n+   * @see org.apache.druid.client.CachingClusteredClient\n+   * @see org.apache.druid.client.DirectDruidClient\n+   */\n+  private class RetryingSequenceIterator implements Iterator<Sequence<T>>\n+  {\n+    private final QueryPlus<T> queryPlus;\n+    private final ResponseContext context;\n+    private Sequence<T> sequence;\n+    private int retryCount = 0;\n+\n+    private RetryingSequenceIterator(QueryPlus<T> queryPlus, ResponseContext context, Sequence<T> baseSequence)\n+    {\n+      this.queryPlus = queryPlus;\n+      this.context = context;\n+      this.sequence = baseSequence;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      if (sequence != null) {\n+        return true;\n+      } else {\n+        final List<SegmentDescriptor> missingSegments = getMissingSegments(context);\n+        if (missingSegments.isEmpty()) {\n+          return false;\n+        } else if (retryCount >= config.getNumTries()) {\n+          if (!config.isReturnPartialResults()) {\n+            throw new SegmentMissingException(\"No results found for segments[%s]\", missingSegments);\n+          } else {\n+            return false;\n+          }\n+        } else {\n+          LOG.info(\"[%,d] missing segments found. Retry attempt [%,d]\", missingSegments.size(), retryCount++);", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTI1MQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451251", "bodyText": "Oops, just copied it from the original code. Split incrementing count from the logging.", "author": "jihoonson", "createdAt": "2020-06-26T23:31:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTQ2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTgwMg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446335802", "bodyText": "Should be getTotalNumRetries(). \"Num total retries\" makes it sound like the number of times we did a total retry.", "author": "gianm", "createdAt": "2020-06-26T18:09:56Z", "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  private int numTotalRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @VisibleForTesting\n+  int getNumTotalRetries()", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTI3NA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451274", "bodyText": "Renamed.", "author": "jihoonson", "createdAt": "2020-06-26T23:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNzI1Ng==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446337256", "bodyText": "Could you add some javadocs to these methods explaining what they do?\nIt's non-obvious that dropSegmentFromServer doesn't modify the server view, and that dropSegmentFromServerAndAddNewServerForSegment does modify the server view, but only for the new server, not the old one.\nAnywhere there is a lack of symmetry and obviousness like this, doc comments are especially important.", "author": "gianm", "createdAt": "2020-06-26T18:12:46Z", "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);\n+    Assert.assertEquals(expectedTimeseriesResult(queryResult.size()), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryUntilWeGetFullResult()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(100, false), // retry up to 100\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertTrue(0 < queryRunner.getNumTotalRetries());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testFailWithPartialResultsAfterRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    dropSegmentFromServer(servers.get(0));\n+\n+    expectedException.expect(SegmentMissingException.class);\n+    expectedException.expectMessage(\"No results found for segments\");\n+    try {\n+      sequence.toList();\n+    }\n+    finally {\n+      Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    }\n+  }\n+\n+  private void prepareCluster(int numServers)\n+  {\n+    for (int i = 0; i < numServers; i++) {\n+      final DataSegment segment = newSegment(SCHEMA_INFO.getDataInterval(), i);\n+      addServer(\n+          SimpleServerView.createServer(i + 1),\n+          segment,\n+          segmentGenerator.generate(segment, SCHEMA_INFO, Granularities.NONE, 10)\n+      );\n+    }\n+  }\n+\n+  private Pair<SegmentId, QueryableIndex> dropSegmentFromServer(DruidServer fromServer)", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTI4Ng==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451286", "bodyText": "Added for both methods.", "author": "jihoonson", "createdAt": "2020-06-26T23:31:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNzI1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDY1NA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446340654", "bodyText": "Is there a guarantee that there will be more than zero retries? Why won't the the correct, new server for the segment be selected the first time?", "author": "gianm", "createdAt": "2020-06-26T18:20:07Z", "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);\n+    Assert.assertEquals(expectedTimeseriesResult(queryResult.size()), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryUntilWeGetFullResult()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(100, false), // retry up to 100\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertTrue(0 < queryRunner.getNumTotalRetries());", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTM0OQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451349", "bodyText": "See #10082 (comment).", "author": "jihoonson", "createdAt": "2020-06-26T23:31:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTAzNg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446341036", "bodyText": "Could we use exact sizes here? (If we can't be sure what we'll get, maybe check that there's one of two sizes, and run the test repeatedly for a minimum number of times and verify that we actually get both.)", "author": "gianm", "createdAt": "2020-06-26T18:21:04Z", "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTM1Nw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451357", "bodyText": "Changed to test exact sizes.", "author": "jihoonson", "createdAt": "2020-06-26T23:32:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTAzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTI1OQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446341259", "bodyText": "A general comment on testing: we should have a real integration test for this. It is the kind of thing that is tough to get right in unit tests, as evidence by the fact that we already had an existing RetryQueryRunnerTest, but it isn't testing the right thing.\nI reviewed the unit test and it seems reasonable enough (with some comments), but for this particular functionality we need an integration test that uses the real servers and a real network.", "author": "gianm", "createdAt": "2020-06-26T18:21:37Z", "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest", "originalCommit": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ1MTc2NA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446451764", "bodyText": "I agree the integration tests will be more useful but still want to keep the unit tests since it's way easier to debug. I am working on integration tests as commented in the PR description which will be added in a follow-up PR. BTW, I think the idea described in #10082 (comment) seems useful for integration tests. That way, we can emulate the what can really happen in real clusters.", "author": "jihoonson", "createdAt": "2020-06-26T23:34:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTI1OQ=="}], "type": "inlineReview"}, {"oid": "b7b6e8e3da104e493e693c4f8e7f5cda0d015c9d", "url": "https://github.com/apache/druid/commit/b7b6e8e3da104e493e693c4f8e7f5cda0d015c9d", "message": "Merge branch 'master' of github.com:apache/druid into fix-retry-query", "committedDate": "2020-06-26T21:39:02Z", "type": "commit"}, {"oid": "f1f3db30dcac12d035b41d3e3ad89af30cdd413d", "url": "https://github.com/apache/druid/commit/f1f3db30dcac12d035b41d3e3ad89af30cdd413d", "message": "address comments", "committedDate": "2020-06-26T23:28:57Z", "type": "commit"}, {"oid": "d403b6b9ce0af1087db2c7f6e50fa42ed526b6a1", "url": "https://github.com/apache/druid/commit/d403b6b9ce0af1087db2c7f6e50fa42ed526b6a1", "message": "fix unit tests", "committedDate": "2020-06-27T03:35:48Z", "type": "commit"}, {"oid": "5182e4744200184a715732843220fa4b04e84dbb", "url": "https://github.com/apache/druid/commit/5182e4744200184a715732843220fa4b04e84dbb", "message": "always initialize response context in cachingClusteredClient", "committedDate": "2020-06-28T01:44:03Z", "type": "commit"}, {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5", "url": "https://github.com/apache/druid/commit/5a6cf8d198bb91b92d8610514eb606feb5e890c5", "message": "fix subquery", "committedDate": "2020-06-28T03:05:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446626252", "bodyText": "Hmm, I feel like this could be done without polluting the production code with test stuff.\nHow about:\n\nmaking RetryingSequenceIterator visible for testing so you can extend and override hasNext to do this runnable thing\nmove building the retry sequence in the run method into a new method that you can override, so to have it make the test iterator with the runnable instead", "author": "clintropolis", "createdAt": "2020-06-28T09:29:20Z", "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final ConcurrentHashMap<String, Integer> idToRemainingResponses =\n+        (ConcurrentHashMap<String, Integer>) Preconditions.checkNotNull(\n+            context.get(Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS),\n+            \"%s in responseContext\",\n+            Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName()\n+        );\n+\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        idToRemainingResponses.get(queryPlus.getQuery().getMostRelevantId()),\n+        \"Number of remaining responses for query[%s]\",\n+        queryPlus.getQuery().getMostRelevantId()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);\n+    }\n+\n+    final Object maybeMissingSegments = context.get(ResponseContext.Key.MISSING_SEGMENTS);\n+    if (maybeMissingSegments == null) {\n+      return Collections.emptyList();\n+    }\n+\n+    return jsonMapper.convertValue(\n+        maybeMissingSegments,\n+        new TypeReference<List<SegmentDescriptor>>()\n+        {\n+        }\n+    );\n+  }\n+\n+  /**\n+   * A lazy iterator populating {@link Sequence} by retrying the query. The first returned sequence is always the base\n+   * sequence from the baseQueryRunner. Subsequent sequences are created dynamically whenever it retries the query. All\n+   * the sequences populated by this iterator will be merged (not combined) with the base sequence.\n+   *\n+   * The design of this iterator depends on how {@link MergeSequence} works; the MergeSequence pops an item from\n+   * each underlying sequence and pushes them to a {@link java.util.PriorityQueue}. Whenever it pops from the queue,\n+   * it pushes a new item from the sequence where the returned item was originally from. Since the first returned\n+   * sequence from this iterator is always the base sequence, the MergeSequence will call {@link Sequence#toYielder}\n+   * on the base sequence first which in turn initializing query distribution tree. Once this tree is built, the query\n+   * servers (historicals and realtime tasks) will lock all segments to read and report missing segments to the broker.\n+   * If there are missing segments reported, this iterator will rewrite the query with those reported segments and\n+   * reissue the rewritten query.\n+   *\n+   * @see org.apache.druid.client.CachingClusteredClient\n+   * @see org.apache.druid.client.DirectDruidClient\n+   */\n+  private class RetryingSequenceIterator implements Iterator<Sequence<T>>\n+  {\n+    private final QueryPlus<T> queryPlus;\n+    private final ResponseContext context;\n+    private final QueryRunner<T> baseQueryRunner;\n+    private final Runnable runnableAfterFirstAttempt;\n+\n+    private boolean first = true;\n+    private Sequence<T> sequence = null;\n+    private int retryCount = 0;\n+\n+    private RetryingSequenceIterator(\n+        QueryPlus<T> queryPlus,\n+        ResponseContext context,\n+        QueryRunner<T> baseQueryRunner,\n+        Runnable runnableAfterFirstAttempt\n+    )\n+    {\n+      this.queryPlus = queryPlus;\n+      this.context = context;\n+      this.baseQueryRunner = baseQueryRunner;\n+      this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      if (first) {\n+        sequence = baseQueryRunner.run(queryPlus, context);\n+        // runnableAfterFirstAttempt is only for testing, it must be no-op for production code.\n+        runnableAfterFirstAttempt.run();", "originalCommit": "5a6cf8d198bb91b92d8610514eb606feb5e890c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NDM5OA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447174398", "bodyText": "I don't necessarily agree that extending a production class in test code is cleaner than adding a test-only option to production code. They're equivalent in terms of test authenticity, and I'm biased towards thinking that extending things is bad in general (because it splits interconnected logic over two different files). So the test-only option sounds good to me.", "author": "gianm", "createdAt": "2020-06-29T18:37:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjk1Mw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447336953", "bodyText": "I agree with @gianm. I'm more worried about that the tests may be able to test different code than production if we forget to sync when we modify this part later in the future.", "author": "jihoonson", "createdAt": "2020-06-30T00:30:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUyMDYwNw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447520607", "bodyText": "I suppose we can agree to disagree; I don't feel super strongly about this particular case since it's not a hot iterator, but I still think the test code in here complicates things overall and makes it harder to casually know what's going on, and wouldn't have done this in this way.", "author": "clintropolis", "createdAt": "2020-06-30T08:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MzAxOQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447863019", "bodyText": "I think the JVM will probably ignore the runnable in production.\n\nI still think the test code in here complicates things overall and makes it harder to casually know what's going on, and wouldn't have done this in this way.\n\nI thought this only makes the code ugly, didn't think it makes things complicated. If that's the case, I think we can talk about how to make it better. Do you think we can do it as a follow-up?", "author": "jihoonson", "createdAt": "2020-06-30T17:35:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5MTI3NQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447991275", "bodyText": "Ah, I don't think it makes it overly complex or anything, just more complex than if it wasn't there, so more ugly like you said. I don't think this necessarily even needs to change, I'm just being a hater \ud83d\ude1b", "author": "clintropolis", "createdAt": "2020-06-30T21:31:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjk4MQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446626981", "bodyText": "nit: I think getMostSpecificId() might be a better name for this method\ntangent: it might be nice if the debug logs in DirectDruidClient logged both queryId and subQueryId if set", "author": "clintropolis", "createdAt": "2020-06-28T09:36:02Z", "path": "processing/src/main/java/org/apache/druid/query/Query.java", "diffHunk": "@@ -154,6 +161,17 @@ default String getSqlQueryId()\n     return null;\n   }\n \n+  /**\n+   * Returns a most relevant ID of this query; if it is a subquery, this will return its subquery ID.\n+   * If it is a regular query without subqueries, this will return its query ID.\n+   * This method should be called after the relevant ID is assigned using {@link #withId} or {@link #withSubQueryId}.\n+   */\n+  default String getMostRelevantId()", "originalCommit": "5a6cf8d198bb91b92d8610514eb606feb5e890c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjk2NQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447336965", "bodyText": "Renamed as suggested and added a debug log.", "author": "jihoonson", "createdAt": "2020-06-30T00:30:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY0MTUwNg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r446641506", "bodyText": "I'm surprised that the builder doesn't have an option to set queryId.", "author": "clintropolis", "createdAt": "2020-06-28T11:58:10Z", "path": "server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java", "diffHunk": "@@ -497,10 +499,11 @@ public void testTimeseriesCaching()\n     );\n \n \n-    TimeseriesQuery query = builder.intervals(\"2011-01-01/2011-01-10\")\n-                                   .aggregators(RENAMED_AGGS)\n-                                   .postAggregators(RENAMED_POST_AGGS)\n-                                   .build();\n+    TimeseriesQuery query = (TimeseriesQuery) builder.intervals(\"2011-01-01/2011-01-10\")\n+                                                     .aggregators(RENAMED_AGGS)\n+                                                     .postAggregators(RENAMED_POST_AGGS)\n+                                                     .build()\n+                                                     .withId(\"queryId\");", "originalCommit": "5a6cf8d198bb91b92d8610514eb606feb5e890c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bcf4a71141184dc7c2333d4e64a44ca2c8f18026", "url": "https://github.com/apache/druid/commit/bcf4a71141184dc7c2333d4e64a44ca2c8f18026", "message": "address comments", "committedDate": "2020-06-30T00:11:28Z", "type": "commit"}, {"oid": "6a96ea8a4f0ec5cacee884e04d236977fc76c52d", "url": "https://github.com/apache/druid/commit/6a96ea8a4f0ec5cacee884e04d236977fc76c52d", "message": "fix test", "committedDate": "2020-06-30T02:26:31Z", "type": "commit"}, {"oid": "e600e5b81dbb1e4d853d39bf8738363421aa5b99", "url": "https://github.com/apache/druid/commit/e600e5b81dbb1e4d853d39bf8738363421aa5b99", "message": "query id for builders", "committedDate": "2020-06-30T06:38:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUyNTcxNA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447525714", "bodyText": "nit: I think a dedicated type would be more appropriate than using a generic Pair, since it makes it a lot easier to follow what is going on than having to keep a mental model of what lhs and rhs are.\nSomething like:\nclass ClusterQueryResults<T>\n{\n    Sequence<T> results;\n    int numServers;\n}\n\nThis would be much easier to understand, and has the added benefit of when we realize we need a 3rd thing we can just add it.", "author": "clintropolis", "createdAt": "2020-06-30T08:58:15Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -283,13 +301,23 @@ public CachingClusteredClient(\n       return contextBuilder.build();\n     }\n \n-    Sequence<T> run(final UnaryOperator<TimelineLookup<String, ServerSelector>> timelineConverter)\n+    /**\n+     * Builds a query distribution and merge plan.\n+     *\n+     * This method returns an empty sequence if the query datasource is unknown or there is matching result-level cache.\n+     * Otherwise, it creates a sequence merging sequences from the regular broker cache and remote servers. If parallel\n+     * merge is enabled, it can merge and *combine* the underlying sequences in parallel.\n+     *\n+     * @return a pair of a sequence merging results from remote query servers and the number of remote servers\n+     *         participating in query processing.\n+     */\n+    NonnullPair<Sequence<T>, Integer> run(final UnaryOperator<TimelineLookup<String, ServerSelector>> timelineConverter)", "originalCommit": "e600e5b81dbb1e4d853d39bf8738363421aa5b99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg2MDAwMw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r447860003", "bodyText": "\ud83d\udc4d I guess I was excited about NonnullPair and overused it.", "author": "jihoonson", "createdAt": "2020-06-30T17:31:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUyNTcxNA=="}], "type": "inlineReview"}, {"oid": "22edff0be8bced4b6baffc152eaa4c82a5719fe7", "url": "https://github.com/apache/druid/commit/22edff0be8bced4b6baffc152eaa4c82a5719fe7", "message": "make queryId optional in the builders and ClusterQueryResult", "committedDate": "2020-06-30T17:30:09Z", "type": "commit"}, {"oid": "91b5748a1cfd659d4a08e8ed72cb6b070c134947", "url": "https://github.com/apache/druid/commit/91b5748a1cfd659d4a08e8ed72cb6b070c134947", "message": "fix test", "committedDate": "2020-06-30T20:00:15Z", "type": "commit"}, {"oid": "45db79ff2944d34b13de9eb4ae52f1a16c5efe05", "url": "https://github.com/apache/druid/commit/45db79ff2944d34b13de9eb4ae52f1a16c5efe05", "message": "suppress tests and unused methods", "committedDate": "2020-06-30T22:34:57Z", "type": "commit"}, {"oid": "4d27ec05790b47dfde2a05e7adbffe9561007823", "url": "https://github.com/apache/druid/commit/4d27ec05790b47dfde2a05e7adbffe9561007823", "message": "exclude groupBy builder", "committedDate": "2020-06-30T22:51:08Z", "type": "commit"}, {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "url": "https://github.com/apache/druid/commit/f1af75fd72a34b509ec8c695a116d5d1cff34c47", "message": "fix jacoco exclusion", "committedDate": "2020-06-30T23:51:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3NjI2NA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448076264", "bodyText": "we are planning to parallelize running subqueries\n\nI'm not sure if we are, but, we also haven't decided definitely not to.\nI think it would also be okay to not handle parallel queries here, but instead build in a sanity check that verifies the subqueries are issued in series. Maybe by verifying that when the id changes, the previous number of responses remaining must be down to zero. It's up to you. I think your current code is okay too.", "author": "gianm", "createdAt": "2020-07-01T02:09:32Z", "path": "processing/src/main/java/org/apache/druid/query/context/ResponseContext.java", "diffHunk": "@@ -112,6 +114,30 @@\n         \"uncoveredIntervalsOverflowed\",\n             (oldValue, newValue) -> (boolean) oldValue || (boolean) newValue\n     ),\n+    /**\n+     * Map of most relevant query ID to remaining number of responses from query nodes.\n+     * The value is initialized in {@code CachingClusteredClient} when it initializes the connection to the query nodes,\n+     * and is updated whenever they respond (@code DirectDruidClient). {@code RetryQueryRunner} uses this value to\n+     * check if the {@link #MISSING_SEGMENTS} is valid.\n+     *\n+     * Currently, the broker doesn't run subqueries in parallel, the remaining number of responses will be updated\n+     * one by one per subquery. However, since we are planning to parallelize running subqueries, we store them", "originalCommit": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEzNTM4OA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448135388", "bodyText": "I'm not sure if we are, but, we also haven't decided definitely not to.\n\nHeh, rephrased to sound more chill.\n\nI think it would also be okay to not handle parallel queries here, but instead build in a sanity check that verifies the subqueries are issued in series. Maybe by verifying that when the id changes, the previous number of responses remaining must be down to zero. It's up to you. I think your current code is okay too.\n\nHmm, how could it be used? Is it to make sure that the subqueries will be issued in series until we parallelize running them?", "author": "jihoonson", "createdAt": "2020-07-01T06:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3NjI2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3ODYxOA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448078618", "bodyText": "Why only log this if the id and subquery id are both nonnull?\nMeaning: why not log it in all cases?\nBtw, since this is going to happen for every server + query pair, it will be very chatty, so I'd suggest trace-level instead of debug.", "author": "gianm", "createdAt": "2020-07-01T02:19:07Z", "path": "server/src/main/java/org/apache/druid/client/DirectDruidClient.java", "diffHunk": "@@ -231,7 +237,19 @@ private InputStream dequeue() throws InterruptedException\n \n           final boolean continueReading;\n           try {\n+            if (query.getId() != null && query.getSubQueryId() != null) {", "originalCommit": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEzNTQ0Mg==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448135442", "bodyText": "Changed to always print trace-level log.", "author": "jihoonson", "createdAt": "2020-07-01T06:09:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3ODYxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4MjY5Nw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448082697", "bodyText": "If this happens, is it a bug? Or might this happen for some legitimate reason?\nIf it is a bug: please include a comment that this message means there was a bug. (So people that get the message and search for it in the code will see that it is a sign of a bug.)\nIf there could be a legitimate reason: in this case we should improve the error message to help the user understand what the legitimate reason might be.\n(a nit: spelling: should be \"missing responses\" rather than \"missing responds\".)", "author": "gianm", "createdAt": "2020-07-01T02:35:38Z", "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final ConcurrentHashMap<String, Integer> idToRemainingResponses =\n+        (ConcurrentHashMap<String, Integer>) Preconditions.checkNotNull(\n+            context.get(Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS),\n+            \"%s in responseContext\",\n+            Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName()\n+        );\n+\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        idToRemainingResponses.get(queryPlus.getQuery().getMostSpecificId()),\n+        \"Number of remaining responses for query[%s]\",\n+        queryPlus.getQuery().getMostSpecificId()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);", "originalCommit": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEzNTQ5MA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448135490", "bodyText": "Oops thanks. For the comment, I wrote some comments here. Do you think it's not clear?", "author": "jihoonson", "createdAt": "2020-07-01T06:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4MjY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4NzA1MQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448087051", "bodyText": "I'm not sure where to put this comment, but I noticed that the MISSING_SEGMENTS array could get truncated by the historical that generated it; see ResponseContext.serializeWith. It looks like this was discussed in #2331. We can't allow this if we are going to rely on the missing segments list for query correctness. I think that means we need to introduce an option that tells the QueryResource that it should throw an error rather than truncate, and we should always set that option when communicating from the Broker to data servers.", "author": "gianm", "createdAt": "2020-07-01T02:54:02Z", "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)", "originalCommit": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEwOTYzOQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448109639", "bodyText": "Ah, this is a really good point, I agree it should be an option to be an exception if the list is truncated.\nI'm not sure what better we can really do without a fairly large refactoring involving making the broker keep track of which segments went to which direct druid client/sequence, and allowing historicals to basically give up if missing so many that the list would be truncated, returning an empty sequence and an indicator that all of the segments it was queried for should be retried.", "author": "clintropolis", "createdAt": "2020-07-01T04:34:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4NzA1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEzNTc5NA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448135794", "bodyText": "Hmm, I was aware of this issue, but ignored since the max size of the serialized responseContext is 7k. The missingSegments is a list of SegmentDescriptor which has an interval, a version, and a partitionNumber. Since its serialized size is about 80 bytes, each historical can report up to about 90 missing segments. I thought it wouldn't happen often, but well, I guess it depends on how your cluster is configured and how many segments the query reads.\n\nI'm not sure what better we can really do without a fairly large refactoring involving making the broker keep track of which segments went to which direct druid client/sequence, and allowing historicals to basically give up if missing so many that the list would be truncated, returning an empty sequence and an indicator that all of the segments it was queried for should be retried.\n\nHmm, I was thinking that we don't have to keep track of all segments, but can send a flag in the query context to historicals which makes the query failed immediately if the serialized responseContext is truncated. The broker will catch the failure and cancel the entire query. This won't retry on the truncated serialization, but I think it makes sense since it's a sort of failure of the retry system.", "author": "jihoonson", "createdAt": "2020-07-01T06:10:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4NzA1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODIyOTk3Ng==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448229976", "bodyText": "Added a flag and made the historicals fail when response context is truncated. Manually tested this new behavior.", "author": "jihoonson", "createdAt": "2020-07-01T09:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4NzA1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEwMjEyMQ==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448102121", "bodyText": "Oops, not including a context here was a mistake, thanks for fixing it.\nNow that I think about it, though, making a new context here will mean we aren't going to properly return context from subqueries up to the original caller. This includes not reporting missing segments in the case where RetryQueryRunnerConfig.isReturnPartialResults = true.\nIt would be better to share the context that was created in QueryLifecycle. Is it feasible to do this? Maybe by moving some of this logic to be lazy and happen inside the returned QueryRunner? (It will get a copy of the context.)\nBtw, this sounds like it might be tough to do, so we could also address it with documentation about known limitations. But I think we either need to fix it, or document it.", "author": "gianm", "createdAt": "2020-07-01T04:01:55Z", "path": "server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java", "diffHunk": "@@ -329,15 +329,18 @@ private DataSource inlineIfNecessary(\n         }\n       } else if (canRunQueryUsingLocalWalker(subQuery) || canRunQueryUsingClusterWalker(subQuery)) {\n         // Subquery needs to be inlined. Assign it a subquery id and run it.\n-        final Query subQueryWithId = subQuery.withSubQueryId(UUID.randomUUID().toString());\n+        final Query subQueryWithId = subQuery.withDefaultSubQueryId();\n \n         final Sequence<?> queryResults;\n \n         if (dryRun) {\n           queryResults = Sequences.empty();\n         } else {\n           final QueryRunner subqueryRunner = subQueryWithId.getRunner(this);\n-          queryResults = subqueryRunner.run(QueryPlus.wrap(subQueryWithId));\n+          queryResults = subqueryRunner.run(\n+              QueryPlus.wrap(subQueryWithId),\n+              DirectDruidClient.makeResponseContextForQuery()", "originalCommit": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEzNjI0MA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448136240", "bodyText": "Now that I think about it, though, making a new context here will mean we aren't going to properly return context from subqueries up to the original caller. This includes not reporting missing segments in the case where RetryQueryRunnerConfig.isReturnPartialResults = true.\n\nThanks. Looking at the keys in responseContext, it makes sense to share it between subqueries (at least for some of them such as cpuConsumed) and seems doable. But I would like to do it in a separate PR since this PR is already big. I will document it as a known issue.\nBTW, the responseContext seems to be used for diverse purposes now. Some of them are:\n\nTo inform something important to users after query is done, such as uncoveredIntervals.\nTo store some intermediate state in the broker during query processing, such as remainingResponsesFromQueryServers.\nTo gather some metrics from query servers such as cpuConsumed.\nA part of the cache validation mechanism using ETag.\n\nI think it would probably be better split those into the first one and others at some point.", "author": "jihoonson", "createdAt": "2020-07-01T06:12:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEwMjEyMQ=="}], "type": "inlineReview"}, {"oid": "d7de425745b16c3d3aa3bfe4a38a46f0697c34f6", "url": "https://github.com/apache/druid/commit/d7de425745b16c3d3aa3bfe4a38a46f0697c34f6", "message": "add tests for builders", "committedDate": "2020-07-01T04:17:55Z", "type": "commit"}, {"oid": "a9ce3d5045af48deb7c4f46ea2b4c8355ef944eb", "url": "https://github.com/apache/druid/commit/a9ce3d5045af48deb7c4f46ea2b4c8355ef944eb", "message": "address comments", "committedDate": "2020-07-01T05:59:47Z", "type": "commit"}, {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94", "url": "https://github.com/apache/druid/commit/d960cf8e1b7621962b69c907e3436ef61c5d3c94", "message": "don't truncate", "committedDate": "2020-07-01T09:14:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyNTk0MA==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448525940", "bodyText": "This should be added to the documentation (all of these error codes are spelled out in a table in querying/querying.md).", "author": "gianm", "createdAt": "2020-07-01T17:53:41Z", "path": "processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java", "diffHunk": "@@ -105,6 +106,8 @@ private static String getErrorCodeFromThrowable(Throwable e)\n       return RESOURCE_LIMIT_EXCEEDED;\n     } else if (e instanceof UnsupportedOperationException) {\n       return UNSUPPORTED_OPERATION;\n+    } else if (e instanceof TruncatedResponseContextException) {\n+      return TRUNCATED_RESPONSE_CONTEXT;", "originalCommit": "d960cf8e1b7621962b69c907e3436ef61c5d3c94", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyNjc4Mw==", "url": "https://github.com/apache/druid/pull/10082#discussion_r448526783", "bodyText": "Should be withFailOnTruncatedResponseContext, not setFailOnTruncatedResponseContext, because nothing's being set (a modified copy is being returned).", "author": "gianm", "createdAt": "2020-07-01T17:55:12Z", "path": "processing/src/main/java/org/apache/druid/query/QueryContexts.java", "diffHunk": "@@ -344,6 +346,19 @@ public String toString()\n     return defaultTimeout;\n   }\n \n+  public static <T> Query<T> setFailOnTruncatedResponseContext(Query<T> query)", "originalCommit": "d960cf8e1b7621962b69c907e3436ef61c5d3c94", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}