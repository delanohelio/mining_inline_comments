{"pr_number": 10676, "pr_title": "Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other", "pr_createdAt": "2020-12-14T16:05:15Z", "pr_url": "https://github.com/apache/druid/pull/10676", "timeline": [{"oid": "d9e7f9bd2b5cd1b2e8df07b66ff9b7aea1e41443", "url": "https://github.com/apache/druid/commit/d9e7f9bd2b5cd1b2e8df07b66ff9b7aea1e41443", "message": "Add ability to wait for segment availability for batch jobs", "committedDate": "2020-12-09T14:15:16Z", "type": "commit"}, {"oid": "3d4de8ca8ddee96357c48955f74fb95d4d3d2cec", "url": "https://github.com/apache/druid/commit/3d4de8ca8ddee96357c48955f74fb95d4d3d2cec", "message": "IT updates", "committedDate": "2020-12-11T21:14:24Z", "type": "commit"}, {"oid": "027938ea80e385136b32c405f8046b9cef675fc1", "url": "https://github.com/apache/druid/commit/027938ea80e385136b32c405f8046b9cef675fc1", "message": "fix queries in legacy hadoop IT", "committedDate": "2020-12-11T23:39:57Z", "type": "commit"}, {"oid": "8b9d26d07527b8b2b6b1a8ed09dc0ac36e353797", "url": "https://github.com/apache/druid/commit/8b9d26d07527b8b2b6b1a8ed09dc0ac36e353797", "message": "Fix broken indexing integration tests", "committedDate": "2020-12-14T23:09:39Z", "type": "commit"}, {"oid": "72cd38a438c16a4cadb4bdedc7b643ffde0b7230", "url": "https://github.com/apache/druid/commit/72cd38a438c16a4cadb4bdedc7b643ffde0b7230", "message": "address an lgtm flag", "committedDate": "2020-12-15T00:14:22Z", "type": "commit"}, {"oid": "f47a8bdb1dfe9602fa64459c3cf8c8ec13808cae", "url": "https://github.com/apache/druid/commit/f47a8bdb1dfe9602fa64459c3cf8c8ec13808cae", "message": "spell checker still flagging for hadoop doc. adding under that file header too", "committedDate": "2020-12-15T20:42:48Z", "type": "commit"}, {"oid": "d5ed3c838e6939f4779bc34b65b9bcae321507b3", "url": "https://github.com/apache/druid/commit/d5ed3c838e6939f4779bc34b65b9bcae321507b3", "message": "fix compaction IT", "committedDate": "2020-12-15T20:45:47Z", "type": "commit"}, {"oid": "71d6f180e31bec39cf5a3917b44c6a9afd984cfc", "url": "https://github.com/apache/druid/commit/71d6f180e31bec39cf5a3917b44c6a9afd984cfc", "message": "Updates to wait for availability method", "committedDate": "2020-12-15T23:13:11Z", "type": "commit"}, {"oid": "0170dcaa6b81ff72b117c84faeb9a755006544d3", "url": "https://github.com/apache/druid/commit/0170dcaa6b81ff72b117c84faeb9a755006544d3", "message": "improve unit testing for patch", "committedDate": "2020-12-15T23:13:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MTcxNA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544091714", "bodyText": "nit: Each time we call the waitForSegmentAvailability function will communicate with coordinator no matter what the value of waitTimeout is and bring extra pressure to the Coordiantor, because there may be hundreds of thousands of batch tasks per day.\nMaybe we can do a double check here, like when waitTimeout<=0 then skip all the waitForSegmentAvailability  if possible , just in case that call waitForSegmentAvailability function without checking waitTimeout.", "author": "zhangyue19921010", "createdAt": "2020-12-16T08:09:25Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -576,6 +582,73 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(TaskToolbox toolbox, ExecutorService exec, List<DataSegment> segmentsToWaitFor, long waitTimeout)\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return false;\n+    }\n+    log.info(\"Waiting for segments to be loaded by the cluster...\");\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()", "originalCommit": "0170dcaa6b81ff72b117c84faeb9a755006544d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMzc5MQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544413791", "bodyText": "ya, that makes sense. Will add", "author": "capistrant", "createdAt": "2020-12-16T15:55:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MTcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MTg3OA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544091878", "bodyText": "nit: Could we extract ingestionSchema.getTuningConfig().getAwaitSegmentAvailabilityTimeoutMillis() at class level if possible?", "author": "zhangyue19921010", "createdAt": "2020-12-16T08:09:39Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -491,6 +497,31 @@ private boolean isParallelMode()\n     return isParallelMode(baseInputSource, ingestionSchema.getTuningConfig());\n   }\n \n+  /**\n+   * Attempt to wait for indexed segments to become available on the cluster.\n+   * @param reportsMap Map containing information with published segments that we are going to wait for.\n+   */\n+  private void waitForSegmentAvailability(Map<String, PushedSegmentsReport> reportsMap)\n+  {\n+    ArrayList<DataSegment> segmentsToWaitFor = new ArrayList<>();\n+    reportsMap.values()\n+              .forEach(report -> {\n+                segmentsToWaitFor.addAll(report.getNewSegments());\n+              });\n+    ExecutorService availabilityExec = Execs.singleThreaded(\"ParallelTaskAvailabilityWaitExec\");\n+    try {\n+      segmentAvailabilityConfirmationCompleted = waitForSegmentAvailability(\n+          toolbox,\n+          availabilityExec,\n+          segmentsToWaitFor,\n+          ingestionSchema.getTuningConfig().getAwaitSegmentAvailabilityTimeoutMillis()", "originalCommit": "0170dcaa6b81ff72b117c84faeb9a755006544d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5MzgxNg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r545293816", "bodyText": "I'm fine with that. addressed in latest commits", "author": "capistrant", "createdAt": "2020-12-17T18:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MTg3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4MDU4MQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544180581", "bodyText": "not sure but removing items from the collection while iterating it may cause an error. Instead, we could use a counter and a completableFuture together. Something like this\nCompletableFuture<Void> uberFuture = CompletableFuture.completedFuture(null);\nfor (DataSegment s : segmentsToWaitFor) {\n      CompletableFuture<Void> future = new CompletableFuture<>();\n      notifier.registerSegmentHandoffCallback(\n          new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n          exec,\n          () -> {\n            log.info(\n                \"Confirmed availability for [%s]. Removing from list of segments to wait for\",\n                s.getId()\n            );\n           future.complete(null);\n         }\n      );\n     uberFuture = uberFuture.thenCombine(future, (a, b) -> null);\n    }\nuberFuture.get(waitTimeout, TimeUnit.MILLISECONDS)", "author": "abhishekagarwal87", "createdAt": "2020-12-16T10:23:03Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -576,6 +582,73 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(TaskToolbox toolbox, ExecutorService exec, List<DataSegment> segmentsToWaitFor, long waitTimeout)\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return false;\n+    }\n+    log.info(\"Waiting for segments to be loaded by the cluster...\");\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n+                                             .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource());\n+\n+    notifier.start();\n+    for (DataSegment s : segmentsToWaitFor) {\n+      notifier.registerSegmentHandoffCallback(\n+          new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n+          exec,\n+          () -> {\n+            log.info(\n+                \"Confirmed availability for [%s]. Removing from list of segments to wait for\",\n+                s.getId()\n+            );\n+            synchronized (availabilityCondition) {\n+              segmentsToWaitFor.remove(s);", "originalCommit": "0170dcaa6b81ff72b117c84faeb9a755006544d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMjgwNQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544412805", "bodyText": "yes, I've been trying to come up with an alternative to avoid concurrent access risks. thanks so much for this idea", "author": "capistrant", "createdAt": "2020-12-16T15:54:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4MDU4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5MzY1Mg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r545293652", "bodyText": "I ended up trying a countdown latch with timeout since I am more familiar with those from use in the past as compared to your suggestion. let me know what you think of it", "author": "capistrant", "createdAt": "2020-12-17T18:04:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4MDU4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTEzNTYwMA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551135600", "bodyText": "Looks good to me @capistrant. Thanks.", "author": "abhishekagarwal87", "createdAt": "2021-01-04T06:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4MDU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4MTk5MA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r544181990", "bodyText": "Nit: Since the runnable is a simple call without blocking or computation, we could just pass Execs.directExecutor here.", "author": "abhishekagarwal87", "createdAt": "2020-12-16T10:24:58Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -576,6 +582,73 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(TaskToolbox toolbox, ExecutorService exec, List<DataSegment> segmentsToWaitFor, long waitTimeout)\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return false;\n+    }\n+    log.info(\"Waiting for segments to be loaded by the cluster...\");\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n+                                             .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource());\n+\n+    notifier.start();\n+    for (DataSegment s : segmentsToWaitFor) {\n+      notifier.registerSegmentHandoffCallback(\n+          new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n+          exec,", "originalCommit": "0170dcaa6b81ff72b117c84faeb9a755006544d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "237325f909f79c3dbb51f03b1d7c841cb20fe062", "url": "https://github.com/apache/druid/commit/237325f909f79c3dbb51f03b1d7c841cb20fe062", "message": "fix bad indentation", "committedDate": "2020-12-17T16:32:46Z", "type": "commit"}, {"oid": "848805ebbf31ac6c1d6e4d85ac4c7d0de2e5ed88", "url": "https://github.com/apache/druid/commit/848805ebbf31ac6c1d6e4d85ac4c7d0de2e5ed88", "message": "refactor waitForSegmentAvailability", "committedDate": "2020-12-17T18:00:25Z", "type": "commit"}, {"oid": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "url": "https://github.com/apache/druid/commit/8169c88acbef88f7cd9e4fb6d346ac60903512f9", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2020-12-28T21:27:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTEzNjA0Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551136043", "bodyText": "shouldn't this return true here? If there are no segments to confirm, the confirmation is done.", "author": "abhishekagarwal87", "createdAt": "2021-01-04T06:26:13Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -576,6 +584,64 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.warn(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return false;", "originalCommit": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIyNDkwMQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552224901", "bodyText": "hmm, I suppose you are right. I guess I initially thought that since I didn't confirm any segments were loaded, I report back that segments were not confirmed to be loaded. However, that false value would be more confusing to an end user of the report compared to returning true... so true seems like right way to go", "author": "capistrant", "createdAt": "2021-01-05T22:00:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTEzNjA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MDU3Mg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551660572", "bodyText": "Can you give some more details on how this will be used in your application? Do you want to track handoff failures of each task? I'm wondering if handoff time is also important.", "author": "jihoonson", "createdAt": "2021-01-05T01:11:38Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/IngestionStatsAndErrorsTaskReportData.java", "diffHunk": "@@ -41,17 +41,22 @@\n   @Nullable\n   private String errorMsg;\n \n+  @JsonProperty\n+  private boolean segmentAvailabilityConfirmed;\n+\n   public IngestionStatsAndErrorsTaskReportData(\n       @JsonProperty(\"ingestionState\") IngestionState ingestionState,\n       @JsonProperty(\"unparseableEvents\") Map<String, Object> unparseableEvents,\n       @JsonProperty(\"rowStats\") Map<String, Object> rowStats,\n-      @JsonProperty(\"errorMsg\") @Nullable String errorMsg\n+      @JsonProperty(\"errorMsg\") @Nullable String errorMsg,\n+      @JsonProperty(\"segmentAvailabilityConfirmed\") boolean segmentAvailabilityConfirmed", "originalCommit": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIyMDA2NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552220064", "bodyText": "my company deploys a large multi-tenant cluster with a services layer for ingestion that our tenants use. these tenants don't just want to know when their task succeeds, they also want to know when data from batch ingest is available for querying. This solution allows us to prevent the ingestion services layer and/or individual tenants from banging on Druid APIs trying to see if their data is available after ingestion.", "author": "capistrant", "createdAt": "2021-01-05T21:49:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MDU3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDk0MjcxNg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r564942716", "bodyText": "my company deploys a large multi-tenant cluster with a services layer for ingestion that our tenants use. these tenants don't just want to know when their task succeeds, they also want to know when data from batch ingest is available for querying. This solution allows us to prevent the ingestion services layer and/or individual tenants from banging on Druid APIs trying to see if their data is available after ingestion.\n\nI understand this, but my question is more like what people expect when segment handoff fails. In streaming ingestion, the handoff failure causes task failure (this behavior seems arguable, but that's what it does now) and thus people's expectation is that they could see some data dropped after handoff failures until new tasks read the same data and publishes the same segments again. However, since there is no realtime querying in batch ingestion, I don't think tasks should fail on handoff failures (which is what this PR does! \ud83d\ude42), but then what will be people's expectation? Are they going to be just OK with handoff failures and wait indefinitely until historicals load new segments (the current behavior)? Do they want to know why the handoff failed? Do they want to know how long it took before the handoff failed? These questions are not clear to me.", "author": "jihoonson", "createdAt": "2021-01-27T00:55:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MDU3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NTM5MTg0Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r565391843", "bodyText": "my company deploys a large multi-tenant cluster with a services layer for ingestion that our tenants use. these tenants don't just want to know when their task succeeds, they also want to know when data from batch ingest is available for querying. This solution allows us to prevent the ingestion services layer and/or individual tenants from banging on Druid APIs trying to see if their data is available after ingestion.\n\nI understand this, but my question is more like what people expect when segment handoff fails. In streaming ingestion, the handoff failure causes task failure (this behavior seems arguable, but that's what it does now) and thus people's expectation is that they could see some data dropped after handoff failures until new tasks read the same data and publishes the same segments again. However, since there is no realtime querying in batch ingestion, I don't think tasks should fail on handoff failures (which is what this PR does! \ud83d\ude42), but then what will be people's expectation? Are they going to be just OK with handoff failures and wait indefinitely until historicals load new segments (the current behavior)? Do they want to know why the handoff failed? Do they want to know how long it took before the handoff failed? These questions are not clear to me.\n\ngood question. for my specific case the service that end users interact with really wanted to be able to answer this question for the end user:\n\nIs the data that I ingested in this job completely loaded for querying?\n\nFor us a simple yes/no will suffice. The cluster operators would have the goal of having 100% of jobs successfully handoff data before the timeout, but when that doesn't happen our users simply want to know that they may need to wait longer. We are simply trying to be transparent and report the point in time status. The onus of finding out when the data is fully loaded if this timeout expired before loading, would fall on a different solution (TBD).\nYou're right, we intentionally did not fail these tasks because Historical nodes loading the segments is detached from whether or not the data was written to deepstore/metastore (if that failed the task should and likely would fail due to existing code paths). We don't want our end users thinking they need to re-run their jobs when this is much more likely to be an issue of the coordinator not having assigned servers to load segments by the time the timeout expired.\nWhy the handoff failed would be something I as an operator am more interested compared to a user (unless that user is also an operator). I think that would be very difficult to communicate in these reports since the indexing task doesn't know much about what the rest of the cluster is doing.\nKnowing how long it took before the time out could be found in the spec, but I guess it could be useful to add that value to the report as well if you think users would want to have quick reference. I think that rather than having that static value, it could be cool to have the dynamic time waited for handoff. Maybe it is the static value because we hit the timeout. but as an operator I would enjoy seeing how long each successful job waited for handoff. what do you think about that?", "author": "capistrant", "createdAt": "2021-01-27T15:16:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MDU3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjM4MDA5NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r566380095", "bodyText": "For us a simple yes/no will suffice. The cluster operators would have the goal of having 100% of jobs successfully handoff data before the timeout, but when that doesn't happen our users simply want to know that they may need to wait longer. We are simply trying to be transparent and report the point in time status. The onus of finding out when the data is fully loaded if this timeout expired before loading, would fall on a different solution (TBD).\n\nCool, are you working on \"the different solution\"? That would be interesting too.\n\nWhy the handoff failed would be something I as an operator am more interested compared to a user (unless that user is also an operator). I think that would be very difficult to communicate in these reports since the indexing task doesn't know much about what the rest of the cluster is doing.\n\nI agree. I think we need more visibility on the coordinator behavior.\n\nKnowing how long it took before the time out could be found in the spec, but I guess it could be useful to add that value to the report as well if you think users would want to have quick reference. I think that rather than having that static value, it could be cool to have the dynamic time waited for handoff. Maybe it is the static value because we hit the timeout. but as an operator I would enjoy seeing how long each successful job waited for handoff. what do you think about that?\n\nThat seems useful to me too \ud83d\udc4d\nFor the time to fail handoff, due to the above issue of the lack of ability to know the cause of handoff failures, I guess I was wondering if the report can be a false alarm. For example, the report can say it failed to confirm the segments handed off, but maybe the handoff could be even not triggered at all for some reason. I don't think this can happen for now, but is possible in the future if someone else modifies this area for some good reason. segmentAvailabilityConfirmationCompleted + time to fail handoff can be an indicator of such unexpected failures. I would say this is not a blocker for this PR, but it seems useful to me.", "author": "jihoonson", "createdAt": "2021-01-28T20:17:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MDU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551662522", "bodyText": "Wondering if you can reuse StreamAppenderatorDriver.registerHandoff() or StreamAppenderatorDriver.publishAndRegisterHandoff() as they seem pretty similar to this new method. You would need to move that method out to BaseAppenderatorDriver.", "author": "jihoonson", "createdAt": "2021-01-05T01:19:00Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -576,6 +584,64 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(", "originalCommit": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIzMDk3NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552230974", "bodyText": "interesting suggestion. definitely open to it. I'll take a look as soon as I have time", "author": "capistrant", "createdAt": "2021-01-05T22:14:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTM4ODAxOA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r555388018", "bodyText": "My initial thoughts:\n\nI agree that the code is working to achieve the same goal. so re-use would be nice\nIndexTask appears to be straightforward if we were to take this approach.\nParallel Indexing and Hadoop Indexing do not seem as straightforward the way I understand things\n\nparallel: we'd like to do this handoff wait from the supervisor, IMO. However there is no existing utilization of Appenderator or AppendoratorDriver at this level of parallel indexing.\nhadoop: Hadoop does not use Appendorator or AppendoratorDriver so we would have to bolt that on with an Appendorator impl that is pretty much only there for show since hadoop indexing does not have any use other than the fact that it would be needed for this implementation (as far as I can tell)\n\n\n\nI'm sure I could make it work eventually. But I wonder if the jumping through hoops to get the task code ready to be compatible for using the registerHandoff in BaseAppenderatorDriver is worth it both in terms of effort and code legibility.", "author": "capistrant", "createdAt": "2021-01-11T22:44:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDkzMTE3NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r564931175", "bodyText": "Good point. I agree. I think it would be better to not do such refactoring for parallel or hadoop task in this PR. But it would be still nice to reuse the same logic in both streaming and batch ingestion. Maybe we can extract this logic as a utility method?", "author": "jihoonson", "createdAt": "2021-01-27T00:25:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NTQ4MDI1Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r565480253", "bodyText": "Good point. I agree. I think it would be better to not do such refactoring for parallel or hadoop task in this PR. But it would be still nice to reuse the same logic in both streaming and batch ingestion. Maybe we can extract this logic as a utility method?\n\nHm, I guess I'm a little bit confused on this comment. which logic are you suggesting be shared? Since the code in StreamAppenderatorDriver is so closely coupled with the appenderator concept, I struggle to see what can be extracted. The callback function required for the appenderator is incompatible with batch ingestion as it stands today. Are you suggesting that we use the same method for both but use different callback implementations based on the ingestion type? I guess I don't understand the value there if that is the case. Otherwise I may be missing your point entirely and just need a nudge in the right direction", "author": "capistrant", "createdAt": "2021-01-27T17:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjM3MDg0Mg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r566370842", "bodyText": "Yeah, I think you are right.", "author": "jihoonson", "createdAt": "2021-01-28T20:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MjUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MzU0OQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551663549", "bodyText": "Why is this always false? Does it make more sense to be always true because realtime tasks will fail when handoff fails?", "author": "jihoonson", "createdAt": "2021-01-05T01:23:19Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskRunner.java", "diffHunk": "@@ -1058,7 +1058,8 @@ private synchronized void persistSequences() throws IOException\n                 ingestionState,\n                 getTaskCompletionUnparseableEvents(),\n                 getTaskCompletionRowStats(),\n-                errorMsg\n+                errorMsg,\n+                false", "originalCommit": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIyMzQxNw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552223417", "bodyText": "well I guess this would be true if the task succeeds and false if it does not instead of hardcoded. I was tunnel visioned on this part thinking only what my team needed and not what actual behavior should be for all different types of ingestion.", "author": "capistrant", "createdAt": "2021-01-05T21:57:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MzU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI0MDM4Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552240383", "bodyText": "I went with errorMsg == null and added a javadoc stating the errorMsg String should be null on success and non-null on failure. Current code follows that pattern", "author": "capistrant", "createdAt": "2021-01-05T22:36:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MzU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDkzMjM3Ng==", "url": "https://github.com/apache/druid/pull/10676#discussion_r564932376", "bodyText": "Oh yeah, I thought IngestionStatsAndErrorsTaskReportData is available only when the task succeeded. I think you are correct.", "author": "jihoonson", "createdAt": "2021-01-27T00:28:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MzU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2NDA4MQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r551664081", "bodyText": "Testing with true would be better because missing booleans are defaulted to false by Jackson in Druid.", "author": "jihoonson", "createdAt": "2021-01-05T01:25:07Z", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/TaskReportSerdeTest.java", "diffHunk": "@@ -55,7 +55,8 @@ public void testSerde() throws Exception\n             ImmutableMap.of(\n                 \"number\", 1234\n             ),\n-            \"an error message\"\n+            \"an error message\",\n+            false", "originalCommit": "8169c88acbef88f7cd9e4fb6d346ac60903512f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIyMzUyMQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r552223521", "bodyText": "makes sense, will change", "author": "capistrant", "createdAt": "2021-01-05T21:57:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2NDA4MQ=="}], "type": "inlineReview"}, {"oid": "32fdff51d315f71fa44bcf5522a9dc823124593b", "url": "https://github.com/apache/druid/commit/32fdff51d315f71fa44bcf5522a9dc823124593b", "message": "Fixes based off of review comments", "committedDate": "2021-01-05T22:28:13Z", "type": "commit"}, {"oid": "b4f2e09d90ce5b3c01ed309f5beedbf554eef261", "url": "https://github.com/apache/druid/commit/b4f2e09d90ce5b3c01ed309f5beedbf554eef261", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-01-06T16:48:40Z", "type": "commit"}, {"oid": "9295df14e44d970cd1e71c893e0e5572279db905", "url": "https://github.com/apache/druid/commit/9295df14e44d970cd1e71c893e0e5572279db905", "message": "cleanup to get compile after merging with master", "committedDate": "2021-01-06T17:20:24Z", "type": "commit"}, {"oid": "abf7f99f6412975b83d7c83e171ef4cc35d54321", "url": "https://github.com/apache/druid/commit/abf7f99f6412975b83d7c83e171ef4cc35d54321", "message": "fix failing test after previous logic update", "committedDate": "2021-01-06T17:25:38Z", "type": "commit"}, {"oid": "d2e9918d3ba2e9e388645de44bd7138866a14e52", "url": "https://github.com/apache/druid/commit/d2e9918d3ba2e9e388645de44bd7138866a14e52", "message": "add back code that must have gotten deleted during conflict resolution", "committedDate": "2021-01-06T20:11:54Z", "type": "commit"}, {"oid": "92f3bec29adbe27c011f56a31017306e415a7cb1", "url": "https://github.com/apache/druid/commit/92f3bec29adbe27c011f56a31017306e415a7cb1", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-01-11T20:17:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjkyMjMzNg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r556922336", "bodyText": "I think this should be errorMsg == null. also add javadoc\n/**\n   * Return a map of reports for the task.\n   *\n   * A successfull task should always have a null errorMsg. A falied task should always have a non-null\n   * errorMsg. Nullable error message for the task. null if task succeeded.\n   *\n   * @return Map of reports for the task.\n   */", "author": "capistrant", "createdAt": "2021-01-13T22:40:17Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -584,7 +584,8 @@ protected boolean isFirehoseDrainableByClosing(FirehoseFactory firehoseFactory)\n                 ingestionState,\n                 getTaskCompletionUnparseableEvents(),\n                 getTaskCompletionRowStats(),\n-                errorMsg\n+                errorMsg,\n+                false", "originalCommit": "92f3bec29adbe27c011f56a31017306e415a7cb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTE1MjEyNQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r561152125", "bodyText": "I think this needs an assert", "author": "capistrant", "createdAt": "2021-01-20T17:39:42Z", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/TaskSerdeTest.java", "diffHunk": "@@ -268,7 +268,8 @@ public void testIndexTaskSerde() throws Exception\n                 null,\n                 null,\n                 null,\n-                null\n+                null,", "originalCommit": "92f3bec29adbe27c011f56a31017306e415a7cb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDk0MjIxOQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r564942219", "bodyText": "I think we should change the log level of some info logs to debug in this class which are printed per segment. It could be a lot in batch ingestion.", "author": "jihoonson", "createdAt": "2021-01-27T00:54:47Z", "path": "server/src/main/java/org/apache/druid/segment/handoff/CoordinatorBasedSegmentHandoffNotifier.java", "diffHunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package org.apache.druid.segment.realtime.plumber;\n+package org.apache.druid.segment.handoff;", "originalCommit": "92f3bec29adbe27c011f56a31017306e415a7cb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "222d3c2bfa9257a7a6c765597cd87d5bfb3587e4", "url": "https://github.com/apache/druid/commit/222d3c2bfa9257a7a6c765597cd87d5bfb3587e4", "message": "update some logging code", "committedDate": "2021-01-28T16:53:31Z", "type": "commit"}, {"oid": "88df46c4e08bb448e8aa594910be3ed60fb64b9e", "url": "https://github.com/apache/druid/commit/88df46c4e08bb448e8aa594910be3ed60fb64b9e", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-01-28T16:54:43Z", "type": "commit"}, {"oid": "452b649088340b757cc3656bd2c241da73c8fb60", "url": "https://github.com/apache/druid/commit/452b649088340b757cc3656bd2c241da73c8fb60", "message": "fixes to get compilation working after merge with master", "committedDate": "2021-01-28T17:49:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjM2ODcyOQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r566368729", "bodyText": "The interrupted state is cleared out after it's checked. We should set the state of the current thread back. Please add Thread.currentThread().interrupt().", "author": "jihoonson", "createdAt": "2021-01-28T19:56:18Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -580,6 +588,64 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return true;\n+    } else if (waitTimeout <= 0) {\n+      log.warn(\"Asked to wait for availability for <= 0 seconds?! Requested waitTimeout: [%s]\", waitTimeout);\n+      return false;\n+    }\n+    log.info(\"Waiting for [%d] segments to be loaded by the cluster...\", segmentsToWaitFor.size());\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n+                                             .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource());\n+    ExecutorService exec = Execs.directExecutor();\n+    CountDownLatch doneSignal = new CountDownLatch(segmentsToWaitFor.size());\n+\n+    notifier.start();\n+    for (DataSegment s : segmentsToWaitFor) {\n+      notifier.registerSegmentHandoffCallback(\n+          new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n+          exec,\n+          () -> {\n+            log.debug(\n+                \"Confirmed availability for [%s]. Removing from list of segments to wait for\",\n+                s.getId()\n+            );\n+            doneSignal.countDown();\n+          }\n+      );\n+    }\n+\n+    try {\n+      return doneSignal.await(waitTimeout, TimeUnit.MILLISECONDS);\n+    }\n+    catch (InterruptedException e) {", "originalCommit": "452b649088340b757cc3656bd2c241da73c8fb60", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "376cfa097cea19a86141b3dc63ce091d80c156f5", "url": "https://github.com/apache/druid/commit/376cfa097cea19a86141b3dc63ce091d80c156f5", "message": "reset interrupt flag in catch block after code review pointed it out", "committedDate": "2021-01-28T20:45:34Z", "type": "commit"}, {"oid": "c63611a33fe7114014432830b0d69227340c6ad6", "url": "https://github.com/apache/druid/commit/c63611a33fe7114014432830b0d69227340c6ad6", "message": "small changes following self-review", "committedDate": "2021-01-28T20:51:36Z", "type": "commit"}, {"oid": "1772199f6b001eccffc3034314c77623843ae4d2", "url": "https://github.com/apache/druid/commit/1772199f6b001eccffc3034314c77623843ae4d2", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-02-10T15:32:36Z", "type": "commit"}, {"oid": "16274c3f35ead6f147a0fde334570a72dbedbd22", "url": "https://github.com/apache/druid/commit/16274c3f35ead6f147a0fde334570a72dbedbd22", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-03-17T14:50:20Z", "type": "commit"}, {"oid": "503bdb448c52c7e94efb136162ee907e3f90a13f", "url": "https://github.com/apache/druid/commit/503bdb448c52c7e94efb136162ee907e3f90a13f", "message": "fixup some issues brought on by merge with master", "committedDate": "2021-03-17T20:06:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyMjMxNw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601822317", "bodyText": "nit: This is a long elsewhere. I don't think this matters though.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              int DEFAULT_AWAIT_SEGMENT_AVAILABILITY_TIMEOUT_MILLIS = 0;\n          \n          \n            \n              long DEFAULT_AWAIT_SEGMENT_AVAILABILITY_TIMEOUT_MILLIS = 0;", "author": "suneet-s", "createdAt": "2021-03-25T20:37:16Z", "path": "server/src/main/java/org/apache/druid/segment/indexing/TuningConfig.java", "diffHunk": "@@ -40,6 +40,7 @@\n   int DEFAULT_MAX_SAVED_PARSE_EXCEPTIONS = 0;\n   int DEFAULT_MAX_ROWS_IN_MEMORY = 1_000_000;\n   boolean DEFAULT_SKIP_BYTES_IN_MEMORY_OVERHEAD_CHECK = false;\n+  int DEFAULT_AWAIT_SEGMENT_AVAILABILITY_TIMEOUT_MILLIS = 0;", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyNDg0MA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601824840", "bodyText": "nit: did you mean to add this twice?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            awaitSegmentAvailabilityTimeoutMillis", "author": "suneet-s", "createdAt": "2021-03-25T20:41:34Z", "path": "website/.spelling", "diffHunk": "@@ -970,6 +970,7 @@ InputSplit\n JobHistory\n a.example.com\n assumeGrouped\n+awaitSegmentAvailabilityTimeoutMillis", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg3NTYyOQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601875629", "bodyText": "The way I understood it is that each file has it's own dictionary. The first is in the hadoop index file section and the second is in the native index file section", "author": "capistrant", "createdAt": "2021-03-25T22:19:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyNDg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyOTkzMg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601829932", "bodyText": "Naive question: Should this be\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  @JsonProperty(\"segmentAvailabilityConfirmed\") boolean segmentAvailabilityConfirmed\n          \n          \n            \n                  @JsonProperty(\"segmentAvailabilityConfirmed\") @Nullable Boolean segmentAvailabilityConfirmed\n          \n      \n    \n    \n  \n\nSeeing the json properties automatically make me think about version mismatches - but I don't exactly know how this is used - so I'm just asking in the hope you can save me some time from digging :)", "author": "suneet-s", "createdAt": "2021-03-25T20:49:38Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/IngestionStatsAndErrorsTaskReportData.java", "diffHunk": "@@ -41,17 +41,22 @@\n   @Nullable\n   private String errorMsg;\n \n+  @JsonProperty\n+  private boolean segmentAvailabilityConfirmed;\n+\n   public IngestionStatsAndErrorsTaskReportData(\n       @JsonProperty(\"ingestionState\") IngestionState ingestionState,\n       @JsonProperty(\"unparseableEvents\") Map<String, Object> unparseableEvents,\n       @JsonProperty(\"rowStats\") Map<String, Object> rowStats,\n-      @JsonProperty(\"errorMsg\") @Nullable String errorMsg\n+      @JsonProperty(\"errorMsg\") @Nullable String errorMsg,\n+      @JsonProperty(\"segmentAvailabilityConfirmed\") boolean segmentAvailabilityConfirmed", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg5MDk0NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601890945", "bodyText": "So these will be created/written by individual indexing tasks. then stored wherever the cluster stores task logs. And I believe the only way they are ever accessed by Druid is streamed from their location directly to an API caller without ever deserializing them. So I don't think there is any possibility for issues during an upgrade here.", "author": "capistrant", "createdAt": "2021-03-25T22:56:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyOTkzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzMTk1NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601831955", "bodyText": "This shouldn't be a warn message since the default is 0. Maybe you want a less than check, which should technically never happen, since there is a check in the constructor.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                } else if (waitTimeout <= 0) {\n          \n          \n            \n                } else if (waitTimeout < 0) {", "author": "suneet-s", "createdAt": "2021-03-25T20:53:14Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -566,6 +574,65 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return true;\n+    } else if (waitTimeout <= 0) {", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg5MTQwMA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601891400", "bodyText": "updated for the < case only. Like you said, it shouldn't ever happen, but someday in the future if it does somehow this should make troubleshooting easier for an operator", "author": "capistrant", "createdAt": "2021-03-25T22:57:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzMTk1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzMzMwNg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601833306", "bodyText": "nit: The question mark in this message makes it seem like this is an un-expected state. Should this be logged at a warn level instead?\nHonestly, if I didn't read the question mark at the end, I would have thought this is a reasonable info level message.", "author": "suneet-s", "createdAt": "2021-03-25T20:55:34Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -566,6 +574,65 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg3OTM0OQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601879349", "bodyText": "Well it will happen in a case when a job doesn't actually ingest any data. It isn't necessarily a bad thing, but it is something I wanted to make note of. I'm going to remove the !? to make it seem less important. it really is just informative.", "author": "capistrant", "createdAt": "2021-03-25T22:28:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzMzMwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzNzU0NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601837544", "bodyText": "Instead of catching this in the finally block, I think a try-with-resource pattern would be safer. I don't think anything can fail in the for loop where we're registering callbacks, but if it does - the SegmentHandoffNotifier won't clean up after itself\ntry (SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory().createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource())) {\n    // register handoffs and wait for signal\n}", "author": "suneet-s", "createdAt": "2021-03-25T21:02:44Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -566,6 +574,65 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return true;\n+    } else if (waitTimeout <= 0) {\n+      log.warn(\"Asked to wait for availability for <= 0 seconds?! Requested waitTimeout: [%s]\", waitTimeout);\n+      return false;\n+    }\n+    log.info(\"Waiting for [%d] segments to be loaded by the cluster...\", segmentsToWaitFor.size());\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n+                                             .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource());\n+    ExecutorService exec = Execs.directExecutor();\n+    CountDownLatch doneSignal = new CountDownLatch(segmentsToWaitFor.size());\n+\n+    notifier.start();\n+    for (DataSegment s : segmentsToWaitFor) {\n+      notifier.registerSegmentHandoffCallback(\n+          new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n+          exec,\n+          () -> {\n+            log.debug(\n+                \"Confirmed availability for [%s]. Removing from list of segments to wait for\",\n+                s.getId()\n+            );\n+            doneSignal.countDown();\n+          }\n+      );\n+    }\n+\n+    try {\n+      return doneSignal.await(waitTimeout, TimeUnit.MILLISECONDS);\n+    }\n+    catch (InterruptedException e) {\n+      log.warn(\"Interrupted while waiting for segment availablity; Unable to confirm availability!\");\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n+    finally {\n+      notifier.close();", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUxMzQ3Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603513473", "bodyText": "@suneet-s  I agree with this thought. Unsure on how exact implementation should look. Pasting a block below before pushing it:\ntry(\n        SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n                                                 .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource())\n    ) {\n\n      ExecutorService exec = Execs.directExecutor();\n      CountDownLatch doneSignal = new CountDownLatch(segmentsToWaitFor.size());\n\n      notifier.start();\n      for (DataSegment s : segmentsToWaitFor) {\n        notifier.registerSegmentHandoffCallback(\n            new SegmentDescriptor(s.getInterval(), s.getVersion(), s.getShardSpec().getPartitionNum()),\n            exec,\n            () -> {\n              log.debug(\n                  \"Confirmed availability for [%s]. Removing from list of segments to wait for\",\n                  s.getId()\n              );\n              doneSignal.countDown();\n            }\n        );\n      }\n\n      try {\n        return doneSignal.await(waitTimeout, TimeUnit.MILLISECONDS);\n      }\n      catch (InterruptedException e) {\n        log.warn(\"Interrupted while waiting for segment availablity; Unable to confirm availability!\");\n        Thread.currentThread().interrupt();\n        return false;\n      }\n    }", "author": "capistrant", "createdAt": "2021-03-29T18:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzNzU0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUyNzE5OQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603527199", "bodyText": "I guess the main question is would be in regards to that existing try/catch. Should that catch be extracted to the try with resource block instead of being nested?", "author": "capistrant", "createdAt": "2021-03-29T18:42:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzNzU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzOTM5NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601839395", "bodyText": "Another naive question: Is it possible for the segments to be handed off before the callback was registered?\nI haven't dug deep into this part of Druid yet, so I'm just if it's something you've considered", "author": "suneet-s", "createdAt": "2021-03-25T21:06:08Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -566,6 +574,65 @@ static Granularity findGranularityFromSegments(List<DataSegment> segments)\n     }\n   }\n \n+  /**\n+   * Wait for segments to become available on the cluster. If waitTimeout is reached, giveup on waiting. This is a\n+   * QoS method that can be used to make Batch Ingest tasks wait to finish until their ingested data is available on\n+   * the cluster. Doing so gives an end user assurance that a Successful task status means their data is available\n+   * for querying.\n+   *\n+   * @param toolbox {@link TaskToolbox} object with for assisting with task work.\n+   * @param segmentsToWaitFor {@link List} of segments to wait for availability.\n+   * @param waitTimeout Millis to wait before giving up\n+   * @return True if all segments became available, otherwise False.\n+   */\n+  protected boolean waitForSegmentAvailability(\n+      TaskToolbox toolbox,\n+      List<DataSegment> segmentsToWaitFor,\n+      long waitTimeout\n+  )\n+  {\n+    if (segmentsToWaitFor.isEmpty()) {\n+      log.info(\"Asked to wait for segments to be available, but I wasn't provided with any segments!?\");\n+      return true;\n+    } else if (waitTimeout <= 0) {\n+      log.warn(\"Asked to wait for availability for <= 0 seconds?! Requested waitTimeout: [%s]\", waitTimeout);\n+      return false;\n+    }\n+    log.info(\"Waiting for [%d] segments to be loaded by the cluster...\", segmentsToWaitFor.size());\n+\n+    SegmentHandoffNotifier notifier = toolbox.getSegmentHandoffNotifierFactory()\n+                                             .createSegmentHandoffNotifier(segmentsToWaitFor.get(0).getDataSource());\n+    ExecutorService exec = Execs.directExecutor();\n+    CountDownLatch doneSignal = new CountDownLatch(segmentsToWaitFor.size());\n+\n+    notifier.start();\n+    for (DataSegment s : segmentsToWaitFor) {\n+      notifier.registerSegmentHandoffCallback(", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg4MDcwMA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601880700", "bodyText": "I suppose it is possible. But it should not matter. CoordinatorBasedSegmentHandoffNotifier should handle that gracefully. We add the callbacks and that notifier is periodically iterating over them and checking for handoff completion which is just an api call to the coordinator. So if I create callback for segment A after it was handed off, the notifier should just complete the callback successfully the next time it does the handoff checks.", "author": "capistrant", "createdAt": "2021-03-25T22:31:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzOTM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzQ3MDk2MQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603470961", "bodyText": "Ah perfect! Thanks for the explanation", "author": "suneet-s", "createdAt": "2021-03-29T17:14:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgzOTM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0MzE5NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601843194", "bodyText": "I think we should add a comment here that says something to the effect of \"For streaming ingestion, segments are considered to be available immediately if the task is successful. If the task failed, the segments are not available\"\nAt least that's what I think this is trying to say.", "author": "suneet-s", "createdAt": "2021-03-25T21:13:02Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskRunner.java", "diffHunk": "@@ -1066,7 +1075,8 @@ private synchronized void persistSequences() throws IOException\n                 ingestionState,\n                 getTaskCompletionUnparseableEvents(),\n                 getTaskCompletionRowStats(),\n-                errorMsg\n+                errorMsg,\n+                errorMsg == null", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg4MjA4Nw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601882087", "bodyText": "yes, it is saying that a successful streaming task means segments were confirmed to be handed off. an unsuccessful task means we are not positive that segments were handed off. I'm unsure if it is possible to have a streaming task confirm that segments are handed off and still fail, but I assume it is not very common.", "author": "capistrant", "createdAt": "2021-03-25T22:34:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0MzE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601844470", "bodyText": "Do we need a CompactionTaskTest where awaitSegmentAvailabilityTimeoutMillis is non null", "author": "suneet-s", "createdAt": "2021-03-25T21:15:24Z", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java", "diffHunk": "@@ -1447,6 +1452,7 @@ private void assertIngestionSchema(\n             null,\n             null,\n             null,\n+            null,\n             null", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg4NDA3OQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601884079", "bodyText": "I think we might. I don't think I was writing this with the intention of having compaction tasks support this. It really wouldn't add value since compaction isn't materially changing the underlying data. But I guess since compaction is spinning up a parallel indexing task, we kinda need to consider it so I will have to check on this.", "author": "capistrant", "createdAt": "2021-03-25T22:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTkyNjk5NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601926994", "bodyText": "Interesting point. I think there are some things we should think about first.\n\nIt's true that currently compaction doesn't change the underlying data much, but it can make some changes such as filtering out some unnecessary dimensions or adding new metrics. You can also change the query granularity now. In the future, I can imagine that you can even transform your data using compaction with a new support for transformSpec.\nThe compaction task is a bit special and different from other batch tasks in how it publishes segments. All other batch tasks can push segments in the middle of indexing, but should publish all those segments at the end of indexing. However, the compaction task can process each time chunk at a time when there is no change in segment granularity. In this case, it can publish segments whenever it finishes processing individual time chunk. It can also go through all time chunks even when there are some time chunks that it fails to compact. The final task status will be FAILED when it succeeds to compact only some time chunks but fails for others.\nCompacting datasources is usually not the single-shot type job. Rather, you would run multiple small compaction tasks over time as in auto compaction. In that case, you would want to know what time chunks are compacted and what are not, so that you can determine what result you can get when you query certain time chunks. For the compaction that is manually set up outside druid, tracking of individual compaction tasks could be useful for this purpose. However, for auto compaction, it won't provide much value since compaction tasks are submitted by the coordinator not users. So, we need another way such as adding a new coordinator API that returns such compaction status.\n\nFrom these, we would probably want something similar but different for compaction from the one proposed here. I would suggest to do it in a different PR.", "author": "jihoonson", "createdAt": "2021-03-26T00:31:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTk4MDA5Ng==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601980096", "bodyText": "@jihoonson when it comes to judging segment handoff, isn't a compaction task a parallel indexing task under the hood? I just tested a compaction task submit with tuning config and the new config for handoff set to 5 minutes. The task ran and logs indicate it confirmed handoff of newly created compacted segment.\nShould not allow the handoff to occur when it is a compact type task? I'm not sure how we would do that, since the TuningConfig is not modifiable as written and if the task(s) ran by the compact task are just parallel indexing tasks, I'm not sure how we would tell them not to honor the handoff wait config", "author": "capistrant", "createdAt": "2021-03-26T02:48:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMjAwMTY4MA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r602001680", "bodyText": "@jihoonson when it comes to judging segment handoff, isn't a compaction task a parallel indexing task under the hood? I just tested a compaction task submit with tuning config and the new config for handoff set to 5 minutes. The task ran and logs indicate it confirmed handoff of newly created compacted segment.\n\nIt is the parallel index task under the hood, but one compaction task can run multiple parallel indexing tasks. With the changes in this PR, each of those parallel indexing tasks will wait for segment handoff independently. The handoff wait timeout will also apply separately for each parallel task. I think this is OK as long as it's documented, but what I meant is, maybe we could do this differently, such as waiting for segments to be handed off all together after running all parallel tasks.\n\nShould not allow the handoff to occur when it is a compact type task? I'm not sure how we would do that, since the TuningConfig is not modifiable as written and if the task(s) ran by the compact task are just parallel indexing tasks, I'm not sure how we would tell them not to honor the handoff wait config\n\nIt seems reasonable to me to not support until we figure out what we want to do. An easy way to do so is making a copy of tuningConfig in compaction task but with awaitSegmentAvailabilityTimeoutMillis always being set to 0. We should document that compaction tasks don't wait for handoff if you want to do this.", "author": "jihoonson", "createdAt": "2021-03-26T04:01:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzQ3MDI3NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603470274", "bodyText": "Given this discussion, can we add a guardrail that says if someone tries to issue a compaction task when awaitSegmentAvailabilityTimeoutMillis is non null and != 0 that this is not supported. I think that's the easiest way forward to get the PR merged and it's a nice UX because it tells users that what they are doing is untested. Similar comment for auto-compaction (and other ingestion tasks that I may have missed)", "author": "suneet-s", "createdAt": "2021-03-29T17:13:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzQ5MTc5Mw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603491793", "bodyText": "Given this discussion, can we add a guardrail that says if someone tries to issue a compaction task when awaitSegmentAvailabilityTimeoutMillis is non null and != 0 that this is not supported. I think that's the easiest way forward to get the PR merged and it's a nice UX because it tells users that what they are doing is untested. Similar comment for auto-compaction (and other ingestion tasks that I may have missed)\n\nAre you expecting a job submit failure then?", "author": "capistrant", "createdAt": "2021-03-29T17:46:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwMTM2Nw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603501367", "bodyText": "Yes, that's what I was imagining. That being said, I'm open to other options if you had something in mind.\nI'm basing this on the principle that if I (the user) do something that Druid doesn't expect, Druid tells me I'm doing something unexpected, and failing fast is better than failing in some obscure way later on in the process.\nA clear error message on the failed job like \"awaitSegmentAvailabilityTimeoutMillis  is not supported for compaction. Please remove this from the spec and re-submit the compaction job.\" would tell users exactly how to fix the failed job - hence, the nice UX.\nA future PR for this support with auto-compaction / compaction would be super cool though.", "author": "suneet-s", "createdAt": "2021-03-29T18:01:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUyMzQ2NQ==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603523465", "bodyText": "I like this idea of it failing. I will work on changes as soon as I can, hopefully early this week.", "author": "capistrant", "createdAt": "2021-03-29T18:36:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDk2NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601844964", "bodyText": "Do we need an IndexTaskSerdeTest where awaitSegmentAvailabilityTimeoutMillis is not null", "author": "suneet-s", "createdAt": "2021-03-25T21:16:20Z", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskSerdeTest.java", "diffHunk": "@@ -267,6 +272,7 @@ public void testBestEffortRollupWithHashedPartitionsSpec()\n         true,\n         10,\n         100,\n+        null,\n         null", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg4NTM4Ng==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601885386", "bodyText": "there was not any new explicit test written the other tests in that file do use -1L and 1L as test values. I just updated another of the existing tests to use 0L as well.", "author": "capistrant", "createdAt": "2021-03-25T22:42:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NDk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NzYyNg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601847626", "bodyText": "Is it worth testing what happens when this is non zero?\nDo we also want to check that the IngestionStatsAndErrorsTaskReportData reports segmentAvailabilityConfirmed as false now?", "author": "suneet-s", "createdAt": "2021-03-25T21:21:05Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/coordinator/duty/ITAutoCompactionTest.java", "diffHunk": "@@ -376,6 +376,11 @@ private void loadData(String indexTask) throws Exception\n   {\n     String taskSpec = getResourceAsString(indexTask);\n     taskSpec = StringUtils.replace(taskSpec, \"%%DATASOURCE%%\", fullDatasourceName);\n+    taskSpec = StringUtils.replace(\n+        taskSpec,\n+        \"%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%\",\n+        jsonMapper.writeValueAsString(\"0\")", "originalCommit": "503bdb448c52c7e94efb136162ee907e3f90a13f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg4NjE3Mg==", "url": "https://github.com/apache/druid/pull/10676#discussion_r601886172", "bodyText": "ya so this relates back to your comment on compaction above. It seems that the smart thing to do is add testing since it is supported by default despite my not intentionally supporting it", "author": "capistrant", "createdAt": "2021-03-25T22:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTg0NzYyNg=="}], "type": "inlineReview"}, {"oid": "0fa4f410cedb5a413d03ebb84bf6a618043bed0b", "url": "https://github.com/apache/druid/commit/0fa4f410cedb5a413d03ebb84bf6a618043bed0b", "message": "small changes after review", "committedDate": "2021-03-25T22:54:07Z", "type": "commit"}, {"oid": "367483667d91159c21f7777848ff176f559d0046", "url": "https://github.com/apache/druid/commit/367483667d91159c21f7777848ff176f559d0046", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-03-25T23:09:47Z", "type": "commit"}, {"oid": "8c3a49968413fa433d68b60cab25bf61435dd809", "url": "https://github.com/apache/druid/commit/8c3a49968413fa433d68b60cab25bf61435dd809", "message": "cleanup a little bit after merge with master", "committedDate": "2021-03-25T23:10:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwODEyMw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603508123", "bodyText": "Do we want to remove the retry loop in here https://github.com/apache/druid/pull/10676/files#diff-6dbfd938d89c6c209d36efc181277afa535629677dfe6b5a1710190c2f9d8ee3L308-L311\nif IngestionStatsAndErrorsTaskReport say that the segment Availability was confirmed. If it isn't true on the first attempt, that means the task report was wrong - am I understanding this correctly?", "author": "suneet-s", "createdAt": "2021-03-29T18:11:53Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITBatchIndexTest.java", "diffHunk": "@@ -280,6 +334,18 @@ private void submitTaskAndWait(\n       );\n     }\n \n+    if (segmentAvailabilityConfirmationPair.lhs != null && segmentAvailabilityConfirmationPair.lhs) {\n+      TaskReport reportRaw = indexer.getTaskReport(taskID).get(\"ingestionStatsAndErrors\");\n+      IngestionStatsAndErrorsTaskReport report = (IngestionStatsAndErrorsTaskReport) reportRaw;\n+      IngestionStatsAndErrorsTaskReportData reportData = (IngestionStatsAndErrorsTaskReportData) report.getPayload();\n+      if (segmentAvailabilityConfirmationPair.rhs != null) {\n+        Assert.assertEquals(\n+            Boolean.valueOf(reportData.isSegmentAvailabilityConfirmed()),\n+            segmentAvailabilityConfirmationPair.rhs\n+        );\n+      }\n+    }\n+", "originalCommit": "8c3a49968413fa433d68b60cab25bf61435dd809", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUxNjc1NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603516754", "bodyText": "trying to track down exactly what retry loop you are referring to. When I click that link my browser is just loading at the top of the page so I'm not sure exactly where you are wanting me to look.", "author": "capistrant", "createdAt": "2021-03-29T18:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwODEyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzU5ODUxMw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603598513", "bodyText": "Maybe this link is better https://github.com/apache/druid/blob/master/integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITBatchIndexTest.java#L308-L312\n    if (waitForSegmentsToLoad) {\n      ITRetryUtil.retryUntilTrue(\n          () -> coordinator.areSegmentsLoaded(dataSourceName), \"Segment Load\"\n      );\n    }", "author": "suneet-s", "createdAt": "2021-03-29T20:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwODEyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNDQ2NDQ5MA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r604464490", "bodyText": "hmm, my gut is that no we don't want to remove that without some (potentially large) surgery to the indexing IT suite. I added on my own logic for checking the ingestion report for handoff status. But that is separate from what other tests are using this loop for. I would bet that the two ideas could be merged together more gracefully by replacing waitForSegmentsToLoad by using the new handoff config and waiting for the task to complete and then checking the report and failing if the report doesn't indicate the segments were loaded. Not sure if it is in scope for this PR, especially after we revoked support for the handoff for compaction tasks (unless there are no Compaction IT out there that use waitForSegmentsToLoad=true, I'd have to check).", "author": "capistrant", "createdAt": "2021-03-30T22:09:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwODEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwOTk4OA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r603509988", "bodyText": "Some comments on this pattern since I've seen it across a few tests\n\nDoes this change mean we lose coverage on a missing SEGMENT_AVAIL_TIMEOUT_MILLIS? Since this is the default mode, it would be good to have coverage for that missing change\nDo we have / want tests where this is set to a high enough number that we don't need a re-try loop in the integration tests while waiting for the segments to be available", "author": "suneet-s", "createdAt": "2021-03-29T18:15:02Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITCombiningFirehoseFactoryIndexTest.java", "diffHunk": "@@ -59,13 +61,28 @@ public void testIndexData() throws Exception\n           throw new RuntimeException(e);\n         }\n       };\n+      final Function<String, String> transform = spec -> {\n+        try {\n+          return StringUtils.replace(\n+              spec,\n+              \"%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%\",\n+              jsonMapper.writeValueAsString(\"0\")", "originalCommit": "8c3a49968413fa433d68b60cab25bf61435dd809", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNDQ2ODQwMw==", "url": "https://github.com/apache/druid/pull/10676#discussion_r604468403", "bodyText": "Some comments on this pattern since I've seen it across a few tests\n\nDoes this change mean we lose coverage on a missing SEGMENT_AVAIL_TIMEOUT_MILLIS? Since this is the default mode, it would be good to have coverage for that missing change\n\n\nThere IT tests out there that indirectly test this. By indirectly, I mean that I did not add the tests but the tests do not have the config in the tuningConfig. For instance, https://github.com/capistrant/incubator-druid/blob/batch-ingest-wait-for-handoff/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITIndexerTest.java#L146, is an index job that doesn't add the transform and the underlying spec file does not have the config in it. https://github.com/capistrant/incubator-druid/blob/batch-ingest-wait-for-handoff/integration-tests/src/test/resources/indexer/wikipedia_reindex_task.json#L49\n\n\nDo we have / want tests where this is set to a high enough number that we don't need a re-try loop in the integration tests while waiting for the segments to be available\n\n\ntechnically that retry loop is redundant in the case that the test uses true,true for the Pair object that determines if we should check the ingestion report for true as the value for the handoff. this would be an example of that, https://github.com/capistrant/incubator-druid/blob/batch-ingest-wait-for-handoff/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITIndexerTest.java#L205", "author": "capistrant", "createdAt": "2021-03-30T22:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUwOTk4OA=="}], "type": "inlineReview"}, {"oid": "f6c91680a7fd4b751754e302862d3ad6eb57e1f5", "url": "https://github.com/apache/druid/commit/f6c91680a7fd4b751754e302862d3ad6eb57e1f5", "message": "Fix potential resource leak in AbstractBatchIndexTask", "committedDate": "2021-03-30T18:17:17Z", "type": "commit"}, {"oid": "edc26dddaa0ac2bebd3580041136673c1cca7724", "url": "https://github.com/apache/druid/commit/edc26dddaa0ac2bebd3580041136673c1cca7724", "message": "syntax fix", "committedDate": "2021-03-30T18:55:33Z", "type": "commit"}, {"oid": "7b25321dd2f5eb0f92d3fbb379a57abbc8792f30", "url": "https://github.com/apache/druid/commit/7b25321dd2f5eb0f92d3fbb379a57abbc8792f30", "message": "Add a Compcation TuningConfig type", "committedDate": "2021-03-31T17:23:15Z", "type": "commit"}, {"oid": "9498cb07ba6c83a1a36f57580d42c337dd7fc75f", "url": "https://github.com/apache/druid/commit/9498cb07ba6c83a1a36f57580d42c337dd7fc75f", "message": "add docs stipulating the lack of support by Compaction tasks for the new config", "committedDate": "2021-03-31T21:04:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzQzNDY1NA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r607434654", "bodyText": "I think this change means that compaction jobs that are submitted with a ParallelIndexTuningConfig (using type = index_parallel) will start failing after this change - is this correct?\nIf so, I think instead of introducing this as a breaking change, we can just add the Precondition check that you have in the CompactionTuningConfig  into the constructor for the compaction task.\nIf this isn't a breaking change, I like that there's a separate tuningConfig for compaction tasks, so that in the future, this config can be more easily optimized.\ncc @maytasm Since I've seen you make some improvements around compaction recently", "author": "suneet-s", "createdAt": "2021-04-06T01:55:29Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -146,7 +151,7 @@\n   @Nullable\n   private final ClientCompactionTaskGranularitySpec granularitySpec;\n   @Nullable\n-  private final ParallelIndexTuningConfig tuningConfig;\n+  private final CompactionTuningConfig tuningConfig;", "originalCommit": "7b25321dd2f5eb0f92d3fbb379a57abbc8792f30", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzQzNTcxMA==", "url": "https://github.com/apache/druid/pull/10676#discussion_r607435710", "bodyText": "I take this back. getTuningConfig deals with the different types of tuningConfigs. \ud83e\udd26", "author": "suneet-s", "createdAt": "2021-04-06T01:58:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzQzNDY1NA=="}], "type": "inlineReview"}, {"oid": "d39dfa3869377d36cb1946424b65bef17eeb8182", "url": "https://github.com/apache/druid/commit/d39dfa3869377d36cb1946424b65bef17eeb8182", "message": "Merge branch 'master' into batch-ingest-wait-for-handoff", "committedDate": "2021-04-06T20:21:06Z", "type": "commit"}, {"oid": "747ee337f27efb2f36cad1c2bbe3058646e826b3", "url": "https://github.com/apache/druid/commit/747ee337f27efb2f36cad1c2bbe3058646e826b3", "message": "Fixup compilation errors after merge with master", "committedDate": "2021-04-06T20:41:10Z", "type": "commit"}, {"oid": "381cdfe8d6404eedbc1df01f58235d7f4a9bf515", "url": "https://github.com/apache/druid/commit/381cdfe8d6404eedbc1df01f58235d7f4a9bf515", "message": "Remove erreneous newline", "committedDate": "2021-04-07T01:17:56Z", "type": "commit"}]}