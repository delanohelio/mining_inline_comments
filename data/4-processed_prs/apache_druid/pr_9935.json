{"pr_number": 9935, "pr_title": "optimize announceHistoricalSegments", "pr_createdAt": "2020-05-27T04:51:34Z", "pr_url": "https://github.com/apache/druid/pull/9935", "timeline": [{"oid": "110b9a346632248d0053298d2683f7fcc27ffeb6", "url": "https://github.com/apache/druid/commit/110b9a346632248d0053298d2683f7fcc27ffeb6", "message": "optimize announceHistoricalSegment", "committedDate": "2020-05-27T04:37:07Z", "type": "commit"}, {"oid": "909081e1ee4ed9243b3c490a3197aa5fafce1e94", "url": "https://github.com/apache/druid/commit/909081e1ee4ed9243b3c490a3197aa5fafce1e94", "message": "optimize announceHistoricalSegment", "committedDate": "2020-05-27T06:19:54Z", "type": "commit"}, {"oid": "c6c4230ad8122b3762b25fbbee8104786a3b4484", "url": "https://github.com/apache/druid/commit/c6c4230ad8122b3762b25fbbee8104786a3b4484", "message": "Merge branch 'optimize_announceHistoricalSegment' of github.com:xiangqiao123/druid into optimize_announceHistoricalSegment", "committedDate": "2020-05-27T06:21:54Z", "type": "commit"}, {"oid": "e7061f281f275b752e4d770cd667f345f2504476", "url": "https://github.com/apache/druid/commit/e7061f281f275b752e4d770cd667f345f2504476", "message": "revert offline SegmentTransactionalInsertAction uses a separate lock", "committedDate": "2020-05-27T09:14:07Z", "type": "commit"}, {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "url": "https://github.com/apache/druid/commit/605bc5b32f25ae2a1b88af09a89417ac2450f87f", "message": "optimize segmentExistsBatch: Avoid too many elements in the in condition", "committedDate": "2020-05-28T05:21:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw==", "url": "https://github.com/apache/druid/pull/9935#discussion_r440493023", "bodyText": "Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.", "author": "jihoonson", "createdAt": "2020-06-15T23:02:11Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTQxOQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446461419", "bodyText": "There is a typo in ANNOUNCE_HISTORICAL_SEGMENG_BATCH (SEGMENG). I would recommend to rename to be more clear such as MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "author": "jihoonson", "createdAt": "2020-06-27T00:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3MjI5Ng==", "url": "https://github.com/apache/druid/pull/9935#discussion_r449572296", "bodyText": "The number 100 is just experience.\nVariable name has been changed to MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "author": "xiangqiao123", "createdAt": "2020-07-03T13:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463433", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }", "author": "jihoonson", "createdAt": "2020-06-27T00:49:45Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NjE2NQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446466165", "bodyText": "There is a log.infoSegments method that takes a collection of segments that should be used if logging a potentially large list of segments.", "author": "clintropolis", "createdAt": "2020-06-27T01:14:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NzI2NQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r449577265", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }\n\nModified as required.", "author": "xiangqiao123", "createdAt": "2020-07-03T13:13:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzYyNg==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463626", "bodyText": "This will copy the whole set of toInsertSegments which doesn't seem necessary. Can we use toInsertSegment instead in the below for loop?", "author": "jihoonson", "createdAt": "2020-06-27T00:51:38Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTIwMA==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465200", "bodyText": "This will corrupt the logs too. How about modifying as the below?\n      final List<List<DataSegment>> partitionedSegments = Lists.partition(\n          new ArrayList<>(toInsertSegments),\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\n      );\n\n      PreparedBatch preparedBatch = handle.prepareBatch(\n          StringUtils.format(\n              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n              dbTables.getSegmentsTable(),\n              connector.getQuoteString()\n          )\n      );\n      \n      for (List<DataSegment> partition : partitionedSegments) {\n        for (DataSegment segment : partition) {\n          preparedBatch.add()\n                       .bind(\"id\", segment.getId().toString())\n                       .bind(\"dataSource\", segment.getDataSource())\n                       .bind(\"created_date\", DateTimes.nowUtc().toString())\n                       .bind(\"start\", segment.getInterval().getStart().toString())\n                       .bind(\"end\", segment.getInterval().getEnd().toString())\n                       .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n                       .bind(\"version\", segment.getVersion())\n                       .bind(\"used\", usedSegments.contains(segment))\n                       .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n        }\n        final int[] affectedRows = preparedBatch.execute();\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -> eachAffectedRows == 1);\n        if (succeeded) {\n          log.infoSegments(partition, \"Published segments to DB\");\n        } else {\n          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())\n                                                             .filter(i -> affectedRows[i] != 1)\n                                                             .mapToObj(partition::get)\n                                                             .collect(Collectors.toList());\n          throw new ISE(\n              \"Failed to publish segments to DB: %s\",\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }", "author": "jihoonson", "createdAt": "2020-06-27T01:04:57Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTMxOA==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465318", "bodyText": "Same here. Better to be log.errorSegments(segments, \"Exception inserting segments\");", "author": "jihoonson", "createdAt": "2020-06-27T01:06:09Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            } else {\n+              throw new ISE(\n+                  \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            }\n+          }\n+        }\n       }\n     }\n     catch (Exception e) {\n-      log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), used);\n+      for (DataSegment segment : segments) {\n+        log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), usedSegments.contains(segment));\n+      }", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "835f0d9977764ee9a22799de063de402befc8299", "url": "https://github.com/apache/druid/commit/835f0d9977764ee9a22799de063de402befc8299", "message": "add unit test && Modified according to cr", "committedDate": "2020-07-03T14:23:53Z", "type": "commit"}, {"oid": "a702cd11a406bbf5c63c04d102ed258b527e3704", "url": "https://github.com/apache/druid/commit/a702cd11a406bbf5c63c04d102ed258b527e3704", "message": "Merge branch 'master' into optimize_announceHistoricalSegment", "committedDate": "2020-07-03T14:40:44Z", "type": "commit"}]}