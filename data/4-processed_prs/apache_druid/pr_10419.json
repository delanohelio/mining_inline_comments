{"pr_number": 10419, "pr_title": "Automatically determine numShards for parallel ingestion hash partitioning", "pr_createdAt": "2020-09-22T10:36:56Z", "pr_url": "https://github.com/apache/druid/pull/10419", "timeline": [{"oid": "61f8caa5537b732dacd6ecacb22842152458e1a6", "url": "https://github.com/apache/druid/commit/61f8caa5537b732dacd6ecacb22842152458e1a6", "message": "Automatically determine numShards for parallel ingestion hash partitioning", "committedDate": "2020-09-22T10:26:35Z", "type": "commit"}, {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e", "url": "https://github.com/apache/druid/commit/85e06516fc84377eedf6b948ba2acd9fe946797e", "message": "Fix inspection, tests, coverage", "committedDate": "2020-09-22T20:22:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA3NzYyNg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493077626", "bodyText": "Does it make sense to keep the @VisibleForTesting since this is now public (so that it can be used in PartialDimensionCardinalityTask)?", "author": "ccaominh", "createdAt": "2020-09-22T22:53:32Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions\n   }\n \n   @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n+  public static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTEyNQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185125", "bodyText": "Removed the @VisibleForTesting", "author": "jon-wei", "createdAt": "2020-09-23T04:09:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA3NzYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc0Nw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493080747", "bodyText": "Using \"single_dim\" instead of \"ranged\" in the error message maps better to the naming in the docs", "author": "ccaominh", "createdAt": "2020-09-22T23:02:54Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTE3Mw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185173", "bodyText": "Changed the message to use \"single_dim\"", "author": "jon-wei", "createdAt": "2020-09-23T04:09:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493083500", "bodyText": "What do you think about changing the logging level to reduce the logging amount since this is printed for each row?", "author": "ccaominh", "createdAt": "2020-09-22T23:11:32Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExODExOQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493118119", "bodyText": "I'll remove this, it was a debugging-only message that I don't think should be retained", "author": "jon-wei", "createdAt": "2020-09-23T01:13:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTIwNg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185206", "bodyText": "This has been removed", "author": "jon-wei", "createdAt": "2020-09-23T04:09:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493085031", "bodyText": "What do you think about using HllSketch instead of HyperLogLogCollector since HllSketch provides much more accurate estimates if the cardinality does not exceed the sketch's k value: http://datasketches.apache.org/docs/HLL/HllSketchVsDruidHyperLogLogCollector.html. Using HllSketch also will be more accurate and faster when the partial HLLs are merged.\nUsing HllSketch would mean that the implementation for parallel ingestion is different from the one for sequential ingestion though.", "author": "ccaominh", "createdAt": "2020-09-22T23:15:57Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());\n+\n+      HyperLogLogCollector hllCollector = intervalToCardinalities.computeIfAbsent(", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExNzk2NQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493117965", "bodyText": "Using HllSketch would mean that the implementation for parallel ingestion is different from the one for sequential ingestion though.\n\nThat was my reasoning for using HyperLogLogCollector, but I think it makes sense to change these to use HllSketch. I'll update this one to use HllSketch and a follow-on could be to do the same for IndexTask.", "author": "jon-wei", "createdAt": "2020-09-23T01:13:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTMwNA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185304", "bodyText": "I've updated this to use HllSketch instead", "author": "jon-wei", "createdAt": "2020-09-23T04:09:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493091201", "bodyText": "I believe the logic in PartialHashSegmentGenerateTask.isReady() will need to be adjusted; otherwise if PartialDimensionCardinalityTask runs and grabs the locks here it'll get stuck. For example, PartialRangeSegmentGenerateTask.isReady() does not grab the locks since they're acquired earlier by PartialDimensionDistributionTask.\nI think a good place to add a test would be in HashPartitionMultiPhaseParallelIndexingTest. Currently, its test cases all specify a value of 2 for numShards, so we can add cases with null.", "author": "ccaominh", "createdAt": "2020-09-22T23:36:13Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzExODI0Ng==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493118246", "bodyText": "Thanks, will look into this", "author": "jon-wei", "createdAt": "2020-09-23T01:14:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTgwMg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185802", "bodyText": "I didn't see any failures when I was testing this on a cluster or in unit tests with the current locking (maybe related to the javadoc comment on isReady(): \"This method must be idempotent, as it may be run multiple times per task.\"?).\nI updated PartialHashSegmentGenerateTask.isReady() to skip the lock acquisition if the numShardsOverride is set (indicating that that cardinality phase ran).", "author": "jon-wei", "createdAt": "2020-09-23T04:12:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5OTg4Nw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493899887", "bodyText": "I think acquiring locks here should be fine since it's idempotent. The supervisor task and all its subtasks share the same lock based on their groupId. I actually think it's better to call tryTimeChunkLock() in every subtask since it will make the task fail early when its lock is revoked. Otherwise, the task will fail when it publishes segments which happens at the last stage in batch ingestion.", "author": "jihoonson", "createdAt": "2020-09-23T21:15:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkyMDk4Mw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493920983", "bodyText": "I seem to remember running into the self deadlock issue when implementing #8925, which is why the locks are only acquired in the first phase of the range partitioning subtasks. I don't remember if I discovered this via RangePartitionMultiPhaseParallelIndexingTest or ITPerfectRollupParallelIndexTest, but if the problem isn't showing up in this PR's tests, then that's good.", "author": "ccaominh", "createdAt": "2020-09-23T22:02:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0OTAwNQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493949005", "bodyText": "I wasn't able to reproduce the self deadlock with RangePartitionMultiPhaseParallelIndexingTest or ITPerfectRollupParallelIndexTest in master, so either I misremembered stuff or the issue has been fixed in the meantime.", "author": "ccaominh", "createdAt": "2020-09-23T23:17:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MjUyOQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493092529", "bodyText": "https://github.com/apache/druid/blob/master/docs/ingestion/native-batch.md#hash-based-partitioning needs to be updated to say that numShards is no longer required and also to mention the new partial dimension cardinality task.", "author": "ccaominh", "createdAt": "2020-09-22T23:40:12Z", "path": "core/src/main/java/org/apache/druid/indexer/partitions/HashedPartitionsSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public Integer getNumShards()\n   @Override\n   public String getForceGuaranteedRollupIncompatiblityReason()\n   {\n-    return getNumShards() == null ? NUM_SHARDS + \" must be specified\" : FORCE_GUARANTEED_ROLLUP_COMPATIBLE;\n+    return FORCE_GUARANTEED_ROLLUP_COMPATIBLE;", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NTkxMg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493185912", "bodyText": "Updated the docs to mark numShards as optional and added a description of the new cardinality scan phase.", "author": "jon-wei", "createdAt": "2020-09-23T04:12:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MjUyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NDc3NA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493094774", "bodyText": "Checking isForceGuaranteedRollup() is probably redundant here", "author": "ccaominh", "createdAt": "2020-09-22T23:47:41Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",\n+          ingestionSchema.getTuningConfig().getPartitionsSpec()\n+      );\n+    }\n+\n+    final Integer numShardsOverride;\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n+    if (ingestionSchema.getTuningConfig().isForceGuaranteedRollup() && partitionsSpec.getNumShards() == null) {", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4NjAwMQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493186001", "bodyText": "Removed the isForceGuaranteedRollup check here", "author": "jon-wei", "createdAt": "2020-09-23T04:12:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NDc3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NTk0MQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493095941", "bodyText": "I like the extensive set of tests!", "author": "ccaominh", "createdAt": "2020-09-22T23:51:39Z", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTaskTest.java", "diffHunk": "@@ -0,0 +1,398 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import org.apache.druid.client.indexing.NoopIndexingServiceClient;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.data.input.impl.InlineInputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskState;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.DynamicPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.PartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskInfoProvider;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.stats.DropwizardRowIngestionMetersFactory;\n+import org.apache.druid.indexing.common.task.IndexTaskClientFactory;\n+import org.apache.druid.java.util.common.Intervals;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.UniformGranularitySpec;\n+import org.apache.druid.testing.junit.LoggerCaptureRule;\n+import org.apache.logging.log4j.core.LogEvent;\n+import org.easymock.Capture;\n+import org.easymock.EasyMock;\n+import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n+import org.joda.time.Interval;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+@RunWith(Enclosed.class)\n+public class PartialDimensionCardinalityTaskTest", "originalCommit": "85e06516fc84377eedf6b948ba2acd9fe946797e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1ff17937ae0bce578d0e3c12dd36eb489c7bc884", "url": "https://github.com/apache/druid/commit/1ff17937ae0bce578d0e3c12dd36eb489c7bc884", "message": "Docs and some PR comments", "committedDate": "2020-09-23T02:12:01Z", "type": "commit"}, {"oid": "0125a564bb64478ef5ef0a5939df835506865dec", "url": "https://github.com/apache/druid/commit/0125a564bb64478ef5ef0a5939df835506865dec", "message": "Adjust locking", "committedDate": "2020-09-23T03:23:24Z", "type": "commit"}, {"oid": "66365b5eb3d90c30788c3e076e4a2fcaad1acc11", "url": "https://github.com/apache/druid/commit/66365b5eb3d90c30788c3e076e4a2fcaad1acc11", "message": "Use HllSketch instead of HyperLogLogCollector", "committedDate": "2020-09-23T04:08:43Z", "type": "commit"}, {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "url": "https://github.com/apache/druid/commit/6d694d2113a9bc55d981f3ec99620b007ef25e44", "message": "Fix tests", "committedDate": "2020-09-23T04:22:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzM5MTU4OA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493391588", "bodyText": "how about using 0 instead here?", "author": "abhishekagarwal87", "createdAt": "2020-09-23T09:54:18Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzU2NA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977564", "bodyText": "Changed this to 0", "author": "jon-wei", "createdAt": "2020-09-24T00:50:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzM5MTU4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQxNzk3MA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493417970", "bodyText": "out of curiosity, what is the effect if we pass jsonMapper.writeValueAsBytes(groupKey) directly to hllSketch? does it affect performance or accuracy in any way? hllSketch is already doing hashing on the byte array input.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T10:22:36Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),\n+          inputRow\n+      );\n+\n+      try {\n+        hllSketch.update(\n+            IndexTask.HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(groupKey)).asBytes()", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzY4Ng==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977686", "bodyText": "Good point, I got rid of the first level of hashing there, it should be more accurate this way", "author": "jon-wei", "createdAt": "2020-09-24T00:51:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQxNzk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493875893", "bodyText": "Please add the new parameter and its description.", "author": "jihoonson", "createdAt": "2020-09-23T20:29:52Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkzOTQyNg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493939426", "bodyText": "I don't think I added any new parameters to HashedPartitionsSpec, were you referring to something else?", "author": "jon-wei", "createdAt": "2020-09-23T22:55:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0MDUxNw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493940517", "bodyText": "Oh, I should have been more specific. I meant, the newly supported parameter, maxRowsPerSegment (or targetRowsPerSegment which I suggested below). Neither of them is not described here since we didn't support it before. Maybe https://github.com/apache/druid/blob/master/docs/ingestion/hadoop.md#hash-based-partitioning helps.", "author": "jihoonson", "createdAt": "2020-09-23T22:58:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NzczMg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977732", "bodyText": "Added docs for targetRowsPerSegment", "author": "jon-wei", "createdAt": "2020-09-24T00:51:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MTQ2Ng==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493891466", "bodyText": "It seems like that it will guarantee that you will not have segments than the computed numShards, but it will not be guaranteed that number of rows per segment doesn't exceed maxRowsPerSegment since the partition dimensions can be skewed. Is this correct? Then, I suggest targetRowsPerSegment since it's not a hard limit.", "author": "jihoonson", "createdAt": "2020-09-23T20:58:59Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.\n+- The `partial_dimension_cardinality` phase is an optional phase that only runs if `numShards` is not specified.\n+The Parallel task splits the input data and assigns them to worker tasks based on the split hint spec.\n+Each worker task (type `partial_dimension_cardinality`) gathers estimates of partitioning dimensions cardinality for\n+each time chunk. The Parallel task will aggregate these estimates from the worker tasks and determine the highest\n+cardinality across all of the time chunks in the input data, dividing this cardinality by `maxRowsPerSegment` to", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3Nzc5Mg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977792", "bodyText": "I went with targetRowsPerSegment", "author": "jon-wei", "createdAt": "2020-09-24T00:51:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MTQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NDkwNQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493894905", "bodyText": "Hmm.. Should we fail instead? Since the timeline in the coordinator and the broker will explode if you have this many segments per interval.", "author": "jihoonson", "createdAt": "2020-09-23T21:05:39Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;\n+    for (Union union : finalCollectors.values()) {\n+      maxCardinality = Math.max(maxCardinality, (long) union.getEstimate());\n+    }\n+\n+    LOG.info(\"Estimated max cardinality: \" + maxCardinality);\n+\n+    // determine numShards based on maxRowsPerSegment and the highest per-interval cardinality\n+    long numShards = maxCardinality / maxRowsPerSegment;\n+    if (maxCardinality % maxRowsPerSegment != 0) {\n+      // if there's a remainder add 1 so we stay under maxRowsPerSegment\n+      numShards += 1;\n+    }\n+    try {\n+      return Math.toIntExact(numShards);\n+    }\n+    catch (ArithmeticException ae) {\n+      return Integer.MAX_VALUE;", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3Nzk0Nw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493977947", "bodyText": "I changed this to throw an exception now", "author": "jon-wei", "createdAt": "2020-09-24T00:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NDkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NjY0Mw==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493896643", "bodyText": "When I wrote this, my intention was showing the name of phases (e.g., https://github.com/apache/druid/pull/10419/files#diff-05bbc55d565a3d9462353d9b4771cb09R34). This phase name is not shown anywhere currently, but will be available in the task live reports and metrics after #10352. How about removing underscores from the here too?", "author": "jihoonson", "createdAt": "2020-09-23T21:09:08Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODI0NA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978244", "bodyText": "Removed the underscores here", "author": "jon-wei", "createdAt": "2020-09-24T00:53:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NjY0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMTY0OA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493901648", "bodyText": "This timestamp should be bucketed based on the query granularity. See https://github.com/apache/druid/blob/master/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java#L146.", "author": "jihoonson", "createdAt": "2020-09-23T21:19:17Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODYwMg==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978602", "bodyText": "Ah, thanks, I fixed this and updated PartialDimensionCardinalityTaskTest.sendsCorrectReportWithMultipleIntervalsInData() to test that case", "author": "jon-wei", "createdAt": "2020-09-24T00:54:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMTY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMjI3OQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493902279", "bodyText": "nit: inputRow is safely non-null since FilteringCloseableInputRowIterator filters out all null rows. See AbstractBatchIndexTask.defaultRowFilter().", "author": "jihoonson", "createdAt": "2020-09-23T21:20:37Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODA0NA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978044", "bodyText": "Removed the null check", "author": "jon-wei", "createdAt": "2020-09-24T00:52:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMjI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMzc4NA==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493903784", "bodyText": "nit: DefaultIndexTaskInputRowIteratorBuilder effectively does nothing here since its core functionality has been moved to FilteringCloseableInputRowIterator in #10336. I haven't cleaned up this interface yet.  Now, it's only useful in range partitioning as some more useful inputRowHandlers are appended to the default builder.", "author": "jihoonson", "createdAt": "2020-09-23T21:23:56Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()", "originalCommit": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODE5MQ==", "url": "https://github.com/apache/druid/pull/10419#discussion_r493978191", "bodyText": "I changed this to just use the iterator above this directly", "author": "jon-wei", "createdAt": "2020-09-24T00:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMzc4NA=="}], "type": "inlineReview"}, {"oid": "ee473864db6caeb7582ae692393afd7025e6f1f7", "url": "https://github.com/apache/druid/commit/ee473864db6caeb7582ae692393afd7025e6f1f7", "message": "Address some PR comments", "committedDate": "2020-09-24T00:26:33Z", "type": "commit"}, {"oid": "5e792972c9ae40b0380040774898afa582242e15", "url": "https://github.com/apache/druid/commit/5e792972c9ae40b0380040774898afa582242e15", "message": "Fix granularity bug", "committedDate": "2020-09-24T00:50:08Z", "type": "commit"}, {"oid": "b424712ec01fd9b4495daa22ef20c2300d493e2b", "url": "https://github.com/apache/druid/commit/b424712ec01fd9b4495daa22ef20c2300d493e2b", "message": "Small doc fix", "committedDate": "2020-09-24T00:52:55Z", "type": "commit"}]}