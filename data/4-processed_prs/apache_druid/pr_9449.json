{"pr_number": 9449, "pr_title": "Add Sql InputSource", "pr_createdAt": "2020-03-02T21:31:01Z", "pr_url": "https://github.com/apache/druid/pull/9449", "timeline": [{"oid": "64b24758bb91ff2174e80e24cd068b3987cc2ca3", "url": "https://github.com/apache/druid/commit/64b24758bb91ff2174e80e24cd068b3987cc2ca3", "message": "Add Sql InputSource", "committedDate": "2020-03-02T21:26:06Z", "type": "commit"}, {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55", "url": "https://github.com/apache/druid/commit/61f62fe01a64534a485b0f8d9176ca6f18186b55", "message": "Add spelling", "committedDate": "2020-03-02T22:16:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r416549940", "bodyText": "can this be part of new module as InputSource seems to replacement for firehose related interfaces ?", "author": "pjain1", "createdAt": "2020-04-28T11:50:33Z", "path": "server/src/main/java/org/apache/druid/guice/FirehoseModule.java", "diffHunk": "@@ -58,7 +59,8 @@ public void configure(Binder binder)\n                 new NamedType(CombiningFirehoseFactory.class, \"combining\"),\n                 new NamedType(FixedCountFirehoseFactory.class, \"fixedCount\"),\n                 new NamedType(SqlFirehoseFactory.class, \"sql\"),\n-                new NamedType(InlineFirehoseFactory.class, \"inline\")\n+                new NamedType(InlineFirehoseFactory.class, \"inline\"),\n+                new NamedType(SqlInputSource.class, \"sql\")", "originalCommit": "61f62fe01a64534a485b0f8d9176ca6f18186b55", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM0OTIxNQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420349215", "bodyText": "Thanks. Extracted it to a separate module.", "author": "a2l007", "createdAt": "2020-05-05T19:20:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM1MzE4Nw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420353187", "bodyText": "Thanks, however I would have just called it InputSourceModule and made it available to all processes by adding it in makeInjectorWithModules of Initialization.java similar to FirehoseModule.", "author": "pjain1", "createdAt": "2020-05-05T19:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM2MTQ1NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420361454", "bodyText": "I can refactor the module name, but I believe this module wouldn't be needed in the broker, historical and router processes since its specific to indexing.", "author": "a2l007", "createdAt": "2020-05-05T19:42:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUzNTI0Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420535242", "bodyText": "ok", "author": "pjain1", "createdAt": "2020-05-06T04:11:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}], "type": "inlineReview"}, {"oid": "23a7c414b97cad0c871de331a8339c22333ace0c", "url": "https://github.com/apache/druid/commit/23a7c414b97cad0c871de331a8339c22333ace0c", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-05T16:24:30Z", "type": "commit"}, {"oid": "d1185e49933fea9baf4e4b28c8d43417e7c543b0", "url": "https://github.com/apache/druid/commit/d1185e49933fea9baf4e4b28c8d43417e7c543b0", "message": "Use separate DruidModule", "committedDate": "2020-05-05T19:18:46Z", "type": "commit"}, {"oid": "7f9743b112d910d50e6348ddb098dd1532650027", "url": "https://github.com/apache/druid/commit/7f9743b112d910d50e6348ddb098dd1532650027", "message": "Change module name", "committedDate": "2020-05-05T20:05:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTQ1Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421819452", "bodyText": "\ud83d\udc4d", "author": "jihoonson", "createdAt": "2020-05-07T21:59:17Z", "path": "core/src/main/java/org/apache/druid/java/util/common/parsers/CloseableIterator.java", "diffHunk": "@@ -66,10 +66,10 @@ public void close() throws IOException\n \n     return new CloseableIterator<R>()\n     {\n-      CloseableIterator<R> iterator = findNextIeteratorIfNecessary();\n+      CloseableIterator<R> iterator = findNextIteratorIfNecessary();", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMDU0Ng==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421820546", "bodyText": "Maybe worth mentioning one more time that these SQLs are executed in two sub tasks when you run a Parallel task.", "author": "jihoonson", "createdAt": "2020-05-07T22:01:53Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate sqls\n+within the interval `2013-01-01/2013-01-02`.", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTA5Nw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421821097", "bodyText": "sqls should be properly capitalized as SQLs.", "author": "jihoonson", "createdAt": "2020-05-07T22:03:19Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate sqls", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNjQxOQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421826419", "bodyText": "Could you add a comment on why we fetch all result in local storage first? I remember this is to avoid holding database connections for too long time. It would help other developers.", "author": "jihoonson", "createdAt": "2020-05-07T22:16:47Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNzAzNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421827037", "bodyText": "nit: this doesn't have to be done in this PR, but how about making JsonIterator a CloseableIterator? It already implements Iterator and Closeable so it would be pretty simple.", "author": "jihoonson", "createdAt": "2020-05-07T22:18:31Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNjM1OA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421836358", "bodyText": "Would you add more detailed docs for this parameter? It should probably mention that you have to load some extension to read from a particular type of database.", "author": "jihoonson", "createdAt": "2020-05-07T22:44:20Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details.|Yes|", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODI1OA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421838258", "bodyText": "One more thing, I remember that many people from our community have been asking about how to use SqlFirehose. What do you think about adding a section that explains how to use it in production environment? To be honest, it's not clear for me what are best practices to make a scalable and efficient pipeline using this input source. For example, how do you parallelize each ingestion task (which means, how do you split queries)? How do you handle data updates in database especially after ingestion job is done? How often do you run ingestion jobs? and so on.", "author": "jihoonson", "createdAt": "2020-05-07T22:49:37Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,43 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### Sql Input Source", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MjUyNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432592527", "bodyText": "@jihoonson I have added more details related to this in the docs. Please take a look. Since this is a one time ingestion from rdbms, updates to the source db data for a specific interval would require a new sql ingestion task be spawned for the same interval which will replace the old segments.\nBy the way, I would like to add a continuous ingestion from RDBMS feature as well which could use the indexing service with a RDBMS supervisor. Do you think this would be a useful feature?", "author": "a2l007", "createdAt": "2020-05-29T16:15:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTk1NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899954", "bodyText": "I would definitely vote for having such a supervisor! That will be super useful.", "author": "jihoonson", "createdAt": "2020-05-31T01:07:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODI1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNDc4MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421834780", "bodyText": "Why did you choose this package for the sql ingestion classes?\nOther implementations of InputSource lives under druid-core not druid-server.\nAnd while we're on the subject of packages - have you thought about making this an extension?", "author": "suneet-s", "createdAt": "2020-05-07T22:39:54Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzE2NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593164", "bodyText": "This feature has a dependency on the SQLFirehoseDatabaseConnector which resides in druid-server. This can be refactored to druid-core in a follow up PR as it might involve changes to the SqlFirehoseFactory  as well.\nIn order to use this InputSource, it has to be paired with a metadata storage extension that provides connectorConfig support. Making this itself an extension would require multiple extensions to be loaded for a single InputSource, which may not be user-friendly.", "author": "a2l007", "createdAt": "2020-05-29T16:17:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNDc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNTIzMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421835231", "bodyText": "javadocs please - same on all the new classes. Also this module name sounds very broad - have you thought about scoping this to a SqlInputModule ? Once we know the right package structure, I think the name of the module will make more sense.", "author": "suneet-s", "createdAt": "2020-05-07T22:41:12Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzQ2MQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593461", "bodyText": "This module is intended to support additional input sources in the future such as replacements for ClippedFirehoseFactory and CombiningFirehoseFactory, which is why the module name is rather broad.", "author": "a2l007", "createdAt": "2020-05-29T16:17:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNTIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421837636", "bodyText": "Can you write a ModuleTest that verifies all the injections are made correctly?", "author": "suneet-s", "createdAt": "2020-05-07T22:47:51Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule\n+{\n+  @Override\n+  public List<? extends Module> getJacksonModules()", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzcyNQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593725", "bodyText": "I believe it is being tested in SqlInputSourceTest.\nCould you please provide an example of what type of test you\u2019re looking for?", "author": "a2l007", "createdAt": "2020-05-29T16:18:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2Njc0NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434766745", "bodyText": "I was thinking of a test where we verify that the \"sql\" named type is registered to the SqlInputSource class correctly. Here's an example of a module test I've added for a custom extension https://github.com/implydata/indexed-table-loader/blob/master/src/test/java/io/imply/druid/indextable/loader/IndexedTableLoaderDruidModuleTest.java", "author": "suneet-s", "createdAt": "2020-06-03T18:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODY5Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421838692", "bodyText": "Why can't open() do this for you? Callers of the interface won't know about the inner workings of each implementation, so if we have a remediation, we should do this automatically.\nInject a Supplier in to this class so you can get a temp dir, this will also make it easier to test.", "author": "suneet-s", "createdAt": "2020-05-07T22:50:55Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NDA2Ng==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432594066", "bodyText": "The contract for InputStream.open defines that it needs to be an InputStream directly on the input entity. Since, there isn\u2019t a direct way to do this and in the interest of not maintaining the sql connection open for a long time, SqlInputSource isn\u2019t supporting open().", "author": "a2l007", "createdAt": "2020-05-29T16:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzOTY4Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421839682", "bodyText": "Is this possible? The annotations indicate that this should always be NonNull\nIf this can be null, then I think it should be checked at the constructor - right now, there is the possiblilty that we create temp files that are never cleaned up if an exception is thrown on this line.", "author": "suneet-s", "createdAt": "2020-05-07T22:53:56Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MDYxMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421840611", "bodyText": "Can you add more comments in here for what this is trying to do", "author": "suneet-s", "createdAt": "2020-05-07T22:56:46Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjEwNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842107", "bodyText": "Are there any size restrictions here? What happens if I try to ingest a very large sql output? How big do the indexing machines need to be? How long can the db connection be kept open so that we can keep writing records to the temp file? Are there any logs that indicate what I need to do operationally to support this?", "author": "suneet-s", "createdAt": "2020-05-07T23:01:17Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NDQzMw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432594433", "bodyText": "Yeah this is something that I\u2019d like to fix in a follow up PR. There isn\u2019t a way to restrict the file sizes presently  and I feel size limiting on fetched files should be supported at the InputEntity interface level. I have added some info in the docs related to the size limit for now.", "author": "a2l007", "createdAt": "2020-05-29T16:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0Mjg5Mw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842893", "bodyText": "Do we need null checks on all the json provided fields? Or is that handled by some annotation I'm not seeing?\nWhat is the default boolean value if foldCase is not specified?", "author": "suneet-s", "createdAt": "2020-05-07T23:03:52Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NjAzMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432596031", "bodyText": "Added null checks. Default for foldcase is false", "author": "a2l007", "createdAt": "2020-05-29T16:22:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0Mjg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MzYxNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421843617", "bodyText": "EqualsVerifier test please for equals and hashcode", "author": "suneet-s", "createdAt": "2020-05-07T23:06:05Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);\n+  }\n+\n+  @Override\n+  public int estimateNumSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.size();\n+  }\n+\n+  @Override\n+  public SplittableInputSource<String> withSplit(InputSplit<String> split)\n+  {\n+    return new SqlInputSource(\n+        Collections.singletonList(split.get()),\n+        foldCase,\n+        sqlFirehoseDatabaseConnector,\n+        objectMapper\n+    );\n+  }\n+\n+  @Override\n+  protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nullable File temporaryDirectory)\n+  {\n+    final SqlInputFormat inputFormat = new SqlInputFormat(objectMapper);\n+    return new InputEntityIteratingReader(\n+        inputRowSchema,\n+        inputFormat,\n+        createSplits(inputFormat, null)\n+            .map(split -> new SqlEntity(split.get(), sqlFirehoseDatabaseConnector, foldCase, objectMapper)).iterator(),\n+        temporaryDirectory\n+    );\n+  }\n+\n+  @Override\n+  public boolean needsFormat()\n+  {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    SqlInputSource that = (SqlInputSource) o;\n+    return foldCase == that.foldCase &&\n+           sqls.equals(that.sqls) &&\n+           sqlFirehoseDatabaseConnector.equals(that.sqlFirehoseDatabaseConnector) &&\n+           objectMapper.equals(that.objectMapper);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(sqls, sqlFirehoseDatabaseConnector, objectMapper, foldCase);\n+  }", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0OTI5Mw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421849293", "bodyText": "Can you add unit tests for this class please - same with SqlInputFormat", "author": "suneet-s", "createdAt": "2020-05-07T23:23:29Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()\n+    {\n+      @Override\n+      public File file()\n+      {\n+        return tempFile;\n+      }\n+\n+      @Override\n+      public void close()\n+      {\n+        if (!tempFile.delete()) {\n+          LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+        }\n+      }\n+    };\n+  }", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "url": "https://github.com/apache/druid/commit/b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-25T12:52:52Z", "type": "commit"}, {"oid": "633323490f39fc279a874ae72d549c6e34762211", "url": "https://github.com/apache/druid/commit/633323490f39fc279a874ae72d549c6e34762211", "message": "Fix docs", "committedDate": "2020-05-28T22:18:49Z", "type": "commit"}, {"oid": "af4d5fab2495c0ee7c461d693053a2fa38c15f12", "url": "https://github.com/apache/druid/commit/af4d5fab2495c0ee7c461d693053a2fa38c15f12", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-28T22:18:54Z", "type": "commit"}, {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "url": "https://github.com/apache/druid/commit/0328eaded9f4b85649316bfca72a79e5b7ed1db9", "message": "Use sqltestutils for tests", "committedDate": "2020-05-29T16:13:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTQwMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899400", "bodyText": "Where is the interval 2013-01-01/2013-01-02 from?", "author": "jihoonson", "createdAt": "2020-05-31T00:55:19Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs within the interval `2013-01-01/2013-01-02`.", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTY3NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899674", "bodyText": "I'm not sure what it means by \"avoid unwanted data being retrieved and stored locally\". Does this mean the subtask can modify the sql to filter out data out of the interval in the granularitySpec? Would you point me out where it is implemented?", "author": "jihoonson", "createdAt": "2020-05-31T01:00:54Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs within the interval `2013-01-01/2013-01-02`.\n+Each of the SQL queries will be run in its own sub-task and thus for the above example, there would be two sub-tasks.\n+\n+Compared to the other native batch InputSources, SQL InputSource behaves differently in terms of reading the input data and so it would be helpful to consider the following points before using this InputSource in a production environment:\n+\n+* During indexing, each sub-task would execute one of the SQL queries and the results are stored locally on disk. The sub-tasks then proceed to read the data from these local input files and generate segments. Presently, there isn\u2019t any restriction on the size of the generated files and this would require the MiddleManagers or Indexers to have sufficient disk capacity based on the volume of data being indexed.\n+\n+* Filtering the SQL queries based on the intervals specified in the `granularitySpec` can avoid unwanted data being retrieved and stored locally by the indexing sub-tasks.", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUyNDAzNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r433524037", "bodyText": "No, I meant to say that the SQL queries should have date time range based WHERE clauses with the same interval as the interval specified in the granularitySpec. If the query doesnt have date time based filters, the query would be pulling data outside the required intervals, which is unwanted. Hope it is clear now. I have added an example as well.", "author": "a2l007", "createdAt": "2020-06-01T22:30:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTY3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUyNTMzNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r433525337", "bodyText": "That makes sense \ud83d\ude42", "author": "jihoonson", "createdAt": "2020-06-01T22:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTgwOA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899808", "bodyText": "The tempFile will not be deleted if an exception is thrown in any lines above. We should catch all exceptions and delete the file properly.", "author": "jihoonson", "createdAt": "2020-05-31T01:04:07Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTg5NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899895", "bodyText": "Thanks for making the JsonIterator a CloseableIterator. Now you can return jsonIterator directly and remove this.", "author": "jihoonson", "createdAt": "2020-05-31T01:05:59Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Reader exclusively for {@link SqlEntity}\n+ */\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    //The results are fetched into local storage as this avoids having to keep a persistent database connection for a long time\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()\n+    {\n+    }, inputStream, closer, objectMapper);\n+    return new CloseableIterator<Map<String, Object>>()", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjkwMDIyMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432900220", "bodyText": "Please move jg to the above try-resource clause so that i can be closed safely.", "author": "jihoonson", "createdAt": "2020-05-31T01:12:53Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "03de8c0e6a40c61ed9e9399766bc633886d4682a", "url": "https://github.com/apache/druid/commit/03de8c0e6a40c61ed9e9399766bc633886d4682a", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-01T13:41:38Z", "type": "commit"}, {"oid": "62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "url": "https://github.com/apache/druid/commit/62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "message": "Add additional tests", "committedDate": "2020-06-01T20:16:21Z", "type": "commit"}, {"oid": "7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "url": "https://github.com/apache/druid/commit/7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-01T20:16:33Z", "type": "commit"}, {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "url": "https://github.com/apache/druid/commit/53cb148570ccb48903a3f6d587287bdc9220f0fc", "message": "Fix inspection", "committedDate": "2020-06-01T21:14:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434759392", "bodyText": "Can we mark this as a big warning in the docs? I think a similar warning should be made on line 1316 indicating that this functionality is experimental and not yet recommended for production use.", "author": "suneet-s", "createdAt": "2020-06-03T18:09:05Z", "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -1310,6 +1311,56 @@ A spec that applies a filter and reads a subset of the original datasource's col\n This spec above will only return the `page`, `user` dimensions and `added` metric.\n Only rows where `page` = `Druid` will be returned.\n \n+### SQL Input Source\n+\n+The SQL input source is used to read data directly from RDBMS.\n+The SQL input source is _splittable_ and can be used by the [Parallel task](#parallel-task), where each worker task will read from one SQL query from the list of queries.\n+Since this input source has a fixed input format for reading events, no `inputFormat` field needs to be specified in the ingestion spec when using this input source.\n+\n+|property|description|required?|\n+|--------|-----------|---------|\n+|type|This should be \"sql\".|Yes|\n+|database|Specifies the database connection details. The database type corresponds to the extension that supplies the `connectorConfig` support and this extension must be loaded into Druid. For database types `mysql` and `postgresql`, the `connectorConfig` support is provided by [mysql-metadata-storage](../development/extensions-core/mysql.md) and [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extensions respectively.|Yes|\n+|foldCase|Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results.|No|\n+|sqls|List of SQL queries where each SQL query would retrieve the data to be indexed.|Yes|\n+\n+An example SqlInputSource spec is shown below:\n+\n+```json\n+...\n+    \"ioConfig\": {\n+      \"type\": \"index_parallel\",\n+      \"inputSource\": {\n+        \"type\": \"sql\",\n+        \"database\": {\n+            \"type\": \"mysql\",\n+            \"connectorConfig\": {\n+                \"connectURI\": \"jdbc:mysql://host:port/schema\",\n+                \"user\": \"user\",\n+                \"password\": \"password\"\n+            }\n+        },\n+        \"sqls\": [\"SELECT * FROM table1 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\", \"SELECT * FROM table2 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\"]\n+    },\n+...\n+```\n+\n+The spec above will read all events from two separate SQLs for the interval `2013-01-01/2013-01-02`.\n+Each of the SQL queries will be run in its own sub-task and thus for the above example, there would be two sub-tasks.\n+\n+Compared to the other native batch InputSources, SQL InputSource behaves differently in terms of reading the input data and so it would be helpful to consider the following points before using this InputSource in a production environment:", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMjA3MQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435422071", "bodyText": "Is this necessary? This inputsource has been used in a couple of production type environments internally and as long as the indexer is allocated enough disk space by the cluster operator, it shouldnt run into any issues.\nI've added some text asking the user to review these points before using the input source.", "author": "a2l007", "createdAt": "2020-06-04T17:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzODcyMw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435638723", "bodyText": "I wouldn't say the SqlInputSource is an experimental. The experimental feature in Druid means either 1) the feature hasn't been stabilized yet and there could be potential bugs while using it or 2) the feature might be stable but can be removed in the future since we know there is a better way. In this case, I believe there will be a better way for ingesting from databases such as the SqlSupervisor mentioned here, but even in a better way, we can build the SqlSupervisor on top of the SqlInputSource (probably the supervisor can use it). Also, we have been providing the SqlFirehose as a non-experimental feature.", "author": "jihoonson", "createdAt": "2020-06-05T01:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTIzMw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435641233", "bodyText": "Maybe instead of experimental, we should call this feature Beta. It indicates that there is future work coming and we may want to change the behavior or configuration of the InputSource, so users shouldn't rely on the API / behavior being consistent across releases.\nSince there are no integration tests, I'm concerned about calling this GA, because someone would have to run through these tests manually before each release to make sure we did not accidentally break the SqlInputSource", "author": "suneet-s", "createdAt": "2020-06-05T01:25:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NTYwOA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435645608", "bodyText": "Maybe instead of experimental, we should call this feature Beta. It indicates that there is future work coming and we may want to change the behavior or configuration of the InputSource, so users shouldn't rely on the API / behavior being consistent across releases.\n\nI agree that more fine-grained feature lifecycle would be nice, but it should be discussed separately in the dev mailing list. For APIs/behaviors, all Druid InputSources are UnstableApi which may change in breaking ways at any time even between minor Druid release lines.\n\nSince there are no integration tests, I'm concerned about calling this GA, because someone would have to run through these tests manually before each release to make sure we did not accidentally break the SqlInputSource\n\nSince all inputSources are UnstableApis, I wouldn't call any of them GA. Integration tests sound nice, we should add them in the near future.", "author": "jihoonson", "createdAt": "2020-06-05T01:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1OTM5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODU4Nw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434778587", "bodyText": "Sorry I missed this earlier: Why did we chose json serialization? What happens if the object returned from the result set can not be serialized by the json generator?", "author": "suneet-s", "createdAt": "2020-06-03T18:43:20Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMjQ5MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435422490", "bodyText": "The design was made to align with one of the multiple use cases where events from json sources were imported into RDBMS and this inputsource would be used to index off of that. The data types supported by file formats would work with this Inputsource as well. Going forward I\u2019d like to take a stab at evaluating if this result file can be hooked up to read from one of the TextReader implementations. The tricky part right now is that the intermediate row parsing logic for SqlReader is completely different compared to the TextReader one.", "author": "a2l007", "createdAt": "2020-06-04T17:21:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODU4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4MDIwMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434780200", "bodyText": "It looks like this logic is duplicated from SQLMetadataStorageActionHandler#isStatementException. Can you consolidate them please", "author": "suneet-s", "createdAt": "2020-06-03T18:46:15Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NDQxMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434784410", "bodyText": "This breaks the contract of HashMap which allows the key to be null. StringUtils.toLowerCase(null) will throw an NPE", "author": "suneet-s", "createdAt": "2020-06-03T18:54:06Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+      return new CleanableFile()\n+      {\n+        @Override\n+        public File file()\n+        {\n+          return tempFile;\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+          if (!tempFile.delete()) {\n+            LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+          }\n+        }\n+      };\n+    }\n+    catch (Exception e) {\n+      if (!tempFile.delete()) {\n+        LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private static class CaseFoldedMap extends HashMap<String, Object>\n+  {\n+    public static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public Object get(Object obj)\n+    {\n+      return super.get(StringUtils.toLowerCase((String) obj));", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NzI4NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434787284", "bodyText": "Does this need to check either inputFormat or splitHintSpec to create the Stream of InputSplits? SImilar comment for function below. Not sure if I understand how these should be used.", "author": "suneet-s", "createdAt": "2020-06-03T18:59:25Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMjgzNg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435422836", "bodyText": "This inputsource does not support custom input formats as defined in SqlInputSource.needsFormat(). The splits are performed based on the number of sql queries and therefore the splitHintSpec is not used as well.", "author": "a2l007", "createdAt": "2020-06-04T17:21:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NzI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434796895", "bodyText": "Why do we need a smile object mapper instead of just the default object mapper", "author": "suneet-s", "createdAt": "2020-06-03T19:17:29Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMzg0OA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435423848", "bodyText": "Smile would be a better option here as it is more compact and would have less overhead while the query results are written to file.", "author": "a2l007", "createdAt": "2020-06-04T17:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MDQwMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435640400", "bodyText": "ah yeah this makes sense. I saw the constructors in SqlEntity and SqlInputFormat use an ObjectMapper that wasn't annotated with @Smile so I was confused why only the InputSource needed a smile object mapper. It's because this is the only guice injected ObjectMapper, the other constructors are not guicified, so they also use the smile ObjectMapper", "author": "suneet-s", "createdAt": "2020-06-05T01:21:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ=="}], "type": "inlineReview"}, {"oid": "eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "url": "https://github.com/apache/druid/commit/eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "message": "Add module test", "committedDate": "2020-06-04T17:18:29Z", "type": "commit"}, {"oid": "dd9d5bb96c010790725c081ae2d0efab15ae49b0", "url": "https://github.com/apache/druid/commit/dd9d5bb96c010790725c081ae2d0efab15ae49b0", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-04T17:18:54Z", "type": "commit"}, {"oid": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "url": "https://github.com/apache/druid/commit/fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "message": "Fix md in docs", "committedDate": "2020-06-04T18:09:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU5MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435641590", "bodyText": "nit: @VisibleForTesting is no longer true.", "author": "suneet-s", "createdAt": "2020-06-05T01:26:38Z", "path": "server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java", "diffHunk": "@@ -174,7 +174,7 @@ public void insert(\n   }\n \n   @VisibleForTesting", "originalCommit": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "08b662647de8ad752194399561ff5102df492547", "url": "https://github.com/apache/druid/commit/08b662647de8ad752194399561ff5102df492547", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-05T13:27:37Z", "type": "commit"}, {"oid": "9550f53ef3616ad7cb996a60782b462a706e3e07", "url": "https://github.com/apache/druid/commit/9550f53ef3616ad7cb996a60782b462a706e3e07", "message": "Remove annotation", "committedDate": "2020-06-05T14:07:46Z", "type": "commit"}]}