{"pr_number": 9819, "pr_title": "refactor SeekableStreamSupervisor usage of RecordSupplier", "pr_createdAt": "2020-05-04T04:42:41Z", "pr_url": "https://github.com/apache/druid/pull/9819", "timeline": [{"oid": "3100efc78c2edcedc123588ddc02f3a2c7ae9d7c", "url": "https://github.com/apache/druid/commit/3100efc78c2edcedc123588ddc02f3a2c7ae9d7c", "message": "refactor SeekableStreamSupervisor usage of RecordSupplier to reduce contention between background threads and main thread, refactor KinesisRecordSupplier, refactor Kinesis lag metric collection and emitting", "committedDate": "2020-05-04T04:38:41Z", "type": "commit"}, {"oid": "b88703d420a962fab9a99ea33b15543c8bbb4b45", "url": "https://github.com/apache/druid/commit/b88703d420a962fab9a99ea33b15543c8bbb4b45", "message": "fix style and test", "committedDate": "2020-05-04T08:16:14Z", "type": "commit"}, {"oid": "b34a585f996057a9fe360cd9eae639d771ddf22f", "url": "https://github.com/apache/druid/commit/b34a585f996057a9fe360cd9eae639d771ddf22f", "message": "cleanup, refactor, javadocs, test", "committedDate": "2020-05-05T11:52:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUxNTAyNw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r420515027", "bodyText": "I think underscoreJoiner should be all caps", "author": "jon-wei", "createdAt": "2020-05-06T02:35:13Z", "path": "core/src/main/java/org/apache/druid/indexer/TaskIdUtils.java", "diffHunk": "@@ -31,6 +32,8 @@\n {\n   private static final Pattern INVALIDCHARS = Pattern.compile(\"(?s).*[^\\\\S ].*\");\n \n+  private static final Joiner underscoreJoiner = Joiner.on(\"_\");", "originalCommit": "b34a585f996057a9fe360cd9eae639d771ddf22f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUyNjA0Nw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r420526047", "bodyText": "Suggest renaming this to not have Resources in the name since it doesn't use PartitionResource", "author": "jon-wei", "createdAt": "2020-05-06T03:27:50Z", "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -679,87 +694,204 @@ public String getPosition(StreamPartition<String> partition)\n     );\n   }\n \n-  @Override\n-  public void close()\n+  /**\n+   * Fetch the partition lag, given a stream and set of current partition offsets. This operates independently from\n+   * the {@link PartitionResource} which have been assigned to this record supplier.\n+   */\n+  public Map<String, Long> getPartitionsTimeLag(String stream, Map<String, String> currentOffsets)\n   {\n-    if (this.closed) {\n-      return;\n-    }\n-\n-    assign(ImmutableSet.of());\n-\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw new RuntimeException(e);\n+    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n+    for (Map.Entry<String, String> partitionOffset : currentOffsets.entrySet()) {\n+      StreamPartition<String> partition = new StreamPartition<>(stream, partitionOffset.getKey());\n+      long currentLag = getPartitionResourcesTimeLag(partition, partitionOffset.getValue());\n+      partitionLag.put(partitionOffset.getKey(), currentLag);\n     }\n-\n-    this.closed = true;\n+    return partitionLag;\n   }\n \n-  // this is only used for tests\n+  /**\n+   * This method is only used for tests to verify that {@link PartitionResource} in fact tracks it's current lag\n+   * as it is polled for records. This isn't currently used in production at all, but could be one day if we were\n+   * to prefer to get the lag from the running tasks in the same API call which fetches the current task offsets,\n+   * instead of directly calling the AWS Kinesis API with the offsets returned from those tasks\n+   * (see {@link #getPartitionsTimeLag}, which accepts a map of current partition offsets).\n+   */\n   @VisibleForTesting\n-  Map<String, Long> getPartitionTimeLag()\n+  Map<String, Long> getPartitionResourcesTimeLag()\n   {\n     return partitionResources.entrySet()\n                              .stream()\n                              .collect(\n-                                 Collectors.toMap(k -> k.getKey().getPartitionId(), k -> k.getValue().getPartitionTimeLag())\n+                                 Collectors.toMap(\n+                                     k -> k.getKey().getPartitionId(),\n+                                     k -> k.getValue().getPartitionTimeLag()\n+                                 )\n                              );\n   }\n \n-  public Map<String, Long> getPartitionTimeLag(Map<String, String> currentOffsets)\n-      throws InterruptedException, TimeoutException, ExecutionException\n+  @VisibleForTesting\n+  public int bufferSize()\n   {\n-    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n-\n-    List<Future<AbstractMap.SimpleEntry<String, Long>>> longo = scheduledExec.invokeAll(\n-        partitionResources.entrySet()\n-                          .stream()\n-                          .map(partition -> {\n-                            final String partitionId = partition.getKey().getPartitionId();\n-                            return partition.getValue().getLagCallable(partitionId, currentOffsets.get(partitionId));\n-                          })\n-                          .collect(Collectors.toList())\n-\n-    );\n-    for (Future<AbstractMap.SimpleEntry<String, Long>> future : longo) {\n-      AbstractMap.SimpleEntry<String, Long> result = future.get(fetchSequenceNumberTimeout, TimeUnit.MILLISECONDS);\n-      partitionLag.put(result.getKey(), result.getValue());\n-    }\n+    return records.size();\n+  }\n \n-    return partitionLag;\n+  @VisibleForTesting\n+  public boolean isBackgroundFetchRunning()\n+  {\n+    return partitionsFetchStarted.get();\n   }\n \n-  private void seekInternal(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n+  /**\n+   * Check that a {@link PartitionResource} has been assigned to this record supplier, and if so call\n+   * {@link PartitionResource#seek} to move it to the latest offsets. Note that this method does not restart background\n+   * fetch, which should have been stopped prior to calling this method by a call to\n+   * {@link #filterBufferAndResetBackgroundFetch}.\n+   */\n+  private void partitionSeek(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n   {\n     PartitionResource resource = partitionResources.get(partition);\n     if (resource == null) {\n       throw new ISE(\"Partition [%s] has not been assigned\", partition);\n     }\n+    resource.seek(iteratorEnum, sequenceNumber);\n+  }\n \n-    log.debug(\n-        \"Seeking partition [%s] to [%s]\",\n-        partition.getPartitionId(),\n-        sequenceNumber != null ? sequenceNumber : iteratorEnum.toString()\n-    );\n+  /**\n+   * Given a partition and a {@link ShardIteratorType}, create a shard iterator and fetch\n+   * {@link #FETCH_SEQUENCE_NUMBER_RECORD_COUNT} records and return the first sequence number from the result set.\n+   * This method is thread safe as it does not depend on the internal state of the supplier (it doesn't use the\n+   * {@link PartitionResource} which have been assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n+  @Nullable\n+  private String getSequenceNumber(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n+  {\n+    return wrapExceptions(() -> {\n+      String shardIterator =\n+          kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n+                 .getShardIterator();\n+      long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n+      GetRecordsResult recordsResult = null;\n+\n+      while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n+\n+        if (closed) {\n+          log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n+          return null;\n+        }\n+        try {\n+          // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n+          // In the case where the shard is constantly removing records that are past their retention period, it is possible\n+          // that we never find the first record in the shard if we use a limit of 1.\n+          recordsResult = kinesis.getRecords(\n+              new GetRecordsRequest().withShardIterator(shardIterator).withLimit(FETCH_SEQUENCE_NUMBER_RECORD_COUNT)\n+          );\n+        }\n+        catch (ProvisionedThroughputExceededException e) {\n+          log.warn(\n+              e,\n+              \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n+              + \"that the request rate for the stream is too high, or the requested data is too large for \"\n+              + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n+              + \"the number of shards to increase throughput.\"\n+          );\n+          try {\n+            Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);\n+            continue;\n+          }\n+          catch (InterruptedException e1) {\n+            log.warn(e1, \"Thread interrupted!\");\n+            Thread.currentThread().interrupt();\n+            break;\n+          }\n+        }\n \n-    resource.shardIterator = wrapExceptions(() -> kinesis.getShardIterator(\n-        partition.getStream(),\n-        partition.getPartitionId(),\n-        iteratorEnum.toString(),\n-        sequenceNumber\n-    ).getShardIterator());\n+        List<Record> records = recordsResult.getRecords();\n+\n+        if (!records.isEmpty()) {\n+          return records.get(0).getSequenceNumber();\n+        }\n+\n+        shardIterator = recordsResult.getNextShardIterator();\n+      }\n+\n+      if (shardIterator == null) {\n+        log.info(\"Partition[%s] returned a null shard iterator, is the shard closed?\", partition.getPartitionId());\n+        return KinesisSequenceNumber.END_OF_SHARD_MARKER;\n+      }\n+\n+\n+      // if we reach here, it usually means either the shard has no more records, or records have not been\n+      // added to this shard\n+      log.warn(\n+          \"timed out while trying to fetch position for shard[%s], millisBehindLatest is [%s], likely no more records in shard\",\n+          partition.getPartitionId(),\n+          recordsResult != null ? recordsResult.getMillisBehindLatest() : \"UNKNOWN\"\n+      );\n+      return null;\n+    });\n   }\n \n-  private void filterBufferAndResetFetchRunnable(Set<StreamPartition<String>> partitions) throws InterruptedException\n+  /**\n+   * Given a {@link StreamPartition} and an offset, create a 'shard iterator' for the offset and fetch a single record\n+   * in order to get the lag: {@link GetRecordsResult#getMillisBehindLatest()}. This method is thread safe as it does\n+   * not depend on the internal state of the supplier (it doesn't use the {@link PartitionResource} which have been\n+   * assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n+  private Long getPartitionResourcesTimeLag(StreamPartition<String> partition, String offset)", "originalCommit": "b34a585f996057a9fe360cd9eae639d771ddf22f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTI5MzQ4OQ==", "url": "https://github.com/apache/druid/pull/9819#discussion_r421293489", "bodyText": "Woops, this was a rename done with intellij that caught this method in the crossfire, I only meant to rename getPartitionResourcesTimeLag() since it does actually get the lag from the PartitionResource collection. Fixed.", "author": "clintropolis", "createdAt": "2020-05-07T07:25:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUyNjA0Nw=="}], "type": "inlineReview"}, {"oid": "59e5156a21a641937ad9eb8c5b812d6de101062f", "url": "https://github.com/apache/druid/commit/59e5156a21a641937ad9eb8c5b812d6de101062f", "message": "fixes", "committedDate": "2020-05-07T07:25:00Z", "type": "commit"}, {"oid": "f1c5e732dca61246bb3bb819515331b083923a5f", "url": "https://github.com/apache/druid/commit/f1c5e732dca61246bb3bb819515331b083923a5f", "message": "keep collecting current offsets and lag if unhealthy in background reporting thread", "committedDate": "2020-05-07T07:38:21Z", "type": "commit"}, {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "url": "https://github.com/apache/druid/commit/2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "message": "Merge remote-tracking branch 'upstream/master' into kinesis-fixes", "committedDate": "2020-05-12T19:39:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTg5Mw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425349893", "bodyText": "Should this method compute the lag instead of latestSequenceFromStream? This will modify the current behavior since there could be some difference between computed lag and actual lag as highestCurrentOffsets will be computed periodically. However, I think it would be fine since 1) the behavior of this method is consistent across Kafka and Kinesis and 2) the lag metric doesn't have to be very real time.", "author": "jihoonson", "createdAt": "2020-05-14T18:34:39Z", "path": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "diffHunk": "@@ -334,16 +334,35 @@ protected boolean useExclusiveStartSequenceNumberForNonFirstSequence()\n   }\n \n   @Override\n-  protected void updateLatestSequenceFromStream(\n-      RecordSupplier<Integer, Long> recordSupplier,\n-      Set<StreamPartition<Integer>> partitions\n-  )\n+  protected void updatePartitionLagFromStream()", "originalCommit": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY3NjcxNw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425676717", "bodyText": "We still need to keep latestSequenceFromStream around since it's also included in the supervisor report payload, but I guess we could precompute the record lag here as well instead of lazily from the current highest offsets and latestSequenceFromStream whenever it is used in a report or emitted as lag metric and it probably wouldn't change the behavior that much.\nThough, since generating a report also would be using the highest current offsets and including them in the payload, it might be strange if the record lag didn't match what you could manually compute from the reports current and highest offsets. Because of this report dissonance, I haven't changed it to precompute it yet, but I did leave a comment about what is going on and why we are only fetching the latest offsets instead of doing anything about lag.", "author": "clintropolis", "createdAt": "2020-05-15T09:22:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0NTcxNw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r426045717", "bodyText": "Sounds good to me.", "author": "jihoonson", "createdAt": "2020-05-15T21:02:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQyMzc5MA==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425423790", "bodyText": "Seems worth canceling a ScheduledFuture for fetchRecords() here since it can be called in assign().", "author": "jihoonson", "createdAt": "2020-05-14T20:53:41Z", "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -112,100 +145,75 @@ private static boolean isServiceExceptionRecoverable(AmazonServiceException ex)\n     // to indicate that this shard has no more records to read\n     @Nullable\n     private volatile String shardIterator;\n-    private volatile boolean started;\n-    private volatile boolean stopRequested;\n-\n     private volatile long currentLagMillis;\n \n-    PartitionResource(StreamPartition<String> streamPartition)\n+    private final AtomicBoolean fetchStarted = new AtomicBoolean();\n+\n+    private PartitionResource(StreamPartition<String> streamPartition)\n     {\n       this.streamPartition = streamPartition;\n     }\n \n-    void startBackgroundFetch()\n+    private void startBackgroundFetch()\n     {\n-      if (started) {\n+      if (!backgroundFetchEnabled) {\n         return;\n       }\n+      // if seek has been called\n+      if (shardIterator == null) {\n+        log.warn(\n+            \"Skipping background fetch for stream[%s] partition[%s] since seek has not been called for this partition\",\n+            streamPartition.getStream(),\n+            streamPartition.getPartitionId()\n+        );\n+        return;\n+      }\n+      if (fetchStarted.compareAndSet(false, true)) {\n+        log.info(\n+            \"Starting scheduled fetch for stream[%s] partition[%s]\",\n+            streamPartition.getStream(),\n+            streamPartition.getPartitionId()\n+        );\n \n-      log.info(\n-          \"Starting scheduled fetch runnable for stream[%s] partition[%s]\",\n-          streamPartition.getStream(),\n-          streamPartition.getPartitionId()\n-      );\n-\n-      stopRequested = false;\n-      started = true;\n-\n-      rescheduleRunnable(fetchDelayMillis);\n-    }\n-\n-    void stopBackgroundFetch()\n-    {\n-      log.info(\n-          \"Stopping scheduled fetch runnable for stream[%s] partition[%s]\",\n-          streamPartition.getStream(),\n-          streamPartition.getPartitionId()\n-      );\n-      stopRequested = true;\n+        scheduleBackgroundFetch(fetchDelayMillis);\n+      }\n     }\n \n-    long getPartitionTimeLag()\n+    private void stopBackgroundFetch()\n     {\n-      return currentLagMillis;\n+      if (fetchStarted.compareAndSet(true, false)) {\n+        log.info(", "originalCommit": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY3NTA2MQ==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425675061", "bodyText": "Modified it to save the ScheduledFuture as currentFetch and cancel it if it isn't done or null.", "author": "clintropolis", "createdAt": "2020-05-15T09:19:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQyMzc5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzY2Nw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425433667", "bodyText": "Hmm, would it be nice if backgroundFetchEnabled is checked in this method or in the callers of this method to make it clear that this method will do nothing if backgroundFetchEnabled = false?", "author": "jihoonson", "createdAt": "2020-05-14T21:12:33Z", "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -847,36 +881,46 @@ private void checkIfClosed()\n   }\n \n   /**\n-   * Returns an array with the content between the position and limit of \"buffer\". This may be the buffer's backing\n-   * array itself. Does not modify position or limit of the buffer.\n+   * This method must be called before a seek operation ({@link #seek}, {@link #seekToLatest}, or\n+   * {@link #seekToEarliest}).\n+   *\n+   * When called, it will nuke the {@link #scheduledExec} that is shared by all {@link PartitionResource}, filters\n+   * records from the buffer for partitions which will have a seek operation performed, and stops background fetch for\n+   * each {@link PartitionResource} to prepare for the seek. If background fetch is not currently running, the\n+   * {@link #scheduledExec} will not be re-created.\n    */\n-  private static byte[] toByteArray(final ByteBuffer buffer)\n+  private void filterBufferAndResetBackgroundFetch(Set<StreamPartition<String>> partitions) throws InterruptedException\n   {\n-    if (buffer.hasArray()\n-        && buffer.arrayOffset() == 0\n-        && buffer.position() == 0\n-        && buffer.array().length == buffer.limit()) {\n-      return buffer.array();\n-    } else {\n-      final byte[] retVal = new byte[buffer.remaining()];\n-      buffer.duplicate().get(retVal);\n-      return retVal;\n-    }\n-  }\n+    checkIfClosed();", "originalCommit": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY3MzkwMQ==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425673901", "bodyText": "Added check for backgroundFetchEnabled to the if statement, but checkIfClose will still be called, is that cool? (since you put the comment on this line)", "author": "clintropolis", "createdAt": "2020-05-15T09:17:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzY2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0NTc2NQ==", "url": "https://github.com/apache/druid/pull/9819#discussion_r426045765", "bodyText": "Yeah, seems it should check here to avoid unnecessary seek (this method is called in seek()).", "author": "jihoonson", "createdAt": "2020-05-15T21:02:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzY2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0MDEzMA==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425440130", "bodyText": "nit: wondering if it's better to use fuzzy exponential backoff using RetryUtils. I guess it could be better by avoiding retries which could be repeated at the same time.", "author": "jihoonson", "createdAt": "2020-05-14T21:25:53Z", "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -665,180 +694,185 @@ public String getPosition(StreamPartition<String> partition)\n     );\n   }\n \n-  @Override\n-  public void close()\n+  /**\n+   * Fetch the partition lag, given a stream and set of current partition offsets. This operates independently from\n+   * the {@link PartitionResource} which have been assigned to this record supplier.\n+   */\n+  public Map<String, Long> getPartitionsTimeLag(String stream, Map<String, String> currentOffsets)\n   {\n-    if (this.closed) {\n-      return;\n-    }\n-\n-    assign(ImmutableSet.of());\n-\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw new RuntimeException(e);\n+    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n+    for (Map.Entry<String, String> partitionOffset : currentOffsets.entrySet()) {\n+      StreamPartition<String> partition = new StreamPartition<>(stream, partitionOffset.getKey());\n+      long currentLag = getPartitionTimeLag(partition, partitionOffset.getValue());\n+      partitionLag.put(partitionOffset.getKey(), currentLag);\n     }\n-\n-    this.closed = true;\n+    return partitionLag;\n   }\n \n-  // this is only used for tests\n+  /**\n+   * This method is only used for tests to verify that {@link PartitionResource} in fact tracks it's current lag\n+   * as it is polled for records. This isn't currently used in production at all, but could be one day if we were\n+   * to prefer to get the lag from the running tasks in the same API call which fetches the current task offsets,\n+   * instead of directly calling the AWS Kinesis API with the offsets returned from those tasks\n+   * (see {@link #getPartitionsTimeLag}, which accepts a map of current partition offsets).\n+   */\n   @VisibleForTesting\n-  Map<String, Long> getPartitionTimeLag()\n+  Map<String, Long> getPartitionResourcesTimeLag()\n   {\n     return partitionResources.entrySet()\n                              .stream()\n                              .collect(\n-                                 Collectors.toMap(k -> k.getKey().getPartitionId(), k -> k.getValue().getPartitionTimeLag())\n+                                 Collectors.toMap(\n+                                     k -> k.getKey().getPartitionId(),\n+                                     k -> k.getValue().getPartitionTimeLag()\n+                                 )\n                              );\n   }\n \n-  public Map<String, Long> getPartitionTimeLag(Map<String, String> currentOffsets)\n+  @VisibleForTesting\n+  public int bufferSize()\n   {\n-    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n-    for (Map.Entry<StreamPartition<String>, PartitionResource> partition : partitionResources.entrySet()) {\n-      final String partitionId = partition.getKey().getPartitionId();\n-      partitionLag.put(partitionId, partition.getValue().getPartitionTimeLag(currentOffsets.get(partitionId)));\n-    }\n-    return partitionLag;\n+    return records.size();\n   }\n \n-  private void seekInternal(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n+  @VisibleForTesting\n+  public boolean isBackgroundFetchRunning()\n   {\n-    PartitionResource resource = partitionResources.get(partition);\n-    if (resource == null) {\n-      throw new ISE(\"Partition [%s] has not been assigned\", partition);\n-    }\n-\n-    log.debug(\n-        \"Seeking partition [%s] to [%s]\",\n-        partition.getPartitionId(),\n-        sequenceNumber != null ? sequenceNumber : iteratorEnum.toString()\n-    );\n-\n-    resource.shardIterator = wrapExceptions(() -> kinesis.getShardIterator(\n-        partition.getStream(),\n-        partition.getPartitionId(),\n-        iteratorEnum.toString(),\n-        sequenceNumber\n-    ).getShardIterator());\n-\n-    checkPartitionsStarted = true;\n+    return partitionsFetchStarted.get();\n   }\n \n-  private void filterBufferAndResetFetchRunnable(Set<StreamPartition<String>> partitions) throws InterruptedException\n+  /**\n+   * Check that a {@link PartitionResource} has been assigned to this record supplier, and if so call\n+   * {@link PartitionResource#seek} to move it to the latest offsets. Note that this method does not restart background\n+   * fetch, which should have been stopped prior to calling this method by a call to\n+   * {@link #filterBufferAndResetBackgroundFetch}.\n+   */\n+  private void partitionSeek(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n   {\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw e;\n+    PartitionResource resource = partitionResources.get(partition);\n+    if (resource == null) {\n+      throw new ISE(\"Partition [%s] has not been assigned\", partition);\n     }\n-\n-    scheduledExec = Executors.newScheduledThreadPool(\n-        fetchThreads,\n-        Execs.makeThreadFactory(\"KinesisRecordSupplier-Worker-%d\")\n-    );\n-\n-    // filter records in buffer and only retain ones whose partition was not seeked\n-    BlockingQueue<OrderedPartitionableRecord<String, String>> newQ = new LinkedBlockingQueue<>(recordBufferSize);\n-\n-    records.stream()\n-           .filter(x -> !partitions.contains(x.getStreamPartition()))\n-           .forEachOrdered(newQ::offer);\n-\n-    records = newQ;\n-\n-    // restart fetching threads\n-    partitionResources.values().forEach(x -> x.started = false);\n-    checkPartitionsStarted = true;\n-  }\n-\n-  @Nullable\n-  private String getSequenceNumberInternal(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n-  {\n-    return wrapExceptions(() -> getSequenceNumberInternal(\n-        partition,\n-        kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n-               .getShardIterator()\n-    ));\n+    resource.seek(iteratorEnum, sequenceNumber);\n   }\n \n+  /**\n+   * Given a partition and a {@link ShardIteratorType}, create a shard iterator and fetch\n+   * {@link #FETCH_SEQUENCE_NUMBER_RECORD_COUNT} records and return the first sequence number from the result set.\n+   * This method is thread safe as it does not depend on the internal state of the supplier (it doesn't use the\n+   * {@link PartitionResource} which have been assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n   @Nullable\n-  private String getSequenceNumberInternal(StreamPartition<String> partition, String shardIterator)\n+  private String getSequenceNumber(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n   {\n-    long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n-    GetRecordsResult recordsResult = null;\n-\n-    while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n-\n-      if (closed) {\n-        log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n-        return null;\n-      }\n-      try {\n-        // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n-        // In the case where the shard is constantly removing records that are past their retention period, it is possible\n-        // that we never find the first record in the shard if we use a limit of 1.\n-        recordsResult = kinesis.getRecords(new GetRecordsRequest().withShardIterator(shardIterator).withLimit(1000));\n-      }\n-      catch (ProvisionedThroughputExceededException e) {\n-        log.warn(\n-            e,\n-            \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n-            + \"that the request rate for the stream is too high, or the requested data is too large for \"\n-            + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n-            + \"the number of shards to increase throughput.\"\n-        );\n+    return wrapExceptions(() -> {\n+      String shardIterator =\n+          kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n+                 .getShardIterator();\n+      long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n+      GetRecordsResult recordsResult = null;\n+\n+      while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n+\n+        if (closed) {\n+          log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n+          return null;\n+        }\n         try {\n-          Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);\n-          continue;\n+          // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n+          // In the case where the shard is constantly removing records that are past their retention period, it is possible\n+          // that we never find the first record in the shard if we use a limit of 1.\n+          recordsResult = kinesis.getRecords(\n+              new GetRecordsRequest().withShardIterator(shardIterator).withLimit(FETCH_SEQUENCE_NUMBER_RECORD_COUNT)\n+          );\n         }\n-        catch (InterruptedException e1) {\n-          log.warn(e1, \"Thread interrupted!\");\n-          Thread.currentThread().interrupt();\n-          break;\n+        catch (ProvisionedThroughputExceededException e) {\n+          log.warn(\n+              e,\n+              \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n+              + \"that the request rate for the stream is too high, or the requested data is too large for \"\n+              + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n+              + \"the number of shards to increase throughput.\"\n+          );\n+          try {\n+            Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);", "originalCommit": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY3NDk4Mw==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425674983", "bodyText": "It seems better to me, and more consistent with the rest of the codebase, though I'm not sure what to do for retry count since the current behavior would be an indefinite number of retries in the case of ProvisionedThroughputExceededException. However, I picked 10, though I can do higher if we are worried this will commonly happen...", "author": "clintropolis", "createdAt": "2020-05-15T09:19:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0MDEzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0NTgwOQ==", "url": "https://github.com/apache/druid/pull/9819#discussion_r426045809", "bodyText": "I guess we can add another mode to RetryUtils.retry() that checks the timeout instead of retryCount, but the current way looks ok to me.", "author": "jihoonson", "createdAt": "2020-05-15T21:02:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0MDEzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0NjY2Mg==", "url": "https://github.com/apache/druid/pull/9819#discussion_r425446662", "bodyText": "This seems duplicate. Also compute() will execute the function and replace the value even when the key exists. You can do newlyDiscovered.computeIfAbsent(taskGroupId, ArrayList::new).add(partitionId);.", "author": "jihoonson", "createdAt": "2020-05-14T21:39:58Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/supervisor/SeekableStreamSupervisor.java", "diffHunk": "@@ -1998,16 +2009,37 @@ private boolean updatePartitionDataFromStream()\n       partitionGroup.add(partitionId);\n \n       if (partitionOffsets.putIfAbsent(partitionId, getNotSetMarker()) == null) {\n-        log.info(\n+        log.debug(\n             \"New partition [%s] discovered for stream [%s], added to task group [%d]\",\n             partitionId,\n             ioConfig.getStream(),\n             taskGroupId\n         );\n+\n+        newlyDiscovered.compute(taskGroupId, (groupId, partitions) -> {\n+          if (partitions == null) {\n+            partitions = new ArrayList<>();\n+          }\n+          partitions.add(partitionId);", "originalCommit": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b909e9e4cd0fd93b39d691a047757841bef41112", "url": "https://github.com/apache/druid/commit/b909e9e4cd0fd93b39d691a047757841bef41112", "message": "review stuffs", "committedDate": "2020-05-15T09:17:15Z", "type": "commit"}, {"oid": "56fcb2cb58535ed08a750b2c562cda452ce1eeb7", "url": "https://github.com/apache/druid/commit/56fcb2cb58535ed08a750b2c562cda452ce1eeb7", "message": "add comment", "committedDate": "2020-05-15T09:21:48Z", "type": "commit"}]}