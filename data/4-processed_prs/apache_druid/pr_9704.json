{"pr_number": 9704, "pr_title": "Refresh query docs.", "pr_createdAt": "2020-04-15T10:24:57Z", "pr_url": "https://github.com/apache/druid/pull/9704", "timeline": [{"oid": "707d685a70599dd948d374373ae50fa5436fe14d", "url": "https://github.com/apache/druid/commit/707d685a70599dd948d374373ae50fa5436fe14d", "message": "Refresh query docs.\n\nLarger changes:\n\n- New doc: querying/datasource.md describes the various kinds of\ndatasources you can use, and has examples for both SQL and native.\n- New doc: querying/query-execution.md describes how native queries\nare executed at a high level. It doesn't go into the details of specific\nquery engines or how queries run at a per-segment level. But I think it\nwould be good to add or link that content here in the future.\n- Refreshed doc: querying/sql.md updated to refer to joins, reformatted\na bit, added a new \"Query translation\" section that explains how\nqueries are translated from SQL to native, and removed configuration\ndetails (moved to configuration/index.md).\n- Refreshed doc: querying/joins.md updated to refer to join datasources.\n\nSmaller changes:\n\n- Add helpful banners to the top of query documentation pages telling\npeople whether a given page describes SQL, native, or both.\n- Add SQL metrics to operations/metrics.md.\n- Add some color and cross-links in various places.\n- Add native query component docs to the sidebar, and renamed them so\nthey look nicer.\n- Remove Select query from the sidebar.\n- Fix Broker SQL configs in configuration/index.md. Remove them from\nquerying/sql.md.\n- Combined querying/searchquery.md and querying/searchqueryspec.md.", "committedDate": "2020-04-15T10:20:56Z", "type": "commit"}, {"oid": "698471e8131daa38820eb729712f7d17ca4c7587", "url": "https://github.com/apache/druid/commit/698471e8131daa38820eb729712f7d17ca4c7587", "message": "Updates.", "committedDate": "2020-04-15T10:40:15Z", "type": "commit"}, {"oid": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "url": "https://github.com/apache/druid/commit/e5de6353fc4dbfacb21b661c79137a1001f4a667", "message": "Fix numbering.", "committedDate": "2020-04-15T10:42:20Z", "type": "commit"}, {"oid": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "url": "https://github.com/apache/druid/commit/2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "message": "Fix glitches.", "committedDate": "2020-04-15T15:56:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1NjIwOQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408956209", "bodyText": "What changes went in to bring these expressions out of an experimental feature?", "author": "suneet-s", "createdAt": "2020-04-15T16:00:17Z", "path": "docs/misc/math-expr.md", "diffHunk": "@@ -22,9 +22,14 @@ title: \"Expressions\"\n   ~ under the License.\n   -->\n \n-\n-> This feature is still experimental. It has not been optimized for performance yet, and its implementation is known to\n->  have significant inefficiencies.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1Njg1OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408956859", "bodyText": "oops sorry hit comment instead of finishing up my review, still going through the rest of these doc changes", "author": "suneet-s", "createdAt": "2020-04-15T16:01:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1NjIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NjM1Nw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409066357", "bodyText": "IIRC, a couple things:\n\nImproved performance by adding various optimized selectors like SingleStringInputDimensionSelector, SingleLongInputCachingExpressionColumnValueSelector, SingleStringInputCachingExpressionColumnValueSelector.\nBattle tested by having expressions be the main way of handling pre- and post-aggregation projections in Druid SQL.", "author": "gianm", "createdAt": "2020-04-15T18:59:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1NjIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyMzg5NQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409123895", "bodyText": "Should this be noted in the 0.18 Release notes? It looks like these changes were first included in 0.17. Not sure if this is just stale docs, or something we want to call out explicitly. As a Druid user, I just assumed this was not experimental.\ncc @jihoonson", "author": "suneet-s", "createdAt": "2020-04-15T20:47:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1NjIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzMjcxOA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409132718", "bodyText": "It looks worth mentioning in the release notes. Thanks.", "author": "jihoonson", "createdAt": "2020-04-15T21:04:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1NjIwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODkyOTk0Nw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408929947", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n          \n          \n            \n            In [Druid SQL](sql.html#from), table datasources reside in the `druid` schema. This is the default schema, so table", "author": "sthetland", "createdAt": "2020-04-15T15:24:49Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODkzMDU4OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408930589", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n          \n          \n            \n            they reside in the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.", "author": "sthetland", "createdAt": "2020-04-15T15:25:42Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk0NTY4Nw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408945687", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n          \n          \n            \n            |`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource to prevent collisions with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|", "author": "sthetland", "createdAt": "2020-04-15T15:45:49Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk0ODE5NA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408948194", "bodyText": "Is it that result \"may not always be complete\"? ... or any other way to characterize the incorrectness, if possible?", "author": "sthetland", "createdAt": "2020-04-15T15:49:17Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect\n+to implement both early and deferred condition evaluation, which we expect to improve performance considerably for\n+common use cases.\n+\n+#### Future work for joins\n+\n+Joins are an area of active development in Druid. The following features are missing today but may appear in\n+future versions:\n+\n+- Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value).\n+- RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not\n+always be correct.", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk1MzQ5Ng==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408953496", "bodyText": "optional... if it's a bit clearer.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n          \n          \n            \n            Druid SQL is less rigid in handling join datasources than native queries. In cases where a SQL query does", "author": "sthetland", "createdAt": "2020-04-15T15:56:28Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk2MDYxMg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408960612", "bodyText": "Slightly reworded but should it say \"avoid\" or just \"use wisely\"?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Whenever possible, for best performance it is good to avoid joins at query time. Often this can be accomplished by\n          \n          \n            \n            For best performance, we recommend avoiding joins at query time whenever possible. Often this can be accomplished by", "author": "sthetland", "createdAt": "2020-04-15T16:06:59Z", "path": "docs/querying/joins.md", "diffHunk": "@@ -22,33 +22,13 @@ title: \"Joins\"\n   ~ under the License.\n   -->\n \n+Druid has two features related to joining of data:\n \n-Apache Druid has limited support for joins through [query-time lookups](../querying/lookups.md). The common use case of\n-query-time lookups is to replace one dimension value (e.g. a String ID) with another value (e.g. a human-readable String value). This is similar to a star-schema join.\n+1. [Join](datasource.md#join) operators. These are available using a [join datasource](datasource.md#join) in native\n+queries, or using the [JOIN operator](sql.md#query-syntax) in Druid SQL. Refer to the\n+[join datasource](datasource.md#join) documentation for information about how joins work in Druid.\n+2. [Query-time lookups](lookups.md), simple key-to-value mappings. These are preloaded on all servers and can be\n+accessed with or without an explicit join operator. Refer to the [lookups](lookups.md) documentation for more details.\n \n-Druid does not yet have full support for joins. Although Druid\u2019s storage format would allow for the implementation\n-of joins (there is no loss of fidelity for columns included as dimensions), full support for joins have not yet been implemented yet\n-for the following reasons:\n-\n-1. Scaling join queries has been, in our professional experience,\n-a constant bottleneck of working with distributed databases.\n-2. The incremental gains in functionality are perceived to be\n-of less value than the anticipated problems with managing\n-highly concurrent, join-heavy workloads.\n-\n-A join query is essentially the merging of two or more streams of data based on a shared set of keys. The primary\n-high-level strategies for join queries we are aware of are a hash-based strategy or a\n-sorted-merge strategy. The hash-based strategy requires that all but\n-one data set be available as something that looks like a hash table,\n-a lookup operation is then performed on this hash table for every\n-row in the \u201cprimary\u201d stream. The sorted-merge strategy assumes\n-that each stream is sorted by the join key and thus allows for the incremental\n-joining of the streams. Each of these strategies, however,\n-requires the materialization of some number of the streams either in\n-sorted order or in a hash table form.\n-\n-When all sides of the join are significantly large tables (> 1 billion\n-records), materializing the pre-join streams requires complex\n-distributed memory management. The complexity of the memory\n-management is only amplified by the fact that we are targeting highly\n-concurrent, multi-tenant workloads.\n+Whenever possible, for best performance it is good to avoid joins at query time. Often this can be accomplished by", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NzU2OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408977569", "bodyText": "is this intentional? In the page text, it's mostly hyphenated, but sometimes two words. Or we could use the code form like groupBy, etc, which would then be \"postAggregation\".. okay, but not preferred, I'd say\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"title\": \"Postaggregations\"\n          \n          \n            \n                    \"title\": \"postAggregation\"", "author": "sthetland", "createdAt": "2020-04-15T16:32:54Z", "path": "website/i18n/en.json", "diffHunk": "@@ -408,17 +417,21 @@\n         \"title\": \"Multi-value dimensions\"\n       },\n       \"querying/multitenancy\": {\n-        \"title\": \"Multitenancy considerations\"\n+        \"title\": \"Multitenancy considerations\",\n+        \"sidebar_label\": \"Multitenancy\"\n       },\n       \"querying/post-aggregations\": {\n-        \"title\": \"Post-Aggregations\"\n+        \"title\": \"Postaggregations\"", "originalCommit": "e5de6353fc4dbfacb21b661c79137a1001f4a667", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5MTQwNg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408991406", "bodyText": "What do you think about \"Post-aggregations\"?\nThis text will appear in the sidebar, where starting with a lowercase would look weird.", "author": "gianm", "createdAt": "2020-04-15T16:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NzU2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5NzMyOQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408997329", "bodyText": "\"Post-aggregations\" works for me.", "author": "sthetland", "createdAt": "2020-04-15T17:04:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NzU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3MzM5NQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408973395", "bodyText": "nit: singular?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n          \n          \n            \n            A datasource in Apache Druid is a thing that you can query. The most common kind of datasource is a table datasource,", "author": "suneet-s", "createdAt": "2020-04-15T16:26:17Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4ODA4NA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409088084", "bodyText": "I'm not sure this is better so I'm inclined to leave it.", "author": "gianm", "createdAt": "2020-04-15T19:39:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3MzM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NDExMA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408974110", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            datasource. But at query time, there are many other types of datasources available.\n          \n          \n            \n            datasource. But at query time, there are many other types of datasources available that are described [below](#datasource-type).\n          \n      \n    \n    \n  \n\nThere's only 1 sub section in this doc right now but that might change in the future. Do you think it's worth a deep link?", "author": "suneet-s", "createdAt": "2020-04-15T16:27:24Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4NzgwMw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409087803", "bodyText": "I don't think so, because the thing being linked to is immediately below.", "author": "gianm", "createdAt": "2020-04-15T19:38:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NDExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NDg4NA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408974884", "bodyText": "this isn't necessarily true? - eg. Join data sources", "author": "suneet-s", "createdAt": "2020-04-15T16:28:38Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4NzI1NQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409087255", "bodyText": "Joins are still something you provide in the FROM clause. Right?", "author": "gianm", "createdAt": "2020-04-15T19:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NDg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5NzEzMQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409097131", "bodyText": "oops - I thought the \"JOIN\" statement was on the same level as FROM, WHERE, etc. \ud83e\udd26\nMaybe a better example is subqueries that are part of a filter\nSELECT table.key, table.metric from table where table.key = (select ANY_VALUE(lookyloo.k) from lookup.lookyloo where lookyloo.v='FILTER')", "author": "suneet-s", "createdAt": "2020-04-15T19:56:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NDg4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3ODIzMw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408978233", "bodyText": "I think this should be the last in the list as it can build on top of other concepts described in the different datasource types.", "author": "suneet-s", "createdAt": "2020-04-15T16:34:00Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4NTUyNg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409085526", "bodyText": "My rationale here was I wanted to put the ones that were usable in SQL (table, lookup, query, join) at the top. I'm not sure which rationale is more powerful. What do you think?", "author": "gianm", "createdAt": "2020-04-15T19:34:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyNjA0Mw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409126043", "bodyText": "As someone brand new to Druid, I personally prefer an approach that builds on related concepts because a lot of these datasource types are foreign to me. However I also see merits in putting all the SQL usable datasources together.\nI think this is a bigger docs architecture discussion, so I'm happy to leave this as is.", "author": "suneet-s", "createdAt": "2020-04-15T20:50:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3ODIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4MTAzMA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408981030", "bodyText": "Maybe move this under the query datasource section since that's what the broker uses when dealing with subqueries", "author": "suneet-s", "createdAt": "2020-04-15T16:38:23Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect\n+to implement both early and deferred condition evaluation, which we expect to improve performance considerably for\n+common use cases.\n+\n+#### Future work for joins\n+\n+Joins are an area of active development in Druid. The following features are missing today but may appear in\n+future versions:\n+\n+- Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value).\n+- RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not\n+always be correct.\n+- Performance-related optimizations as mentioned in the [previous section](#join-performance).\n+- Join algorithms other than broadcast hash-joins.\n+\n+### `union`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--Native-->\n ```json\n {\n-       \"type\": \"union\",\n-       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"union\",\n+    \"dataSources\": [\"<dataSourceName1>\", \"<dataSourceName2>\", \"<dataSourceName3>\"]\n+  },\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n \n-Note that the data sources being unioned should have the same schema.\n-Union Queries should be always sent to a Broker/Router process and are *NOT* supported directly by the Historical processes.\n+Union datasources allow you to treat two or more table datasources as a single datasource. The datasources being unioned\n+do not need to have identical schemas. If they do not fully match up, then columns that exist in one table but not\n+another will be treated as if they contained all null values in the tables where they do not exist.\n \n-### Query datasource\n+Union datasources are not available in Druid SQL.\n \n-This is used for nested groupBys and is only currently supported for groupBys.\n+### `inline`", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4NDk0Mg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409084942", "bodyText": "But they are also a standalone datasource type.", "author": "gianm", "createdAt": "2020-04-15T19:33:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4MTAzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4NDI0Ng==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408984246", "bodyText": "Is this only true if the datasource that is being queried is a table. If it's a lookup or an inline datasource, I think this would be done on the broker, we could still run in to memory issues, and perf is still bad, but we don't pay the transfer cost.", "author": "suneet-s", "createdAt": "2020-04-15T16:43:25Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5MTY3MA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409091670", "bodyText": "Hmm I agree this is worded confusingly. I replaced it with the following.\n\nPerformance tip: In most cases, subquery results are fully buffered in memory on the Broker and then further processing occurs on the Broker itself. This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits on the Broker. See the Query execution documentation for more details on how subqueries are executed and what limits will apply.", "author": "gianm", "createdAt": "2020-04-15T19:46:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4NDI0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4OTk4MQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408989981", "bodyText": "I don't think these intervals match the intervals used by the sql query in the tab above", "author": "suneet-s", "createdAt": "2020-04-15T16:52:25Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5MzgyOA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409093828", "bodyText": "They do if site_traffic dataset doesn't have any data outside this interval \ud83d\ude42\nMy thinking here was that we're not holding these out as the queries that Druid SQL would generate based on these native queries. So I took the opportunity to make them look nicer. In addition to using prettier intervals I also used prettier column names.", "author": "gianm", "createdAt": "2020-04-15T19:50:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk4OTk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5MjY4OA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408992688", "bodyText": "Overall comment: All the SQL queries don't have time filters. As someone new to Druid, I'd look at this page and start issuing SQL commands without time filters. We should point out that this will cause performance issues on anything other than small test data. Don't know what the best way to do that is though", "author": "suneet-s", "createdAt": "2020-04-15T16:56:50Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n ", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5NDgxNA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408994814", "bodyText": "nit: all servers is vague. Are they pre-loaded on the master nodes too? I think it can be configured so that only the nodes that need lookups have them pre-loaded. My understanding is - brokers (always), historicals (always?), overlord, MM/ Indexer (only if it's needed by ingestion).", "author": "suneet-s", "createdAt": "2020-04-15T17:00:06Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5NDg3OA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409094878", "bodyText": "I replaced it with \"all servers that are involved in queries\".", "author": "gianm", "createdAt": "2020-04-15T19:52:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk5NDgxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAyMjYyMQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409022621", "bodyText": "Maybe make this block a warning block?", "author": "suneet-s", "createdAt": "2020-04-15T17:45:52Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MzYyOQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409083629", "bodyText": "Hmm. If you include the list below it (which this paragraph is connected to) it's almost the entirety of the section. I think it would be too big of a warning block. Any suggestions for something smaller that would make sense as a warning?", "author": "gianm", "createdAt": "2020-04-15T19:30:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAyMjYyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzMDQyMA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409130420", "bodyText": "Maybe the section on line 242 could be highlighted to drive home the point this paragraph is trying to make. It also has the benefit of linking to the query translation docs that tell a user how to read the query plan to know whether what Druid is trying to do makes sense or not.\nSince Druid SQL is less rigid in handling join datasource than native queries, it is important to understand how Druid translates SQL to native queries, refer to the\n[Druid SQL](sql.md#query-translation) documentation.", "author": "suneet-s", "createdAt": "2020-04-15T20:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAyMjYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAyNDUyNA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409024524", "bodyText": "I think this is an extension of the first point about using the LOOKUP function instead of a join?", "author": "suneet-s", "createdAt": "2020-04-15T17:49:06Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MzE2Mg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409083162", "bodyText": "It is. I just wanted to say it twice so people get the point.", "author": "gianm", "createdAt": "2020-04-15T19:30:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAyNDUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNDgwNA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409034804", "bodyText": "Is it possible to issue these queries today? I tried doing it from SQL and the query wouldn't plan. IIRC native queries also do not allow these conditions.\nMaybe something like\nCROSS joins are supported via sql, but it does so using sub-queries, so it really should not be used in production use cases", "author": "suneet-s", "createdAt": "2020-04-15T18:06:36Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect\n+to implement both early and deferred condition evaluation, which we expect to improve performance considerably for\n+common use cases.\n+\n+#### Future work for joins\n+\n+Joins are an area of active development in Druid. The following features are missing today but may appear in\n+future versions:\n+\n+- Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value).\n+- RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MjkzMw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409082933", "bodyText": "There are some tests for the functionality and they do pass. They're in CalciteQueryTest (SQL) and HashJoinSegmentStorageAdapterTest (native). What query did you try to run?", "author": "gianm", "createdAt": "2020-04-15T19:29:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNDgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNTM1Mg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409035352", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                \"dataSources\": [\"<dataSourceName1>\", \"<dataSourceName2>\", \"<dataSourceName3>\"]\n          \n          \n            \n                \"dataSources\": [\"<tableDataSourceName1>\", \"<tableDataSourceName2>\", \"<tableDataSourceName3>\"]", "author": "suneet-s", "createdAt": "2020-04-15T18:07:35Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect\n+to implement both early and deferred condition evaluation, which we expect to improve performance considerably for\n+common use cases.\n+\n+#### Future work for joins\n+\n+Joins are an area of active development in Druid. The following features are missing today but may appear in\n+future versions:\n+\n+- Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value).\n+- RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not\n+always be correct.\n+- Performance-related optimizations as mentioned in the [previous section](#join-performance).\n+- Join algorithms other than broadcast hash-joins.\n+\n+### `union`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--Native-->\n ```json\n {\n-       \"type\": \"union\",\n-       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"union\",\n+    \"dataSources\": [\"<dataSourceName1>\", \"<dataSourceName2>\", \"<dataSourceName3>\"]", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MjE0Mg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409082142", "bodyText": "Sure, sounds good.", "author": "gianm", "createdAt": "2020-04-15T19:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNTM1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNzc3MA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409037770", "bodyText": "Are there any performance notable implications of using union datasources?", "author": "suneet-s", "createdAt": "2020-04-15T18:11:43Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,\n+  SUM(sales.revenue) AS country_revenue\n+FROM\n+  sales\n+  INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k\n+GROUP BY\n+  countries.v\n+```\n+<!--Native-->\n ```json\n {\n-\t\"type\": \"table\",\n-\t\"name\": \"<string_value>\"\n+  \"queryType\": \"groupBy\",\n+  \"dataSource\": {\n+    \"type\": \"join\",\n+    \"left\": \"sales\",\n+    \"right\": {\n+      \"type\": \"lookup\",\n+      \"lookup\": \"store_to_country\"\n+    },\n+    \"rightPrefix\": \"r.\",\n+    \"condition\": \"store == \\\"r.k\\\"\",\n+    \"joinType\": \"INNER\"\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"dimensions\": [\n+    { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" }\n+  ],\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" }\n+  ]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows\n+you to join arbitrarily many datasources.\n+\n+In Druid {{DRUIDVERSION}}, joins are implemented with a broadcast hash-join algorithm. This means that all tables\n+other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This\n+feature is intended mainly to allow joining regular Druid tables with [lookup](#lookup), [inline](#inline), and\n+[query](#query) datasources.\n+\n+For information about how Druid executes queries involving joins, refer to the\n+[Query execution](query-execution.html#join) page.\n+\n+#### Joins in SQL\n+\n+SQL joins take the form:\n+\n+```\n+<o1> [ INNER | LEFT [OUTER] ] JOIN <o2> ON <condition>\n+```\n+\n+The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together.\n+Conditions like `t1.x = t2.x`, or `LOWER(t1.x) = t2.x`, or `t1.x = t2.x AND t1.y = t2.y` can all be handled. Conditions\n+like `t1.x <> t2.x` cannot currently be handled.\n \n-### Union datasource\n+Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does\n+something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have\n+a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the\n+SQL layer will generate subqueries include:\n \n-This data source unions two or more table data sources.\n+- Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept\n+a table on the left-hand side, but not the right, so a subquery is needed.\n \n+- Join conditions where the expressions on either side are of different types.\n+\n+- Join conditions where the right-hand expression is not a direct column access.\n+\n+For more information about how Druid translates SQL to native queries, refer to the\n+[Druid SQL](sql.md#query-translation) documentation.\n+\n+#### Joins in native queries\n+\n+Native join datasources have the following properties. All are required.\n+\n+|Field|Description|\n+|-----|-----------|\n+|`left`|Left-hand datasource. Must be of type `table`, `join`, `lookup`, `query`, or `inline`. Placing another join as the left datasource allows you to join arbitrarily many datasources.|\n+|`right`|Right-hand datasource. Must be of type `lookup`, `query`, or `inline`. Note that this is more rigid than what Druid SQL requires.|\n+|`rightPrefix`|String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string `__time`. Any columns from the left-hand side that start with your `rightPrefix` will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side.|\n+|`condition`|[Expression](../misc/math-expr.md) that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression.|\n+|`joinType`|`INNER` or `LEFT`.|\n+\n+#### Join performance\n+\n+Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes:\n+\n+1. Joins are especially useful with [lookup datasources](#lookup), but in most cases, the\n+[`LOOKUP` function](sql.html#string-functions) performs better than a join. Consider using the `LOOKUP` function if\n+it is appropriate for your use case.\n+2. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in\n+your queries. Refer to the [Druid SQL](sql.md#query-translation) documentation for more details about when this happens\n+and how to detect it.\n+3. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match.\n+For example, since lookup keys are always strings, the condition `druid.d JOIN lookup.l ON d.field = l.field` will\n+perform best if `d.field` is a string.\n+4. As of Druid {{DRUIDVERSION}}, the join operator must evaluate the condition for each row. In the future, we expect\n+to implement both early and deferred condition evaluation, which we expect to improve performance considerably for\n+common use cases.\n+\n+#### Future work for joins\n+\n+Joins are an area of active development in Druid. The following features are missing today but may appear in\n+future versions:\n+\n+- Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value).\n+- RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not\n+always be correct.\n+- Performance-related optimizations as mentioned in the [previous section](#join-performance).\n+- Join algorithms other than broadcast hash-joins.\n+\n+### `union`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--Native-->\n ```json\n {\n-       \"type\": \"union\",\n-       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"union\",\n+    \"dataSources\": [\"<dataSourceName1>\", \"<dataSourceName2>\", \"<dataSourceName3>\"]\n+  },\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n }\n ```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n \n-Note that the data sources being unioned should have the same schema.\n-Union Queries should be always sent to a Broker/Router process and are *NOT* supported directly by the Historical processes.\n+Union datasources allow you to treat two or more table datasources as a single datasource. The datasources being unioned\n+do not need to have identical schemas. If they do not fully match up, then columns that exist in one table but not\n+another will be treated as if they contained all null values in the tables where they do not exist.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3ODY2MA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409078660", "bodyText": "It is indirectly described here.", "author": "jihoonson", "createdAt": "2020-04-15T19:21:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNzc3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MTQ0NA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409081444", "bodyText": "yes, they're discussed in query-execution. I'll add a link.", "author": "gianm", "createdAt": "2020-04-15T19:26:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzNzc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NzkyOQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r408977929", "bodyText": "Should this be store_to_country.v instead of countries.v?", "author": "ccaominh", "createdAt": "2020-04-15T16:33:27Z", "path": "docs/querying/datasource.md", "diffHunk": "@@ -22,43 +22,317 @@ title: \"Datasources\"\n   ~ under the License.\n   -->\n \n+Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource,\n+and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true\n+[during data ingestion](../ingestion/index.html), where ingestion is always creating or writing into a table\n+datasource. But at query time, there are many other types of datasources available.\n \n-A data source is the Apache Druid equivalent of a database table. However, a query can also masquerade as a data source, providing subquery-like functionality. Query data sources are currently supported only by [GroupBy](../querying/groupbyquery.md) queries.\n+In the [Druid SQL](sql.html) language, datasources are provided in the [`FROM` clause](sql.html#from).\n \n-### Table datasource\n-The table data source is the most common type. It's represented by a string, or by the full structure:\n+The word \"datasource\" is generally spelled `dataSource` (with a capital S) when it appears in API requests and\n+responses.\n \n+## Datasource type\n+\n+### `table`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT column1, column2 FROM \"druid\".\"dataSourceName\"\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": \"dataSourceName\",\n+  \"columns\": [\"column1\", \"column2\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+The table datasource is the most common type. This is the kind of datasource you get when you perform\n+[data ingestion](../ingestion/index.html). They are split up into segments, distributed around the cluster,\n+and queried in parallel.\n+\n+In [Druid SQL](sql.html#from), table datasources reside in the the `druid` schema. This is the default schema, so table\n+datasources can be referenced as either `druid.dataSourceName` or simply `dataSourceName`.\n+\n+In native queries, table datasources can be referenced using their names as strings (as in the example above), or by\n+using JSON objects of the form:\n+\n+```json\n+\"dataSource\": {\n+  \"type\": \"table\",\n+  \"name\": \"dataSourceName\"\n+}\n+```\n+\n+To see a list of all table datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'`.\n+\n+### `lookup`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+SELECT k, v FROM lookup.countries\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"scan\",\n+  \"dataSource\": {\n+    \"type\": \"lookup\",\n+    \"lookup\": \"countries\"\n+  },\n+  \"columns\": [\"k\", \"v\"],\n+  \"intervals\": [\"0000/3000\"]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Lookup datasources correspond to Druid's key-value [lookup](lookups.html) objects. In [Druid SQL](sql.html#from),\n+they reside in the the `lookup` schema. They are preloaded in memory on all servers, so they can be accessed rapidly.\n+They can be joined onto regular tables using the [join operator](#join).\n+\n+Lookup datasources are key-value oriented and always have exactly two columns: `k` (the key) and `v` (the value), and\n+both are always strings.\n+\n+To see a list of all lookup datasources, use the SQL query\n+`SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'`.\n+\n+> Performance tip: Lookups can be joined with a base table either using an explicit [join](#join), or by using the\n+> SQL [`LOOKUP` function](sql.html#string-functions).\n+> However, the join operator must evaluate the condition on each row, whereas the\n+> `LOOKUP` function can defer evaluation until after an aggregation phase. This means that the `LOOKUP` function is\n+> usually faster than joining to a lookup datasource.\n+\n+### `query`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Uses a subquery to count hits per page, then takes the average.\n+SELECT\n+  AVG(cnt) AS average_hits_per_page\n+FROM\n+  (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page)\n+```\n+<!--Native-->\n+```json\n+{\n+  \"queryType\": \"timeseries\",\n+  \"dataSource\": {\n+    \"type\": \"query\",\n+    \"query\": {\n+      \"queryType\": \"groupBy\",\n+      \"dataSource\": \"site_traffic\",\n+      \"intervals\": [\"0000/3000\"],\n+      \"granularity\": \"all\",\n+      \"dimensions\": [\"page\"],\n+      \"aggregations\": [\n+        { \"type\": \"count\", \"name\": \"hits\" }\n+      ]\n+    }\n+  },\n+  \"intervals\": [\"0000/3000\"],\n+  \"granularity\": \"all\",\n+  \"aggregations\": [\n+    { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" },\n+    { \"type\": \"count\", \"name\": \"pages\" }\n+  ],\n+  \"postAggregations\": [\n+    { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" }\n+  ]\n+}\n+```\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a\n+`dataSource`. In SQL, they can appear in the following places, always surrounded by parentheses:\n+\n+- The FROM clause: `FROM (<subquery>)`.\n+- As inputs to a JOIN: `<table-or-subquery-1> t1 INNER JOIN <table-or-subquery-2> t2 ON t1.<col1> = t2.<col2>`.\n+- In the WHERE clause: `WHERE <column> { IN | NOT IN } (<subquery>)`. These are translated to joins by the SQL planner.\n+\n+> Performance tip: Subquery results need to be fully transferred to the Broker as part of query execution.\n+> This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits.\n+> See the [Query execution](query-execution.md) documentation for more details on how subqueries are executed and what\n+> limits will apply.\n+\n+### `join`\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+<!--SQL-->\n+```sql\n+-- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country.\n+SELECT\n+  countries.v AS country,", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MDkyNA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409080924", "bodyText": "Yes it should. Thanks.", "author": "gianm", "createdAt": "2020-04-15T19:25:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODk3NzkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAxMTc4OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409011789", "bodyText": "Should this be store_to_country.v instead of countries.v?", "author": "ccaominh", "createdAt": "2020-04-15T17:27:50Z", "path": "docs/querying/lookups.md", "diffHunk": "@@ -55,21 +56,37 @@ Other lookup types are available as extensions, including:\n Query Syntax\n ------------\n \n-In [Druid SQL](sql.html), lookups can be queried using the `LOOKUP` function, for example:\n+In [Druid SQL](sql.html), lookups can be queried using the [`LOOKUP` function](sql.md#string-functions), for example:\n \n+```sql\n+SELECT\n+  LOOKUP(store, 'store_to_country') AS country,\n+  SUM(revenue)\n+FROM sales\n+GROUP BY 1\n ```\n-SELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1\n+\n+They can also be queried using the [JOIN operator](datasource.md#join):\n+\n+```sql\n+SELECT\n+  countries.v AS country,", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA4MTIwNg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409081206", "bodyText": "Yes it should. Thanks.", "author": "gianm", "createdAt": "2020-04-15T19:26:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAxMTc4OQ=="}], "type": "inlineReview"}, {"oid": "4adc23ac42e710c6c782de917abf1031ff9e9c25", "url": "https://github.com/apache/druid/commit/4adc23ac42e710c6c782de917abf1031ff9e9c25", "message": "Add new words to spellcheck file.", "committedDate": "2020-04-15T19:25:16Z", "type": "commit"}, {"oid": "3e4232e41baca214aece9aeee488ee799bf7c557", "url": "https://github.com/apache/druid/commit/3e4232e41baca214aece9aeee488ee799bf7c557", "message": "Assorted changes.", "committedDate": "2020-04-15T19:52:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NTk4NA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409065984", "bodyText": "As a counter-balance to this point, I think we should discuss the motivations for using some form of join functionality with the caveat that performance is slower. eg. Dimensions that change after ingestion, reduce ingestion cost for some performance loss.", "author": "suneet-s", "createdAt": "2020-04-15T18:59:15Z", "path": "docs/querying/joins.md", "diffHunk": "@@ -22,33 +22,13 @@ title: \"Joins\"\n   ~ under the License.\n   -->\n \n+Druid has two features related to joining of data:\n \n-Apache Druid has limited support for joins through [query-time lookups](../querying/lookups.md). The common use case of\n-query-time lookups is to replace one dimension value (e.g. a String ID) with another value (e.g. a human-readable String value). This is similar to a star-schema join.\n+1. [Join](datasource.md#join) operators. These are available using a [join datasource](datasource.md#join) in native\n+queries, or using the [JOIN operator](sql.md#query-syntax) in Druid SQL. Refer to the\n+[join datasource](datasource.md#join) documentation for information about how joins work in Druid.\n+2. [Query-time lookups](lookups.md), simple key-to-value mappings. These are preloaded on all servers and can be\n+accessed with or without an explicit join operator. Refer to the [lookups](lookups.md) documentation for more details.\n \n-Druid does not yet have full support for joins. Although Druid\u2019s storage format would allow for the implementation\n-of joins (there is no loss of fidelity for columns included as dimensions), full support for joins have not yet been implemented yet\n-for the following reasons:\n-\n-1. Scaling join queries has been, in our professional experience,\n-a constant bottleneck of working with distributed databases.\n-2. The incremental gains in functionality are perceived to be\n-of less value than the anticipated problems with managing\n-highly concurrent, join-heavy workloads.\n-\n-A join query is essentially the merging of two or more streams of data based on a shared set of keys. The primary\n-high-level strategies for join queries we are aware of are a hash-based strategy or a\n-sorted-merge strategy. The hash-based strategy requires that all but\n-one data set be available as something that looks like a hash table,\n-a lookup operation is then performed on this hash table for every\n-row in the \u201cprimary\u201d stream. The sorted-merge strategy assumes\n-that each stream is sorted by the join key and thus allows for the incremental\n-joining of the streams. Each of these strategies, however,\n-requires the materialization of some number of the streams either in\n-sorted order or in a hash table form.\n-\n-When all sides of the join are significantly large tables (> 1 billion\n-records), materializing the pre-join streams requires complex\n-distributed memory management. The complexity of the memory\n-management is only amplified by the fact that we are targeting highly\n-concurrent, multi-tenant workloads.\n+Whenever possible, for best performance it is good to avoid joins at query time. Often this can be accomplished by\n+joining data before it is loaded into Druid.", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExOTc5MQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409119791", "bodyText": "I think those examples are really the same thing. I combined them added another. Here is the result. How does it look?\n\nWhenever possible, for best performance it is good to avoid joins at query time. Often this can be accomplished by joining data before it is loaded into Druid. However, there are situations where joins or lookups are the best solution available despite the performance overhead, including:\n\nThe fact-to-dimension (star and snowflake schema) case: you need to change dimension values after initial ingestion, and aren't able to reingest to do this. In this case, you can use lookups for your dimension tables.\nYour workload requires joins or filters on subqueries.", "author": "gianm", "createdAt": "2020-04-15T20:39:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NTk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzMTU3OA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409131578", "bodyText": "looks good. It appears I can't resolve these comments so I've just been emoji-ing the ones I agree with so that I don't spam you with notifications", "author": "suneet-s", "createdAt": "2020-04-15T21:01:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA2NTk4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3MjMxMw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409072313", "bodyText": "nit: The title looks strange as one word. Is postaggregation a word?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            title: \"Postaggregations\"\n          \n          \n            \n            title: \"Post Aggregations\"", "author": "suneet-s", "createdAt": "2020-04-15T19:10:15Z", "path": "docs/querying/post-aggregations.md", "diffHunk": "@@ -1,6 +1,6 @@\n ---\n id: post-aggregations\n-title: \"Post-Aggregations\"\n+title: \"Postaggregations\"", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5Nzk4OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409097989", "bodyText": "How about \"Post-aggregations\".", "author": "gianm", "createdAt": "2020-04-15T19:58:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3MjMxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3NzY5Mg==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409077692", "bodyText": "Should there be a section for datsource specific parameters? To talk about query contexts for join queries (filer pushdown ,etc.) or is that covered in another section", "author": "suneet-s", "createdAt": "2020-04-15T19:19:59Z", "path": "docs/querying/query-context.md", "diffHunk": "@@ -46,26 +55,28 @@ The query context is used for various query configuration parameters. The follow\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n \n+## Query-type-specific parameters", "originalCommit": "2f9d2b1b352118065b7af76cb5a30fc2d3b5a4ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5ODQ5Mw==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409098493", "bodyText": "Hmm, I'm not sure. I don't think those parameters are documented right now. Should they be or are they 'secret' internal parameters?", "author": "gianm", "createdAt": "2020-04-15T19:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3NzY5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA5OTY5OQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409099699", "bodyText": "/cc @jon-wei", "author": "gianm", "createdAt": "2020-04-15T20:01:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTA3NzY5Mg=="}], "type": "inlineReview"}, {"oid": "ab2619525c37d9896085393e5e1cfe5e7dbdec14", "url": "https://github.com/apache/druid/commit/ab2619525c37d9896085393e5e1cfe5e7dbdec14", "message": "Further adjustments.", "committedDate": "2020-04-15T20:39:49Z", "type": "commit"}, {"oid": "fdbbc4f6cffb182dd091230d0288662c1e9f9397", "url": "https://github.com/apache/druid/commit/fdbbc4f6cffb182dd091230d0288662c1e9f9397", "message": "Add missing punctuation.", "committedDate": "2020-04-15T20:41:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyMTMyOA==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409121328", "bodyText": "Do we want to talk about direct querying that was added in #9502", "author": "suneet-s", "createdAt": "2020-04-15T20:42:37Z", "path": "docs/querying/lookups.md", "diffHunk": "@@ -55,21 +56,37 @@ Other lookup types are available as extensions, including:\n Query Syntax\n ------------\n \n-In [Druid SQL](sql.html), lookups can be queried using the `LOOKUP` function, for example:\n+In [Druid SQL](sql.html), lookups can be queried using the [`LOOKUP` function](sql.md#string-functions), for example:\n \n+```sql\n+SELECT\n+  LOOKUP(store, 'store_to_country') AS country,\n+  SUM(revenue)\n+FROM sales\n+GROUP BY 1\n ```\n-SELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1\n+\n+They can also be queried using the [JOIN operator](datasource.md#join):", "originalCommit": "3e4232e41baca214aece9aeee488ee799bf7c557", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyNDQ2NQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409124465", "bodyText": "I don't think so, because the assumption should be you can direct query it (why not!)", "author": "gianm", "createdAt": "2020-04-15T20:48:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyMTMyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzMzI3NQ==", "url": "https://github.com/apache/druid/pull/9704#discussion_r409133275", "bodyText": "^ fair point, this documentation makes me think there are only 2 ways to query a lookup, but maybe it's because I'm biased because I've tried in the past and have not been able to query it before that PR :)", "author": "suneet-s", "createdAt": "2020-04-15T21:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEyMTMyOA=="}], "type": "inlineReview"}]}