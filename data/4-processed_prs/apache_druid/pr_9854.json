{"pr_number": 9854, "pr_title": "Integration Tests.", "pr_createdAt": "2020-05-12T10:37:21Z", "pr_url": "https://github.com/apache/druid/pull/9854", "timeline": [{"oid": "6efdbbf312a91cb57f9f7362541d6ec982e36970", "url": "https://github.com/apache/druid/commit/6efdbbf312a91cb57f9f7362541d6ec982e36970", "message": "Integration Tests.\nAdded docker-compose with druid-cluster configuration.\nRefactored shell scripts. split code in a few files", "committedDate": "2020-05-12T10:08:27Z", "type": "commit"}, {"oid": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "url": "https://github.com/apache/druid/commit/afa4facc8ccc21673cc48e5d9d980cb0cba98501", "message": "Integration Tests.\nAdded environment variable: DRUID_INTEGRATION_TEST_GROUP", "committedDate": "2020-05-14T11:12:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODM5Nw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425508397", "bodyText": "nit: change skip.start.docker to skip.run.docker to be consistent", "author": "maytasm", "createdAt": "2020-05-15T00:54:26Z", "path": "integration-tests/pom.xml", "diffHunk": "@@ -374,21 +375,23 @@\n                         <artifactId>exec-maven-plugin</artifactId>\n                         <executions>\n                             <execution>\n-                                <id>build-and-start-druid-cluster</id>\n+                                <id>docker-package</id>\n                                 <goals>\n                                     <goal>exec</goal>\n                                 </goals>\n                                 <phase>pre-integration-test</phase>\n                                 <configuration>\n                                     <environmentVariables>\n-                                    <DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>${start.hadoop.docker}</DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>\n-                                    <DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>${skip.start.docker}</DRUID_INTEGRATION_TEST_SKIP_START_DOCKER>\n-                                    <DRUID_INTEGRATION_TEST_JVM_RUNTIME>${jvm.runtime}</DRUID_INTEGRATION_TEST_JVM_RUNTIME>\n-                                    <DRUID_INTEGRATION_TEST_GROUP>${groups}</DRUID_INTEGRATION_TEST_GROUP>\n-                                    <DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH>${override.config.path}</DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH>\n-                                    <DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH>${resource.file.dir.path}</DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH>>\n+                                        <DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>${start.hadoop.docker}</DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER>\n+                                        <DRUID_INTEGRATION_TEST_JVM_RUNTIME>${jvm.runtime}</DRUID_INTEGRATION_TEST_JVM_RUNTIME>\n+                                        <DRUID_INTEGRATION_TEST_GROUP>${groups}</DRUID_INTEGRATION_TEST_GROUP>\n+                                        <DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH>${override.config.path}</DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH>\n+                                        <DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH>${resource.file.dir.path}</DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH>\n+                                        <DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER>${skip.build.docker}</DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER>\n+                                        <DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER>${skip.start.docker}</DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER>", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4MTgzMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425681833", "bodyText": "I keep \"start\" because it was added before.\nActually I dont like all variables: \"skip.*.docker\" because they have wrong naming. It is better to name it: \"docker.build.skip\", \"docker.run.skip\", \"docker.stop.skip\"\nAnd I think that we dont need \"docker.stop.skip\" variable because if you dont want to \"run\" cluster then we need to skip step \"stop\".", "author": "agricenko", "createdAt": "2020-05-15T09:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODM5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxNTA3NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425715074", "bodyText": "The purpose of DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER is only to prevent the tearing down of Druid cluster after the test completed. This is so that you can continue to inspect the cluster after the test finished.\nNow that I think about it, maybe we can remove DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER. I actually don't think it is that useful. Also, feel free to rename the variables. I also think \"docker.build.skip\", \"docker.run.skip\", \"docker.stop.skip\" are better. Please also update the README if you do rename them.", "author": "maytasm", "createdAt": "2020-05-15T10:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODM5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1NDUwMg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425754502", "bodyText": "Fixed.\nNew variable names: \"docker.run.skip\", \"docker.build.skip\"\nRemoved \"skip.stop,docker\" variable\nUpdated readme", "author": "agricenko", "createdAt": "2020-05-15T12:02:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODM5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425508702", "bodyText": "Where is the below deleted?\nWhy is the int-tests-config-file removed?", "author": "maytasm", "createdAt": "2020-05-15T00:55:31Z", "path": "integration-tests/pom.xml", "diffHunk": "@@ -446,61 +450,7 @@\n                             </suiteXmlFiles>\n                         </configuration>\n                     </plugin>\n-                    <plugin>\n-                        <groupId>de.thetaphi</groupId>\n-                        <artifactId>forbiddenapis</artifactId>\n-                        <configuration>\n-                            <signaturesFiles>\n-                                <!-- Needed because of https://github.com/policeman-tools/forbidden-apis/issues/126 -->\n-                                <signaturesFile>../codestyle/joda-time-forbidden-apis.txt</signaturesFile>\n-                                <signaturesFile>../codestyle/druid-forbidden-apis.txt</signaturesFile>\n-                            </signaturesFiles>\n-                        </configuration>\n-                    </plugin>\n-                </plugins>\n-            </build>\n-        </profile>\n-        <profile>\n-            <id>int-tests-config-file</id>\n-            <build>\n-                <plugins>\n-                    <plugin>\n-                        <groupId>org.apache.maven.plugins</groupId>\n-                        <artifactId>maven-failsafe-plugin</artifactId>\n-                        <executions>\n-                            <execution>\n-                                <id>integration-tests</id>\n-                                <phase>integration-test</phase>\n-                                <goals>\n-                                    <goal>integration-test</goal>\n-                                    <goal>verify</goal>\n-                                </goals>\n-                            </execution>\n-                        </executions>\n-                        <configuration>\n-                            <properties>\n-                                <property>\n-                                    <name>testrunfactory</name>\n-                                    <value>org.testng.DruidTestRunnerFactory</value>\n-                                </property>\n-                            </properties>\n-                            <argLine>\n-                                -Duser.timezone=UTC\n-                                -Dfile.encoding=UTF-8\n-                                -Ddruid.test.config.type=configFile\n-                                -Ddruid.test.config.configFile=${env.CONFIG_FILE}\n-                                -Ddruid.client.https.trustStorePath=client_tls/truststore.jks\n-                                -Ddruid.client.https.trustStorePassword=druid123\n-                                -Ddruid.client.https.keyStorePath=client_tls/client.jks\n-                                -Ddruid.client.https.certAlias=druid\n-                                -Ddruid.client.https.keyManagerPassword=druid123\n-                                -Ddruid.client.https.keyStorePassword=druid123\n-                            </argLine>\n-                            <suiteXmlFiles>\n-                                <suiteXmlFile>src/test/resources/testng.xml</suiteXmlFile>\n-                            </suiteXmlFiles>\n-                        </configuration>\n-                    </plugin>\n+", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4MDA5Mg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425680092", "bodyText": "I dont understand the reason of this Profile. We can run tests with integration-tests profile and add 3 arguments: \"skip.start.docker=true\" \"skip.stop.docker=true\" \"skip.build.docker=true\" to get the same behaviour as with \"int-tests-config-file\" profile", "author": "agricenko", "createdAt": "2020-05-15T09:28:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcwMzIxMg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425703212", "bodyText": "With the integration-tests profile, the test will run against docker cluster (tests will try to talk to env.DOCKER_IP). Those 3 arguments: \"skip.start.docker=true\" \"skip.stop.docker=true\" \"skip.build.docker=true\" does not change this fact. Whatever you set those argument to, using the integration-tests profile, tests will still talk to env.DOCKER_IP. Hence, if you do have skip.start.docker=true or skip.build.docker=true, you will need to make sure that somehow there is docker running at env.DOCKER_IP since the test will expect there to be druid at env.DOCKER_IP.\nOn the other hand, the int-tests-config-file will not assume docker is running at all. It will ignore those 3 arguments: \"skip.start.docker=true\" \"skip.stop.docker=true\" \"skip.build.docker=true\" and also ignore env.DOCKER_IP. It will read a file located at env.CONFIG_FILE for the ip of druid nodes. You can provide the ips of the druid node and other dependency services. Those services can be running on cloud, or locally, or on your network, etc.  You can even use this profile and set the config file to point to your production cluster (not recommended though haha)", "author": "maytasm", "createdAt": "2020-05-15T10:12:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1MTUxNw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425751517", "bodyText": "I dont know how is possible to run docker containers on any IP because we have hard coded ip addresses in shell scripts that expect some container in some IP address\nAnyway I will roll back this profile", "author": "agricenko", "createdAt": "2020-05-15T11:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc2MzE0Ng==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425763146", "bodyText": "rolled back", "author": "agricenko", "createdAt": "2020-05-15T12:20:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjAwOTQwNw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426009407", "bodyText": "I mean that this profile, int-tests-config-file, is not for running integration tests with Docker. It can run integration tests against Druid running at the given ip. The Druid that is running at the given ip does not need to be run using Docker.", "author": "maytasm", "createdAt": "2020-05-15T19:38:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUwODcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxMjgwNQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425512805", "bodyText": "nit:new line", "author": "maytasm", "createdAt": "2020-05-15T01:12:15Z", "path": "integration-tests/script/copy_resources.sh", "diffHunk": "@@ -0,0 +1,80 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# setup client keystore\n+./docker/tls/generate-client-certs-and-keystores.sh\n+rm -rf docker/client_tls\n+cp -r client_tls docker/client_tls\n+\n+# Make directories if they dont exist\n+mkdir -p $SHARED_DIR/hadoop_xml\n+mkdir -p $SHARED_DIR/hadoop-dependencies\n+mkdir -p $SHARED_DIR/logs\n+mkdir -p $SHARED_DIR/tasklogs\n+mkdir -p $SHARED_DIR/docker/extensions\n+mkdir -p $SHARED_DIR/docker/credentials\n+\n+# install druid jars\n+rm -rf $SHARED_DIR/docker\n+cp -R docker $SHARED_DIR/docker\n+mvn -B dependency:copy-dependencies -DoutputDirectory=$SHARED_DIR/docker/lib\n+\n+# move extensions into a seperate extension folder\n+# For druid-s3-extensions\n+mkdir -p $SHARED_DIR/docker/extensions/druid-s3-extensions\n+mv $SHARED_DIR/docker/lib/druid-s3-extensions-* $SHARED_DIR/docker/extensions/druid-s3-extensions\n+# For druid-azure-extensions\n+mkdir -p $SHARED_DIR/docker/extensions/druid-azure-extensions\n+mv $SHARED_DIR/docker/lib/druid-azure-extensions-* $SHARED_DIR/docker/extensions/druid-azure-extensions\n+# For druid-google-extensions\n+mkdir -p $SHARED_DIR/docker/extensions/druid-google-extensions\n+mv $SHARED_DIR/docker/lib/druid-google-extensions-* $SHARED_DIR/docker/extensions/druid-google-extensions\n+# For druid-hdfs-storage\n+mkdir -p $SHARED_DIR/docker/extensions/druid-hdfs-storage\n+mv $SHARED_DIR/docker/lib/druid-hdfs-storage-* $SHARED_DIR/docker/extensions/druid-hdfs-storage\n+# For druid-kinesis-indexing-service\n+mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n+mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n+# For druid-parquet-extensions\n+mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n+mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n+# For druid-orc-extensions\n+mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n+mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n+\n+# Pull Hadoop dependency if needed\n+if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n+then\n+  java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+  curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar\n+fi\n+\n+# install logging config\n+cp src/main/resources/log4j2.xml $SHARED_DIR/docker/lib/log4j2.xml\n+\n+# copy the integration test jar, it provides test-only extension implementations\n+cp target/druid-integration-tests*.jar $SHARED_DIR/docker/lib\n+\n+# one of the integration tests needs the wikiticker sample data\n+mkdir -p $SHARED_DIR/wikiticker-it\n+cp ../examples/quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz $SHARED_DIR/wikiticker-it/wikiticker-2015-09-12-sampled.json.gz\n+cp docker/wiki-simple-lookup.json $SHARED_DIR/wikiticker-it/wiki-simple-lookup.json\n+\n+# copy other files if needed\n+if [ -n \"$DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH\" ]\n+then\n+  cp -a $DRUID_INTEGRATION_TEST_RESOURCE_FILE_DIR_PATH/. $SHARED_DIR/docker/credentials/\n+fi", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyMzQ3MA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425723470", "bodyText": "fixed", "author": "agricenko", "createdAt": "2020-05-15T10:54:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxMjgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxMjk1MQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425512951", "bodyText": "nit:new line", "author": "maytasm", "createdAt": "2020-05-15T01:12:47Z", "path": "integration-tests/script/copy_hadoop_resources.sh", "diffHunk": "@@ -0,0 +1,44 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# wait for hadoop namenode to be up\n+echo \"Waiting for hadoop namenode to be up\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -mkdir -p /druid\"\n+while [ $? -ne 0 ]\n+do\n+   sleep 2\n+   docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -mkdir -p /druid\"\n+done\n+echo \"Finished waiting for Hadoop namenode\"\n+\n+# Setup hadoop druid dirs\n+echo \"Setting up druid hadoop dirs\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -mkdir -p /druid\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -mkdir -p /druid/segments\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -mkdir -p /quickstart\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -chmod 777 /druid\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -chmod 777 /druid/segments\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -chmod 777 /quickstart\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -chmod -R 777 /tmp\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -chmod -R 777 /user\"\n+# Copy data files to Hadoop container\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -put /shared/wikiticker-it/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\"\n+docker exec -t druid-it-hadoop sh -c \"./usr/local/hadoop/bin/hdfs dfs -put /resources/data/batch_index /batch_index\"\n+echo \"Finished setting up druid hadoop dirs\"\n+\n+echo \"Copying Hadoop XML files to shared\"\n+docker exec -t druid-it-hadoop sh -c \"cp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\"\n+echo \"Copied Hadoop XML files to shared\"", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyMzM5OQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425723399", "bodyText": "fixed", "author": "agricenko", "createdAt": "2020-05-15T10:54:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxMjk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425514931", "bodyText": "stop_cluster.sh has a check for DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER\nThe check for DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER is too skip teardown of the cluster after the test finish running (for debugging). In this case, where we are stopping the cluster so that we can start then fresh to run integration test, we should skip checking DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER here.", "author": "maytasm", "createdAt": "2020-05-15T01:21:26Z", "path": "integration-tests/build_run_cluster.sh", "diffHunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+echo $DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH\n+\n+export DIR=$(cd $(dirname $0) && pwd)\n+export HADOOP_DOCKER_DIR=$DIR/../examples/quickstart/tutorial/hadoop/docker\n+export DOCKERDIR=$DIR/docker\n+export SERVICE_SUPERVISORDS_DIR=$DOCKERDIR/service-supervisords\n+export ENVIRONMENT_CONFIGS_DIR=$DOCKERDIR/environment-configs\n+export SHARED_DIR=${HOME}/shared\n+export SUPERVISORDIR=/usr/lib/druid/conf\n+export RESOURCEDIR=$DIR/src/test/resources\n+\n+# so docker IP addr will be known during docker build\n+echo ${DOCKER_IP:=127.0.0.1} > $DOCKERDIR/docker_ip\n+\n+if !($DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER); then\n+  sh ./script/copy_resources.sh\n+  sh ./script/docker_build_containers.sh\n+fi\n+\n+if !($DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER); then\n+  sh ./stop_cluster.sh", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4ODQyNA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425688424", "bodyText": "This block is the same as we have in master, I only removed dublicated code. see: run_cluster.sh#line:24\nWe need to stop container 2 times:\n\nbefore - if you have already runned druid cluster and you want to run it again then you need to stop it\nafter - stop druid containers after integration tests\n\nAlso step (1) needed in case if your tests failed then step (2) not executed", "author": "agricenko", "createdAt": "2020-05-15T09:44:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxMzczMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425713733", "bodyText": "That is correct.\nI think we want to stop container in number 1 if DRUID_INTEGRATION_TEST_SKIP_START_DOCKER is not true and we want to stop container in number 2 if DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER is not false.\nHowever, in number 1, we do not have to care about what is the value of DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER. For example, number 1 we should stop (so that we can run before starting test) if DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER=true and DRUID_INTEGRATION_TEST_SKIP_START_DOCKER=false", "author": "maytasm", "createdAt": "2020-05-15T10:33:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxMzg5NQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425713895", "bodyText": "In other words, the DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER only applies to number 2 not number 1", "author": "maytasm", "createdAt": "2020-05-15T10:33:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxNDQ1MA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425714450", "bodyText": "The purpose of DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER is only to prevent the tearing down of Druid cluster after the test completed. This is so that you can continue to inspect the cluster after the test finished.", "author": "maytasm", "createdAt": "2020-05-15T10:34:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1NjYyMA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425756620", "bodyText": "I am confused a little bit about do we need to change something or not.\nWith new updates in pull request:\nWe dont have \"DRUID_INTEGRATION_TEST_SKIP_STOP_DOCKER\" variable.\nWe skip stop step if we DRUID_INTEGRATION_TEST_SKIP_START_DOCKER=true", "author": "agricenko", "createdAt": "2020-05-15T12:07:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1MDY1Ng==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426050656", "bodyText": "I think it is fine as is in the updated patch", "author": "maytasm", "createdAt": "2020-05-15T21:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNDkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNTczMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425515731", "bodyText": "Why the split DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER and DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER?\nI don't think we should ever want to BUILD but not RUN. I am also not really sure if having not BUILD and run is that useful. If your cluster is not running then you are not sure what state it is in, might as well be better to build and start fresh.", "author": "maytasm", "createdAt": "2020-05-15T01:24:51Z", "path": "integration-tests/build_run_cluster.sh", "diffHunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+echo $DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH\n+\n+export DIR=$(cd $(dirname $0) && pwd)\n+export HADOOP_DOCKER_DIR=$DIR/../examples/quickstart/tutorial/hadoop/docker\n+export DOCKERDIR=$DIR/docker\n+export SERVICE_SUPERVISORDS_DIR=$DOCKERDIR/service-supervisords\n+export ENVIRONMENT_CONFIGS_DIR=$DOCKERDIR/environment-configs\n+export SHARED_DIR=${HOME}/shared\n+export SUPERVISORDIR=/usr/lib/druid/conf\n+export RESOURCEDIR=$DIR/src/test/resources\n+\n+# so docker IP addr will be known during docker build\n+echo ${DOCKER_IP:=127.0.0.1} > $DOCKERDIR/docker_ip\n+\n+if !($DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER); then", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4NDM3Mw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425684373", "bodyText": "usually if you develop tests you need to build docker container only once. You dont need to do this more then once because you do not develop any of things that is in container.\nYou can run \"build\" flow without \"running tests\" in case if do this on CI and you want to build once container before runnning all types of integration tests.\nYour build flow will be next:\n\nbuild docker container\nrun parallel:\n2.1 s3 tests on builded container\n2.2 azure tests on builded container\n2.3 gcs tests on builded container\n2.4 etc", "author": "agricenko", "createdAt": "2020-05-15T09:36:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxNTY2NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425715664", "bodyText": "Sounds good. Can you also make sure to update the README", "author": "maytasm", "createdAt": "2020-05-15T10:37:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyNTk1NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425725954", "bodyText": "I think we can keep skip.build and skip.start.\nWe can then get rid of skip.stop.", "author": "maytasm", "createdAt": "2020-05-15T10:59:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc3MjcwMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425772701", "bodyText": "fixed,\nadded info about docker.build.skip into readme\nremoved docker.stop.skip variable", "author": "agricenko", "createdAt": "2020-05-15T12:39:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNTczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425516425", "bodyText": "copy_hadoop_resources.sh have to be run after hadoop cluster started BUT BEFORE the other druid containers start.", "author": "maytasm", "createdAt": "2020-05-15T01:27:39Z", "path": "integration-tests/build_run_cluster.sh", "diffHunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+echo $DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH\n+\n+export DIR=$(cd $(dirname $0) && pwd)\n+export HADOOP_DOCKER_DIR=$DIR/../examples/quickstart/tutorial/hadoop/docker\n+export DOCKERDIR=$DIR/docker\n+export SERVICE_SUPERVISORDS_DIR=$DOCKERDIR/service-supervisords\n+export ENVIRONMENT_CONFIGS_DIR=$DOCKERDIR/environment-configs\n+export SHARED_DIR=${HOME}/shared\n+export SUPERVISORDIR=/usr/lib/druid/conf\n+export RESOURCEDIR=$DIR/src/test/resources\n+\n+# so docker IP addr will be known during docker build\n+echo ${DOCKER_IP:=127.0.0.1} > $DOCKERDIR/docker_ip\n+\n+if !($DRUID_INTEGRATION_TEST_SKIP_BUILD_DOCKER); then\n+  sh ./script/copy_resources.sh\n+  sh ./script/docker_build_containers.sh\n+fi\n+\n+if !($DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER); then\n+  sh ./stop_cluster.sh\n+  sh ./script/docker_run_cluster.sh\n+fi\n+\n+if ($DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER); then\n+  sh ./script/copy_hadoop_resources.sh", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY5MDI1NQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425690255", "bodyText": "I dont understand why we do this before running all containers.\nWe only add more delay to wait for containers.\nCould you please explain why do we need to create all folders and other things before running all containers?", "author": "agricenko", "createdAt": "2020-05-15T09:47:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxMTc2Ng==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425711766", "bodyText": "Sure. Once you have started the Hadoop containers, hadoop will create a bunch of configuration files at /usr/local/hadoop/etc/hadoop/*.xml on the hadoop containers. These files will then have to be copy to the shared folder. Then our Druid java process will need to have these files in it's classpath when it startup. These cannot be added while Druid is already running, hence, that's why Druid processes are started after Hadoop", "author": "maytasm", "createdAt": "2020-05-15T10:29:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc3Njk5Mg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425776992", "bodyText": "If this files can't be added after Druid started then why I dont have any errors on running integration tests with hadoop?", "author": "agricenko", "createdAt": "2020-05-15T12:46:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1MTg2NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426051864", "bodyText": "Can you verify if the hadoop tests still work if before you run the hadoop integration test your shared folder is empty. Since we never clean up the shared directory after the tests finish running, I think the reason you don't have any error is because your Druid cluster is picking up old files in .../shared/hadoop_xml from your previous runs.", "author": "maytasm", "createdAt": "2020-05-15T21:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ3MTQ2OA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426471468", "bodyText": "verified:\n\ndeleted ~/shared/hadoop_xml/*\ns3 tests passed", "author": "agricenko", "createdAt": "2020-05-18T08:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ3NDAzMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426474033", "bodyText": "I think you want to run hdfs-deep-storage group since that test index_hadoop task.", "author": "maytasm", "createdAt": "2020-05-18T09:01:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0NDEwMA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r433544100", "bodyText": "In case anyone is looking at this,\nThe hdfs-deep-storage test pass as is in this current patch.\nI suspect that while we do start Druid before the hadoop_xml files get created, we do not run the ingestion task yet.\nWe do know where the files will be created so the classpath was set correctly when Druid starts. Hence, when we run the hadoop index ingestion task in the integration tests, those hadoop_xml files was created already.", "author": "maytasm", "createdAt": "2020-06-01T23:39:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNjQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNzk3Mg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425517972", "bodyText": "Can you add \"hdfs-deep-storage\", \"s3-ingestion\", \"kinesis-index\", and \"kinesis-data-format\" to this list too.", "author": "maytasm", "createdAt": "2020-05-15T01:33:27Z", "path": "integration-tests/script/docker_run_cluster.sh", "diffHunk": "@@ -0,0 +1,63 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Create docker network\n+{\n+  docker network create --subnet=172.172.172.0/24 druid-it-net\n+}\n+\n+# setup all enviornment variables to be pass to the containers\n+COMMON_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/common -e DRUID_INTEGRATION_TEST_GROUP\"\n+BROKER_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/broker\"\n+COORDINATOR_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/coordinator\"\n+HISTORICAL_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/historical\"\n+MIDDLEMANAGER_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/middlemanager\"\n+OVERLORD_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/overlord\"\n+ROUTER_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/router\"\n+ROUTER_CUSTOM_CHECK_TLS_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/router-custom-check-tls\"\n+ROUTER_NO_CLIENT_AUTH_TLS_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/router-no-client-auth-tls\"\n+ROUTER_PERMISSIVE_TLS_ENV=\"--env-file=$ENVIRONMENT_CONFIGS_DIR/router-permissive-tls\"\n+\n+if [ -z \"$DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH\" ]\n+then\n+    echo \"\\$DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH is not set. No override config file provided\"\n+    if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"s3-deep-storage\" ] || \\\n+    [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"gcs-deep-storage\" ] || \\\n+    [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"azure-deep-storage\" ]; then", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyNTc4Ng==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425725786", "bodyText": "added", "author": "agricenko", "createdAt": "2020-05-15T10:59:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxNzk3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxOTk0Mw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425519943", "bodyText": "What is the purpose of this since we already have integration-tests/docker/docker-compose.override-env.yml\nNote that the env file in environment-configs/override-examples/ is never meant to be use as-is or modified directly. User should copy this and write their own override env file and use integration-tests/docker/docker-compose.override-env.yml", "author": "maytasm", "createdAt": "2020-05-15T01:41:14Z", "path": "integration-tests/docker/docker-compose.gcs.yml", "diffHunk": "@@ -0,0 +1,182 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyNzg5MQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425727891", "bodyText": "See my comment in \"docker-compose.azure.yml\" comments", "author": "agricenko", "createdAt": "2020-05-15T11:04:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxOTk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1Njk1Mw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425756953", "bodyText": "removed docker-compose.gcs.yml file", "author": "agricenko", "createdAt": "2020-05-15T12:07:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUxOTk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDAxNA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425520014", "bodyText": "What is the purpose of this since we already have integration-tests/docker/docker-compose.override-env.yml\nNote that the env file in environment-configs/override-examples/ is never meant to be use as-is or modified directly. User should copy this and write their own override env file and use integration-tests/docker/docker-compose.override-env.yml", "author": "maytasm", "createdAt": "2020-05-15T01:41:38Z", "path": "integration-tests/docker/docker-compose.azure.yml", "diffHunk": "@@ -0,0 +1,182 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcwOTI1Mw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425709253", "bodyText": "Usually developers prepare docker-compose files for each \"use case\". And all this docker-compose files it's something stable, you do some changes in this files rarely.\nIf you have all docker-compose files for each use case you can see how many combinations you have, you can see what the difference between each type of configuration.\nI understand that all files in \"environment-configs/override-examples/\" this is more like example configuration but for new developer who want to understand how to run \"s3\" tests he need to check only docker-compose.s3.yml.\nWe have 2 ways how to run \"druid containers for s3\" tests:\n\nrun hadoop + druid:\ndocker-compose -f ./integration-tests/docker/docker-compose.s3.yml up -d\n\n2.run hadoop:\ndocker-compose -f ./integration-tests/docker/docker-compose.hadoop.yml up -d\nrun druid-containers:\nOVERRIDE_ENV=<full_path_to_env_file> docker-compose -f ./integration-tests/docker/docker-compose.override-env.yml up -d", "author": "agricenko", "createdAt": "2020-05-15T10:24:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxMDgyOA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425710828", "bodyText": "I am fine to remove all \"azure\", \"gcs\", \"s3\" docker-compose files but to run docker containers by docker-compose we need to run 2 commands or create own \"docker-compose\" files for each run configuration.", "author": "agricenko", "createdAt": "2020-05-15T10:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxOTYwNw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425719607", "bodyText": "I think \"azure\", \"gcs\", \"s3\" docker-compose are not necessary and can add to confusion. Those \"environment-configs/override-examples/\" are examples. Even if you do want to run the s3 tests, you will likely need to modify the files from \"environment-configs/override-examples/\". Also, I think it make more sense to separate hadoop from Druid clusters. Our configurations for Druid can simply be running with default env and running with override env. Then hadoop can optionally be started or not started.", "author": "maytasm", "createdAt": "2020-05-15T10:45:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1NzI3OA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425757278", "bodyText": "removed docker-compose.s3.yml, docker-compose.azure.yml, docker-compose.gcs.yml files", "author": "agricenko", "createdAt": "2020-05-15T12:08:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDAxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDQ2Mg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425520462", "bodyText": "What is the purpose of this since we already have integration-tests/docker/docker-compose.override-env.yml\nNote that the env file in environment-configs/override-examples/ is never meant to be use as-is or modified directly. User should copy this and write their own override env file and use integration-tests/docker/docker-compose.override-env.yml", "author": "maytasm", "createdAt": "2020-05-15T01:43:17Z", "path": "integration-tests/docker/docker-compose.s3.yml", "diffHunk": "@@ -0,0 +1,182 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxODg0MQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425718841", "bodyText": "See my comment in \"docker-compose.azure.yml\" comments", "author": "agricenko", "createdAt": "2020-05-15T10:44:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDQ2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc1NzM4MQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425757381", "bodyText": "removed docker-compose.s3.yml file", "author": "agricenko", "createdAt": "2020-05-15T12:08:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDQ2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425520923", "bodyText": "Do we need both integration-tests/docker/docker-compose.test-env.yml and integration-tests/docker/docker-compose.yml?", "author": "maytasm", "createdAt": "2020-05-15T01:45:09Z", "path": "integration-tests/docker/docker-compose.test-env.yml", "diffHunk": "@@ -0,0 +1,173 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyMTAyMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425721023", "bodyText": "The issue is that we have 2 different configuration to run integration tests.\nsecurity tests required: \"DRUID_INTEGRATION_TEST_GROUP\" env variables\ns3, azure, etc required: \"OVERRIDE_ENV\"\nI can merge both this ways but then I will need to create file with empty configuration in (environment-configs/override-examples/)", "author": "agricenko", "createdAt": "2020-05-15T10:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyNDI3NQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425724275", "bodyText": "I'm a little confused. When is integration-tests/docker/docker-compose.yml use?\nI see that s3, azure, gcs uses docker-compose.override-env.yml and everything else uses docker-compose.test-env.yml.\nI think it will be very helpful if you can list out the purpose of each yml", "author": "maytasm", "createdAt": "2020-05-15T10:56:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyNDUxOA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425724518", "bodyText": "Also I think every tests needs DRUID_INTEGRATION_TEST_GROUP", "author": "maytasm", "createdAt": "2020-05-15T10:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc2MjkxMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425762911", "bodyText": "I expect to use \"docker-compose.yml\" for all tests that do not need custom env configs\nBut unfortunately this is not works, because druid.sh prepare data for \"query\" and \"security\" tests (this is wrong because this must be done by tests)\n\nI think that we can remove \"docker-compose.test-env.yml\"\nI move DRUID_INTEGRATION_TEST_GROUP to docker-compose.yml\n\ndocker-compose - it's files for containers, not for tests. If your tests need \"DRUID_INTEGRATION_TEST_GROUP\" variable you pass this variable my maven run or by some config file.", "author": "agricenko", "createdAt": "2020-05-15T12:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc2MzYzMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425763631", "bodyText": "removed docker-compose.test-env.yml\nAdded DRUID_INTEGRATION_TEST_GROUP env to docker-compose.yml", "author": "agricenko", "createdAt": "2020-05-15T12:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMDkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425521442", "bodyText": "Is it possible to use the environment variables we define in the scripts for the path to mount volumes and env files here. (i.e.  COMMON_ENV, SERVICE_SUPERVISORDS_DIR, SUPERVISORDIR, RESOURCEDIR, etc.)", "author": "maytasm", "createdAt": "2020-05-15T01:47:06Z", "path": "integration-tests/docker/docker-compose.base.yml", "diffHunk": "@@ -0,0 +1,271 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more", "originalCommit": "afa4facc8ccc21673cc48e5d9d980cb0cba98501", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcxODA2NQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425718065", "bodyText": "yes it's possible, we can create file with env variables, but then this docker-compose file will be bad readable.\nWhat is the reason to use variable like: \"COMMON_ENV\"?\nEnv variables used in case if you want to customize something\n------- Currently -------\nThis part is readable, you can understand that we mount file \"zookeeper.conf\" from \"current dir/service-supervisords/zookeeper.conf\" to some path in your container and you can see this path\nvolumes:\n- ${HOME}/shared:/shared\n- ./service-supervisords/zookeeper.conf:/usr/lib/druid/conf/zookeeper.conf\n- ./service-supervisords/kafka.conf:/usr/lib/druid/conf/kafka.conf\n------- With env variables like in run_cluster.sh -------\nlook at this configuration from person who is new. What he see? He see a lot of env variables, he don't know what the values of this variables, he need to spend some time to find where this variables declared to see path. This configuration is hard to understand\nvolumes:\n- ${HOME}/shared:/shared\n- ${SERVICE_SUPERVISORDS_DIR}/zookeeper.conf:${SUPERVISORDIR}/zookeeper.conf\n- ${SERVICE_SUPERVISORDS_DIR}/kafka.conf:${SUPERVISORDIR}/kafka.conf", "author": "agricenko", "createdAt": "2020-05-15T10:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcyMTk2OA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425721968", "bodyText": "I think this is to keep it in sync with the other scripts. For example, if I move the folder path in build_run_cluster.sh to something different then the path will be wrong in this file.", "author": "maytasm", "createdAt": "2020-05-15T10:51:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc0Mjk3Ng==", "url": "https://github.com/apache/druid/pull/9854#discussion_r425742976", "bodyText": "you need to split \"env variables for customizing server\" and \"env variables that you created for development\".\nWe need to avoid \"env variable for development\" in docker-compose files.\nIf you have some configuration for your contaner this config files must be in close folder.\nIf you want to move docker compose files you need to move then with configuration files. they need to be in the same folder because they have the same context\nFolders paths, configs paths, any other things that related to container config they must be in \"docker-compose\" files, not in shell scripts, java pom.xml, etc", "author": "agricenko", "createdAt": "2020-05-15T11:37:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA2MTI0OQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426061249", "bodyText": "Should the SHARED_DIR be passed to yml the same way as OVERRIDE_ENV (instead of hardcoding it to ${HOME}/shared). Since SHARED_DIR does not have to be in the same directory as the yml and can be anywhere.", "author": "maytasm", "createdAt": "2020-05-15T21:46:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQwNjA2OQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426406069", "bodyText": "Do we have different configurations for shared_dir path?", "author": "agricenko", "createdAt": "2020-05-18T06:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAwMDU1NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r427000554", "bodyText": "Right now it is hardcoded to outside of Druid directory at ${HOME}/shared\nMaybe that should be changed (not related to your patch)", "author": "maytasm", "createdAt": "2020-05-19T02:53:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA5NjczMg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r427096732", "bodyText": "Currently it is done in correct way.\nAll paths that related to server configuration that need to be done in yaml and you dont need to create env variable for this.", "author": "agricenko", "createdAt": "2020-05-19T07:47:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyMTQ0Mg=="}], "type": "inlineReview"}, {"oid": "61c4a1016a3be2bda2f1c0b23d8e3cf9b8b8eef0", "url": "https://github.com/apache/druid/commit/61c4a1016a3be2bda2f1c0b23d8e3cf9b8b8eef0", "message": "Integration Tests. Removed nit", "committedDate": "2020-05-15T10:54:10Z", "type": "commit"}, {"oid": "5b6ece926fb6378cf0332fa6794bc8510c77dc97", "url": "https://github.com/apache/druid/commit/5b6ece926fb6378cf0332fa6794bc8510c77dc97", "message": "Integration Tests. Updated if block in docker_run_cluster.sh.", "committedDate": "2020-05-15T10:59:12Z", "type": "commit"}, {"oid": "410d0cdf80045d9b274fa4f54841f9749fb0c47b", "url": "https://github.com/apache/druid/commit/410d0cdf80045d9b274fa4f54841f9749fb0c47b", "message": "Integration Tests. Readme. Added Docker-compose section.", "committedDate": "2020-05-15T11:24:51Z", "type": "commit"}, {"oid": "55ee7a5134eeefc3df36e56696f31069c098b903", "url": "https://github.com/apache/druid/commit/55ee7a5134eeefc3df36e56696f31069c098b903", "message": "Integration Tests. removed yml files for s3, gcs, azure.\nRenamed variables for skip start/stop/build docker.\nUpdated readme.\nRollback maven profile: int-tests-config-file", "committedDate": "2020-05-15T12:01:20Z", "type": "commit"}, {"oid": "578bca258cd81d23eb15e2f9345f34f0c56120df", "url": "https://github.com/apache/druid/commit/578bca258cd81d23eb15e2f9345f34f0c56120df", "message": "Integration Tests. Removed docker-compose.test-env.yml file.\nAdded DRUID_INTEGRATION_TEST_GROUP variable to docker-compose.yml", "committedDate": "2020-05-15T12:19:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NDg2NQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426054865", "bodyText": "Can you mention that this is only if there is no code change to Druid. For example, your integration tests that you are writing might actually catch real bugs in the code. Then after you attempt to fix bug in code, you will need to build druid again.", "author": "maytasm", "createdAt": "2020-05-15T21:26:59Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn\n+\n+Build druid-cluster, druid-hadoop docker images. From root module run maven command:\n+```\n+mvn clean install -pl integration-tests -P integration-tests -Ddocker.run.skip=true -Dmaven.test.skip=true\n+```\n+\n+Run druid cluster by docker-compose:\n+```\n+docker-compose -f integration-tests/docker/docker-compose.yml up\n+```\n+\n+Run integration tests:\n+```\n+mvn verify -pl integration-tests -P integration-tests -Dgroups=batch-index -Djvm.runtime=8 -Pskip-static-checks -Ddruid.console.skip=true -Dmaven.javadoc.skip=true -Ddocker.run.skip=true -Ddocker.build.skip=true\n+```\n+\n+Run s3 | azure | gcs integration tests:\n+\n+Prepare configuration for integration tests:\n+```\n+   integration-tests/docker/environment-configs/override-examples/*\n+```\n+\n+Run docker compose for one of group:\n+```\n+    docker-compose -f integration-tests/docker/docker-compose-s3.yml up\n+```\n+\n+Run maven command to execute tests. \n+> See: Running a Test That Uses Cloud \n+\n ## Tips & tricks for debugging and developing integration tests\n \n ### Useful mvn command flags\n \n-- -Dskip.start.docker=true to skip starting docker containers. This can save ~3 minutes by skipping building and bringing \n+- -Ddocker.build.skip=true to skip build druid containers. This can save ~4 minutes to build druid cluster and druid hadoop.\n+You need to build druid containers only once, after you can skip docker build step. ", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ4OTI4NA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426489284", "bodyText": "added more details", "author": "agricenko", "createdAt": "2020-05-18T09:25:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NDg2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAwMDkxNw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r427000917", "bodyText": "Thanks!", "author": "maytasm", "createdAt": "2020-05-19T02:54:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NDg2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NjM3MQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426056371", "bodyText": "I think this is not needed as this is still the same as the instruction above. Since we already have the instruction to run the test in ## Running tests and instruction on using -Ddocker.x.skip, I think this is redundant.", "author": "maytasm", "createdAt": "2020-05-15T21:31:05Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn\n+\n+Build druid-cluster, druid-hadoop docker images. From root module run maven command:\n+```\n+mvn clean install -pl integration-tests -P integration-tests -Ddocker.run.skip=true -Dmaven.test.skip=true\n+```\n+\n+Run druid cluster by docker-compose:\n+```\n+docker-compose -f integration-tests/docker/docker-compose.yml up\n+```\n+\n+Run integration tests:", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5MDI4OQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426490289", "bodyText": "removed", "author": "agricenko", "createdAt": "2020-05-18T09:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NjM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAwMTU0MA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r427001540", "bodyText": "LGTM", "author": "maytasm", "createdAt": "2020-05-19T02:57:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NjM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzE1Mw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426057153", "bodyText": "We do not need to run build and docker-compose manually right?\nIf we run mvn verify -pl integration-tests -P integration-tests... then it should automatically build and run docker containers. If that is the case, I think this section can be rename to instruction for using/manually bringing up docker containers instead of running tests.", "author": "maytasm", "createdAt": "2020-05-15T21:33:22Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5MTgxNg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426491816", "bodyText": "on run integration tests you also run docker containers.\nRenamed section", "author": "agricenko", "createdAt": "2020-05-18T09:29:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzQxMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426057411", "bodyText": "I think this is not needed. If we run mvn verify -pl integration-tests -P integration-tests... then it should automatically build and run docker containers. If that is the case, the instruction in Running a Test That Uses Cloud  should be enough", "author": "maytasm", "createdAt": "2020-05-15T21:34:07Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn\n+\n+Build druid-cluster, druid-hadoop docker images. From root module run maven command:\n+```\n+mvn clean install -pl integration-tests -P integration-tests -Ddocker.run.skip=true -Dmaven.test.skip=true\n+```\n+\n+Run druid cluster by docker-compose:\n+```\n+docker-compose -f integration-tests/docker/docker-compose.yml up\n+```\n+\n+Run integration tests:\n+```\n+mvn verify -pl integration-tests -P integration-tests -Dgroups=batch-index -Djvm.runtime=8 -Pskip-static-checks -Ddruid.console.skip=true -Dmaven.javadoc.skip=true -Ddocker.run.skip=true -Ddocker.build.skip=true\n+```\n+\n+Run s3 | azure | gcs integration tests:", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5MTg3MA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426491870", "bodyText": "removed", "author": "agricenko", "createdAt": "2020-05-18T09:29:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzU3OA==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426057578", "bodyText": "I think this file no longer exist?", "author": "maytasm", "createdAt": "2020-05-15T21:34:40Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn\n+\n+Build druid-cluster, druid-hadoop docker images. From root module run maven command:\n+```\n+mvn clean install -pl integration-tests -P integration-tests -Ddocker.run.skip=true -Dmaven.test.skip=true\n+```\n+\n+Run druid cluster by docker-compose:\n+```\n+docker-compose -f integration-tests/docker/docker-compose.yml up\n+```\n+\n+Run integration tests:\n+```\n+mvn verify -pl integration-tests -P integration-tests -Dgroups=batch-index -Djvm.runtime=8 -Pskip-static-checks -Ddruid.console.skip=true -Dmaven.javadoc.skip=true -Ddocker.run.skip=true -Ddocker.build.skip=true\n+```\n+\n+Run s3 | azure | gcs integration tests:\n+\n+Prepare configuration for integration tests:\n+```\n+   integration-tests/docker/environment-configs/override-examples/*\n+```\n+\n+Run docker compose for one of group:\n+```\n+    docker-compose -f integration-tests/docker/docker-compose-s3.yml up", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5MjEyNg==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426492126", "bodyText": "removed", "author": "agricenko", "createdAt": "2020-05-18T09:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1ODEwMw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426058103", "bodyText": "Thanks for adding \"Build druid-cluster, druid-hadoop docker images. From root module run maven command:\" and \"Run druid cluster by docker-compose:\". I think it might be nice if we document the variation of yml we have here too", "author": "maytasm", "createdAt": "2020-05-15T21:36:16Z", "path": "integration-tests/README.md", "diffHunk": "@@ -68,16 +68,48 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Running tests by docker-compose + mvn\n+\n+Build druid-cluster, druid-hadoop docker images. From root module run maven command:", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5ODgwNw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426498807", "bodyText": "Added details about all yamls", "author": "agricenko", "createdAt": "2020-05-18T09:41:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1ODEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1OTg2OQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426059869", "bodyText": "Do we still need all of these variables?\nI think SERVICE_SUPERVISORDS_DIR, SUPERVISORDIR, RESOURCEDIR can be reomved since the yml doesn't use them", "author": "maytasm", "createdAt": "2020-05-15T21:41:47Z", "path": "integration-tests/build_run_cluster.sh", "diffHunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+echo $DRUID_INTEGRATION_TEST_OVERRIDE_CONFIG_PATH\n+\n+export DIR=$(cd $(dirname $0) && pwd)\n+export HADOOP_DOCKER_DIR=$DIR/../examples/quickstart/tutorial/hadoop/docker\n+export DOCKERDIR=$DIR/docker\n+export SERVICE_SUPERVISORDS_DIR=$DOCKERDIR/service-supervisords", "originalCommit": "578bca258cd81d23eb15e2f9345f34f0c56120df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQ5MjcxMQ==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426492711", "bodyText": "Probably we do not need all of them, need to check", "author": "agricenko", "createdAt": "2020-05-18T09:31:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1OTg2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjUzMzM0Nw==", "url": "https://github.com/apache/druid/pull/9854#discussion_r426533347", "bodyText": "removed not need variables", "author": "agricenko", "createdAt": "2020-05-18T10:41:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1OTg2OQ=="}], "type": "inlineReview"}, {"oid": "1776ea2711f30aba860260fba11f867c6028f1aa", "url": "https://github.com/apache/druid/commit/1776ea2711f30aba860260fba11f867c6028f1aa", "message": "Integration Tests. Readme. Added details about docker-compose", "committedDate": "2020-05-18T09:42:07Z", "type": "commit"}, {"oid": "4076bdba1c58ea568f4b21dbdb3bb7700f00fc43", "url": "https://github.com/apache/druid/commit/4076bdba1c58ea568f4b21dbdb3bb7700f00fc43", "message": "Integration Tests. cleanup shell scripts", "committedDate": "2020-05-18T10:14:14Z", "type": "commit"}, {"oid": "c478c1ca1ee60e04093e36aabab8d512c4b6a1a3", "url": "https://github.com/apache/druid/commit/c478c1ca1ee60e04093e36aabab8d512c4b6a1a3", "message": "Merge remote-tracking branch 'remotes/origin/master' into integration_tests_improvements_docker_compose", "committedDate": "2020-06-02T11:57:00Z", "type": "commit"}, {"oid": "56d086d26fe255aa6a0c4c8715bd8fc3fd69fa84", "url": "https://github.com/apache/druid/commit/56d086d26fe255aa6a0c4c8715bd8fc3fd69fa84", "message": "Merge branch 'master' into integration_tests_improvements_docker_compose", "committedDate": "2020-06-02T12:15:44Z", "type": "commit"}]}