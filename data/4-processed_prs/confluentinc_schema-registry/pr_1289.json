{"pr_number": 1289, "pr_title": "First cut at JSON Schema support", "pr_createdAt": "2020-01-09T23:29:39Z", "pr_url": "https://github.com/confluentinc/schema-registry/pull/1289", "timeline": [{"oid": "3ff85720fe79d6f7d8d5fb1956a5f735ec955fc0", "url": "https://github.com/confluentinc/schema-registry/commit/3ff85720fe79d6f7d8d5fb1956a5f735ec955fc0", "message": "First cut at JSON Schema support", "committedDate": "2020-01-10T21:43:16Z", "type": "forcePushed"}, {"oid": "a7dc0128b0a0523754bab4a224b22dd8f5026cf5", "url": "https://github.com/confluentinc/schema-registry/commit/a7dc0128b0a0523754bab4a224b22dd8f5026cf5", "message": "First cut at JSON Schema support", "committedDate": "2020-01-13T23:16:03Z", "type": "forcePushed"}, {"oid": "b92e65f80a138d66ce467e627d8bc431c744b755", "url": "https://github.com/confluentinc/schema-registry/commit/b92e65f80a138d66ce467e627d8bc431c744b755", "message": "First cut at JSON Schema support", "committedDate": "2020-01-14T21:37:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MDY3NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366650674", "bodyText": "typo : ProtobufSchemaProvider", "author": "OneCricketeer", "createdAt": "2020-01-15T00:59:12Z", "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/KafkaSchemaRegistry.java", "diffHunk": "@@ -176,6 +176,11 @@ public KafkaSchemaRegistry(SchemaRegistryConfig config,\n     SchemaProvider avroSchemaProvider = new AvroSchemaProvider();\n     avroSchemaProvider.configure(schemaProviderConfigs);\n     schemaProviders.add(avroSchemaProvider);\n+    // Ensure JSON SchemaProvider is registered\n+    SchemaProvider jsonSchemaProvider = new ProtobufSchemaProvider();", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MTE2MQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366671161", "bodyText": "Good catch", "author": "rayokota", "createdAt": "2020-01-15T02:34:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MDY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MDkyMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366650920", "bodyText": "Could a mapper be protected and inherited from ClusterTestHarness?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:00:11Z", "path": "core/src/test/java/io/confluent/kafka/schemaregistry/rest/json/RestApiTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.rest.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.client.rest.RestService;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.requests.RegisterSchemaRequest;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();\n+\n+  private static final ObjectMapper MAPPER = new ObjectMapper();", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MTIzMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366651233", "bodyText": "I find it strange that avro-serializer is necessary", "author": "OneCricketeer", "createdAt": "2020-01-15T01:01:34Z", "path": "json-schema-converter/pom.xml", "diffHunk": "@@ -0,0 +1,71 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-parent</artifactId>\n+        <version>5.5.0-SNAPSHOT</version>\n+    </parent>\n+\n+    <licenses>\n+        <license>\n+            <name>Confluent Community License</name>\n+            <url>http://www.confluent.io/confluent-community-license</url>\n+            <distribution>repo</distribution>\n+        </license>\n+        <license>\n+            <name>Apache License 2.0</name>\n+            <url>http://www.apache.org/licenses/LICENSE-2.0.html</url>\n+            <distribution>repo</distribution>\n+        </license>\n+    </licenses>\n+\n+    <artifactId>kafka-connect-json-schema-converter</artifactId>\n+    <packaging>jar</packaging>\n+    <name>kafka-connect-json-schema-converter</name>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>io.confluent</groupId>\n+            <artifactId>kafka-json-schema-provider</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>io.confluent</groupId>\n+            <artifactId>kafka-json-schema-serializer</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>io.confluent</groupId>\n+            <artifactId>kafka-avro-serializer</artifactId>", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3NDE0OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366674149", "bodyText": "Yes, I'm thinking of factoring out common stuff from kafka-avro-serializer", "author": "rayokota", "createdAt": "2020-01-15T02:48:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MTIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MjY3Mw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366652673", "bodyText": "Should you be using Jackson equivalents of ArrayNode and ObjectNode rather than org.json?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:07:47Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/JsonSchema.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+\n+package io.confluent.kafka.schemaregistry.json;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ArrayNode;\n+import com.fasterxml.jackson.databind.node.BinaryNode;\n+import com.fasterxml.jackson.databind.node.BooleanNode;\n+import com.fasterxml.jackson.databind.node.NullNode;\n+import com.fasterxml.jackson.databind.node.NumericNode;\n+import com.fasterxml.jackson.databind.node.TextNode;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.everit.json.schema.Schema;\n+import org.everit.json.schema.ValidationException;\n+import org.everit.json.schema.loader.SchemaLoader;\n+import org.json.JSONArray;\n+import org.json.JSONObject;", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MTYwMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366671603", "bodyText": "The underlying JSON Schema library uses org.json.  I translate back and forth between org.json and Jackson at various points.", "author": "rayokota", "createdAt": "2020-01-15T02:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MjY3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjI3OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672278", "bodyText": "Ah. https://github.com/everit-org/json-schema/blob/master/core/pom.xml#L220-L222", "author": "OneCricketeer", "createdAt": "2020-01-15T02:39:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MjY3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1MzEzMQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366653131", "bodyText": "Is there a preference of methods over Predicate instances?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:09:31Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/JsonSchema.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+\n+package io.confluent.kafka.schemaregistry.json;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ArrayNode;\n+import com.fasterxml.jackson.databind.node.BinaryNode;\n+import com.fasterxml.jackson.databind.node.BooleanNode;\n+import com.fasterxml.jackson.databind.node.NullNode;\n+import com.fasterxml.jackson.databind.node.NumericNode;\n+import com.fasterxml.jackson.databind.node.TextNode;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.everit.json.schema.Schema;\n+import org.everit.json.schema.ValidationException;\n+import org.everit.json.schema.loader.SchemaLoader;\n+import org.json.JSONArray;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.json.diff.Difference;\n+import io.confluent.kafka.schemaregistry.json.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+\n+public class JsonSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(JsonSchemaProvider.class);\n+\n+  public static final String TYPE = \"JSON\";\n+\n+  private static final Object NONE_MARKER = new Object();\n+\n+  private final Schema schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, String> resolvedReferences;\n+\n+  private final ObjectMapper objectMapper = Jackson.newObjectMapper();\n+\n+  @VisibleForTesting\n+  public JsonSchema(JsonNode jsonNode) throws JsonProcessingException {\n+    this(new ObjectMapper().writeValueAsString(jsonNode));\n+  }\n+\n+  public JsonSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public JsonSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    SchemaLoader.SchemaLoaderBuilder builder = SchemaLoader.builder();\n+    if (resolvedReferences != null) {\n+      try {\n+        for (Map.Entry<String, String> dep : resolvedReferences.entrySet()) {\n+          builder.registerSchemaByURI(new URI(dep.getKey()), new JSONObject(dep.getValue()));\n+        }\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(\"Invalid dependency name\", e);\n+      }\n+    }\n+    builder.schemaJson(new JSONObject(schemaString));\n+    SchemaLoader loader = builder.build();\n+    this.schemaObj = loader.load().build();\n+    this.version = version;\n+    this.references = references;\n+    this.resolvedReferences = resolvedReferences;\n+  }\n+\n+  public JsonSchema(Schema schemaObj) {\n+    this(schemaObj, null);\n+  }\n+\n+  public JsonSchema(Schema schemaObj, Integer version) {\n+    this.schemaObj = schemaObj;\n+    this.version = version;\n+    this.references = Collections.emptyList();\n+    this.resolvedReferences = Collections.emptyMap();\n+  }\n+\n+  public Schema rawSchema() {\n+    return schemaObj;\n+  }\n+\n+  @Override\n+  public String schemaType() {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public String name() {\n+    return schemaObj.getTitle();\n+  }\n+\n+  @Override\n+  public String canonicalString() {\n+    return schemaObj.toString();\n+  }\n+\n+  public Integer version() {\n+    return version;\n+  }\n+\n+  @Override\n+  public List<SchemaReference> references() {\n+    return references;\n+  }\n+\n+  public Map<String, String> resolvedReferences() {\n+    return resolvedReferences;\n+  }\n+\n+  public void validate(Object value) throws JsonProcessingException, ValidationException {\n+    Object primitiveValue = NONE_MARKER;\n+    if (isPrimitive(value)) {\n+      primitiveValue = value;\n+    } else if (value instanceof BinaryNode) {\n+      primitiveValue = ((BinaryNode) value).asText();\n+    } else if (value instanceof BooleanNode) {\n+      primitiveValue = ((BooleanNode) value).asBoolean();\n+    } else if (value instanceof NullNode) {\n+      primitiveValue = null;\n+    } else if (value instanceof NumericNode) {\n+      primitiveValue = ((NumericNode) value).numberValue();\n+    } else if (value instanceof TextNode) {\n+      primitiveValue = ((TextNode) value).asText();\n+    }\n+    if (primitiveValue != NONE_MARKER) {\n+      rawSchema().validate(primitiveValue);\n+    } else {\n+      Object jsonObject;\n+      if (value instanceof ArrayNode) {\n+        jsonObject = objectMapper.treeToValue(((ArrayNode) value), JSONArray.class);\n+      } else if (value instanceof JsonNode) {\n+        jsonObject = objectMapper.treeToValue(((JsonNode) value), JSONObject.class);\n+      } else if (value.getClass().isArray()) {\n+        jsonObject = objectMapper.convertValue(value, JSONArray.class);\n+      } else {\n+        jsonObject = objectMapper.convertValue(value, JSONObject.class);\n+      }\n+      rawSchema().validate(jsonObject);\n+    }\n+  }\n+\n+  private static boolean isPrimitive(Object value) {\n+    return value == null\n+        || value instanceof Boolean\n+        || value instanceof Number\n+        || value instanceof String;\n+  }", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1Mzk2NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366653965", "bodyText": "Are there plans to support higher drafts?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:12:58Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/JsonSchemaUtils.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.JsonNodeFactory;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.kjetland.jackson.jsonSchema.JsonSchemaConfig;\n+import com.kjetland.jackson.jsonSchema.JsonSchemaGenerator;\n+\n+import java.io.IOException;\n+import java.io.StringWriter;\n+import java.nio.charset.StandardCharsets;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+\n+public class JsonSchemaUtils {\n+\n+  private static final Object NONE_MARKER = new Object();\n+\n+  private static final ObjectMapper jsonMapper = Jackson.newObjectMapper();\n+\n+  static final String ENVELOPE_SCHEMA_FIELD_NAME = \"schema\";\n+  static final String ENVELOPE_PAYLOAD_FIELD_NAME = \"payload\";\n+\n+  public static ObjectNode envelope(JsonNode schema, JsonNode payload) {\n+    ObjectNode result = JsonNodeFactory.instance.objectNode();\n+    result.set(ENVELOPE_SCHEMA_FIELD_NAME, schema);\n+    result.set(ENVELOPE_PAYLOAD_FIELD_NAME, payload);\n+    return result;\n+  }\n+\n+  public static JsonSchema copyOf(JsonSchema schema) {\n+    return new JsonSchema(schema.canonicalString(),\n+        schema.references(),\n+        schema.resolvedReferences(),\n+        schema.version()\n+    );\n+  }\n+\n+  public static JsonSchema getSchema(Object object) {\n+    if (object == null) {\n+      return null;\n+    }\n+    if (object instanceof JsonNode) {\n+      JsonNode jsonValue = (JsonNode) object;\n+      if (jsonValue.isObject() && jsonValue.has(ENVELOPE_SCHEMA_FIELD_NAME)) {\n+        return new JsonSchema(jsonValue.get(ENVELOPE_SCHEMA_FIELD_NAME).toString());\n+      }\n+    }\n+    JsonSchemaConfig config = JsonSchemaConfig.nullableJsonSchemaDraft4();  // allow nulls", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjA1MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672050", "bodyText": "This is just a config on the underlying library to allow nullable types for draft 4 or later.", "author": "rayokota", "createdAt": "2020-01-15T02:38:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1Mzk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3NDk0Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366674946", "bodyText": "draft 4 or later\n\nIIRC, draft 6 & 7 are not backwards compatible with 4, so not sure this would support \"or later\"", "author": "OneCricketeer", "createdAt": "2020-01-15T02:52:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1Mzk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3OTkzOA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366679938", "bodyText": "The config is a bit misnamed :  Under the hood nullableJsonSchemaDraft4 toggles the useOneOfForOption and useOneOfForNullables properties on JsonSchemaConfig. from https://github.com/mbknor/mbknor-jackson-jsonSchema/", "author": "rayokota", "createdAt": "2020-01-15T03:19:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1Mzk2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NDg2OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366654869", "bodyText": "Do you need both Joda and JavaTime?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:16:46Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/Jackson.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.datatype.guava.GuavaModule;\n+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;\n+import com.fasterxml.jackson.datatype.joda.JodaModule;\n+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\n+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;\n+\n+import static com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES;\n+\n+/**\n+ * A utility class for Jackson.\n+ */\n+public class Jackson {\n+  private Jackson() { /* singleton */ }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper}.\n+   */\n+  public static ObjectMapper newObjectMapper() {\n+    final ObjectMapper mapper = new ObjectMapper();\n+\n+    return configure(mapper);\n+  }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper} with a custom\n+   * {@link com.fasterxml.jackson.core.JsonFactory}.\n+   *\n+   * @param jsonFactory instance of {@link com.fasterxml.jackson.core.JsonFactory} to use\n+   *     for the created {@link com.fasterxml.jackson.databind.ObjectMapper} instance.\n+   */\n+  public static ObjectMapper newObjectMapper(JsonFactory jsonFactory) {\n+    final ObjectMapper mapper = new ObjectMapper(jsonFactory);\n+\n+    return configure(mapper);\n+  }\n+\n+  private static ObjectMapper configure(ObjectMapper mapper) {\n+    mapper.registerModule(new GuavaModule());\n+    mapper.registerModule(new JodaModule());\n+    mapper.registerModule(new ParameterNamesModule());\n+    mapper.registerModule(new Jdk8Module());\n+    mapper.registerModule(new JavaTimeModule());", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjI0OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672249", "bodyText": "This is to allow the client to use either in their Jackson serializable classes.", "author": "rayokota", "createdAt": "2020-01-15T02:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NDg2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTMxMQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366655311", "bodyText": "Same comment as above", "author": "OneCricketeer", "createdAt": "2020-01-15T01:18:42Z", "path": "json-schema-serializer/pom.xml", "diffHunk": "@@ -0,0 +1,70 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>io.confluent</groupId>\n+        <artifactId>kafka-schema-registry-parent</artifactId>\n+        <version>5.5.0-SNAPSHOT</version>\n+    </parent>\n+\n+    <licenses>\n+        <license>\n+            <name>Confluent Community License</name>\n+            <url>http://www.confluent.io/confluent-community-license</url>\n+            <distribution>repo</distribution>\n+        </license>\n+        <license>\n+            <name>Apache License 2.0</name>\n+            <url>http://www.apache.org/licenses/LICENSE-2.0.html</url>\n+            <distribution>repo</distribution>\n+        </license>\n+    </licenses>\n+\n+    <artifactId>kafka-json-schema-serializer</artifactId>\n+    <packaging>jar</packaging>\n+    <name>kafka-json-schema-serializer</name>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.apache.kafka</groupId>\n+            <artifactId>kafka_${kafka.scala.version}</artifactId>\n+            <version>${kafka.version}</version>\n+            <scope>provided</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>io.confluent</groupId>\n+            <artifactId>kafka-json-schema-provider</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+        <dependency>\n+            <groupId>io.confluent</groupId>\n+            <artifactId>kafka-avro-serializer</artifactId>", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3NDIzOA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366674238", "bodyText": "Yes, I'm thinking of factoring out common stuff from kafka-avro-serializer", "author": "rayokota", "createdAt": "2020-01-15T02:48:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTMxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTY0MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366655640", "bodyText": "Best to extract out the ObjectMapper instance", "author": "OneCricketeer", "createdAt": "2020-01-15T01:20:18Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageFormatter.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import kafka.common.MessageFormatter;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageFormatter, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageFormatter and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-consumer.sh. Then run the following command.\n+ *\n+ * <p>1. To read only the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081\n+ *\n+ * <p>2. To read both the key and the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true\n+ *\n+ * <p>3. To read the key, value, and timestamp of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true \\\n+ * --property print.timestamp=true\n+ */\n+public class JsonSchemaMessageFormatter extends AbstractKafkaJsonSchemaDeserializer\n+    implements MessageFormatter {\n+\n+  private static final byte[] NULL_BYTES = \"null\".getBytes(StandardCharsets.UTF_8);\n+  private boolean printKey = false;\n+  private boolean printTimestamp = false;\n+  private boolean printIds = false;\n+  private boolean printKeyId = false;\n+  private boolean printValueId = false;\n+  private byte[] keySeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] lineSeparator = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] idSeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private Deserializer keyDeserializer;\n+\n+  /**\n+   * Constructor needed by kafka console consumer.\n+   */\n+  public JsonSchemaMessageFormatter() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  JsonSchemaMessageFormatter(\n+      SchemaRegistryClient schemaRegistryClient, Deserializer keyDeserializer\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keyDeserializer = keyDeserializer;\n+  }\n+\n+  @Override\n+  public void init(Properties props) {\n+    if (props == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+    schemaRegistry = createSchemaRegistry(url, originals);\n+\n+    if (props.containsKey(\"print.timestamp\")) {\n+      printTimestamp = props.getProperty(\"print.timestamp\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"print.key\")) {\n+      printKey = props.getProperty(\"print.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"line.separator\")) {\n+      lineSeparator = props.getProperty(\"line.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"key.deserializer\")) {\n+      try {\n+        keyDeserializer = (Deserializer) Class.forName((String) props.get(\"key.deserializer\"))\n+            .newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key deserializer\", e);\n+      }\n+    }\n+    if (props.containsKey(\"print.schema.ids\")) {\n+      printIds = props.getProperty(\"print.schema.ids\").trim().toLowerCase().equals(\"true\");\n+      if (printIds) {\n+        printValueId = true;\n+        if (keyDeserializer == null\n+            || keyDeserializer instanceof AbstractKafkaJsonSchemaDeserializer) {\n+          printKeyId = true;\n+        }\n+      }\n+    }\n+    if (props.containsKey(\"schema.id.separator\")) {\n+      idSeparator = props.getProperty(\"schema.id.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public void writeTo(ConsumerRecord<byte[], byte[]> consumerRecord, PrintStream output) {\n+    if (printTimestamp) {\n+      try {\n+        TimestampType timestampType = consumerRecord.timestampType();\n+        if (timestampType != TimestampType.NO_TIMESTAMP_TYPE) {\n+          output.write(String.format(\"%s:%d\", timestampType, consumerRecord.timestamp())\n+              .getBytes(StandardCharsets.UTF_8));\n+        } else {\n+          output.write(\"NO_TIMESTAMP\".getBytes(StandardCharsets.UTF_8));\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the timestamp\", ioe);\n+      }\n+    }\n+    if (printKey) {\n+      try {\n+        if (keyDeserializer != null) {\n+          Object deserializedKey = consumerRecord.key() == null\n+                                   ? null\n+                                   : keyDeserializer.deserialize(null, consumerRecord.key());\n+          output.write(deserializedKey != null ? deserializedKey.toString()\n+              .getBytes(StandardCharsets.UTF_8) : NULL_BYTES);\n+        } else {\n+          writeTo(consumerRecord.key(), output);\n+        }\n+        if (printKeyId) {\n+          output.write(idSeparator);\n+          int schemaId = schemaIdFor(consumerRecord.key());\n+          output.print(schemaId);\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the key\", ioe);\n+      }\n+    }\n+    try {\n+      writeTo(consumerRecord.value(), output);\n+      if (printValueId) {\n+        output.write(idSeparator);\n+        int schemaId = schemaIdFor(consumerRecord.value());\n+        output.print(schemaId);\n+      }\n+      output.write(lineSeparator);\n+    } catch (IOException ioe) {\n+      throw new SerializationException(\"Error while formatting the value\", ioe);\n+    }\n+  }\n+\n+  private void writeTo(byte[] data, PrintStream output) throws IOException {\n+    Object object = deserialize(data);\n+    output.print(Jackson.newObjectMapper().writeValueAsString(object));", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY4NjY4NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366686684", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T03:57:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTc3OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366655778", "bodyText": "Comment should be probably be updated with JSONSchema", "author": "OneCricketeer", "createdAt": "2020-01-15T01:20:58Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageReader.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import kafka.common.KafkaException;\n+import kafka.common.MessageReader;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.everit.json.schema.ValidationException;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaSerializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageReader, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageReader and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-producer.sh. Then run the following\n+ * command.\n+ *\n+ * <p>1. Send JSON Schema string as value. (make sure there is no space in the schema string)\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.JsonSchemaMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"string\"}'", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjMzMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672333", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T02:39:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTk2NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366655965", "bodyText": "Same comment as above about ObjectMapper extraction", "author": "OneCricketeer", "createdAt": "2020-01-15T01:21:44Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageReader.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import kafka.common.KafkaException;\n+import kafka.common.MessageReader;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.everit.json.schema.ValidationException;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaSerializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageReader, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageReader and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-producer.sh. Then run the following\n+ * command.\n+ *\n+ * <p>1. Send JSON Schema string as value. (make sure there is no space in the schema string)\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.JsonSchemaMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"string\"}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * \"a\"\n+ * \"b\"\n+ *\n+ * <p>2. Send JSON Schema record as value.\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.JsonSchemaMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n+ * \"type\":\"string\"}]}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * {\"f1\": \"value1\"}\n+ *\n+ * <p>3. Send JSON Schema string as key and JSON Schema record as value.\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.JsonSchemaMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property parse.key=true \\\n+ * --property key.schema='{\"type\":\"string\"}' \\\n+ * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n+ * \"type\":\"string\"}]}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * \"key1\" \\t {\"f1\": \"value1\"}\n+ */\n+public class JsonSchemaMessageReader extends AbstractKafkaJsonSchemaSerializer\n+    implements MessageReader {\n+\n+  private String topic = null;\n+  private BufferedReader reader = null;\n+  private Boolean parseKey = false;\n+  private String keySeparator = \"\\t\";\n+  private boolean ignoreError = false;\n+  private JsonSchema keySchema = null;\n+  private JsonSchema valueSchema = null;\n+  private String keySubject = null;\n+  private String valueSubject = null;\n+  private Serializer keySerializer;\n+\n+  /**\n+   * Constructor needed by kafka console producer.\n+   */\n+  public JsonSchemaMessageReader() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  JsonSchemaMessageReader(\n+      SchemaRegistryClient schemaRegistryClient,\n+      JsonSchema keySchema,\n+      JsonSchema valueSchema,\n+      String topic,\n+      boolean parseKey,\n+      BufferedReader reader,\n+      boolean autoRegister\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keySchema = keySchema;\n+    this.valueSchema = valueSchema;\n+    this.topic = topic;\n+    this.keySubject = topic + \"-key\";\n+    this.valueSubject = topic + \"-value\";\n+    this.parseKey = parseKey;\n+    this.reader = reader;\n+    this.autoRegisterSchema = autoRegister;\n+  }\n+\n+  @Override\n+  public void init(java.io.InputStream inputStream, Properties props) {\n+    topic = props.getProperty(\"topic\");\n+    if (props.containsKey(\"parse.key\")) {\n+      parseKey = props.getProperty(\"parse.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\");\n+    }\n+    if (props.containsKey(\"ignore.error\")) {\n+      ignoreError = props.getProperty(\"ignore.error\").trim().toLowerCase().equals(\"true\");\n+    }\n+    reader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+\n+    schemaRegistry = new CachedSchemaRegistryClient(Collections.singletonList(url),\n+        AbstractKafkaSchemaSerDeConfig.MAX_SCHEMAS_PER_SUBJECT_DEFAULT,\n+        Collections.singletonList(new JsonSchemaProvider()),\n+        originals\n+    );\n+    if (!props.containsKey(\"value.schema\")) {\n+      throw new ConfigException(\"Must provide the JSON schema string in value.schema\");\n+    }\n+    String valueSchemaString = props.getProperty(\"value.schema\");\n+    valueSchema = new JsonSchema(valueSchemaString);\n+\n+    keySerializer = getKeySerializer(props);\n+\n+    if (needsKeySchema()) {\n+      if (!props.containsKey(\"key.schema\")) {\n+        throw new ConfigException(\"Must provide the JSON schema string in key.schema\");\n+      }\n+      String keySchemaString = props.getProperty(\"key.schema\");\n+      keySchema = new JsonSchema(keySchemaString);\n+    }\n+    keySubject = topic + \"-key\";\n+    valueSubject = topic + \"-value\";\n+    if (props.containsKey(\"auto.register\")) {\n+      this.autoRegisterSchema = Boolean.parseBoolean(props.getProperty(\"auto.register\").trim());\n+    } else {\n+      this.autoRegisterSchema = true;\n+    }\n+  }\n+\n+  private Serializer getKeySerializer(Properties props) throws ConfigException {\n+    if (props.containsKey(\"key.serializer\")) {\n+      try {\n+        return (Serializer) Class.forName((String) props.get(\"key.serializer\")).newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key serializer\", e);\n+      }\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private boolean needsKeySchema() {\n+    return parseKey && keySerializer == null;\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public ProducerRecord<byte[], byte[]> readMessage() {\n+    try {\n+      String line = reader.readLine();\n+      if (line == null) {\n+        return null;\n+      }\n+      if (!parseKey) {\n+        Object value = jsonToJsonNode(line);\n+        byte[] serializedValue = serializeImpl(valueSubject, value, valueSchema);\n+        return new ProducerRecord<>(topic, serializedValue);\n+      } else {\n+        int keyIndex = line.indexOf(keySeparator);\n+        if (keyIndex < 0) {\n+          if (ignoreError) {\n+            Object value = jsonToJsonNode(line);\n+            byte[] serializedValue = serializeImpl(valueSubject, value, valueSchema);\n+            return new ProducerRecord<>(topic, serializedValue);\n+          } else {\n+            throw new KafkaException(\"No key found in line \" + line);\n+          }\n+        } else {\n+          String keyString = line.substring(0, keyIndex);\n+          String valueString = (keyIndex + keySeparator.length() > line.length())\n+                               ? \"\"\n+                               : line.substring(keyIndex + keySeparator.length());\n+          byte[] serializedKey;\n+          if (keySerializer != null) {\n+            serializedKey = keySerializer.serialize(topic, keyString);\n+          } else {\n+            Object key = jsonToJsonNode(keyString);\n+            serializedKey = serializeImpl(keySubject, key, keySchema);\n+          }\n+          Object value = jsonToJsonNode(valueString);\n+          byte[] serializedValue = serializeImpl(valueSubject, value, valueSchema);\n+          return new ProducerRecord<>(topic, serializedKey, serializedValue);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new KafkaException(\"Error reading from input\", e);\n+    }\n+  }\n+\n+  private Object jsonToJsonNode(String jsonString) {\n+    try {\n+      return Jackson.newObjectMapper().readTree(jsonString);", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY4NjY0Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366686647", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T03:57:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NTk2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1Njc0Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366656747", "bodyText": "This seems duplicated on the DeserializerConfig (minus the DOC)", "author": "OneCricketeer", "createdAt": "2020-01-15T01:25:22Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/serializers/json/KafkaJsonSchemaSerializerConfig.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.serializers.json;\n+\n+import java.util.Map;\n+\n+import io.confluent.common.config.ConfigDef;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+\n+public class KafkaJsonSchemaSerializerConfig extends AbstractKafkaSchemaSerDeConfig {\n+\n+  public static final String FAIL_INVALID_SCHEMA = \"json.fail.invalid.schema\";", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjE0Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672142", "bodyText": "The underlying JSON Schema library uses org.json. I translate back and forth between org.json and Jackson at various points.", "author": "rayokota", "createdAt": "2020-01-15T02:38:37Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/JSONArrayDeserializer.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonParser;\n+import com.fasterxml.jackson.core.JsonToken;\n+import com.fasterxml.jackson.databind.DeserializationContext;\n+import com.fasterxml.jackson.databind.deser.std.StdDeserializer;\n+import com.fasterxml.jackson.databind.util.ClassUtil;\n+import org.json.JSONArray;\n+import org.json.JSONObject;", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjE1Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672157", "bodyText": "The underlying JSON Schema library uses org.json. I translate back and forth between org.json and Jackson at various points.", "author": "rayokota", "createdAt": "2020-01-15T02:38:41Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/JSONObjectDeserializer.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonParser;\n+import com.fasterxml.jackson.core.JsonToken;\n+import com.fasterxml.jackson.databind.DeserializationContext;\n+import com.fasterxml.jackson.databind.deser.std.StdDeserializer;\n+import org.json.JSONException;\n+import org.json.JSONObject;", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MjI5Mw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r366672293", "bodyText": "The underlying JSON Schema library uses org.json. I translate back and forth between org.json and Jackson at various points.", "author": "rayokota", "createdAt": "2020-01-15T02:39:23Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/Jackson.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.datatype.guava.GuavaModule;\n+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;\n+import com.fasterxml.jackson.datatype.joda.JodaModule;\n+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\n+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;\n+\n+import static com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES;\n+\n+/**\n+ * A utility class for Jackson.\n+ */\n+public class Jackson {\n+  private Jackson() { /* singleton */ }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper}.\n+   */\n+  public static ObjectMapper newObjectMapper() {\n+    final ObjectMapper mapper = new ObjectMapper();\n+\n+    return configure(mapper);\n+  }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper} with a custom\n+   * {@link com.fasterxml.jackson.core.JsonFactory}.\n+   *\n+   * @param jsonFactory instance of {@link com.fasterxml.jackson.core.JsonFactory} to use\n+   *     for the created {@link com.fasterxml.jackson.databind.ObjectMapper} instance.\n+   */\n+  public static ObjectMapper newObjectMapper(JsonFactory jsonFactory) {\n+    final ObjectMapper mapper = new ObjectMapper(jsonFactory);\n+\n+    return configure(mapper);\n+  }\n+\n+  private static ObjectMapper configure(ObjectMapper mapper) {\n+    mapper.registerModule(new GuavaModule());\n+    mapper.registerModule(new JodaModule());\n+    mapper.registerModule(new ParameterNamesModule());\n+    mapper.registerModule(new Jdk8Module());\n+    mapper.registerModule(new JavaTimeModule());\n+    mapper.registerModule(new JsonOrgModule());", "originalCommit": "b92e65f80a138d66ce467e627d8bc431c744b755", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzMjQzMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367132430", "bodyText": "Should schema strings be externalized to resource files?", "author": "OneCricketeer", "createdAt": "2020-01-15T22:06:52Z", "path": "core/src/test/java/io/confluent/kafka/schemaregistry/rest/json/RestApiTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.rest.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.client.rest.RestService;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.requests.RegisterSchemaRequest;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();\n+\n+  private static final ObjectMapper MAPPER = new ObjectMapper();\n+\n+  public RestApiTest() {\n+    super(1, true);\n+  }\n+\n+  @Override\n+  protected Properties getSchemaRegistryProperties() {\n+    Properties props = new Properties();\n+    props.setProperty(\"schema.providers\", JsonSchemaProvider.class.getName());\n+    return props;\n+  }\n+\n+  @Test\n+  public void testBasic() throws Exception {\n+    String subject1 = \"testTopic1\";\n+    String subject2 = \"testTopic2\";\n+    int schemasInSubject1 = 10;\n+    List<Integer> allVersionsInSubject1 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject1 = getRandomJsonSchemas(schemasInSubject1);\n+    int schemasInSubject2 = 5;\n+    List<Integer> allVersionsInSubject2 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject2 = getRandomJsonSchemas(schemasInSubject2);\n+    List<String> allSubjects = new ArrayList<String>();\n+\n+    // test getAllSubjects with no existing data\n+    assertEquals(\"Getting all subjects should return empty\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+\n+    // test registering and verifying new schemas in subject1\n+    int schemaIdCounter = 1;\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      String schema = allSchemasInSubject1.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject1);\n+      schemaIdCounter++;\n+      allVersionsInSubject1.add(expectedVersion);\n+    }\n+    allSubjects.add(subject1);\n+\n+    // test re-registering existing schemas\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      int expectedId = i + 1;\n+      String schemaString = allSchemasInSubject1.get(i);\n+      int foundId = restApp.restClient.registerSchema(schemaString,\n+          JsonSchema.TYPE,\n+          Collections.emptyList(),\n+          subject1\n+      );\n+      assertEquals(\"Re-registering an existing schema should return the existing version\",\n+          expectedId,\n+          foundId\n+      );\n+    }\n+\n+    // test registering schemas in subject2\n+    for (int i = 0; i < schemasInSubject2; i++) {\n+      String schema = allSchemasInSubject2.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject2);\n+      schemaIdCounter++;\n+      allVersionsInSubject2.add(expectedVersion);\n+    }\n+    allSubjects.add(subject2);\n+\n+    // test getAllVersions with existing data\n+    assertEquals(\n+        \"Getting all versions from subject1 should match all registered versions\",\n+        allVersionsInSubject1,\n+        restApp.restClient.getAllVersions(subject1)\n+    );\n+    assertEquals(\n+        \"Getting all versions from subject2 should match all registered versions\",\n+        allVersionsInSubject2,\n+        restApp.restClient.getAllVersions(subject2)\n+    );\n+\n+    // test getAllSubjects with existing data\n+    assertEquals(\"Getting all subjects should match all registered subjects\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+  }\n+\n+  @Test\n+  public void testSchemaReferences() throws Exception {\n+    Map<String, String> schemas = getJsonSchemaWithReferences();\n+    String subject = \"reference\";\n+    registerAndVerifySchema(restApp.restClient, schemas.get(\"ref.json\"), 1, subject);\n+\n+    RegisterSchemaRequest request = new RegisterSchemaRequest();\n+    request.setSchema(schemas.get(\"main.json\"));\n+    request.setSchemaType(JsonSchema.TYPE);\n+    SchemaReference ref = new SchemaReference(\"ref.json\", \"reference\", 1);\n+    request.setReferences(Collections.singletonList(ref));\n+    int registeredId = restApp.restClient.registerSchema(request, \"referrer\");\n+    assertEquals(\"Registering a new schema should succeed\", 2, registeredId);\n+\n+    SchemaString schemaString = restApp.restClient.getId(2);\n+    // the newly registered schema should be immediately readable on the master\n+    assertEquals(\"Registered schema should be found\",\n+        MAPPER.readTree(schemas.get(\"main.json\")),\n+        MAPPER.readTree(schemaString.getSchemaString())\n+    );\n+\n+    assertEquals(\"Schema references should be found\",\n+        Collections.singletonList(ref),\n+        schemaString.getReferences()\n+    );\n+  }\n+\n+  public static void registerAndVerifySchema(\n+      RestService restService,\n+      String schemaString,\n+      int expectedId,\n+      String subject\n+  ) throws IOException, RestClientException {\n+    int registeredId = restService.registerSchema(\n+        schemaString,\n+        JsonSchema.TYPE,\n+        Collections.emptyList(),\n+        subject\n+    );\n+    Assert.assertEquals(\n+        \"Registering a new schema should succeed\",\n+        (long) expectedId,\n+        (long) registeredId\n+    );\n+    Assert.assertEquals(\"Registered schema should be found\",\n+        MAPPER.readTree(schemaString),\n+        MAPPER.readTree(restService.getId(expectedId).getSchemaString())\n+    );\n+  }\n+\n+  public static List<String> getRandomJsonSchemas(int num) {\n+    List<String> schemas = new ArrayList<>();\n+    for (int i = 0; i < num; i++) {\n+      String schema = \"{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"f\"\n+          + random.nextInt(Integer.MAX_VALUE)\n+          + \"\\\":\"\n+          + \"{\\\"type\\\":\\\"string\\\"}},\\\"additionalProperties\\\":false}\";\n+      schemas.add(schema);\n+    }\n+    return schemas;\n+  }\n+\n+  public static Map<String, String> getJsonSchemaWithReferences() {\n+    Map<String, String> schemas = new HashMap<>();\n+    String reference = \"{\\\"type\\\":\\\"object\\\",\\\"additionalProperties\\\":false,\\\"definitions\\\":\"\n+        + \"{\\\"ExternalType\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"name\\\":{\\\"type\\\":\\\"string\\\"}},\"\n+        + \"\\\"additionalProperties\\\":false}}}\";", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0OTA5OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r370749099", "bodyText": "+1", "author": "dragosvictor", "createdAt": "2020-01-24T17:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzMjQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNDYyNQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367134625", "bodyText": "Is there a way to refactor this duplicated block?\ne.g. protected void writePrimitives(JsonGenerator g, Class cls, Object ob)", "author": "OneCricketeer", "createdAt": "2020-01-15T22:12:23Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/JSONArraySerializer.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.core.JsonToken;\n+import com.fasterxml.jackson.core.type.WritableTypeId;\n+import com.fasterxml.jackson.databind.JsonMappingException;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializerProvider;\n+import com.fasterxml.jackson.databind.jsontype.TypeSerializer;\n+import org.json.JSONArray;\n+import org.json.JSONObject;\n+\n+import java.io.IOException;\n+import java.lang.reflect.Type;\n+\n+public class JSONArraySerializer extends JSONBaseSerializer<JSONArray> {\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final JSONArraySerializer instance = new JSONArraySerializer();\n+\n+  public JSONArraySerializer() {\n+    super(JSONArray.class);\n+  }\n+\n+  @Override // since 2.6\n+  public boolean isEmpty(SerializerProvider provider, JSONArray value) {\n+    return (value == null) || value.length() == 0;\n+  }\n+\n+  @Override\n+  public void serialize(\n+      JSONArray value,\n+      JsonGenerator g,\n+      SerializerProvider provider\n+  ) throws IOException {\n+    g.writeStartArray();\n+    serializeContents(value, g, provider);\n+    g.writeEndArray();\n+  }\n+\n+  @Override\n+  public void serializeWithType(\n+      JSONArray value, JsonGenerator g, SerializerProvider provider, TypeSerializer typeSer\n+  ) throws IOException {\n+    g.setCurrentValue(value);\n+    WritableTypeId typeIdDef = typeSer.writeTypePrefix(g,\n+        typeSer.typeId(value, JsonToken.START_ARRAY)\n+    );\n+    serializeContents(value, g, provider);\n+    typeSer.writeTypeSuffix(g, typeIdDef);\n+  }\n+\n+  @Override\n+  public JsonNode getSchema(\n+      SerializerProvider provider,\n+      Type typeHint\n+  ) throws JsonMappingException {\n+    return createSchemaNode(\"array\", true);\n+  }\n+\n+  protected void serializeContents(\n+      JSONArray value,\n+      JsonGenerator g,\n+      SerializerProvider provider\n+  ) throws IOException {\n+    for (int i = 0, len = value.length(); i < len; ++i) {\n+      Object ob = value.opt(i);\n+      if (ob == null || ob == JSONObject.NULL) {\n+        g.writeNull();\n+        continue;\n+      }\n+      Class<?> cls = ob.getClass();\n+      if (cls == JSONObject.class) {\n+        JSONObjectSerializer.instance.serialize((JSONObject) ob, g, provider);\n+      } else if (cls == JSONArray.class) {\n+        serialize((JSONArray) ob, g, provider);\n+      } else if (cls == String.class) {\n+        g.writeString((String) ob);\n+      } else if (cls == Integer.class) {\n+        g.writeNumber(((Integer) ob).intValue());\n+      } else if (cls == Long.class) {\n+        g.writeNumber(((Long) ob).longValue());\n+      } else if (cls == Boolean.class) {\n+        g.writeBoolean(((Boolean) ob).booleanValue());\n+      } else if (cls == Double.class) {\n+        g.writeNumber(((Double) ob).doubleValue());", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNTUwOQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367135509", "bodyText": "nit: extra whitespace", "author": "OneCricketeer", "createdAt": "2020-01-15T22:14:29Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/Jackson.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.datatype.guava.GuavaModule;\n+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;\n+import com.fasterxml.jackson.datatype.joda.JodaModule;\n+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\n+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;\n+\n+import static com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES;\n+\n+/**\n+ * A utility class for Jackson.\n+ */\n+public class Jackson {\n+  private Jackson() { /* singleton */ }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper}.\n+   */\n+  public static ObjectMapper newObjectMapper() {\n+    final ObjectMapper mapper = new ObjectMapper();\n+", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNTU4Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367135587", "bodyText": "nit: extra whitespace", "author": "OneCricketeer", "createdAt": "2020-01-15T22:14:40Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/Jackson.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.datatype.guava.GuavaModule;\n+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;\n+import com.fasterxml.jackson.datatype.joda.JodaModule;\n+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\n+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;\n+\n+import static com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES;\n+\n+/**\n+ * A utility class for Jackson.\n+ */\n+public class Jackson {\n+  private Jackson() { /* singleton */ }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper}.\n+   */\n+  public static ObjectMapper newObjectMapper() {\n+    final ObjectMapper mapper = new ObjectMapper();\n+\n+    return configure(mapper);\n+  }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper} with a custom\n+   * {@link com.fasterxml.jackson.core.JsonFactory}.\n+   *\n+   * @param jsonFactory instance of {@link com.fasterxml.jackson.core.JsonFactory} to use\n+   *     for the created {@link com.fasterxml.jackson.databind.ObjectMapper} instance.\n+   */\n+  public static ObjectMapper newObjectMapper(JsonFactory jsonFactory) {\n+    final ObjectMapper mapper = new ObjectMapper(jsonFactory);\n+", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNjI2NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367136265", "bodyText": "Could make the factory nullable.\nreturn jsonFactory == null ? new ObjectMapper() : new ObjectMapper(jsonFactory);\nCall newObjectMapper(null) from overloaded method", "author": "OneCricketeer", "createdAt": "2020-01-15T22:16:20Z", "path": "json-schema-provider/src/main/java/io/confluent/kafka/schemaregistry/json/jackson/Jackson.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.jackson;\n+\n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.datatype.guava.GuavaModule;\n+import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;\n+import com.fasterxml.jackson.datatype.joda.JodaModule;\n+import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\n+import com.fasterxml.jackson.module.paramnames.ParameterNamesModule;\n+\n+import static com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES;\n+\n+/**\n+ * A utility class for Jackson.\n+ */\n+public class Jackson {\n+  private Jackson() { /* singleton */ }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper}.\n+   */\n+  public static ObjectMapper newObjectMapper() {\n+    final ObjectMapper mapper = new ObjectMapper();\n+\n+    return configure(mapper);\n+  }\n+\n+  /**\n+   * Creates a new {@link ObjectMapper} with a custom\n+   * {@link com.fasterxml.jackson.core.JsonFactory}.\n+   *\n+   * @param jsonFactory instance of {@link com.fasterxml.jackson.core.JsonFactory} to use\n+   *     for the created {@link com.fasterxml.jackson.databind.ObjectMapper} instance.\n+   */\n+  public static ObjectMapper newObjectMapper(JsonFactory jsonFactory) {\n+    final ObjectMapper mapper = new ObjectMapper(jsonFactory);", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNjU1MQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367136551", "bodyText": "Could externalize strings", "author": "OneCricketeer", "createdAt": "2020-01-15T22:17:01Z", "path": "json-schema-provider/src/test/java/io/confluent/kafka/schemaregistry/json/JsonSchemaTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ArrayNode;\n+import com.fasterxml.jackson.databind.node.BooleanNode;\n+import com.fasterxml.jackson.databind.node.JsonNodeFactory;\n+import com.fasterxml.jackson.databind.node.NumericNode;\n+import com.fasterxml.jackson.databind.node.TextNode;\n+import org.everit.json.schema.ValidationException;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class JsonSchemaTest {\n+\n+  private static ObjectMapper objectMapper = new ObjectMapper();\n+\n+  private static final String recordSchemaString = \"{\\\"properties\\\": {\\n\"\n+      + \"     \\\"null\\\": {\\\"type\\\": \\\"null\\\"},\\n\"\n+      + \"     \\\"boolean\\\": {\\\"type\\\": \\\"boolean\\\"},\\n\"\n+      + \"     \\\"number\\\": {\\\"type\\\": \\\"number\\\"},\\n\"\n+      + \"     \\\"string\\\": {\\\"type\\\": \\\"string\\\"},\\n\"\n+      + \"  },\\n\"\n+      + \"  \\\"additionalProperties\\\": false\\n\"\n+      + \"}\";", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNzA3Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367137077", "bodyText": "I feel like this could be refactored out to a utility class", "author": "OneCricketeer", "createdAt": "2020-01-15T22:18:17Z", "path": "json-schema-provider/src/test/java/io/confluent/kafka/schemaregistry/json/diff/SchemaDiffTest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.json.diff;\n+\n+import org.everit.json.schema.Schema;\n+import org.everit.json.schema.loader.SchemaLoader;\n+import org.json.JSONArray;\n+import org.json.JSONObject;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+public class SchemaDiffTest {\n+\n+  @SuppressWarnings(\"unchecked\")\n+  @Test\n+  public void checkJsonSchemaCompatibility() {\n+    final JSONArray testCases = new JSONArray(readFile(\"diff-schema-examples.json\"));\n+\n+    for (final Object testCaseObject : testCases) {\n+      final JSONObject testCase = (JSONObject) testCaseObject;\n+      final Schema original = SchemaLoader.load(testCase.getJSONObject(\"original_schema\"));\n+      final Schema update = SchemaLoader.load(testCase.getJSONObject(\"update_schema\"));\n+      final JSONArray changes = (JSONArray) testCase.get(\"changes\");\n+      boolean isCompatible = testCase.getBoolean(\"compatible\");\n+      final List<String> errorMessages = (List<String>) changes.toList()\n+          .stream()\n+          .map(Object::toString)\n+          .collect(toList());\n+      final String description = (String) testCase.get(\"description\");\n+\n+      List<Difference> differences = SchemaDiff.compare(original, update);\n+      final List<Difference> incompatibleDiffs = differences.stream()\n+          .filter(diff -> !SchemaDiff.COMPATIBLE_CHANGES.contains(diff.getType()))\n+          .collect(Collectors.toList());\n+      assertThat(description,\n+          differences.stream()\n+              .map(change -> change.getType().toString() + \" \" + change.getJsonPath())\n+              .collect(toList()),\n+          is(errorMessages)\n+      );\n+      assertEquals(description, isCompatible, incompatibleDiffs.isEmpty());\n+    }\n+  }\n+\n+  @Test\n+  public void testRecursiveCheck() {\n+    final Schema original = SchemaLoader.load(new JSONObject(readFile(\"recursive-schema.json\")));\n+    final Schema newOne = SchemaLoader.load(new JSONObject(readFile(\"recursive-schema.json\")));\n+    Assert.assertTrue(SchemaDiff.compare(original, newOne).isEmpty());\n+  }\n+\n+  @Test\n+  public void testSchemaAddsProperties() {\n+    final Schema first = SchemaLoader.load(new JSONObject(\"{}\"));\n+\n+    final Schema second = SchemaLoader.load(new JSONObject((\"{\\\"properties\\\": {}}\")));\n+    final List<Difference> changes = SchemaDiff.compare(first, second);\n+    Assert.assertTrue(changes.isEmpty());\n+  }\n+\n+  public static String readFile(String fileName) {\n+    ClassLoader classLoader = ClassLoader.getSystemClassLoader();\n+    InputStream is = classLoader.getResourceAsStream(fileName);\n+    if (is != null) {\n+      BufferedReader reader = new BufferedReader(new InputStreamReader(is));\n+      return reader.lines().collect(Collectors.joining(System.lineSeparator()));\n+    }\n+    return null;\n+  }", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzNzg3NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367137874", "bodyText": "Could be moved into MessageFormatter as a default method?", "author": "OneCricketeer", "createdAt": "2020-01-15T22:20:13Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageFormatter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import kafka.common.MessageFormatter;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageFormatter, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageFormatter and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-consumer.sh. Then run the following command.\n+ *\n+ * <p>1. To read only the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081\n+ *\n+ * <p>2. To read both the key and the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true\n+ *\n+ * <p>3. To read the key, value, and timestamp of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true \\\n+ * --property print.timestamp=true\n+ */\n+public class JsonSchemaMessageFormatter extends AbstractKafkaJsonSchemaDeserializer\n+    implements MessageFormatter {\n+\n+  private static final byte[] NULL_BYTES = \"null\".getBytes(StandardCharsets.UTF_8);\n+  private boolean printKey = false;\n+  private boolean printTimestamp = false;\n+  private boolean printIds = false;\n+  private boolean printKeyId = false;\n+  private boolean printValueId = false;\n+  private byte[] keySeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] lineSeparator = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] idSeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private Deserializer keyDeserializer;\n+  private final ObjectMapper objectMapper = Jackson.newObjectMapper();\n+\n+  /**\n+   * Constructor needed by kafka console consumer.\n+   */\n+  public JsonSchemaMessageFormatter() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  JsonSchemaMessageFormatter(\n+      SchemaRegistryClient schemaRegistryClient, Deserializer keyDeserializer\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keyDeserializer = keyDeserializer;\n+  }\n+\n+  @Override\n+  public void init(Properties props) {\n+    if (props == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+    schemaRegistry = createSchemaRegistry(url, originals);\n+\n+    if (props.containsKey(\"print.timestamp\")) {\n+      printTimestamp = props.getProperty(\"print.timestamp\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"print.key\")) {\n+      printKey = props.getProperty(\"print.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"line.separator\")) {\n+      lineSeparator = props.getProperty(\"line.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"key.deserializer\")) {\n+      try {\n+        keyDeserializer = (Deserializer) Class.forName((String) props.get(\"key.deserializer\"))\n+            .newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key deserializer\", e);\n+      }\n+    }\n+    if (props.containsKey(\"print.schema.ids\")) {\n+      printIds = props.getProperty(\"print.schema.ids\").trim().toLowerCase().equals(\"true\");\n+      if (printIds) {\n+        printValueId = true;\n+        if (keyDeserializer == null\n+            || keyDeserializer instanceof AbstractKafkaJsonSchemaDeserializer) {\n+          printKeyId = true;\n+        }\n+      }\n+    }\n+    if (props.containsKey(\"schema.id.separator\")) {\n+      idSeparator = props.getProperty(\"schema.id.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzODIzNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367138234", "bodyText": "I feel like this could be refactored out to a utility class, if it doesn't already exist", "author": "OneCricketeer", "createdAt": "2020-01-15T22:21:04Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageFormatter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import kafka.common.MessageFormatter;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageFormatter, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageFormatter and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-consumer.sh. Then run the following command.\n+ *\n+ * <p>1. To read only the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081\n+ *\n+ * <p>2. To read both the key and the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true\n+ *\n+ * <p>3. To read the key, value, and timestamp of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true \\\n+ * --property print.timestamp=true\n+ */\n+public class JsonSchemaMessageFormatter extends AbstractKafkaJsonSchemaDeserializer\n+    implements MessageFormatter {\n+\n+  private static final byte[] NULL_BYTES = \"null\".getBytes(StandardCharsets.UTF_8);\n+  private boolean printKey = false;\n+  private boolean printTimestamp = false;\n+  private boolean printIds = false;\n+  private boolean printKeyId = false;\n+  private boolean printValueId = false;\n+  private byte[] keySeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] lineSeparator = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] idSeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private Deserializer keyDeserializer;\n+  private final ObjectMapper objectMapper = Jackson.newObjectMapper();\n+\n+  /**\n+   * Constructor needed by kafka console consumer.\n+   */\n+  public JsonSchemaMessageFormatter() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  JsonSchemaMessageFormatter(\n+      SchemaRegistryClient schemaRegistryClient, Deserializer keyDeserializer\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keyDeserializer = keyDeserializer;\n+  }\n+\n+  @Override\n+  public void init(Properties props) {\n+    if (props == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+    schemaRegistry = createSchemaRegistry(url, originals);\n+\n+    if (props.containsKey(\"print.timestamp\")) {\n+      printTimestamp = props.getProperty(\"print.timestamp\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"print.key\")) {\n+      printKey = props.getProperty(\"print.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"line.separator\")) {\n+      lineSeparator = props.getProperty(\"line.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"key.deserializer\")) {\n+      try {\n+        keyDeserializer = (Deserializer) Class.forName((String) props.get(\"key.deserializer\"))\n+            .newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key deserializer\", e);\n+      }\n+    }\n+    if (props.containsKey(\"print.schema.ids\")) {\n+      printIds = props.getProperty(\"print.schema.ids\").trim().toLowerCase().equals(\"true\");\n+      if (printIds) {\n+        printValueId = true;\n+        if (keyDeserializer == null\n+            || keyDeserializer instanceof AbstractKafkaJsonSchemaDeserializer) {\n+          printKeyId = true;\n+        }\n+      }\n+    }\n+    if (props.containsKey(\"schema.id.separator\")) {\n+      idSeparator = props.getProperty(\"schema.id.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public void writeTo(ConsumerRecord<byte[], byte[]> consumerRecord, PrintStream output) {\n+    if (printTimestamp) {\n+      try {\n+        TimestampType timestampType = consumerRecord.timestampType();\n+        if (timestampType != TimestampType.NO_TIMESTAMP_TYPE) {\n+          output.write(String.format(\"%s:%d\", timestampType, consumerRecord.timestamp())\n+              .getBytes(StandardCharsets.UTF_8));\n+        } else {\n+          output.write(\"NO_TIMESTAMP\".getBytes(StandardCharsets.UTF_8));\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the timestamp\", ioe);\n+      }\n+    }\n+    if (printKey) {\n+      try {\n+        if (keyDeserializer != null) {\n+          Object deserializedKey = consumerRecord.key() == null\n+                                   ? null\n+                                   : keyDeserializer.deserialize(null, consumerRecord.key());\n+          output.write(deserializedKey != null ? deserializedKey.toString()\n+              .getBytes(StandardCharsets.UTF_8) : NULL_BYTES);\n+        } else {\n+          writeTo(consumerRecord.key(), output);\n+        }\n+        if (printKeyId) {\n+          output.write(idSeparator);\n+          int schemaId = schemaIdFor(consumerRecord.key());\n+          output.print(schemaId);\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the key\", ioe);\n+      }\n+    }\n+    try {\n+      writeTo(consumerRecord.value(), output);\n+      if (printValueId) {\n+        output.write(idSeparator);\n+        int schemaId = schemaIdFor(consumerRecord.value());\n+        output.print(schemaId);\n+      }\n+      output.write(lineSeparator);\n+    } catch (IOException ioe) {\n+      throw new SerializationException(\"Error while formatting the value\", ioe);\n+    }\n+  }\n+\n+  private void writeTo(byte[] data, PrintStream output) throws IOException {\n+    Object object = deserialize(data);\n+    output.print(objectMapper.writeValueAsString(object));\n+  }\n+\n+  @Override\n+  public void close() {\n+    // nothing to do\n+  }\n+\n+  private int schemaIdFor(byte[] payload) {\n+    ByteBuffer buffer = ByteBuffer.wrap(payload);\n+    if (buffer.get() != MAGIC_BYTE) {\n+      throw new SerializationException(\"Unknown magic byte!\");\n+    }\n+    return buffer.getInt();\n+  }", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzODU4OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367138588", "bodyText": "If the SchemaProvider interface was extracted to a parameter, this method could be re-used elsewhere", "author": "OneCricketeer", "createdAt": "2020-01-15T22:21:55Z", "path": "json-schema-serializer/src/main/java/io/confluent/kafka/formatter/json/JsonSchemaMessageFormatter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import kafka.common.MessageFormatter;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+import io.confluent.kafka.schemaregistry.json.jackson.Jackson;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer;\n+\n+/**\n+ * Example\n+ * To use JsonSchemaMessageFormatter, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for JsonSchemaMessageFormatter and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-consumer.sh. Then run the following command.\n+ *\n+ * <p>1. To read only the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081\n+ *\n+ * <p>2. To read both the key and the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true\n+ *\n+ * <p>3. To read the key, value, and timestamp of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.JsonSchemaMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true \\\n+ * --property print.timestamp=true\n+ */\n+public class JsonSchemaMessageFormatter extends AbstractKafkaJsonSchemaDeserializer\n+    implements MessageFormatter {\n+\n+  private static final byte[] NULL_BYTES = \"null\".getBytes(StandardCharsets.UTF_8);\n+  private boolean printKey = false;\n+  private boolean printTimestamp = false;\n+  private boolean printIds = false;\n+  private boolean printKeyId = false;\n+  private boolean printValueId = false;\n+  private byte[] keySeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] lineSeparator = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] idSeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private Deserializer keyDeserializer;\n+  private final ObjectMapper objectMapper = Jackson.newObjectMapper();\n+\n+  /**\n+   * Constructor needed by kafka console consumer.\n+   */\n+  public JsonSchemaMessageFormatter() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  JsonSchemaMessageFormatter(\n+      SchemaRegistryClient schemaRegistryClient, Deserializer keyDeserializer\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keyDeserializer = keyDeserializer;\n+  }\n+\n+  @Override\n+  public void init(Properties props) {\n+    if (props == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+    schemaRegistry = createSchemaRegistry(url, originals);\n+\n+    if (props.containsKey(\"print.timestamp\")) {\n+      printTimestamp = props.getProperty(\"print.timestamp\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"print.key\")) {\n+      printKey = props.getProperty(\"print.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"line.separator\")) {\n+      lineSeparator = props.getProperty(\"line.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"key.deserializer\")) {\n+      try {\n+        keyDeserializer = (Deserializer) Class.forName((String) props.get(\"key.deserializer\"))\n+            .newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key deserializer\", e);\n+      }\n+    }\n+    if (props.containsKey(\"print.schema.ids\")) {\n+      printIds = props.getProperty(\"print.schema.ids\").trim().toLowerCase().equals(\"true\");\n+      if (printIds) {\n+        printValueId = true;\n+        if (keyDeserializer == null\n+            || keyDeserializer instanceof AbstractKafkaJsonSchemaDeserializer) {\n+          printKeyId = true;\n+        }\n+      }\n+    }\n+    if (props.containsKey(\"schema.id.separator\")) {\n+      idSeparator = props.getProperty(\"schema.id.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public void writeTo(ConsumerRecord<byte[], byte[]> consumerRecord, PrintStream output) {\n+    if (printTimestamp) {\n+      try {\n+        TimestampType timestampType = consumerRecord.timestampType();\n+        if (timestampType != TimestampType.NO_TIMESTAMP_TYPE) {\n+          output.write(String.format(\"%s:%d\", timestampType, consumerRecord.timestamp())\n+              .getBytes(StandardCharsets.UTF_8));\n+        } else {\n+          output.write(\"NO_TIMESTAMP\".getBytes(StandardCharsets.UTF_8));\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the timestamp\", ioe);\n+      }\n+    }\n+    if (printKey) {\n+      try {\n+        if (keyDeserializer != null) {\n+          Object deserializedKey = consumerRecord.key() == null\n+                                   ? null\n+                                   : keyDeserializer.deserialize(null, consumerRecord.key());\n+          output.write(deserializedKey != null ? deserializedKey.toString()\n+              .getBytes(StandardCharsets.UTF_8) : NULL_BYTES);\n+        } else {\n+          writeTo(consumerRecord.key(), output);\n+        }\n+        if (printKeyId) {\n+          output.write(idSeparator);\n+          int schemaId = schemaIdFor(consumerRecord.key());\n+          output.print(schemaId);\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the key\", ioe);\n+      }\n+    }\n+    try {\n+      writeTo(consumerRecord.value(), output);\n+      if (printValueId) {\n+        output.write(idSeparator);\n+        int schemaId = schemaIdFor(consumerRecord.value());\n+        output.print(schemaId);\n+      }\n+      output.write(lineSeparator);\n+    } catch (IOException ioe) {\n+      throw new SerializationException(\"Error while formatting the value\", ioe);\n+    }\n+  }\n+\n+  private void writeTo(byte[] data, PrintStream output) throws IOException {\n+    Object object = deserialize(data);\n+    output.print(objectMapper.writeValueAsString(object));\n+  }\n+\n+  @Override\n+  public void close() {\n+    // nothing to do\n+  }\n+\n+  private int schemaIdFor(byte[] payload) {\n+    ByteBuffer buffer = ByteBuffer.wrap(payload);\n+    if (buffer.get() != MAGIC_BYTE) {\n+      throw new SerializationException(\"Unknown magic byte!\");\n+    }\n+    return buffer.getInt();\n+  }\n+\n+  private SchemaRegistryClient createSchemaRegistry(\n+      String schemaRegistryUrl, Map<String, Object> originals\n+  ) {\n+    return schemaRegistry != null\n+           ? schemaRegistry\n+           : new CachedSchemaRegistryClient(Collections.singletonList(schemaRegistryUrl),\n+               AbstractKafkaSchemaSerDeConfig.MAX_SCHEMAS_PER_SUBJECT_DEFAULT,\n+               Collections.singletonList(new JsonSchemaProvider()),\n+               originals\n+           );\n+  }", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzOTM5OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r367139398", "bodyText": "This method is duplicated in KafkaSchemaRegistry, no?", "author": "OneCricketeer", "createdAt": "2020-01-15T22:23:52Z", "path": "maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java", "diffHunk": "@@ -84,7 +85,7 @@ protected SchemaRegistryClient client() {\n \n   private List<SchemaProvider> defaultSchemaProviders() {\n     return Arrays.asList(\n-        new AvroSchemaProvider(), new ProtobufSchemaProvider()\n+        new AvroSchemaProvider(), new JsonSchemaProvider(), new ProtobufSchemaProvider()\n     );\n   }", "originalCommit": "1201e49883649e47dbf85935cf378c0bd5a9e1b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDg4MDk2OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r370880968", "bodyText": "Question: why the cast ?", "author": "dragosvictor", "createdAt": "2020-01-24T23:08:57Z", "path": "core/src/test/java/io/confluent/kafka/schemaregistry/rest/json/RestApiTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.rest.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.client.rest.RestService;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.requests.RegisterSchemaRequest;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();\n+\n+  private static final ObjectMapper MAPPER = new ObjectMapper();\n+\n+  public RestApiTest() {\n+    super(1, true);\n+  }\n+\n+  @Override\n+  protected Properties getSchemaRegistryProperties() {\n+    Properties props = new Properties();\n+    props.setProperty(\"schema.providers\", JsonSchemaProvider.class.getName());\n+    return props;\n+  }\n+\n+  @Test\n+  public void testBasic() throws Exception {\n+    String subject1 = \"testTopic1\";\n+    String subject2 = \"testTopic2\";\n+    int schemasInSubject1 = 10;\n+    List<Integer> allVersionsInSubject1 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject1 = getRandomJsonSchemas(schemasInSubject1);\n+    int schemasInSubject2 = 5;\n+    List<Integer> allVersionsInSubject2 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject2 = getRandomJsonSchemas(schemasInSubject2);\n+    List<String> allSubjects = new ArrayList<String>();\n+\n+    // test getAllSubjects with no existing data\n+    assertEquals(\"Getting all subjects should return empty\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+\n+    // test registering and verifying new schemas in subject1\n+    int schemaIdCounter = 1;\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      String schema = allSchemasInSubject1.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject1);\n+      schemaIdCounter++;\n+      allVersionsInSubject1.add(expectedVersion);\n+    }\n+    allSubjects.add(subject1);\n+\n+    // test re-registering existing schemas\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      int expectedId = i + 1;\n+      String schemaString = allSchemasInSubject1.get(i);\n+      int foundId = restApp.restClient.registerSchema(schemaString,\n+          JsonSchema.TYPE,\n+          Collections.emptyList(),\n+          subject1\n+      );\n+      assertEquals(\"Re-registering an existing schema should return the existing version\",\n+          expectedId,\n+          foundId\n+      );\n+    }\n+\n+    // test registering schemas in subject2\n+    for (int i = 0; i < schemasInSubject2; i++) {\n+      String schema = allSchemasInSubject2.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject2);\n+      schemaIdCounter++;\n+      allVersionsInSubject2.add(expectedVersion);\n+    }\n+    allSubjects.add(subject2);\n+\n+    // test getAllVersions with existing data\n+    assertEquals(\n+        \"Getting all versions from subject1 should match all registered versions\",\n+        allVersionsInSubject1,\n+        restApp.restClient.getAllVersions(subject1)\n+    );\n+    assertEquals(\n+        \"Getting all versions from subject2 should match all registered versions\",\n+        allVersionsInSubject2,\n+        restApp.restClient.getAllVersions(subject2)\n+    );\n+\n+    // test getAllSubjects with existing data\n+    assertEquals(\"Getting all subjects should match all registered subjects\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+  }\n+\n+  @Test\n+  public void testSchemaReferences() throws Exception {\n+    Map<String, String> schemas = getJsonSchemaWithReferences();\n+    String subject = \"reference\";\n+    registerAndVerifySchema(restApp.restClient, schemas.get(\"ref.json\"), 1, subject);\n+\n+    RegisterSchemaRequest request = new RegisterSchemaRequest();\n+    request.setSchema(schemas.get(\"main.json\"));\n+    request.setSchemaType(JsonSchema.TYPE);\n+    SchemaReference ref = new SchemaReference(\"ref.json\", \"reference\", 1);\n+    request.setReferences(Collections.singletonList(ref));\n+    int registeredId = restApp.restClient.registerSchema(request, \"referrer\");\n+    assertEquals(\"Registering a new schema should succeed\", 2, registeredId);\n+\n+    SchemaString schemaString = restApp.restClient.getId(2);\n+    // the newly registered schema should be immediately readable on the master\n+    assertEquals(\"Registered schema should be found\",\n+        MAPPER.readTree(schemas.get(\"main.json\")),\n+        MAPPER.readTree(schemaString.getSchemaString())\n+    );\n+\n+    assertEquals(\"Schema references should be found\",\n+        Collections.singletonList(ref),\n+        schemaString.getReferences()\n+    );\n+  }\n+\n+  public static void registerAndVerifySchema(\n+      RestService restService,\n+      String schemaString,\n+      int expectedId,\n+      String subject\n+  ) throws IOException, RestClientException {\n+    int registeredId = restService.registerSchema(\n+        schemaString,\n+        JsonSchema.TYPE,\n+        Collections.emptyList(),\n+        subject\n+    );\n+    Assert.assertEquals(\n+        \"Registering a new schema should succeed\",\n+        (long) expectedId,\n+        (long) registeredId", "originalCommit": "f19fa364440904c4cc6feeeb93c848bb066086e8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDg5MDU2Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r370890567", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T23:55:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDg4MDk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDg4MTE5NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1289#discussion_r370881194", "bodyText": "Nit: it's highly improbable, but we could end up with the same number twice here and break the tests every million builds or so :)", "author": "dragosvictor", "createdAt": "2020-01-24T23:09:50Z", "path": "core/src/test/java/io/confluent/kafka/schemaregistry/rest/json/RestApiTest.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.rest.json;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.client.rest.RestService;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.requests.RegisterSchemaRequest;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.json.JsonSchema;\n+import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();\n+\n+  private static final ObjectMapper MAPPER = new ObjectMapper();\n+\n+  public RestApiTest() {\n+    super(1, true);\n+  }\n+\n+  @Override\n+  protected Properties getSchemaRegistryProperties() {\n+    Properties props = new Properties();\n+    props.setProperty(\"schema.providers\", JsonSchemaProvider.class.getName());\n+    return props;\n+  }\n+\n+  @Test\n+  public void testBasic() throws Exception {\n+    String subject1 = \"testTopic1\";\n+    String subject2 = \"testTopic2\";\n+    int schemasInSubject1 = 10;\n+    List<Integer> allVersionsInSubject1 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject1 = getRandomJsonSchemas(schemasInSubject1);\n+    int schemasInSubject2 = 5;\n+    List<Integer> allVersionsInSubject2 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject2 = getRandomJsonSchemas(schemasInSubject2);\n+    List<String> allSubjects = new ArrayList<String>();\n+\n+    // test getAllSubjects with no existing data\n+    assertEquals(\"Getting all subjects should return empty\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+\n+    // test registering and verifying new schemas in subject1\n+    int schemaIdCounter = 1;\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      String schema = allSchemasInSubject1.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject1);\n+      schemaIdCounter++;\n+      allVersionsInSubject1.add(expectedVersion);\n+    }\n+    allSubjects.add(subject1);\n+\n+    // test re-registering existing schemas\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      int expectedId = i + 1;\n+      String schemaString = allSchemasInSubject1.get(i);\n+      int foundId = restApp.restClient.registerSchema(schemaString,\n+          JsonSchema.TYPE,\n+          Collections.emptyList(),\n+          subject1\n+      );\n+      assertEquals(\"Re-registering an existing schema should return the existing version\",\n+          expectedId,\n+          foundId\n+      );\n+    }\n+\n+    // test registering schemas in subject2\n+    for (int i = 0; i < schemasInSubject2; i++) {\n+      String schema = allSchemasInSubject2.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject2);\n+      schemaIdCounter++;\n+      allVersionsInSubject2.add(expectedVersion);\n+    }\n+    allSubjects.add(subject2);\n+\n+    // test getAllVersions with existing data\n+    assertEquals(\n+        \"Getting all versions from subject1 should match all registered versions\",\n+        allVersionsInSubject1,\n+        restApp.restClient.getAllVersions(subject1)\n+    );\n+    assertEquals(\n+        \"Getting all versions from subject2 should match all registered versions\",\n+        allVersionsInSubject2,\n+        restApp.restClient.getAllVersions(subject2)\n+    );\n+\n+    // test getAllSubjects with existing data\n+    assertEquals(\"Getting all subjects should match all registered subjects\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+  }\n+\n+  @Test\n+  public void testSchemaReferences() throws Exception {\n+    Map<String, String> schemas = getJsonSchemaWithReferences();\n+    String subject = \"reference\";\n+    registerAndVerifySchema(restApp.restClient, schemas.get(\"ref.json\"), 1, subject);\n+\n+    RegisterSchemaRequest request = new RegisterSchemaRequest();\n+    request.setSchema(schemas.get(\"main.json\"));\n+    request.setSchemaType(JsonSchema.TYPE);\n+    SchemaReference ref = new SchemaReference(\"ref.json\", \"reference\", 1);\n+    request.setReferences(Collections.singletonList(ref));\n+    int registeredId = restApp.restClient.registerSchema(request, \"referrer\");\n+    assertEquals(\"Registering a new schema should succeed\", 2, registeredId);\n+\n+    SchemaString schemaString = restApp.restClient.getId(2);\n+    // the newly registered schema should be immediately readable on the master\n+    assertEquals(\"Registered schema should be found\",\n+        MAPPER.readTree(schemas.get(\"main.json\")),\n+        MAPPER.readTree(schemaString.getSchemaString())\n+    );\n+\n+    assertEquals(\"Schema references should be found\",\n+        Collections.singletonList(ref),\n+        schemaString.getReferences()\n+    );\n+  }\n+\n+  public static void registerAndVerifySchema(\n+      RestService restService,\n+      String schemaString,\n+      int expectedId,\n+      String subject\n+  ) throws IOException, RestClientException {\n+    int registeredId = restService.registerSchema(\n+        schemaString,\n+        JsonSchema.TYPE,\n+        Collections.emptyList(),\n+        subject\n+    );\n+    Assert.assertEquals(\n+        \"Registering a new schema should succeed\",\n+        (long) expectedId,\n+        (long) registeredId\n+    );\n+    Assert.assertEquals(\"Registered schema should be found\",\n+        MAPPER.readTree(schemaString),\n+        MAPPER.readTree(restService.getId(expectedId).getSchemaString())\n+    );\n+  }\n+\n+  public static List<String> getRandomJsonSchemas(int num) {\n+    List<String> schemas = new ArrayList<>();\n+    for (int i = 0; i < num; i++) {\n+      String schema = \"{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"f\"\n+          + random.nextInt(Integer.MAX_VALUE)", "originalCommit": "f19fa364440904c4cc6feeeb93c848bb066086e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8baf87e7cd230a76140ef9016c9d8bfd2d66efde", "url": "https://github.com/confluentinc/schema-registry/commit/8baf87e7cd230a76140ef9016c9d8bfd2d66efde", "message": "First cut at JSON Schema support", "committedDate": "2020-01-27T19:10:10Z", "type": "commit"}, {"oid": "e9ac2355678b420fac994969bd7ee6064a8bc765", "url": "https://github.com/confluentinc/schema-registry/commit/e9ac2355678b420fac994969bd7ee6064a8bc765", "message": "Incorporate some review feedback", "committedDate": "2020-01-27T19:10:23Z", "type": "commit"}, {"oid": "3d459aad867b0e214209930a4a4c95debfb911ea", "url": "https://github.com/confluentinc/schema-registry/commit/3d459aad867b0e214209930a4a4c95debfb911ea", "message": "Refactor common serializer classes", "committedDate": "2020-01-27T19:10:23Z", "type": "commit"}, {"oid": "276675b8b5f2f77b86e8469a01917e52d39c2536", "url": "https://github.com/confluentinc/schema-registry/commit/276675b8b5f2f77b86e8469a01917e52d39c2536", "message": "Minor API cleanup", "committedDate": "2020-01-27T19:10:24Z", "type": "commit"}, {"oid": "6c3946dadef5c130b7f3276d47fec43e997e3c25", "url": "https://github.com/confluentinc/schema-registry/commit/6c3946dadef5c130b7f3276d47fec43e997e3c25", "message": "Add type.property config for RecordNameStrategy", "committedDate": "2020-01-27T19:10:24Z", "type": "commit"}, {"oid": "10ad6aa13c77c99df41ae653524c05632ddf7455", "url": "https://github.com/confluentinc/schema-registry/commit/10ad6aa13c77c99df41ae653524c05632ddf7455", "message": "Add kafak-connect maven plugin for JSON Schema", "committedDate": "2020-01-27T19:10:24Z", "type": "commit"}, {"oid": "806c619854cd2f808c56e973a73da14874773fc1", "url": "https://github.com/confluentinc/schema-registry/commit/806c619854cd2f808c56e973a73da14874773fc1", "message": "Minor refactoring", "committedDate": "2020-01-27T19:10:24Z", "type": "commit"}, {"oid": "4e5c9495490d8533216ada003306e49e1f2f1f6c", "url": "https://github.com/confluentinc/schema-registry/commit/4e5c9495490d8533216ada003306e49e1f2f1f6c", "message": "Remove unneeded casts", "committedDate": "2020-01-27T19:10:24Z", "type": "commit"}, {"oid": "25d10f46faaed320b8cae7b9329e8880eb63dcfb", "url": "https://github.com/confluentinc/schema-registry/commit/25d10f46faaed320b8cae7b9329e8880eb63dcfb", "message": "Initialize JSONSchemaProvier", "committedDate": "2020-01-27T19:13:32Z", "type": "commit"}, {"oid": "a8b073f3c495228fc611f37f955974077dbf234b", "url": "https://github.com/confluentinc/schema-registry/commit/a8b073f3c495228fc611f37f955974077dbf234b", "message": "Improve logging when JSON schemas are incompatible", "committedDate": "2020-01-27T19:16:31Z", "type": "commit"}, {"oid": "a8b073f3c495228fc611f37f955974077dbf234b", "url": "https://github.com/confluentinc/schema-registry/commit/a8b073f3c495228fc611f37f955974077dbf234b", "message": "Improve logging when JSON schemas are incompatible", "committedDate": "2020-01-27T19:16:31Z", "type": "forcePushed"}]}