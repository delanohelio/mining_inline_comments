{"pr_number": 337, "pr_title": "add AD task cache", "pr_createdAt": "2020-12-20T05:26:36Z", "pr_url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337", "timeline": [{"oid": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/1bf147c0d9ce73b519bcca5a973c72db87f97707", "message": "add AD task cache", "committedDate": "2020-12-20T05:12:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg1ODcxNw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546858717", "bodyText": "so only one task is allowed per detector, right?", "author": "weicongs-amazon", "createdAt": "2020-12-21T18:21:21Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg3MzI4NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546873285", "bodyText": "Yes, user can only run one task for one detector. If a detector has one running task, user can't edit/delete/start the detector unless they stop the detector.", "author": "ylwu-amzn", "createdAt": "2020-12-21T18:54:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg1ODcxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546861455", "bodyText": "do we really need this if this is a private method?", "author": "weicongs-amazon", "createdAt": "2020-12-21T18:27:20Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg3NDM3NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546874374", "bodyText": "This is to reuse the task existence checking logic, otherwise, need to write duplicate code in other methods.", "author": "ylwu-amzn", "createdAt": "2020-12-21T18:56:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDM4Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546910382", "bodyText": "my point is if this is a really useful exception. I assume this won't happen at all. If this happens, what's the logic like?  Anyway, this doesn't hurt anything.", "author": "weicongs-amazon", "createdAt": "2020-12-21T20:17:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjM3Ng==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546916376", "bodyText": "Yeah, good question, in normal workflow this exception will not happen. It may happen for some race condition case. For example, user stops a running detector on AD Kibana plugin, then AD ES plugin will try to stop the running AD task in background. It's possible that the frontend polls the running task's state just after the task removed from the cache. Then the task will not be found.", "author": "ylwu-amzn", "createdAt": "2020-12-21T20:32:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk0MDQ5Nw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546940497", "bodyText": "I think if that's a valid scenario, it's better to fix it in our logic, instead of throwing exception.", "author": "weicongs-amazon", "createdAt": "2020-12-21T21:34:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk0MjI5NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546942294", "bodyText": "Yes, for this case, will catch the exception in TaskManger and return readable error message, will not throw exception directly to frontend. TaskManager will be in next PR.", "author": "ylwu-amzn", "createdAt": "2020-12-21T21:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MTQ1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MjYwNg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546862606", "bodyText": "why do we need clear this up since it's pretty small?", "author": "weicongs-amazon", "createdAt": "2020-12-21T18:29:46Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5MDI0MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546890240", "bodyText": "It looks good to not clear the training data memory as it's pretty small per detector. But we will let user define max executing historical detectors per node via cluster setting. If there are many detectors executing(this is even worse for high cardinality as we need to cache training data for every entity), the cache size may be not small any more. And the training data size is 1000 currently, if we change it to a bigger value in future, the cache will increase as well. These training data is only to train the threshold model. After model trained, they will not be used any more. So we'd better clean up this cache to release resource.", "author": "ylwu-amzn", "createdAt": "2020-12-21T19:31:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MjYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwNzg4Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546907882", "bodyText": "My understanding is that the model itself contributes much bigger than this training dataset. It's ok to keep this right now since the logic isn't very complicated. My original thought is that this optimization doesn't bring enough benefits to us comparing to added complexity into the codes, and optimization can be always done later. Without it, we can keep the cachedMemorySize readonly.", "author": "weicongs-amazon", "createdAt": "2020-12-21T20:11:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2MjYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546864701", "bodyText": "why do we need put rcf model here? I remember there is rcf model caching in the AD entity result execution.  Are we trying to build a new workflow for this one?", "author": "weicongs-amazon", "createdAt": "2020-12-21T18:34:40Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADBatchTaskCache.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_MIN_SAMPLES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_SAMPLES_PER_TREE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.TIME_DECAY;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import com.amazon.opendistroforelasticsearch.ad.ml.HybridThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+/**\n+ * AD batch task cache which will hold RCF, threshold model, shingle and training data.\n+ */\n+public class ADBatchTaskCache {\n+    private final String detectorId;\n+    private RandomCutForest rcfModel;", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg4Njc3NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546886774", "bodyText": "For realtime high cardinality(HC) detector, we will cache entity's RCF model in EntityModel.  In our first release, historical detector doesn't support high cardinality.\nWe have two options for the AD task cache design.\n\n\nRepurpose current cache to support AD task. The pros is we can reuse some existing code, the cons is we need to change current detector cache logic which may impact realtime detector run.\n\n\nCreate separate cache for AD task. So we don't need to worry about the old detector cache design. In next phase, we will not differentiate realtime and historical detectors, so we will create AD tasks for all detectors. Then we can deprecate the old cache mechanism and move to task cache easily. That will be clean and easy to migrate. Of course, the cons will be we will have some duplicate code: cache on detector or entity level, and cache on AD task level.", "author": "ylwu-amzn", "createdAt": "2020-12-21T19:23:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODYzOA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546898638", "bodyText": "What do you mean by \"create AD tasks for all detectors\"?", "author": "kaituo", "createdAt": "2020-12-21T19:49:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMjA0NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546912045", "bodyText": "thanks. It's good with the second option. btw, is this cachedTask including the requirement of realtime detector, or we just have these two now, and consolidate these later by refactoring?", "author": "weicongs-amazon", "createdAt": "2020-12-21T20:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMzE1MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546913151", "bodyText": "It's too early to include the realtime detector's requirements in this task cache design. Prefer to refactor when we finalize the UX and product features.", "author": "ylwu-amzn", "createdAt": "2020-12-21T20:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTkyNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546939924", "bodyText": "Same thought, thanks", "author": "weicongs-amazon", "createdAt": "2020-12-21T21:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg2NDcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwMzgxMw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546903813", "bodyText": "cancel cache does not sound correct English.  You meant invalidate cache?", "author": "kaituo", "createdAt": "2020-12-21T20:01:58Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADBatchTaskCache.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_MIN_SAMPLES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_SAMPLES_PER_TREE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.TIME_DECAY;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import com.amazon.opendistroforelasticsearch.ad.ml.HybridThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+/**\n+ * AD batch task cache which will hold RCF, threshold model, shingle and training data.\n+ */\n+public class ADBatchTaskCache {\n+    private final String detectorId;\n+    private RandomCutForest rcfModel;\n+    private ThresholdingModel thresholdModel;\n+    private boolean thresholdModelTrained;\n+    private Deque<Map.Entry<Long, Optional<double[]>>> shingle;\n+    private List<Double> thresholdModelTrainingData;\n+    private AtomicBoolean cancelled = new AtomicBoolean(false);\n+    private AtomicLong cacheMemorySize = new AtomicLong(0);\n+    private String cancelReason;\n+    private String cancelledBy;", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk0OTkwMg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546949902", "bodyText": "For historical detector, it may run for a long time if the historical date range is long. If user starts a historical detector by mistake, they can stop the detector before the task reach the historical end time. When user stop a historical detector, AD ES plugin will cancel the running task in background. The cancel task is an async process, so we need to cache the cancel reason and cancelled by in cache until the task cancelled successfully.", "author": "ylwu-amzn", "createdAt": "2020-12-21T21:57:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwMzgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwNjEzMg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546906132", "bodyText": "Can you add doc describing the thrown exceptions?  The caller would need to know.", "author": "kaituo", "createdAt": "2020-12-21T20:07:08Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1Mjg2MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546952861", "bodyText": "done", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:05:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwNjEzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwNjE2MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546906160", "bodyText": "Can you add doc describing the thrown exceptions?", "author": "kaituo", "createdAt": "2020-12-21T20:07:12Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1Mjg5Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546952892", "bodyText": "done", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:05:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkwNjE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDQ3OA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546910478", "bodyText": "how do you arrive at 24?  We are gonna change to data to float soon, whose size is of 4 bytes.", "author": "kaituo", "createdAt": "2020-12-21T20:17:53Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        return memoryTracker.estimateModelSize(adTask.getDetector(), NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(adTask.getDetector().getShingleSize());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+            taskCaches.remove(taskId);\n+        }\n+    }\n+\n+    /**\n+     * Cancel AD task.\n+     *\n+     * @param taskId AD task id\n+     * @param reason why need to cancel task\n+     * @param userName user name\n+     */\n+    public void cancel(String taskId, String reason, String userName) {\n+        getBatchTaskCache(taskId).cancel(reason, userName);\n+    }\n+\n+    /**\n+     * Task is cancelled or not.\n+     *\n+     * @param taskId AD task id\n+     * @return true if task is cancelled; otherwise return false\n+     */\n+    public boolean isCancelled(String taskId) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        return taskCache.isCancelled();\n+    }\n+\n+    /**\n+     * Get why task cancelled.\n+     *\n+     * @param taskId AD task id\n+     * @return task cancellation reason\n+     */\n+    public String getCancelReason(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelReason();\n+    }\n+\n+    /**\n+     * Get task cancelled by which user.\n+     *\n+     * @param taskId AD task id\n+     * @return user name\n+     */\n+    public String getCancelledBy(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelledBy();\n+    }\n+\n+    /**\n+     * Get current task count in cache.\n+     *\n+     * @return task count\n+     */\n+    public int size() {\n+        return taskCaches.size();\n+    }\n+\n+    /**\n+     * Clear all tasks.\n+     */\n+    public void clear() {\n+        taskCaches.clear();\n+    }\n+\n+    /**\n+     * Estimate max memory usage of model training data.\n+     * The training data is double and will cache in {@link java.util.ArrayList}.\n+     * Check {@link ADBatchTaskCache#getThresholdModelTrainingData()}\n+     *\n+     * @param size training data point count\n+     * @return how many bytes will consume\n+     */\n+    public long trainingDataMemorySize(int size) {\n+        return 24 * size;", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1MDcwNg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546950706", "bodyText": "Training data will be stored in List<Double>, that will be 24 for each item. Will change this when we change to float. Actually we need to change all related places, not just this one.", "author": "ylwu-amzn", "createdAt": "2020-12-21T21:59:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDQ3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyMzE1NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547423155", "bodyText": "Double is of 8 bytes.  Where does the rest of 16 come from?  I want to make the change easier and the code easier to read as others will have the same questions as me.", "author": "kaituo", "createdAt": "2020-12-22T18:06:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDQ3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ3MTAzNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547471034", "bodyText": "Sure, will add more comments to explain.  I will merge some changes from next PR into this one.", "author": "ylwu-amzn", "createdAt": "2020-12-22T19:37:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDQ3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ5Nzk1OA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547497958", "bodyText": "Double is of 8 bytes.\n\ndouble is 8 bytes, Double object will consumes 16 bytes more. I planed to change the ArrayList to double[] in next PR. As you are asking this now, I will cherry pick that part from next PR to this one.", "author": "ylwu-amzn", "createdAt": "2020-12-22T20:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTEzOA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546911138", "bodyText": "the shingle memory depends on # of features, shingle size, and the data type (float vs double).  Can we hardcode it to 96?", "author": "kaituo", "createdAt": "2020-12-21T20:19:29Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        return memoryTracker.estimateModelSize(adTask.getDetector(), NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(adTask.getDetector().getShingleSize());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+            taskCaches.remove(taskId);\n+        }\n+    }\n+\n+    /**\n+     * Cancel AD task.\n+     *\n+     * @param taskId AD task id\n+     * @param reason why need to cancel task\n+     * @param userName user name\n+     */\n+    public void cancel(String taskId, String reason, String userName) {\n+        getBatchTaskCache(taskId).cancel(reason, userName);\n+    }\n+\n+    /**\n+     * Task is cancelled or not.\n+     *\n+     * @param taskId AD task id\n+     * @return true if task is cancelled; otherwise return false\n+     */\n+    public boolean isCancelled(String taskId) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        return taskCache.isCancelled();\n+    }\n+\n+    /**\n+     * Get why task cancelled.\n+     *\n+     * @param taskId AD task id\n+     * @return task cancellation reason\n+     */\n+    public String getCancelReason(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelReason();\n+    }\n+\n+    /**\n+     * Get task cancelled by which user.\n+     *\n+     * @param taskId AD task id\n+     * @return user name\n+     */\n+    public String getCancelledBy(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelledBy();\n+    }\n+\n+    /**\n+     * Get current task count in cache.\n+     *\n+     * @return task count\n+     */\n+    public int size() {\n+        return taskCaches.size();\n+    }\n+\n+    /**\n+     * Clear all tasks.\n+     */\n+    public void clear() {\n+        taskCaches.clear();\n+    }\n+\n+    /**\n+     * Estimate max memory usage of model training data.\n+     * The training data is double and will cache in {@link java.util.ArrayList}.\n+     * Check {@link ADBatchTaskCache#getThresholdModelTrainingData()}\n+     *\n+     * @param size training data point count\n+     * @return how many bytes will consume\n+     */\n+    public long trainingDataMemorySize(int size) {\n+        return 24 * size;\n+    }\n+\n+    /**\n+     * Estimate max memory usage of shingle data.\n+     * Based on the test, one shingle data point consumes about 96 bytes.\n+     * The shingle data is stored in {@link java.util.Deque}\n+     * Check {@link ADBatchTaskCache#getShingle()}\n+     *\n+     * @param shingleSize shingle data point count\n+     * @return how many bytes will consume\n+     */\n+    public long shingleMemorySize(int shingleSize) {\n+        return 96 * shingleSize;", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyMzkxMQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547423911", "bodyText": "you might miss some of the comments.", "author": "kaituo", "createdAt": "2020-12-22T18:08:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ2NjE5NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547466194", "bodyText": "Good catch, forgot to change this part, will change the calculation.\nActually the 96 is from 2 feature testing", "author": "ylwu-amzn", "createdAt": "2020-12-22T19:26:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTEzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTU0OA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546911548", "bodyText": "You don't consider threshold model size, right?", "author": "kaituo", "createdAt": "2020-12-21T20:20:25Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        return memoryTracker.estimateModelSize(adTask.getDetector(), NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTIxMg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546915212", "bodyText": "When are you gonna release the cache memory?  Also, you need to use canAllocateReserved(..) since canAllocate will consider shared cache memory, which can be always full.", "author": "kaituo", "createdAt": "2020-12-21T20:29:40Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNTQyNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547615424", "bodyText": "you missed my comment", "author": "kaituo", "createdAt": "2020-12-23T03:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTIxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY0ODc5Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547648792", "bodyText": "This cache will be released when task cancelled, failed or finished normally. I thought this small PR will make the review easier , but seems that make things complicated and cause a lot of questions which can be answered by related code. Will send out a bigger and more self-explained PR next time. BTW, you can refer to our detail design doc for the whole picture.\nNot sure why, I totally missed this comment. Will address this.", "author": "ylwu-amzn", "createdAt": "2020-12-23T05:12:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTY5MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546915690", "bodyText": "This should be be memoryTracker.consumeMemory(memoryToConsume, true, HISTORICAL_SINGLE_ENTITY_DETECTOR)", "author": "kaituo", "createdAt": "2020-12-21T20:30:57Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNTQ3NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547615475", "bodyText": "you missed my comment", "author": "kaituo", "createdAt": "2020-12-23T03:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTY5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjEwOA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546916108", "bodyText": "2nd parameter should be true.", "author": "kaituo", "createdAt": "2020-12-21T20:32:09Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NDI1OA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546954258", "bodyText": "Good catch, will fix", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:09:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjY4MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546916681", "bodyText": "This method assumes not no threads calling this method with the same taskId, right? Can we assume that?", "author": "kaituo", "createdAt": "2020-12-21T20:33:36Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NDYwNw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546954607", "bodyText": "Correct, only the same thread in which task running will call this method", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:09:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjY4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyMzc0Nw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547423747", "bodyText": "Can we guarantee that?  You might want to put synchronized around the method.", "author": "kaituo", "createdAt": "2020-12-22T18:08:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjY4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyNjEyOA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547426128", "bodyText": "Will move this method to protected. Prefer not to add synchronized to this method currently as this method will not be called by other places. Other developers may feel confused by adding unnecessary synchronized.", "author": "ylwu-amzn", "createdAt": "2020-12-22T18:13:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNjY4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNDYyNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546934624", "bodyText": "This class is a combination of 4 functions:\n\ndata structure for models.  You can modify EntityModel to cover that.  Current EntityModel does not support shingle.  Once it supports that, it will need your Deque<Map.Entry<Long, Optional<double[]>>> shingle.\nfunctions to create empty models.  We create models during training.  Do we really need to create the empty one here?  It sounds strange that a cache is responsible for training or creating models. Can we let the caller pass in the trained objects in?\ncancel/invalidate the cache.\ntrack cache size\n\nI am thinking whether we can define the class as:\nclass ADBatchTaskCache {\nprivate EntityModel ...\nprivate String cancelReason;\nprivate AtomicLong cacheMemorySize = new AtomicLong(0);\n...\npublic ADBatchTaskCache(EntityModel..) {..}", "author": "kaituo", "createdAt": "2020-12-21T21:19:31Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADBatchTaskCache.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_MIN_SAMPLES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_SAMPLES_PER_TREE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.TIME_DECAY;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import com.amazon.opendistroforelasticsearch.ad.ml.HybridThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+/**\n+ * AD batch task cache which will hold RCF, threshold model, shingle and training data.\n+ */\n+public class ADBatchTaskCache {", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NTY4Mw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546955683", "bodyText": "I'd rather not involve EntityModel here, as historical detector is just for single-entity detector in first phase. We will move to universal flow and support HC detector in next phase. Check response to Weicong's question: #337 (comment)\n\nCan we let the caller pass in the trained objects in?\n\nFor this one, will address this in next PR in task runner.", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:13:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNDYyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyNDE5Mw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547424193", "bodyText": "EntityModel is not specific to HC detectors.", "author": "kaituo", "createdAt": "2020-12-22T18:09:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNDYyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzMjkzNg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547432936", "bodyText": "This is some tradeoff, you can check this reply #337 (comment)", "author": "ylwu-amzn", "createdAt": "2020-12-22T18:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNDYyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTAxNw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546939017", "bodyText": "runtime exception means sth you cannot recover.  Is getting something not existing not recoverable? Is it better to return an optional.empty when it is not there.", "author": "kaituo", "createdAt": "2020-12-21T21:30:56Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk2MjY3Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546962672", "bodyText": "This is to make the error handling easier in TaskManager (I will send out another PR soon). In task manager, we will catch all exceptions, differentiate customer input error and server error in one place. We definitely can return opotional.empty or null to tell TaskManger that this task is not found in cache. But we need extra logic to handle such not found exceptions. In task runner, we need to check task existence for every piece, then return when the task not found. In profile/stats API, we need to do the same thing. So I prefer to throw exception here and catch exceptions in one place, rather than check the returned result and check if it's Opitonal.empty or null in multiple places.", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:33:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzNjM5OQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547436399", "bodyText": "Can you explain these in comments?  In a similar place where Weicong commented, it is uncommon to write get/put by throwing exceptions in this way.  Others will want to know this when they read/modify these methods.", "author": "kaituo", "createdAt": "2020-12-22T18:27:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY1Mjk3MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547652971", "bodyText": "Oh, I didn't see this comment either as it's resolved before I addressed this. Will add some comments in java doc.", "author": "ylwu-amzn", "createdAt": "2020-12-23T05:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTQ5Mg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546939492", "bodyText": "2nd parameter should be true.", "author": "kaituo", "createdAt": "2020-12-21T21:31:56Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        return memoryTracker.estimateModelSize(adTask.getDetector(), NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(adTask.getDetector().getShingleSize());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), false, HISTORICAL_SINGLE_ENTITY_DETECTOR);", "originalCommit": "1bf147c0d9ce73b519bcca5a973c72db87f97707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1Njg0MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r546956840", "bodyText": "done", "author": "ylwu-amzn", "createdAt": "2020-12-21T22:16:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzOTQ5Mg=="}], "type": "inlineReview"}, {"oid": "aaced0b3feceec73cc6b0de316ba9c48045a8456", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/aaced0b3feceec73cc6b0de316ba9c48045a8456", "message": "add java doc for exception", "committedDate": "2020-12-21T22:06:10Z", "type": "commit"}, {"oid": "de29b8a937ef9125bc2c66dad02059b04622822e", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/de29b8a937ef9125bc2c66dad02059b04622822e", "message": "change to reserved memory", "committedDate": "2020-12-21T22:23:26Z", "type": "commit"}, {"oid": "de29b8a937ef9125bc2c66dad02059b04622822e", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/de29b8a937ef9125bc2c66dad02059b04622822e", "message": "change to reserved memory", "committedDate": "2020-12-21T22:23:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyNTg2MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547425860", "bodyText": "When you cancel a task, should we release memory involved?", "author": "kaituo", "createdAt": "2020-12-22T18:13:17Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     * If AD task is already in cache, will throw {@link IllegalArgumentException}\n+     * If there is one AD task in cache for detector, will throw {@link IllegalArgumentException}\n+     * If there is no enough memory for this AD task, will throw {@link LimitExceededException}\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     * If executing task count exceeds limitation, will throw\n+     * {@link LimitExceededException}\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public List<Double> getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    public void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingData().size();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.getThresholdModelTrainingData().clear();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        return memoryTracker.estimateModelSize(adTask.getDetector(), NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(adTask.getDetector().getShingleSize());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+            taskCaches.remove(taskId);\n+        }\n+    }\n+\n+    /**\n+     * Cancel AD task.\n+     *\n+     * @param taskId AD task id\n+     * @param reason why need to cancel task\n+     * @param userName user name\n+     */\n+    public void cancel(String taskId, String reason, String userName) {\n+        getBatchTaskCache(taskId).cancel(reason, userName);", "originalCommit": "de29b8a937ef9125bc2c66dad02059b04622822e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyOTY4NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547429684", "bodyText": "Refer to this reply #337 (comment),\n\nThe cancel task is an async process, so we need to cache the cancel reason and cancelled by in cache until the task cancelled successfully.\n\nThe memory will not be released until the task cancelled successfully.", "author": "ylwu-amzn", "createdAt": "2020-12-22T18:17:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyNTg2MA=="}], "type": "inlineReview"}, {"oid": "d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "message": "fix shingle memory calculation;store threshold model training data in double array", "committedDate": "2020-12-22T20:48:30Z", "type": "commit"}, {"oid": "d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "message": "fix shingle memory calculation;store threshold model training data in double array", "committedDate": "2020-12-22T20:48:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMDA5MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547610091", "bodyText": "when will you call this method?  Threshold model can emit results even if it has only seen 1 rcf score.  We use rcf's total updates to measure whether the models are ready or not.  Simply put, threshold model is always trained.  I wonder why we need a flag to set it trained or not.", "author": "kaituo", "createdAt": "2020-12-23T02:48:53Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     * If AD task is already in cache, will throw {@link IllegalArgumentException}\n+     * If there is one AD task in cache for detector, will throw {@link IllegalArgumentException}\n+     * If there is no enough memory for this AD task, will throw {@link LimitExceededException}\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     * If executing task count exceeds limitation, will throw\n+     * {@link LimitExceededException}\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public double[] getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    public int addThresholdModelTrainingData(String taskId, double... data) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        double[] thresholdModelTrainingData = taskCache.getThresholdModelTrainingData();\n+        AtomicInteger size = taskCache.getThresholdModelTrainingDataSize();\n+        int dataPointsAdded = Math.min(data.length, THRESHOLD_MODEL_TRAINING_SIZE - size.get());\n+        System.arraycopy(data, 0, thresholdModelTrainingData, size.get(), dataPointsAdded);\n+        return size.addAndGet(dataPointsAdded);\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    protected void setThresholdModelTrained(String taskId, boolean trained) {", "originalCommit": "d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYzOTI3MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547639270", "bodyText": "This method will be called when historical detector's threshold model finishes training/cold start.\nFor historical detector, we have all of the data, so we should use these data as much as possible to train and predict better. In realtime detector, we have read some(512) historical sampled data to train model. In historical detector, we use more(1000) data points to train model. For both realtime and historical detectors, the RCF&Threshold model will be continuously trained with following data after cold start. Threshold model will be trained first, then start to output reliable results. So we need to know whether the threshold model is trained or not.\nFrom ML team's suggestion that we should use enough data to train RCF model and they suggest 1000 should be good enough. We may tune this value after performance test and more data quality checking.", "author": "ylwu-amzn", "createdAt": "2020-12-23T04:57:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMDA5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE1NDAyMQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r548154021", "bodyText": "Understood.  1000 is a trade-off between accuracy and usability. In real time, 128 is the threshold to emit results.", "author": "kaituo", "createdAt": "2020-12-23T19:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMDA5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMjcxMg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547612712", "bodyText": "Maybe you make 8 an field called it numberSize?  This would make it easier to change in the future.", "author": "kaituo", "createdAt": "2020-12-23T02:59:22Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     * If AD task is already in cache, will throw {@link IllegalArgumentException}\n+     * If there is one AD task in cache for detector, will throw {@link IllegalArgumentException}\n+     * If there is no enough memory for this AD task, will throw {@link LimitExceededException}\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     * If executing task count exceeds limitation, will throw\n+     * {@link LimitExceededException}\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public double[] getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    public int addThresholdModelTrainingData(String taskId, double... data) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        double[] thresholdModelTrainingData = taskCache.getThresholdModelTrainingData();\n+        AtomicInteger size = taskCache.getThresholdModelTrainingDataSize();\n+        int dataPointsAdded = Math.min(data.length, THRESHOLD_MODEL_TRAINING_SIZE - size.get());\n+        System.arraycopy(data, 0, thresholdModelTrainingData, size.get(), dataPointsAdded);\n+        return size.addAndGet(dataPointsAdded);\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    protected void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingDataSize().get();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.clearTrainingData();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        AnomalyDetector detector = adTask.getDetector();\n+        return memoryTracker.estimateModelSize(detector, NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(detector.getShingleSize(), detector.getEnabledFeatureIds().size());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+            taskCaches.remove(taskId);\n+        }\n+    }\n+\n+    /**\n+     * Cancel AD task.\n+     *\n+     * @param taskId AD task id\n+     * @param reason why need to cancel task\n+     * @param userName user name\n+     */\n+    public void cancel(String taskId, String reason, String userName) {\n+        getBatchTaskCache(taskId).cancel(reason, userName);\n+    }\n+\n+    /**\n+     * Task is cancelled or not.\n+     *\n+     * @param taskId AD task id\n+     * @return true if task is cancelled; otherwise return false\n+     */\n+    public boolean isCancelled(String taskId) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        return taskCache.isCancelled();\n+    }\n+\n+    /**\n+     * Get why task cancelled.\n+     *\n+     * @param taskId AD task id\n+     * @return task cancellation reason\n+     */\n+    public String getCancelReason(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelReason();\n+    }\n+\n+    /**\n+     * Get task cancelled by which user.\n+     *\n+     * @param taskId AD task id\n+     * @return user name\n+     */\n+    public String getCancelledBy(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelledBy();\n+    }\n+\n+    /**\n+     * Get current task count in cache.\n+     *\n+     * @return task count\n+     */\n+    public int size() {\n+        return taskCaches.size();\n+    }\n+\n+    /**\n+     * Clear all tasks.\n+     */\n+    public void clear() {\n+        taskCaches.clear();\n+    }\n+\n+    /**\n+     * Estimate max memory usage of model training data.\n+     * The training data is double and will cache in double array.\n+     * One double consumes 8 bytes.\n+     *\n+     * @param size training data point count\n+     * @return how many bytes will consume\n+     */\n+    public long trainingDataMemorySize(int size) {\n+        return 8 * size;\n+    }\n+\n+    /**\n+     * Estimate max memory usage of shingle data.\n+     * One feature aggregated data point(double) consumes 8 bytes.\n+     * The shingle data is stored in {@link java.util.Deque}. From testing,\n+     * other parts except feature data consume 80 bytes.\n+     *\n+     * Check {@link ADBatchTaskCache#getShingle()}\n+     *\n+     * @param shingleSize shingle data point count\n+     * @param enabledFeatureSize enabled feature count\n+     * @return how many bytes will consume\n+     */\n+    public long shingleMemorySize(int shingleSize, int enabledFeatureSize) {\n+        return (80 + 8 * enabledFeatureSize) * shingleSize;", "originalCommit": "d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMjczOA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547612738", "bodyText": "Maybe you make 8 an field called it numberSize?", "author": "kaituo", "createdAt": "2020-12-23T02:59:31Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     * If AD task is already in cache, will throw {@link IllegalArgumentException}\n+     * If there is one AD task in cache for detector, will throw {@link IllegalArgumentException}\n+     * If there is no enough memory for this AD task, will throw {@link LimitExceededException}\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocate(neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, false, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     * If executing task count exceeds limitation, will throw\n+     * {@link LimitExceededException}\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public double[] getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    public int addThresholdModelTrainingData(String taskId, double... data) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        double[] thresholdModelTrainingData = taskCache.getThresholdModelTrainingData();\n+        AtomicInteger size = taskCache.getThresholdModelTrainingDataSize();\n+        int dataPointsAdded = Math.min(data.length, THRESHOLD_MODEL_TRAINING_SIZE - size.get());\n+        System.arraycopy(data, 0, thresholdModelTrainingData, size.get(), dataPointsAdded);\n+        return size.addAndGet(dataPointsAdded);\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    protected void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingDataSize().get();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.clearTrainingData();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     *\n+     * @param taskId task id\n+     * @return AD batch task cache\n+     */\n+    private ADBatchTaskCache getBatchTaskCache(String taskId) {\n+        if (!contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task not in cache\");\n+        }\n+        return taskCaches.get(taskId);\n+    }\n+\n+    /**\n+     * Calculate AD task cache memory usage.\n+     *\n+     * @param adTask AD task\n+     * @return how many bytes will consume\n+     */\n+    private long calculateADTaskCacheSize(ADTask adTask) {\n+        AnomalyDetector detector = adTask.getDetector();\n+        return memoryTracker.estimateModelSize(detector, NUM_TREES) + trainingDataMemorySize(THRESHOLD_MODEL_TRAINING_SIZE)\n+            + shingleMemorySize(detector.getShingleSize(), detector.getEnabledFeatureIds().size());\n+    }\n+\n+    /**\n+     * Remove task from cache.\n+     *\n+     * @param taskId AD task id\n+     */\n+    public void remove(String taskId) {\n+        if (contains(taskId)) {\n+            memoryTracker.releaseMemory(getBatchTaskCache(taskId).getCacheMemorySize().get(), true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+            taskCaches.remove(taskId);\n+        }\n+    }\n+\n+    /**\n+     * Cancel AD task.\n+     *\n+     * @param taskId AD task id\n+     * @param reason why need to cancel task\n+     * @param userName user name\n+     */\n+    public void cancel(String taskId, String reason, String userName) {\n+        getBatchTaskCache(taskId).cancel(reason, userName);\n+    }\n+\n+    /**\n+     * Task is cancelled or not.\n+     *\n+     * @param taskId AD task id\n+     * @return true if task is cancelled; otherwise return false\n+     */\n+    public boolean isCancelled(String taskId) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        return taskCache.isCancelled();\n+    }\n+\n+    /**\n+     * Get why task cancelled.\n+     *\n+     * @param taskId AD task id\n+     * @return task cancellation reason\n+     */\n+    public String getCancelReason(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelReason();\n+    }\n+\n+    /**\n+     * Get task cancelled by which user.\n+     *\n+     * @param taskId AD task id\n+     * @return user name\n+     */\n+    public String getCancelledBy(String taskId) {\n+        return getBatchTaskCache(taskId).getCancelledBy();\n+    }\n+\n+    /**\n+     * Get current task count in cache.\n+     *\n+     * @return task count\n+     */\n+    public int size() {\n+        return taskCaches.size();\n+    }\n+\n+    /**\n+     * Clear all tasks.\n+     */\n+    public void clear() {\n+        taskCaches.clear();\n+    }\n+\n+    /**\n+     * Estimate max memory usage of model training data.\n+     * The training data is double and will cache in double array.\n+     * One double consumes 8 bytes.\n+     *\n+     * @param size training data point count\n+     * @return how many bytes will consume\n+     */\n+    public long trainingDataMemorySize(int size) {\n+        return 8 * size;", "originalCommit": "d48182d29dee44991ba9c6a0afd3b8b3e0aa7fb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY0MTAxNg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r547641016", "bodyText": "ok, will add it.\nNot sure why, I can't reply your comments #337 (comment) , so I just reply it here\n\nyou missed my comment\n\nWorking on three branches for historical detector,  feature query validation and this PR. I guess I put the fix into a wrong branches. Will address this comment. Thanks.", "author": "ylwu-amzn", "createdAt": "2020-12-23T05:01:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxMjczOA=="}], "type": "inlineReview"}, {"oid": "9088012ca0e00fad15a5fb48b15080395720290b", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/9088012ca0e00fad15a5fb48b15080395720290b", "message": "address comments", "committedDate": "2020-12-23T05:41:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE0ODAyNg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r548148026", "bodyText": "I still suggest we remove the exceptions and use Optional or null. In Java, the convention is to always check conditions whenever possible and not to use exceptions for flow control.  Conditional check is a jump in the byte code while the exception handling is much more complex.  When an exception occurs inside a Java method, the method creates an Exception object and passes the Exception object to the JVM (in Java term, the method \"throw\" an Exception). The Exception object contains the type of the exception, and the state of the program when the exception occurs. The JVM is responsible for finding an exception handler to process the Exception object. It searches backward through the call stack until it finds a matching exception handler for that particular class of Exception object (in Java term, it is called \"catch\" the Exception). If the JVM cannot find a matching exception handler in all the methods in the call stack, it terminates the program.\nIf you don't want to do it, I am fine as well.  Just a note this is not a good practice.\nRef: https://stackoverflow.com/questions/8161042/why-use-an-exception-instead-of-if-else", "author": "kaituo", "createdAt": "2020-12-23T19:05:07Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/task/ADTaskCacheManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.task;\n+\n+import static com.amazon.opendistroforelasticsearch.ad.MemoryTracker.Origin.HISTORICAL_SINGLE_ENTITY_DETECTOR;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.MAX_BATCH_TASK_PER_NODE;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.NUM_TREES;\n+import static com.amazon.opendistroforelasticsearch.ad.settings.AnomalyDetectorSettings.THRESHOLD_MODEL_TRAINING_SIZE;\n+\n+import java.util.Deque;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+\n+import com.amazon.opendistroforelasticsearch.ad.MemoryTracker;\n+import com.amazon.opendistroforelasticsearch.ad.common.exception.LimitExceededException;\n+import com.amazon.opendistroforelasticsearch.ad.ml.ThresholdingModel;\n+import com.amazon.opendistroforelasticsearch.ad.model.ADTask;\n+import com.amazon.opendistroforelasticsearch.ad.model.AnomalyDetector;\n+import com.amazon.randomcutforest.RandomCutForest;\n+\n+public class ADTaskCacheManager {\n+\n+    private final Map<String, ADBatchTaskCache> taskCaches;\n+    private volatile Integer maxAdBatchTaskPerNode;\n+    private final MemoryTracker memoryTracker;\n+    private final int numberSize = 8;\n+\n+    /**\n+     * Constructor to create AD task cache manager.\n+     *\n+     * @param settings ES settings\n+     * @param clusterService ES cluster service\n+     * @param memoryTracker AD memory tracker\n+     */\n+    public ADTaskCacheManager(Settings settings, ClusterService clusterService, MemoryTracker memoryTracker) {\n+        this.maxAdBatchTaskPerNode = MAX_BATCH_TASK_PER_NODE.get(settings);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MAX_BATCH_TASK_PER_NODE, it -> maxAdBatchTaskPerNode = it);\n+        taskCaches = new ConcurrentHashMap<>();\n+        this.memoryTracker = memoryTracker;\n+    }\n+\n+    /**\n+     * Put AD task into cache.\n+     * If AD task is already in cache, will throw {@link IllegalArgumentException}\n+     * If there is one AD task in cache for detector, will throw {@link IllegalArgumentException}\n+     * If there is no enough memory for this AD task, will throw {@link LimitExceededException}\n+     *\n+     * @param adTask AD task\n+     */\n+    public synchronized void put(ADTask adTask) {\n+        String taskId = adTask.getTaskId();\n+        if (contains(taskId)) {\n+            throw new IllegalArgumentException(\"AD task is already running\");\n+        }\n+        if (containsTaskOfDetector(adTask.getDetectorId())) {\n+            throw new IllegalArgumentException(\"There is one task executing for detector\");\n+        }\n+        checkRunningTaskLimit();\n+        long neededCacheSize = calculateADTaskCacheSize(adTask);\n+        if (!memoryTracker.canAllocateReserved(adTask.getDetectorId(), neededCacheSize)) {\n+            throw new LimitExceededException(\"No enough memory to run detector\");\n+        }\n+        memoryTracker.consumeMemory(neededCacheSize, true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        ADBatchTaskCache taskCache = new ADBatchTaskCache(adTask);\n+        taskCache.getCacheMemorySize().set(neededCacheSize);\n+        taskCaches.put(taskId, taskCache);\n+    }\n+\n+    /**\n+     * check if current running batch task on current node exceeds\n+     * max running task limitation.\n+     * If executing task count exceeds limitation, will throw\n+     * {@link LimitExceededException}\n+     */\n+    public void checkRunningTaskLimit() {\n+        if (size() >= maxAdBatchTaskPerNode) {\n+            String error = \"Can't run more than \" + maxAdBatchTaskPerNode + \" historical detectors per data node\";\n+            throw new LimitExceededException(error);\n+        }\n+    }\n+\n+    /**\n+     * Get task RCF model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return RCF model\n+     */\n+    public RandomCutForest getRcfModel(String taskId) {\n+        return getBatchTaskCache(taskId).getRcfModel();\n+    }\n+\n+    /**\n+     * Get task threshold model.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model\n+     */\n+    public ThresholdingModel getThresholdModel(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModel();\n+    }\n+\n+    /**\n+     * Get threshold model training data.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return threshold model training data\n+     */\n+    public double[] getThresholdModelTrainingData(String taskId) {\n+        return getBatchTaskCache(taskId).getThresholdModelTrainingData();\n+    }\n+\n+    public int addThresholdModelTrainingData(String taskId, double... data) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        double[] thresholdModelTrainingData = taskCache.getThresholdModelTrainingData();\n+        AtomicInteger size = taskCache.getThresholdModelTrainingDataSize();\n+        int dataPointsAdded = Math.min(data.length, THRESHOLD_MODEL_TRAINING_SIZE - size.get());\n+        System.arraycopy(data, 0, thresholdModelTrainingData, size.get(), dataPointsAdded);\n+        return size.addAndGet(dataPointsAdded);\n+    }\n+\n+    /**\n+     * Threshold model trained or not.\n+     * If task doesn't exist in cache, will throw {@link java.lang.IllegalArgumentException}.\n+     *\n+     * @param taskId AD task id\n+     * @return true if threshold model trained; otherwise, return false\n+     */\n+    public boolean isThresholdModelTrained(String taskId) {\n+        return getBatchTaskCache(taskId).isThresholdModelTrained();\n+    }\n+\n+    /**\n+     * Set threshold model trained or not.\n+     *\n+     * @param taskId task id\n+     * @param trained threshold model trained or not\n+     */\n+    protected void setThresholdModelTrained(String taskId, boolean trained) {\n+        ADBatchTaskCache taskCache = getBatchTaskCache(taskId);\n+        taskCache.setThresholdModelTrained(trained);\n+        if (trained) {\n+            int size = taskCache.getThresholdModelTrainingDataSize().get();\n+            long cacheSize = trainingDataMemorySize(size);\n+            taskCache.clearTrainingData();\n+            taskCache.getCacheMemorySize().getAndAdd(-cacheSize);\n+            memoryTracker.releaseMemory(cacheSize, true, HISTORICAL_SINGLE_ENTITY_DETECTOR);\n+        }\n+    }\n+\n+    /**\n+     * Get shingle data.\n+     *\n+     * @param taskId AD task id\n+     * @return shingle data\n+     */\n+    public Deque<Map.Entry<Long, Optional<double[]>>> getShingle(String taskId) {\n+        return getBatchTaskCache(taskId).getShingle();\n+    }\n+\n+    /**\n+     * Check if task exists in cache.\n+     *\n+     * @param taskId task id\n+     * @return true if task exists in cache; otherwise, return false.\n+     */\n+    public boolean contains(String taskId) {\n+        return taskCaches.containsKey(taskId);\n+    }\n+\n+    /**\n+     * Check if there is task in cache for detector.\n+     *\n+     * @param detectorId detector id\n+     * @return true if there is task in cache; otherwise return false\n+     */\n+    public boolean containsTaskOfDetector(String detectorId) {\n+        return taskCaches.values().stream().filter(v -> Objects.equals(detectorId, v.getDetectorId())).findAny().isPresent();\n+    }\n+\n+    /**\n+     * Get batch task cache. If task doesn't exist in cache, will throw\n+     * {@link java.lang.IllegalArgumentException}\n+     * We throw exception rather than return {@code Optional.empty} or null\n+     * here, so don't need to check task existence by writing duplicate null\n+     * checking code. All AD task exceptions will be handled in AD task manager.", "originalCommit": "9088012ca0e00fad15a5fb48b15080395720290b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc1OTYxOQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r548759619", "bodyText": "Totally agree to not use exception for flow control. Here the exception is not to control the flow, but rather terminate the task run flow, just like any AD result action to terminate detector job run by throwing exceptions. In next PR I may change the method name to reduce the confusion.", "author": "ylwu-amzn", "createdAt": "2020-12-24T22:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE0ODAyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2MDI0MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/337#discussion_r548760240", "bodyText": "got it.  Please change the method name or add comment to make it clearer.", "author": "kaituo", "createdAt": "2020-12-24T23:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE0ODAyNg=="}], "type": "inlineReview"}]}