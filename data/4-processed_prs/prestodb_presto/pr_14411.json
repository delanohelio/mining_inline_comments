{"pr_number": 14411, "pr_title": "Add Native Parquet Writer in Hive Module and Misc fixes", "pr_createdAt": "2020-04-19T21:24:06Z", "pr_url": "https://github.com/prestodb/presto/pull/14411", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0NzEzOA==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413247138", "bodyText": "static import HiveSessionProperties.getParquetWriterPageSize and HiveSessionProperties.getParquetWriterBlockSize", "author": "zhenxiao", "createdAt": "2020-04-22T19:08:35Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -97,6 +93,11 @@ public ParquetFileWriterFactory(\n             return Optional.empty();\n         }\n \n+        ParquetWriterOptions parquetWriterOptions = ParquetWriterOptions.builder()\n+                .setMaxPageSize(HiveSessionProperties.getParquetWriterPageSize(session))", "originalCommit": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODQ3OQ==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248479", "bodyText": "not needed? already named ignored", "author": "zhenxiao", "createdAt": "2020-04-22T19:10:46Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.Page;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.block.Block;\n+import com.facebook.presto.spi.block.BlockBuilder;\n+import com.facebook.presto.spi.block.RunLengthEncodedBlock;\n+import com.facebook.presto.spi.type.Type;\n+import com.google.common.collect.ImmutableList;\n+import org.openjdk.jol.info.ClassLayout;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements HiveFileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName.getHadoopCompressionCodecClassName());\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore", "originalCommit": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODc4MQ==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248781", "bodyText": "...writing parquet to Hive", "author": "zhenxiao", "createdAt": "2020-04-22T19:11:13Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.Page;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.block.Block;\n+import com.facebook.presto.spi.block.BlockBuilder;\n+import com.facebook.presto.spi.block.RunLengthEncodedBlock;\n+import com.facebook.presto.spi.type.Type;\n+import com.google.common.collect.ImmutableList;\n+import org.openjdk.jol.info.ClassLayout;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements HiveFileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName.getHadoopCompressionCodecClassName());\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore\n+            }\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error committing write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void rollback()\n+    {\n+        try {\n+            try {\n+                parquetWriter.close();\n+            }\n+            finally {\n+                rollbackAction.call();\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error rolling back write parquet to Hive\", e);", "originalCommit": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODk5Mg==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248992", "bodyText": "is this indentation correct?", "author": "zhenxiao", "createdAt": "2020-04-22T19:11:35Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.HiveSessionProperties;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.ParquetOutputFormat;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;\n+\n+    @Inject\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            HiveClientConfig hiveConfig)\n+    {\n+        this(\n+                hdfsEnvironment,", "originalCommit": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5ODE4NQ==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r429598185", "bodyText": "yeah. I think it's correct.", "author": "qqibrow", "createdAt": "2020-05-24T04:34:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0OTM1OA==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413249358", "bodyText": "static import HiveSessionProperties.isParquetOptimizedWriterEnabled", "author": "zhenxiao", "createdAt": "2020-04-22T19:12:11Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.HiveSessionProperties;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.ParquetOutputFormat;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;\n+\n+    @Inject\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            HiveClientConfig hiveConfig)\n+    {\n+        this(\n+                hdfsEnvironment,\n+                typeManager,\n+                nodeVersion,\n+                requireNonNull(hiveConfig, \"hiveConfig is null\").getDateTimeZone());\n+    }\n+\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            DateTimeZone hiveStorageTimeZone)\n+    {\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.nodeVersion = requireNonNull(nodeVersion, \"nodeVersion is null\");\n+        this.hiveStorageTimeZone = requireNonNull(hiveStorageTimeZone, \"hiveStorageTimeZone is null\");\n+    }\n+\n+    @Override\n+    public Optional<HiveFileWriter> createFileWriter(Path path, List<String> inputColumnNames, StorageFormat storageFormat, Properties schema, JobConf conf, ConnectorSession session)\n+    {\n+        if (!HiveSessionProperties.isParquetOptimizedWriterEnabled(session)) {", "originalCommit": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI4OTY5NA==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414289694", "bodyText": "This is not used.", "author": "viczhang861", "createdAt": "2020-04-24T04:47:53Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;", "originalCommit": "6232bbacdd2cc9c73e86165537531bfcf4e7f1c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI4OTgxMA==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414289810", "bodyText": "This is not used.", "author": "viczhang861", "createdAt": "2020-04-24T04:48:13Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;", "originalCommit": "6232bbacdd2cc9c73e86165537531bfcf4e7f1c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI5NTE0Mw==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414295143", "bodyText": "Just curious,  can we read parquet compression directly using HiveSessionProperties::getCompressionCodec(ConnectorSession session)", "author": "viczhang861", "createdAt": "2020-04-24T05:04:48Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -96,6 +97,8 @@ public ParquetFileWriterFactory(\n             return Optional.empty();\n         }\n \n+        CompressionCodecName compressionCodecName = getCompression(conf);", "originalCommit": "a5dcf973135234d497d1b4d093c41b804bcfebc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5ODA3OQ==", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r429598079", "bodyText": "When I made the code change, the source of truth for compression is from config and the conf is set through ConfigurationUtils::configureCompression. Maybe your way would also work.", "author": "qqibrow", "createdAt": "2020-05-24T04:31:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI5NTE0Mw=="}], "type": "inlineReview"}, {"oid": "c4f9baa1f8cca943526bf84b328051b4dc22bb15", "url": "https://github.com/prestodb/presto/commit/c4f9baa1f8cca943526bf84b328051b4dc22bb15", "message": "Set ParquetWriterOptions based on session parameters", "committedDate": "2020-05-25T17:25:19Z", "type": "forcePushed"}, {"oid": "96b6dc6ce6e1eb45f84da4dd0cef8b944c00258f", "url": "https://github.com/prestodb/presto/commit/96b6dc6ce6e1eb45f84da4dd0cef8b944c00258f", "message": "Expose written bytes and buffered bytes in ParquetWriter", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "426d98287425147963f0583e530e6b1772ef0ec7", "url": "https://github.com/prestodb/presto/commit/426d98287425147963f0583e530e6b1772ef0ec7", "message": "Add parquet writer in hive module", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "d6c619cdbb98c26ab67609875b6e293e6a7e50f3", "url": "https://github.com/prestodb/presto/commit/d6c619cdbb98c26ab67609875b6e293e6a7e50f3", "message": "Add test for optimized parquet writer in hive module", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "f5f3b9907a2cfe3ebd0290d0f75a07bdc4981047", "url": "https://github.com/prestodb/presto/commit/f5f3b9907a2cfe3ebd0290d0f75a07bdc4981047", "message": "Add compression in ParquetFileWriterFactory", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "529a4677c5e650a52c3b77646b2b3201d657ca96", "url": "https://github.com/prestodb/presto/commit/529a4677c5e650a52c3b77646b2b3201d657ca96", "message": "Set statistics in RowGroup metadata", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "ed84b7a5a6a33bdae482c82a583bc2454bd8f93c", "url": "https://github.com/prestodb/presto/commit/ed84b7a5a6a33bdae482c82a583bc2454bd8f93c", "message": "resetDictionary right after get dictionary page\n\nresetDictionary in reset() cleaned dictionary regardless of the fact\nthat dictionary page is null or not. resetDictionary should happen\nonly after get dictionary page and the page is not null.", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "abdd47801b40527308aeb8906c47cb82e41dc96c", "url": "https://github.com/prestodb/presto/commit/abdd47801b40527308aeb8906c47cb82e41dc96c", "message": "Add encoding should be called after getBytes() and before reset()", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "1ddbca8f194cac6b0d1ed918073645e9fa34f381", "url": "https://github.com/prestodb/presto/commit/1ddbca8f194cac6b0d1ed918073645e9fa34f381", "message": "Support setting page size and row group size in parquet writer", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "c71f74db070ca8ff586bd6be23c3033dea08bb57", "url": "https://github.com/prestodb/presto/commit/c71f74db070ca8ff586bd6be23c3033dea08bb57", "message": "Set parquet writer page size and row group size in ParquetTester\n\nSet both setting to smaller value could force writer to write multiple\nrow groups and multiple pages in each row group, which could better\nsimulate real world examples.", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "2e7b17abc839ae658bfb3870f70a457ab9ad5ff1", "url": "https://github.com/prestodb/presto/commit/2e7b17abc839ae658bfb3870f70a457ab9ad5ff1", "message": "Set ParquetWriterOptions based on session parameters", "committedDate": "2020-05-25T17:28:39Z", "type": "commit"}, {"oid": "2e7b17abc839ae658bfb3870f70a457ab9ad5ff1", "url": "https://github.com/prestodb/presto/commit/2e7b17abc839ae658bfb3870f70a457ab9ad5ff1", "message": "Set ParquetWriterOptions based on session parameters", "committedDate": "2020-05-25T17:28:39Z", "type": "forcePushed"}]}