{"pr_number": 14850, "pr_title": "Distribute splits to presto on spark tasks based on size", "pr_createdAt": "2020-07-16T22:30:38Z", "pr_url": "https://github.com/prestodb/presto/pull/14850", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUzNTkyNA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456535924", "bodyText": "nit: use OptionalLong", "author": "wenleix", "createdAt": "2020-07-17T16:06:06Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java", "diffHunk": "@@ -282,6 +282,12 @@ public Object getInfo()\n                 .build();\n     }\n \n+    @Override\n+    public Optional<Long> getSplitSizeInBytes()", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTEyOA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585128", "bodyText": "Let's return Optional.empty() here.", "author": "arhimondr", "createdAt": "2020-07-17T17:44:58Z", "path": "presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java", "diffHunk": "@@ -37,4 +38,9 @@\n     List<HostAddress> getPreferredNodes(List<HostAddress> sortedCandidates);\n \n     Object getInfo();\n+\n+    default Optional<Long> getSplitSizeInBytes()\n+    {\n+        throw new UnsupportedOperationException();", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTQ5Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585493", "bodyText": "add checkArgument(minSplitsPerSparkPartition > 0 , \"minSplitsPerSparkPartition must be greater than zero: %s\", minSplitsPerSparkPartition)", "author": "arhimondr", "createdAt": "2020-07-17T17:45:37Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTgzOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585839", "bodyText": "ditto here", "author": "arhimondr", "createdAt": "2020-07-17T17:46:19Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjIyMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586221", "bodyText": "nit: move it inside the loop", "author": "arhimondr", "createdAt": "2020-07-17T17:47:03Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().get())\n+                    .sum();\n+            int taskCountBySplitsDataSizePerPartition = max(1, toIntExact(totalSizeInBytes / estimatedSizeInBytesPerPartition));\n+\n+            int taskCount = min(taskCountBySplitsPerPartition, taskCountBySplitsDataSizePerPartition);\n+\n+            PriorityQueue<SparkPartition> pq = new PriorityQueue();\n+            int partitionId;", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjM2NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586364", "bodyText": "maybe assignSplit?", "author": "arhimondr", "createdAt": "2020-07-17T17:47:21Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586940", "bodyText": "I would simply sort it by size. We should always add a next split to the partition that has the least data assigned (not the least number of splits).", "author": "arhimondr", "createdAt": "2020-07-17T17:48:30Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyOTM5MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456729390", "bodyText": "Make sense,  spark.min-splits-per-partition is only used to determine total number of spark task:  avoid spawn new task that has few splits.  In most cases,  spark task count should be determined by data size per task.", "author": "viczhang861", "createdAt": "2020-07-18T01:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDMzOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660339", "bodyText": "simply this. splitsSizeInBytes  - that.splitsSizeInBytes", "author": "arhimondr", "createdAt": "2020-07-20T20:04:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjM0NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802344", "bodyText": "This avoids overflow when converting Long to Int.", "author": "viczhang861", "createdAt": "2020-07-21T02:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NzI0MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456587241", "bodyText": "splitSize is not available", "author": "arhimondr", "createdAt": "2020-07-17T17:49:05Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)\n+        {\n+            usedSplitSlot = min(usedSplitSlot + 1, maxSplitSlot);\n+            Optional<Long> splitSize = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSize.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes not available\"));", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "url": "https://github.com/prestodb/presto/commit/c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size.", "committedDate": "2020-07-18T00:52:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMTEyNg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457511126", "bodyText": "nit: should this be allMatch? -- Since if some split has size, some other splits don't, we still cannot do auto tune right?", "author": "wenleix", "createdAt": "2020-07-20T15:47:52Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());", "originalCommit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU2Nzc3Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457567773", "bodyText": "I removed double negation to improve readability.", "author": "viczhang861", "createdAt": "2020-07-20T17:16:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMTEyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMjAyNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457512025", "bodyText": "Even the partition count is not decided automatically, we can still use the greedy algorithm to have a balanced split assignment.", "author": "wenleix", "createdAt": "2020-07-20T15:48:59Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {", "originalCommit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457516151", "bodyText": "Curious: Will this be the total number of physical cores on the Spark workers (instead of Spark container CPUs? ). (E.g. the worker has 16 cores, but the Spark container only has 4 CPUs, and this will set minSplitsPerSparkPartition to 16? )", "author": "wenleix", "createdAt": "2020-07-20T15:53:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;\n     private int initialSparkPartitionCount = 16;\n+    private int minSplitsPerSparkPartition = Runtime.getRuntime().availableProcessors();", "originalCommit": "2303a390bd13591864af0cc311277b407833e532", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU2ODU0Ng==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457568546", "bodyText": "@arhimondr Do you know the answer?  I can run some test to verify.", "author": "viczhang861", "createdAt": "2020-07-20T17:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MTIxMg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457661212", "bodyText": "Yeah, I think this default doesn't make a lot of sense.\nI think it is even better to do not set any defaults and keep the autotune disabled. Then it can be manually enabled and the correct settings will be set.", "author": "arhimondr", "createdAt": "2020-07-20T20:06:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ=="}], "type": "inlineReview"}, {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "url": "https://github.com/prestodb/presto/commit/7f267d86eb8656c8f852c931baffb0d0b00b8025", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size.", "committedDate": "2020-07-20T17:14:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1ODUxOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457658519", "bodyText": "maybe sparkInitialPartitionCountAutoTuneEnabled?\ncc: @wenleix", "author": "arhimondr", "createdAt": "2020-07-20T20:00:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1OTQwNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457659405", "bodyText": "Let's extract this into a variable\nboolean splitsDataSizeAvailable  = splits.stream() .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent())\nThen simply boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session) && splitsDataSizeAvailable", "author": "arhimondr", "createdAt": "2020-07-20T20:02:49Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +389,51 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && splits.stream()", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDYyNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660625", "bodyText": "This breakes the non auto tune path. It may start throwing if the split size is not available", "author": "arhimondr", "createdAt": "2020-07-20T20:05:11Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +523,35 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId)\n+        {\n+            this.partitionId = partitionId;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void assignSplit(ScheduledSplit scheduledSplit)\n+        {\n+            OptionalLong splitSizeInBytes = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSizeInBytes.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes is not present\"));", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2OTM5Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457669397", "bodyText": "Good catch ! Is it possible that some splits have size available and some don't, for this case, a combination of even distribution + random distribution will be used.", "author": "viczhang861", "createdAt": "2020-07-20T20:22:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDYyNQ=="}], "type": "inlineReview"}, {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "url": "https://github.com/prestodb/presto/commit/aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-21T02:18:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDE0OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800149", "bodyText": "(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition", "author": "arhimondr", "createdAt": "2020-07-21T02:33:22Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDQwMw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800403", "bodyText": "Can splits be immutable here?", "author": "arhimondr", "createdAt": "2020-07-21T02:34:14Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801177", "bodyText": "What if split is larger? Does it mean that we are not going to schedule it? We are going to break correctness.", "author": "arhimondr", "createdAt": "2020-07-21T02:37:04Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NzYyOA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547628", "bodyText": "This last line of code inside for loop guarantees each split will be assigned to a partition decided by variable partitionId.", "author": "viczhang861", "createdAt": "2020-07-22T05:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NTYxMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458845611", "bodyText": "Oh, nevermind. I missread", "author": "arhimondr", "createdAt": "2020-07-22T14:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0ODYyMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458848621", "bodyText": "So basically if the split size is larger than the split size per partition theres no need to add the partition to the queue, as it will get full after adding this split, right?", "author": "arhimondr", "createdAt": "2020-07-22T14:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801298", "bodyText": "Should this partition be placed into the queue?", "author": "arhimondr", "createdAt": "2020-07-21T02:37:29Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0Nzc5NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547794", "bodyText": "Not needed, this partition cannot accept more splits.", "author": "viczhang861", "createdAt": "2020-07-22T05:44:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NzU2Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458847567", "bodyText": "Could you please elaborate how it works?\nfrom what I see in the upper branch we check if !queue.isEmpty(), so if queue is empty it will not enter the upper branch. But we never place anything into the queue outside of the upper branch?", "author": "arhimondr", "createdAt": "2020-07-22T14:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3ODg0MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458978840", "bodyText": "With the redesign of original thought,  we removed min_splits_per_partition. The goal is now simple:   (1) Avoid skew by setting a max limit per partition. If a single split's data size exceeds this limit, the only thing we can do is to not add any more splits into this partition to make maximal partition size small.  (2) Make splits balanced, this is achieved by heuristic algorithm to always assign a split to partition with smallest size.\nWhat is partitionCount >= initialPartitionCount?\n-- initialPartitionCount is the lower bound of partitions (based on calculation total_size / max_partition_size), so if this is false, always create a new partition to accept any split\nWhat is !queue.isEmpty() && queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes\n-- If this condition is false,  there is no partition with enough capacity to accept this split, create a new partition.\nInvariance for partition in the queue\n-- This partition is not full and contains at least one split", "author": "viczhang861", "createdAt": "2020-07-22T17:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTQ1Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801453", "bodyText": "This code is getting non trivial. I would strongly suggest adding a unit test.", "author": "arhimondr", "createdAt": "2020-07-21T02:38:05Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);\n+                        newPartition.assignSplitWithSize(splitSizeInBytes);", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0Nzg2OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547869", "bodyText": "Unit test added, repeated for 1000 times.", "author": "viczhang861", "createdAt": "2020-07-22T05:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTU3MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801571", "bodyText": "why not simply this. availableCapacityInBytes  - that. availableCapacityInBytes ? (or the vice versa, depending on the order you are trying to achieve)", "author": "arhimondr", "createdAt": "2020-07-21T02:38:30Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +537,40 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long availableCapacityInBytes;\n+\n+        public SparkPartition(int partitionId, long availableCapacityInBytes)\n+        {\n+            this.partitionId = partitionId;\n+            this.availableCapacityInBytes = availableCapacityInBytes;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return availableCapacityInBytes == o.availableCapacityInBytes ?", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjczMw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802733", "bodyText": "Nevermind. DIdn't realize we are dealing with a long.", "author": "arhimondr", "createdAt": "2020-07-21T02:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTg5OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801898", "bodyText": "You can simply do o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() - o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() (or the vice versa, depending on the order you are trying to achieve)", "author": "arhimondr", "createdAt": "2020-07-21T02:39:36Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjY3Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802677", "bodyText": "Nevermind. DIdn't realize we are dealing with a long.", "author": "arhimondr", "createdAt": "2020-07-21T02:42:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTg5OA=="}], "type": "inlineReview"}, {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986", "url": "https://github.com/prestodb/presto/commit/fcf8a651fd84280b60aca8b777ef32c26217b986", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-22T05:37:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0MjgyNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458842825", "bodyText": "nit: reformat", "author": "arhimondr", "createdAt": "2020-07-22T14:38:56Z", "path": "presto-spark-base/pom.xml", "diffHunk": "@@ -175,6 +175,12 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-hive-common</artifactId>\n+            <scope>test</scope>\n+\t</dependency>", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MDU2OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458850568", "bodyText": "You don't have to create a HiveSplit. You can create a private class MockSplit that only implement the getSplitSizeInBytes method", "author": "arhimondr", "createdAt": "2020-07-22T14:48:45Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MTA1MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458851051", "bodyText": "It doesn't feel like this algorithm is worth to be tested with a fuzz testing. Instead i would recommend adding a number of deterministic test cases to verify all possible corner cases of the algorithm.", "author": "arhimondr", "createdAt": "2020-07-22T14:49:20Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MjAyNg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458852026", "bodyText": "Instead of having very generic assertions I would recommend adding more concrete assertions", "author": "arhimondr", "createdAt": "2020-07-22T14:50:32Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(\n+                    \"test_schema\",\n+                    \"test_table\",\n+                    \"\",\n+                    \"path\",\n+                    0,\n+                    splitSizeInBytes,\n+                    splitSizeInBytes,\n+                    new Storage(\n+                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n+                            \"location\",\n+                            Optional.empty(),\n+                            false,\n+                            ImmutableMap.of(),\n+                            ImmutableMap.of()),\n+                    ImmutableList.of(),\n+                    ImmutableList.of(),\n+                    OptionalInt.empty(),\n+                    OptionalInt.empty(),\n+                    NO_PREFERENCE,\n+                    1,\n+                    ImmutableMap.of(),\n+                    Optional.empty(),\n+                    false,\n+                    Optional.empty(),\n+                    NO_CACHE_REQUIREMENT,\n+                    Optional.empty());\n+            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n+            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n+            splits.add(scheduledSplit);\n+        }\n+\n+        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n+        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n+            if (scheduledSplits.size() > 1) {\n+                long totalPartitionSize = scheduledSplits.stream()\n+                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                        .sum();\n+                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTY3OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459059679", "bodyText": "So if we look at this if-statement, it's like this:\n    if (queue is not empty and some other cnoditions) {\n        Do something\n        Add the partition into the queue\n    }\n    else {\n         // queue is empty\n         Do something, but doesn't append anything into the queue\n    }\n\nAs the queue is empty at the beginning, it looks like nothing will be added into the queue. Is there anything I am missing in the code flow? ~ -- or maybe just debug through it to see if the \"else\" branch will ever get triggered?", "author": "wenleix", "createdAt": "2020-07-22T20:21:16Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +385,72 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact((totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n+                long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n+            });\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzYzMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459167631", "bodyText": "Yes, I forgot to add it back to the queue, as a result every split is assigned to a new partition.  Unit test is updated with explicit examples to cover this case.", "author": "viczhang861", "createdAt": "2020-07-23T01:13:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTY3OQ=="}], "type": "inlineReview"}, {"oid": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "url": "https://github.com/prestodb/presto/commit/bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "message": "Add properties for Presto-on-Spark split distribution", "committedDate": "2020-07-22T23:27:13Z", "type": "commit"}, {"oid": "cb194a543593c4d344f68d2325672c972e99b30c", "url": "https://github.com/prestodb/presto/commit/cb194a543593c4d344f68d2325672c972e99b30c", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T01:05:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTM5OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459181399", "bodyText": "nit: Let's simply return result.build(); instead of adding one more level of indentation", "author": "arhimondr", "createdAt": "2020-07-23T02:15:47Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +383,85 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        checkArgument(initialPartitionCount > 0,\n+                \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {", "originalCommit": "cb194a543593c4d344f68d2325672c972e99b30c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjgwNA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459182804", "bodyText": "maxPartitionSize?", "author": "arhimondr", "createdAt": "2020-07-23T02:21:46Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +178,158 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        // black box test\n+        testAssignSplitsToPartitionWithRandomSplitsSize(3);\n+\n+        // auto tune partition + splits with mixed size\n+        List<Long> testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L));\n+        Collections.shuffle(testSplitsSize);\n+        Map<Integer, List<Long>> actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        Map<Integer, List<Long>> expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(11L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(10L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(9L)));\n+        expectedResult.put(3, new ArrayList(Arrays.asList(8L, 1L)));\n+        expectedResult.put(4, new ArrayList(Arrays.asList(7L, 2L)));\n+        expectedResult.put(5, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(6, new ArrayList(Arrays.asList(5L, 4L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // enable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 4L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(2L, 1L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 1L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 2L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(4L, 3L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + large splits\n+        testSplitsSize = new ArrayList(Arrays.asList(5L, 6L, 7L, 8L, 9L, 10L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(10L, 5L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(9L, 6L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(8L, 7L)));\n+        assertEquals(actualResult, expectedResult);\n+    }\n+\n+    private void testAssignSplitsToPartitionWithRandomSplitsSize(int repeatedTimes)\n+    {\n+        int maxSplitSizeInBytes = 2048;\n+        for (int i = 0; i < repeatedTimes; ++i) {\n+            List<Long> splitsSize = new ArrayList<>(1000);\n+            for (int j = 0; j < splitsSize.size(); j++) {\n+                splitsSize.set(j, ThreadLocalRandom.current().nextLong((long) (maxSplitSizeInBytes * 1.2)));\n+            }\n+            assignSplitsToPartition(true, maxSplitSizeInBytes, splitsSize);\n+            assignSplitsToPartition(false, maxSplitSizeInBytes, splitsSize);\n+        }\n+    }\n+\n+    private Map<Integer, List<Long>> assignSplitsToPartition(\n+            boolean autoTunePartitionCount,\n+            long maxSplitsSize,", "originalCommit": "cb194a543593c4d344f68d2325672c972e99b30c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T03:21:10Z", "type": "commit"}, {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T03:21:10Z", "type": "forcePushed"}]}