{"pr_number": 14995, "pr_title": "Add support for druid data ingestion", "pr_createdAt": "2020-08-09T08:05:03Z", "pr_url": "https://github.com/prestodb/presto/pull/14995", "timeline": [{"oid": "bc66fcc259f0e4836cffbcac98a54e0d75eb6493", "url": "https://github.com/prestodb/presto/commit/bc66fcc259f0e4836cffbcac98a54e0d75eb6493", "message": "Add druid table insert/ingestion skeleton code", "committedDate": "2020-08-08T08:03:26Z", "type": "commit"}, {"oid": "6b0d139453a6a714aaef44df259729a191c4877d", "url": "https://github.com/prestodb/presto/commit/6b0d139453a6a714aaef44df259729a191c4877d", "message": "Add ingestion storage path to DruidConfig", "committedDate": "2020-08-08T08:03:26Z", "type": "commit"}, {"oid": "08e66d5b2ebbc3f01c0c60d04acd24c1e15afa9d", "url": "https://github.com/prestodb/presto/commit/08e66d5b2ebbc3f01c0c60d04acd24c1e15afa9d", "message": "Write druid page data to gzip files", "committedDate": "2020-08-08T08:03:26Z", "type": "commit"}, {"oid": "043581ebf193294d3940fcef89fa414443e64850", "url": "https://github.com/prestodb/presto/commit/043581ebf193294d3940fcef89fa414443e64850", "message": "Implement sending ingestion task to druid", "committedDate": "2020-08-09T07:55:21Z", "type": "commit"}, {"oid": "8b41372506afc7105062cd01f3ff428024aa76a3", "url": "https://github.com/prestodb/presto/commit/8b41372506afc7105062cd01f3ff428024aa76a3", "message": "Implement druid ingestion by CTAS", "committedDate": "2020-08-10T03:23:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNDkwMg==", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469604902", "bodyText": "s/DruidIngestInputSourceLocal/DruidIngestLocalInput/g", "author": "zhenxiao", "createdAt": "2020-08-12T23:36:01Z", "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal", "originalCommit": "927a9d68076cc23f7a02b79c314029fa3dc6805d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcxNzc5Mw==", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469717793", "bodyText": "done", "author": "beinan", "createdAt": "2020-08-13T06:10:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNDkwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNTAwNw==", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469605007", "bodyText": "s/DruidIngestInputSourceHDFS/DruidIngestHDFSInput/g", "author": "zhenxiao", "createdAt": "2020-08-12T23:36:22Z", "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal\n+            implements DruidIngestInputSource\n+    {\n+        private final String type;\n+        private final String baseDir;\n+        private final String filter;\n+\n+        public DruidIngestInputSourceLocal(String type, String baseDir, String filter)\n+        {\n+            this.type = type;\n+            this.baseDir = baseDir;\n+            this.filter = filter;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"baseDir\")\n+        public String getBaseDir()\n+        {\n+            return baseDir;\n+        }\n+\n+        @JsonProperty(\"filter\")\n+        public String getFilter()\n+        {\n+            return filter;\n+        }\n+    }\n+\n+    public static class DruidIngestInputSourceHDFS", "originalCommit": "927a9d68076cc23f7a02b79c314029fa3dc6805d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcxNzgzNw==", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469717837", "bodyText": "done", "author": "beinan", "createdAt": "2020-08-13T06:10:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNTAwNw=="}], "type": "inlineReview"}, {"oid": "06eaf54b1c683f7d250f63bfb16087cd2565fd18", "url": "https://github.com/prestodb/presto/commit/06eaf54b1c683f7d250f63bfb16087cd2565fd18", "message": "Implement hdfs input source for druid ingestion", "committedDate": "2020-08-13T06:09:21Z", "type": "commit"}, {"oid": "06eaf54b1c683f7d250f63bfb16087cd2565fd18", "url": "https://github.com/prestodb/presto/commit/06eaf54b1c683f7d250f63bfb16087cd2565fd18", "message": "Implement hdfs input source for druid ingestion", "committedDate": "2020-08-13T06:09:21Z", "type": "forcePushed"}]}