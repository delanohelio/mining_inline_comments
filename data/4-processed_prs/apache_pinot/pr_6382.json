{"pr_number": 6382, "pr_title": "Compatibility test for segment operations upload and delete", "pr_createdAt": "2020-12-24T23:09:25Z", "pr_url": "https://github.com/apache/pinot/pull/6382", "timeline": [{"oid": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "url": "https://github.com/apache/pinot/commit/5eca9cc3fdf43232eef070575d23444bc5dbd340", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2020-12-24T22:34:40Z", "type": "commit"}, {"oid": "e16354ab71abd522b8a1bee49aa1ee00ce153143", "url": "https://github.com/apache/pinot/commit/e16354ab71abd522b8a1bee49aa1ee00ce153143", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2020-12-24T22:47:46Z", "type": "commit"}, {"oid": "6aa246fa7eafa2319c5f9a32d3495a7a8b75f72f", "url": "https://github.com/apache/pinot/commit/6aa246fa7eafa2319c5f9a32d3495a7a8b75f72f", "message": "Merge branch 'compat-test' of https://github.com/amarnathkarthik/incubator-pinot into compat-test", "committedDate": "2020-12-24T23:14:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548763761", "bodyText": "It may be better to also add a deleteOnExit to this file handle. Just in case someone starts the test suite and kills it mid-way?", "author": "mcvsubbu", "createdAt": "2020-12-24T23:39:41Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());", "originalCommit": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NjMyMA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548766320", "bodyText": "In case, if killed mid-way, the next execution will create a new file since the file name is appended with UUID, therefore it should not impact the test. IMO, the general convention when using FileUtils.getTempDirectory() would be that file/directory will be deleted on exit, hence I don't think its required and also file name with prefix pinot-compat-test should provide the context in the case file are listed using OS file system.", "author": "amarnathkarthik", "createdAt": "2020-12-25T00:10:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2ODk0OQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548768949", "bodyText": "I am not worried about the next test using the same dir. I am worried about the cruft left behind. If you can confirm that it is indeed deleted, that will be good. Otherwise, please add a deleteOnExit(). Thanks.", "author": "mcvsubbu", "createdAt": "2020-12-25T00:39:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc3ODE1Nw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548778157", "bodyText": "Fixed.", "author": "amarnathkarthik", "createdAt": "2020-12-25T02:01:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjM4OQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042389", "bodyText": "Thanks", "author": "mcvsubbu", "createdAt": "2020-12-26T23:31:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzc2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzg2Nw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548763867", "bodyText": "Why do we have this? Wy not just use _segmentName?", "author": "mcvsubbu", "createdAt": "2020-12-24T23:41:31Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());\n+      if (onlineSegmentCount.getFirst() <= 0 && onlineSegmentCount.getSecond() <= 0) {\n+        LOGGER.error(\"Uploaded segment {} not found or not in {} state.\", _segmentName, STATE_ONLINE);\n+        return false;\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    String segmentName = driver.getSegmentName();", "originalCommit": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NTYxNQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548765615", "bodyText": "Both should be the same since the segment name is passed when generating the segment but above one should give the actual segment name used when generating. In the case of segment naming logic change to use either sequence id or min/max value, this implementation would always return the correct segment name.", "author": "amarnathkarthik", "createdAt": "2020-12-25T00:02:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzg2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2OTAxOA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548769018", "bodyText": "If the segment name is not the same as what we provided, there is a bug, and you can return false.  We will use this same segment name later on the delete command, and it better work (instead of being some other segment name :)", "author": "mcvsubbu", "createdAt": "2020-12-25T00:40:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzg2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc3ODE4OQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548778189", "bodyText": "Fixed.", "author": "amarnathkarthik", "createdAt": "2020-12-25T02:01:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2Mzg2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764349", "bodyText": "Since it takes some time for the segment to  make it to the external view, it is best to put this in a while loop. I suggest:\nwhile (segmentNotOnline()) { sleep(100ms) if (it took more than max time) { return false } }\nMax time can be hardcoded as 30s.", "author": "mcvsubbu", "createdAt": "2020-12-24T23:46:53Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());", "originalCommit": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDUyMQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764521", "bodyText": "Also, in general we may have more than one replica of the segment, so it may be better to parse the external view for all replicas. True, we will be having only one replica to start with, but I would like to be able to extend the tests  along that dimension if needed.", "author": "mcvsubbu", "createdAt": "2020-12-24T23:48:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NTEwMA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548765100", "bodyText": "Yes, upload Segment has a delay to show up in external view API, instead of sleep already added scheduler delay in getTableExternalView() method. For now, added 5 seconds which I think would be good enough for the compatibility test.", "author": "amarnathkarthik", "createdAt": "2020-12-24T23:56:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2OTA4Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548769083", "bodyText": "I want to avoid the case where an overloaded system has intermittent failures. Let us code it so that we are covered for a reasonably high delay, but will work fast if it is done sooner.", "author": "mcvsubbu", "createdAt": "2020-12-25T00:41:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc3ODIzOA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548778238", "bodyText": "Added loop for verification.", "author": "amarnathkarthik", "createdAt": "2020-12-25T02:02:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDU1Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548764553", "bodyText": "Same comment about looping with sleep and handling multiple replicas.", "author": "mcvsubbu", "createdAt": "2020-12-24T23:49:25Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,175 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());\n+      if (onlineSegmentCount.getFirst() <= 0 && onlineSegmentCount.getSecond() <= 0) {\n+        LOGGER.error(\"Uploaded segment {} not found or not in {} state.\", _segmentName, STATE_ONLINE);\n+        return false;\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    String segmentName = driver.getSegmentName();\n+    File indexDir = new File(outputDir, segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+\n+      Pair<Long, Long> onlineSegmentCount = getOnlineSegmentCount(getTableExternalView());", "originalCommit": "5eca9cc3fdf43232eef070575d23444bc5dbd340", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NTY2Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548765663", "bodyText": "added scheduler delay in getTableExternalView() method. For now, added 5 seconds which I think would be good enough for the compatibility test.", "author": "amarnathkarthik", "createdAt": "2020-12-25T00:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc3ODI0NQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r548778245", "bodyText": "Added loop for verification.", "author": "amarnathkarthik", "createdAt": "2020-12-25T02:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc2NDU1Mw=="}], "type": "inlineReview"}, {"oid": "ab6751039723203cff10196c75d43bcc0f16f9ed", "url": "https://github.com/apache/pinot/commit/ab6751039723203cff10196c75d43bcc0f16f9ed", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2020-12-25T02:00:57Z", "type": "commit"}, {"oid": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "url": "https://github.com/apache/pinot/commit/d61e79d2deeac63d658fc393fb0ce4fe22439301", "message": "Trigger build", "committedDate": "2020-12-25T04:24:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042602", "bodyText": "We cannot assume that this is the last segment to be deleted.\nBetter to call a method called verifySegmentdeleted(),  and have that method fetch the externalView and make sure the segment is not there in it.", "author": "mcvsubbu", "createdAt": "2020-12-26T23:34:15Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,172 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() <= 0) {\n+        if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+          LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+              DEFAULT_MAX_SLEEP_TIME_MS);\n+          return false;\n+        }\n+        LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_WAIT_TIME_MS);\n+        Thread.sleep(DEFAULT_WAIT_TIME_MS);\n+      }\n+      LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, STATE_ONLINE);\n+\n+      return true;\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() > 0) {", "originalCommit": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0Nzg0Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549047843", "bodyText": "do agree for better code readability moving the verification code to a separate method makes sense, but did not quite understand We cannot assume that this is the last segment to be deleted. SegmentOp implemented based on your design that it will be called multiple times during upgrade/downgrade, but whenever it's called it will be for 1 segment. Let me know if my understanding is not correct.\nAlso, Segment in any of these states ONLINE, CONSUMING, ERROR, OFFLINE is considered not deleted?", "author": "amarnathkarthik", "createdAt": "2020-12-27T00:47:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE1NjU5MQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549156591", "bodyText": "Valid question.\nSo, assuming that we create a table and add a segment each between phases of upgrades (there are six) and delete some of them in some phases. So, we will start with 0 segments, and maybe end with 2 or 3, while going up to 6.\nIn this case, a count of online segments can be anythning.\nSo,  all we want to make sure is that the given segment (segment name) is in the state we want it to be. If deleted, then we want to make sure that it disappeared from externalview.\nA segment is deleted when it goes away from externalview. If it is present, it better not be in ERROR state (unless we intend that). For now, let us just implement plain old ADD and DELETE operations and check for ONLINE state in the case  of adding a segment, and for not being there in the case of a delete.\nSo, the best way seems to be to get the externalview, parse it into a json object, and look for specific fields.\nAnd while we are there, might as well account for all replicas being in the same state rather than just one replica. If we add test cases involving replicas this will be one less thing to take care of.", "author": "mcvsubbu", "createdAt": "2020-12-27T19:45:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE2MzM2OQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549163369", "bodyText": "Thanks for the clarification, as suggested implement a method to get an external view if the tables and 2 other methods (1) to get the count for segment name in a specific state, and (2) to get the count of the segment name irrespective of the state (for delete verification)", "author": "amarnathkarthik", "createdAt": "2020-12-27T21:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjgzOQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042839", "bodyText": "we can't assume that this is the first segment that we upload. The tests will upload segments to the same table across each upgrade/downgrade. Better to call a method called verifySegmentInState(\"ONLINE\") and have that method fetch the externalview and make sure that the segment is in the required state. We can then use the same method for realtime table and consuming segments also, if needed. (As in, `verifySegmentInState(\"CONSUMING\")). Thanks.", "author": "mcvsubbu", "createdAt": "2020-12-26T23:36:46Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,172 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, and upload the files to controller.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+\n+      long startTime = System.currentTimeMillis();\n+      while (getOnlineSegmentCount() <= 0) {", "originalCommit": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0ODc3NQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549048775", "bodyText": "Same as above will move to a separate method but need clarification on the state. Call to external view returns 2 table views which are OFFLINE and REALTIME, is there a specific state for each of the table view to consider it to ONLINE?\nCan OFFLINE or REALTIME table view have any of these states ONLINE, CONSUMING, and is it considered as ONLINE?", "author": "amarnathkarthik", "createdAt": "2020-12-27T01:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE2MzM5Mg==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549163392", "bodyText": "Fixed as slated above", "author": "amarnathkarthik", "createdAt": "2020-12-27T21:02:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0MjgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA0Mjg3Mg==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549042872", "bodyText": "Thanks for the fix", "author": "mcvsubbu", "createdAt": "2020-12-26T23:37:17Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/TableOp.java", "diffHunk": "@@ -104,9 +103,12 @@ boolean runOp() {\n \n   private boolean createSchema() {\n     try {\n-      ControllerTest.sendPostRequest(\n-          ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forSchemaCreate(),\n-          FileUtils.readFileToString(new File(_schemaFileName)));\n+      Map<String, String> headers = new HashMap<String, String>() {{\n+        put(\"Content-type\", \"application/json\");\n+      }};\n+      ControllerTest\n+          .sendPostRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forSchemaCreate(),", "originalCommit": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE1NjA2MQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549156061", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private static final int DEFAULT_WAIT_TIME_MS = 5000;\n          \n          \n            \n              private static final int DEFAULT_SLEEP_INTERVAL_MS = 200;", "author": "mcvsubbu", "createdAt": "2020-12-27T19:39:35Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";\n+  private static final int DEFAULT_MAX_SLEEP_TIME_MS = 30000;\n+  private static final int DEFAULT_WAIT_TIME_MS = 5000;", "originalCommit": "d61e79d2deeac63d658fc393fb0ce4fe22439301", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE2MzM5OA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r549163398", "bodyText": "Fixed.", "author": "amarnathkarthik", "createdAt": "2020-12-27T21:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTE1NjA2MQ=="}], "type": "inlineReview"}, {"oid": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "url": "https://github.com/apache/pinot/commit/bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2020-12-27T20:59:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjE1Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551492153", "bodyText": "Please pick this up from CommonConstants", "author": "mcvsubbu", "createdAt": "2021-01-04T18:34:14Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";", "originalCommit": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUwNjUyMQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551506521", "bodyText": "Sure.", "author": "amarnathkarthik", "createdAt": "2021-01-04T19:01:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjE1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyMTkwNA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551621904", "bodyText": "Fixed", "author": "amarnathkarthik", "createdAt": "2021-01-04T23:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjQ2MA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551492460", "bodyText": "nit:  I prefer one per line for readability", "author": "mcvsubbu", "createdAt": "2021-01-04T18:34:53Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -36,15 +57,23 @@\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class SegmentOp extends BaseOp {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentOp.class);\n+  private static final FileFormat DEFAULT_FILE_FORMAT = FileFormat.CSV;\n+  private static final String STATE_ONLINE = \"ONLINE\";\n+  private static final int DEFAULT_MAX_SLEEP_TIME_MS = 30000;\n+  private static final int DEFAULT_SLEEP_INTERVAL_MS = 200;\n+\n   public enum Op {\n-    UPLOAD,\n-    DELETE\n+    UPLOAD, DELETE", "originalCommit": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUwNDEwNQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551504105", "bodyText": "Intellij pinot default code formatter moves to a single line may have to consider making changes to codestyle-intellij.xml to take care of this issue.", "author": "amarnathkarthik", "createdAt": "2021-01-04T18:57:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5MjQ2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551494957", "bodyText": "The code here will work, but seems to be implying that a segment can exist in both realtime and offline tables with the same name.\nYou can choose to just consider the offline side for now. Let us add the realtime side later.", "author": "mcvsubbu", "createdAt": "2021-01-04T18:39:38Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,220 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(STATE_ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getSegmentCountInState(state) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n+    return true;\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for both OFFLINE and REALTIME which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return count for OFFLINE and REALTIME segments.\n+   */\n+  private long getSegmentCountInState(String state)", "originalCommit": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUwNjM0Mg==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551506342", "bodyText": "Implementation checks for the segment in external view in both offline and realtime. Any specific reason why REALTIME should not be considered for this compatibility test? Aren't we planning to ingest the data from Kafka for these use cases?", "author": "amarnathkarthik", "createdAt": "2021-01-04T19:01:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUwOTEyMQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551509121", "bodyText": "Realtime segments are not pushed in this manner. The are created by Pinot and pushed into deep store.\nYou have two choices:\n(1) Pick up the table config and check that the table type is OFFLINE. If not, fail the operation. If it is OFFLINE, then get the offline externalview. This is the more rigorous way of doing it.\n(2) Assume that the operation is specified correctly, and just check the offline side.\nWhen we start adding realtime events and segments get generated, we will move this code to wherever it fits so that we can re-use the method to check for realtime segments being ONLINE or CONSUMING state.", "author": "mcvsubbu", "createdAt": "2021-01-04T19:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUxOTg3Nw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551519877", "bodyText": "ok. Will do with option 2 for now.", "author": "amarnathkarthik", "createdAt": "2021-01-04T19:27:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyMTk0NQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551621945", "bodyText": "Fixed", "author": "amarnathkarthik", "createdAt": "2021-01-04T23:08:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NDk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NjcxNg==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551496716", "bodyText": "segment names are case sensitive. Please do not ignore case.\nAlso, if there are two replicas, and one of them is ONLINE and the other is not, how will this work?\nInstead of counting the number, can we get the state of all replicas of the segment and return true ONLY if they are in the intended state?", "author": "mcvsubbu", "createdAt": "2021-01-04T18:43:06Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +103,220 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n     }\n     return true;\n   }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(STATE_ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n+    }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getSegmentCountInState(state) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n+    return true;\n+  }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for both OFFLINE and REALTIME which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return count for OFFLINE and REALTIME segments.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    long offlineSegmentCount =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equalsIgnoreCase(_segmentName)).filter(v -> v.getValue().values().contains(state))", "originalCommit": "bb5c23fbcfa0f67afc495b829c4bb59503fe0038", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUxNjU4Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551516583", "bodyText": "Considering the below sample table's external view, where replicas are 2, the expectation is both Server_hostname1_8001 and Server_hostname2_8001 should in ONLINE else if its should be derived segment is OFFLINE?\nSample Table External view:\n{\n  \"id\" : \"table_OFFLINE\",\n  \"simpleFields\" : {\n    \"BUCKET_SIZE\" : \"0\",\n    \"INSTANCE_GROUP_TAG\" : \"table_OFFLINE\",\n    \"MAX_PARTITIONS_PER_INSTANCE\" : \"1\",\n    \"NUM_PARTITIONS\" : \"1\",\n    \"REBALANCE_MODE\" : \"CUSTOMIZED\",\n    \"REPLICAS\" : \"2\"\n  },\n  \"mapFields\" : {\n    \"account_summary_additive_daily_0\" : {\n      \"Server_hostname1_8001\" : \"ONLINE\",\n      \"Server_hostname2_8001\" : \"ONLINE\"\n    }\n  },\n  \"listFields\" : {\n  }\n}", "author": "amarnathkarthik", "createdAt": "2021-01-04T19:21:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NjcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTUzMDUzMw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551530533", "bodyText": "Yes, the expectation is that all replicas reach ONLINE. Some replicas may take more time to get there than others, so we should wait until all replicas reach ONLINE state. If any replica reaches ERROR state, we can bail out early with a failure.", "author": "mcvsubbu", "createdAt": "2021-01-04T19:47:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NjcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyMTk4MA==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551621980", "bodyText": "Implemented change to return failure immediately if any of the segments in ERROR state else will check if all the segments are not in OFFLINE and match the one we intend to be.", "author": "amarnathkarthik", "createdAt": "2021-01-04T23:08:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ5NjcxNg=="}], "type": "inlineReview"}, {"oid": "d48a442b8c3951872987bdd3bd6e32f8b6c5c859", "url": "https://github.com/apache/pinot/commit/d48a442b8c3951872987bdd3bd6e32f8b6c5c859", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2021-01-04T23:07:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyOTAxMQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551629011", "bodyText": "Isnt it better to extract the specific segment instance state from the externalview (helix APIs) and explicitly check for specific state we want? Then we can apply this method to any state we like -- CONSUMING or ONLINE or OFFLINE etc.\nSo,\n\nread externalview into json\nExtract the key mapFields (a map)\nExtract the key _segmentName (another map)\nCheck all values to be the same as desired state\n\nThis way, we can re-use the method to check for any state we like (which we will in the future).\nthanks", "author": "mcvsubbu", "createdAt": "2021-01-04T23:28:48Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,221 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(CommonConstants.Helix.StateModel.SegmentStateModel.ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n     }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    long segmentCount;\n+    while ((segmentCount = getSegmentCountInState(state)) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      } else if (segmentCount == -1) {\n+        LOGGER.error(\"Upload segment verification failed, one or more segment(s) is in {} state.\",\n+            CommonConstants.Helix.StateModel.SegmentStateModel.ERROR);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n     return true;\n   }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for OFFLINE which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return -1 in case of ERROR, 0 if in OFFLINE state else return count of state matching parameter.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    final Set<String> segmentState =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equals(_segmentName)).flatMap(x -> x.getValue().values().stream())\n+            .collect(Collectors.toSet()) : Collections.emptySet();\n+\n+    if (segmentState.contains(CommonConstants.Helix.StateModel.SegmentStateModel.ERROR)) {", "originalCommit": "d48a442b8c3951872987bdd3bd6e32f8b6c5c859", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0OTc2NQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551649765", "bodyText": "Thanks for the clarification. Made changes to return -1 in case of ERROR, 1 if all matches the desired state and 0 otherwise.", "author": "amarnathkarthik", "createdAt": "2021-01-05T00:33:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyOTAxMQ=="}], "type": "inlineReview"}, {"oid": "d658cc9569e2bc50000850f695d29f247a0c63ec", "url": "https://github.com/apache/pinot/commit/d658cc9569e2bc50000850f695d29f247a0c63ec", "message": "Compatibility test for segment operations upload and delete", "committedDate": "2021-01-05T00:31:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1OTM0Mw==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551659343", "bodyText": "Better if this is a boolean return?", "author": "mcvsubbu", "createdAt": "2021-01-05T01:06:53Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/compat/tests/SegmentOp.java", "diffHunk": "@@ -82,14 +107,219 @@ public void setTableConfigFileName(String tableConfigFileName) {\n     _tableConfigFileName = tableConfigFileName;\n   }\n \n+  public void setSchemaFileName(String schemaFileName) {\n+    _schemaFileName = schemaFileName;\n+  }\n+\n+  public String getSchemaFileName() {\n+    return _schemaFileName;\n+  }\n+\n+  public void setRecordReaderConfigFileName(String recordReaderConfigFileName) {\n+    _recordReaderConfigFileName = recordReaderConfigFileName;\n+  }\n+\n+  public String getRecordReaderConfigFileName() {\n+    return _recordReaderConfigFileName;\n+  }\n+\n+  public void setSegmentName(String segmentName) {\n+    _segmentName = segmentName;\n+  }\n+\n+  public String getSegmentName() {\n+    return _segmentName;\n+  }\n+\n   @Override\n   boolean runOp() {\n-    switch(_op) {\n+    switch (_op) {\n       case UPLOAD:\n-        System.out.println(\"Generating segment \" + _segmentName + \" from \" + _inputDataFileName + \" and uploading to \" +\n-            _tableConfigFileName);\n+        return createAndUploadSegments();\n       case DELETE:\n+        return deleteSegment();\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Create Segment file, compress to TarGz, upload the files to controller and verify segment upload.\n+   * @return true if all successful, false in case of failure.\n+   */\n+  private boolean createAndUploadSegments() {\n+    File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-compat-test-\" + UUID.randomUUID());\n+    localTempDir.deleteOnExit();\n+    File localOutputTempDir = new File(localTempDir, \"output\");\n+    try {\n+      FileUtils.forceMkdir(localOutputTempDir);\n+      File segmentTarFile = generateSegment(localOutputTempDir);\n+      uploadSegment(segmentTarFile);\n+      return verifySegmentInState(CommonConstants.Helix.StateModel.SegmentStateModel.ONLINE);\n+    } catch (Exception e) {\n+      LOGGER.error(\"Failed to create and upload segment for input data file {}.\", _inputDataFileName, e);\n+      return false;\n+    } finally {\n+      FileUtils.deleteQuietly(localTempDir);\n     }\n+  }\n+\n+  /**\n+   * Generate the Segment(s) and then compress to TarGz file. Supports generation of segment files for one input data\n+   * file.\n+   * @param outputDir to generate the Segment file(s).\n+   * @return File object of the TarGz compressed segment file.\n+   * @throws Exception while generating segment files and/or compressing to TarGz.\n+   */\n+  private File generateSegment(File outputDir)\n+      throws Exception {\n+    TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+    _tableName = tableConfig.getTableName();\n+\n+    Schema schema = JsonUtils.fileToObject(new File(_schemaFileName), Schema.class);\n+    RecordReaderConfig recordReaderConfig =\n+        RecordReaderFactory.getRecordReaderConfig(DEFAULT_FILE_FORMAT, _recordReaderConfigFileName);\n+\n+    SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, schema);\n+    segmentGeneratorConfig.setInputFilePath(_inputDataFileName);\n+    segmentGeneratorConfig.setFormat(DEFAULT_FILE_FORMAT);\n+    segmentGeneratorConfig.setOutDir(outputDir.getAbsolutePath());\n+    segmentGeneratorConfig.setReaderConfig(recordReaderConfig);\n+    segmentGeneratorConfig.setTableName(_tableName);\n+    segmentGeneratorConfig.setSegmentName(_segmentName);\n+\n+    SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();\n+    driver.init(segmentGeneratorConfig);\n+    driver.build();\n+    File indexDir = new File(outputDir, _segmentName);\n+    LOGGER.info(\"Successfully created segment: {} at directory: {}\", _segmentName, indexDir);\n+    File segmentTarFile = new File(outputDir, _segmentName + TarGzCompressionUtils.TAR_GZ_FILE_EXTENSION);\n+    TarGzCompressionUtils.createTarGzFile(indexDir, segmentTarFile);\n+    LOGGER.info(\"Tarring segment from: {} to: {}\", indexDir, segmentTarFile);\n+\n+    return segmentTarFile;\n+  }\n+\n+  /**\n+   * Upload the TarGz Segment file to the controller.\n+   * @param segmentTarFile TarGz Segment file\n+   * @throws Exception when upload segment fails.\n+   */\n+  private void uploadSegment(File segmentTarFile)\n+      throws Exception {\n+    URI controllerURI = FileUploadDownloadClient.getUploadSegmentURI(new URI(ClusterDescriptor.CONTROLLER_URL));\n+    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {\n+      fileUploadDownloadClient.uploadSegment(controllerURI, segmentTarFile.getName(), segmentTarFile, _tableName);\n+    }\n+  }\n+\n+  /**\n+   * Verify given table and segment name in the controller are in the state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return true if segment is in the state provided in the parameter, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentInState(String state)\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    long segmentCount;\n+    while ((segmentCount = getSegmentCountInState(state)) <= 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Upload segment verification failed, count is zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      } else if (segmentCount == -1) {\n+        LOGGER.error(\"Upload segment verification failed, one or more segment(s) is in {} state.\",\n+            CommonConstants.Helix.StateModel.SegmentStateModel.ERROR);\n+        return false;\n+      }\n+      LOGGER.warn(\"Upload segment verification count is zero, will retry after {} ms.\", DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully verified segment {} and its current status is {}.\", _segmentName, state);\n     return true;\n   }\n+\n+  /**\n+   * Deletes the segment for the given segment name and table name.\n+   * @return true if delete successful, else false.\n+   */\n+  private boolean deleteSegment() {\n+    try {\n+      TableConfig tableConfig = JsonUtils.fileToObject(new File(_tableConfigFileName), TableConfig.class);\n+      _tableName = tableConfig.getTableName();\n+\n+      ControllerTest.sendDeleteRequest(ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL)\n+          .forSegmentDelete(_tableName, _segmentName));\n+      return verifySegmentDeleted();\n+    } catch (Exception e) {\n+      LOGGER.error(\"Request to delete the segment {} for the table {} failed.\", _segmentName, _tableName, e);\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Verify given table name and segment name deleted from the controller.\n+   * @return true if no segment found, else false.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  private boolean verifySegmentDeleted()\n+      throws IOException, InterruptedException {\n+    long startTime = System.currentTimeMillis();\n+    while (getCountForSegmentName() > 0) {\n+      if ((System.currentTimeMillis() - startTime) > DEFAULT_MAX_SLEEP_TIME_MS) {\n+        LOGGER.error(\"Delete segment verification failed, count is greater than zero after max wait time {} ms.\",\n+            DEFAULT_MAX_SLEEP_TIME_MS);\n+        return false;\n+      }\n+      LOGGER.warn(\"Delete segment verification count greater than zero, will retry after {} ms.\",\n+          DEFAULT_SLEEP_INTERVAL_MS);\n+      Thread.sleep(DEFAULT_SLEEP_INTERVAL_MS);\n+    }\n+\n+    LOGGER.info(\"Successfully delete the segment {} for the table {}.\", _segmentName, _tableName);\n+    return true;\n+  }\n+\n+  /**\n+   * Retrieve external view for the given table name.\n+   * @return TableViews.TableView of OFFLINE and REALTIME segments.\n+   */\n+  private TableViews.TableView getExternalViewForTable()\n+      throws IOException {\n+    return JsonUtils.stringToObject(ControllerTest.sendGetRequest(\n+        ControllerRequestURLBuilder.baseUrl(ClusterDescriptor.CONTROLLER_URL).forTableExternalView(_tableName)),\n+        TableViews.TableView.class);\n+  }\n+\n+  /**\n+   * Retrieve the number of segments for OFFLINE which are in state matching the parameter.\n+   * @param state of the segment to be verified in the controller.\n+   * @return -1 in case of ERROR, 1 if all matches the state else 0.\n+   */\n+  private long getSegmentCountInState(String state)\n+      throws IOException {\n+    final Set<String> segmentState =\n+        getExternalViewForTable().offline != null ? getExternalViewForTable().offline.entrySet().stream()\n+            .filter(k -> k.getKey().equals(_segmentName)).flatMap(x -> x.getValue().values().stream())\n+            .collect(Collectors.toSet()) : Collections.emptySet();\n+\n+    if (segmentState.contains(CommonConstants.Helix.StateModel.SegmentStateModel.ERROR)) {\n+      return -1;\n+    }\n+\n+    return segmentState.stream().allMatch(x -> x.contains(state)) ? 1 : 0;", "originalCommit": "d658cc9569e2bc50000850f695d29f247a0c63ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY2MzM5NQ==", "url": "https://github.com/apache/pinot/pull/6382#discussion_r551663395", "bodyText": "My bad. Yes, you need an integer return, since we are also handling ERROR.\nThanks,", "author": "mcvsubbu", "createdAt": "2021-01-05T01:22:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1OTM0Mw=="}], "type": "inlineReview"}]}