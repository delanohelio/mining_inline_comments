{"pr_number": 5787, "pr_title": "[Feature] - Spark Pinot Connector", "pr_createdAt": "2020-08-01T12:43:37Z", "pr_url": "https://github.com/apache/pinot/pull/5787", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTgxNQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463981815", "bodyText": "will be good to move all the versions to the properties", "author": "kishoreg", "createdAt": "2020-08-01T17:18:05Z", "path": "pinot-connectors/pinot-spark-connector/pom.xml", "diffHunk": "@@ -0,0 +1,246 @@\n+<?xml version=\"1.0\"?>\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+    <parent>\n+        <artifactId>pinot-connectors</artifactId>\n+        <groupId>org.apache.pinot</groupId>\n+        <version>${revision}${sha1}</version>\n+        <relativePath>..</relativePath>\n+    </parent>\n+    <artifactId>pinot-spark-connector</artifactId>\n+    <name>Pinot Spark Connector</name>\n+    <url>https://pinot.apache.org/</url>\n+    <properties>\n+        <pinot.root>${basedir}/../..</pinot.root>\n+        <spark.version>2.4.5</spark.version>\n+        <circe.version>0.13.0</circe.version>\n+    </properties>\n+\n+    <profiles>\n+        <profile>\n+            <id>scala-2.12</id>\n+            <activation>\n+                <activeByDefault>true</activeByDefault>\n+            </activation>\n+            <properties>\n+                <scala.version>2.12.11</scala.version>\n+                <scala.compat.version>2.12</scala.compat.version>\n+            </properties>\n+            <dependencies>\n+                <dependency>\n+                    <groupId>com.thoughtworks.paranamer</groupId>\n+                    <artifactId>paranamer</artifactId>\n+                    <version>2.8</version>", "originalCommit": "59023484ef269d2d96726247b8221e0dbc040ded", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTg0NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463981845", "bodyText": "what is V2?", "author": "kishoreg", "createdAt": "2020-08-01T17:18:37Z", "path": "pinot-connectors/pinot-spark-connector/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister", "diffHunk": "@@ -0,0 +1 @@\n+org.apache.pinot.connector.spark.datasource.PinotDataSourceV2", "originalCommit": "59023484ef269d2d96726247b8221e0dbc040ded", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MzA2OA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463983068", "bodyText": "version.\nSpark has datasource read/write api v1 and v2. V2 api has support a lot of features like push down filters, columnar read etc. For more details Spark DatasourceV2", "author": "mangrrua", "createdAt": "2020-08-01T17:33:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTg0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTk4Ng==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463981986", "bodyText": "I think we have a new Controller API that provides this as one call cc @fx19880617", "author": "kishoreg", "createdAt": "2020-08-01T17:20:13Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotClusterClient.scala", "diffHunk": "@@ -0,0 +1,208 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.net.{URI, URLEncoder}\n+import java.util.regex.Pattern\n+\n+import org.apache.pinot.connector.spark.decodeTo\n+import org.apache.pinot.connector.spark.exceptions.{HttpStatusCodeException, PinotException}\n+import org.apache.pinot.connector.spark.utils.{HttpUtils, Logging}\n+\n+import scala.util.{Failure, Success, Try}\n+import io.circe.generic.auto._\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.connector.query.GeneratedPQLs\n+import org.apache.pinot.spi.data.Schema\n+\n+/**\n+ * Client that read/write/prepare required data from/to Pinot.\n+ */\n+private[pinot] object PinotClusterClient extends Logging {\n+\n+  def getTableSchema(controllerUrl: String, tableName: String): Schema = {\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      val uri = new URI(s\"http://$controllerUrl/tables/$rawTableName/schema\")\n+      val response = HttpUtils.sendGetRequest(uri)\n+      Schema.fromString(response)\n+    } match {\n+      case Success(response) =>\n+        logDebug(s\"Pinot schema received successfully for table '$rawTableName'\")\n+        response\n+      case Failure(exception) =>\n+        throw PinotException(\n+          s\"An error occurred while getting Pinot schema for table '$rawTableName'\",\n+          exception\n+        )\n+    }\n+  }\n+\n+  /**\n+   * Get available broker urls(host:port) for given table.\n+   * This method is used when if broker instances not defined in the datasource options.\n+   */\n+  def getBrokerInstances(controllerUrl: String, tableName: String): List[String] = {\n+    val brokerPattern = Pattern.compile(\"Broker_(.*)_(\\\\d+)\")\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      val uri = new URI(s\"http://$controllerUrl/tables/$rawTableName/instances\")", "originalCommit": "59023484ef269d2d96726247b8221e0dbc040ded", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MzE1NA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463983154", "bodyText": "What is the changes? How can i use?", "author": "mangrrua", "createdAt": "2020-08-01T17:34:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MTYyOA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464071628", "bodyText": "here: #5685", "author": "xiangfu0", "createdAt": "2020-08-02T12:30:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MTk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjE1NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463982155", "bodyText": "should we move to SQL since we plan to deprecate the PQL?", "author": "kishoreg", "createdAt": "2020-08-01T17:22:18Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/query/GeneratedPQLs.scala", "diffHunk": "@@ -0,0 +1,33 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector.query\n+\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+\n+private[pinot] case class GeneratedPQLs(", "originalCommit": "59023484ef269d2d96726247b8221e0dbc040ded", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MzI4NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463983285", "bodyText": "we can convert pql to sql. i'll update supported filters and query generation for that, and use sql compiler instead of pql-compiler", "author": "mangrrua", "createdAt": "2020-08-01T17:35:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjE1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk5MDI4OQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463990289", "bodyText": "pql was changed with sql", "author": "mangrrua", "createdAt": "2020-08-01T19:02:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjI3Mg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463982272", "bodyText": "is there something special about v2 in spark? Trying to understand why we are naming PinotDataSourceV2 and not just PinotDataSource", "author": "kishoreg", "createdAt": "2020-08-01T17:24:02Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/datasource/PinotDataSourceV2.scala", "diffHunk": "@@ -0,0 +1,36 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.datasource\n+\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader\n+import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}\n+import org.apache.spark.sql.types.StructType\n+\n+class PinotDataSourceV2 extends DataSourceV2 with ReadSupport with DataSourceRegister {", "originalCommit": "59023484ef269d2d96726247b8221e0dbc040ded", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MzUyMQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r463983521", "bodyText": "version2. In spark, generally, version is added as V1 or V2 to the datasource class to learn which datasource api was used", "author": "mangrrua", "createdAt": "2020-08-01T17:39:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4ODE2NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464488165", "bodyText": "Adding docs will help, for folks not yet hands on with Scala.", "author": "mayankshriv", "createdAt": "2020-08-03T15:28:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MjI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1MzgzNw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464053837", "bodyText": "What's this file for?", "author": "xiangfu0", "createdAt": "2020-08-02T09:16:31Z", "path": "config/.scalafmt.conf", "diffHunk": "@@ -0,0 +1,17 @@\n+version = \"2.4.0\"", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTg1MA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464311850", "bodyText": "Defined rules to format scala code.\nThis file removed due to travis fail. It required maven 3.6.0 version.", "author": "mangrrua", "createdAt": "2020-08-03T09:52:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1MzgzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1NDQwMA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464054400", "bodyText": "Worth to mention that if the table is OFFLINE or REALTIME only table, user can also use tbl to query.", "author": "xiangfu0", "createdAt": "2020-08-02T09:22:12Z", "path": "pinot-connectors/pinot-spark-connector/documentation/read_model.md", "diffHunk": "@@ -0,0 +1,145 @@\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+# Read Model\n+\n+Connector can scan offline, hybrid and realtime tables. `table` parameter have to given like below;", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUyNjc4Mg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464526782", "bodyText": "Users can not specify table name without type if table not hybrid. If table hybrid, it checks routing tables of realtime and offline query separately.", "author": "mangrrua", "createdAt": "2020-08-03T16:34:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1NDQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUxMDM1OA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r467510358", "bodyText": "Got it, then please specify this in the user doc, or maybe it's better to add one more option as table type?", "author": "xiangfu0", "createdAt": "2020-08-08T22:25:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1NDQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU2Nzg0MQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r467567841", "bodyText": "Hmm, table type option would be good. Table type option will be required option with the table name. It will be more clear, thanks!\nUpdate: Implemented", "author": "mangrrua", "createdAt": "2020-08-09T10:49:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1NDQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1ODU5OA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464058598", "bodyText": "For filter push down do we support just column with filters like (columnA > 5) or we can support more like transform function on columns like (columnA > columnB, columnA * 10 > columnB)", "author": "xiangfu0", "createdAt": "2020-08-02T10:08:07Z", "path": "pinot-connectors/pinot-spark-connector/documentation/read_model.md", "diffHunk": "@@ -0,0 +1,145 @@\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+# Read Model\n+\n+Connector can scan offline, hybrid and realtime tables. `table` parameter have to given like below;\n+- For offline table `tbl_OFFLINE`\n+- For realtime table `tbl_REALTIME`\n+- For hybrid table `tbl`\n+\n+An example scan;\n+\n+```scala\n+val df = spark.read\n+      .format(\"pinot\")\n+      .option(\"table\", \"airlineStats\")\n+      .load()\n+```\n+\n+Custom schema can be specified directly. If schema is not specified, connector read table schema from Pinot controller, and then convert to the Spark schema. \n+\n+\n+### Architecture\n+\n+Connector reads data from `Pinot Servers` directly. For this operation, firstly, connector creates query with given filters(if filter push down is enabled) and columns, then finds routing table for created query. It creates pinot splits that contains **ONE PINOT SERVER and ONE OR MORE SEGMENT per spark partition**, based on the routing table and `segmentsPerSplit`(detailed explain is defined below). Lastly, each partition read data from specified pinot server in parallel.\n+\n+![Spark-Pinot Connector Architecture](images/spark-pinot-connector-executor-server-interaction.jpg)\n+\n+\n+Each Spark partition open connection with Pinot server, and read data. For example, assume that routing table informations for specified query is like that:\n+\n+```\n+- realtime ->\n+   - realtimeServer1 -> (segment1, segment2, segment3)\n+   - realtimeServer2 -> (segment4)\n+- offline ->\n+   - offlineServer10 -> (segment10, segment20)\n+```\n+\n+If `segmentsPerSplit` is equal to 3, there will be created 3 Spark partition like below;\n+\n+| Spark Partition  | Queried Pinot Server/Segments |\n+| ------------- | ------------- |\n+| partition1  | realtimeServer1 / segment1, segment2, segment3  |\n+| partition2  | realtimeServer2 / segment4  |\n+| partition3  | offlineServer10 / segment10, segment20 |\n+\n+\n+If `segmentsPerSplit` is equal to 1, there will be created 6 Spark partition;\n+\n+| Spark Partition  | Queried Pinot Server/Segments |\n+| ------------- | ------------- |\n+| partition1  | realtimeServer1 / segment1 |\n+| partition2  | realtimeServer1 / segment2  |\n+| partition3  | realtimeServer1 / segment3 |\n+| partition4  | realtimeServer2 / segment4 |\n+| partition5  | offlineServer10 / segment10 |\n+| partition6  | offlineServer10 / segment20 |\n+\n+\n+If `segmentsPerSplit` value is too low, that means more parallelism. But this also mean that a lot of connection will be opened with Pinot servers, and will increase QPS on the Pinot servers. \n+\n+If `segmetnsPerSplit` value is too high, that means less parallelism. Each Pinot server will scan more segments per request.  \n+\n+**Note:** Pinot servers prunes segments based on the segment metadata when query comes. In some cases(for example filtering based on the some columns), some servers may not return data. Therefore, some Spark partitions will be empty. In this cases, `repartition()` may be applied for efficient data analysis after loading data to Spark.\n+\n+\n+### Filter And Column Push Down\n+Connector supports filter and column push down. Filters and columns are pushed to the pinot servers. Filter and column push down improves the performance while reading data because of its minimizing data transfer between Pinot and Spark. In default, filter push down enabled. If filters are desired to be applied in Spark, `usePushDownFilters` should be set as `false`.\n+\n+Connector supports `Equal, In, LessThan, LessThanOrEqual, Greater, GreaterThan, Not, TEXT_MATCH, And, Or` filters for now.", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUyNDEwMg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464524102", "bodyText": "The filters section of the readme outdated. I've changed pql with sql, but i did forget to change supported filters section. The connector supports all sql filters now. I'll fix it", "author": "mangrrua", "createdAt": "2020-08-03T16:29:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA1ODU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA2MzEzMA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464063130", "bodyText": "new line", "author": "xiangfu0", "createdAt": "2020-08-02T10:59:28Z", "path": "pinot-connectors/pinot-spark-connector/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister", "diffHunk": "@@ -0,0 +1 @@\n+org.apache.pinot.connector.spark.datasource.PinotDataSourceV2", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA2Njg1OQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464066859", "bodyText": "pinot-spi has already defined enum: org.apache.pinot.spi.config.table.TableType\nAlso you may want to check some utils for table name: org.apache.pinot.spi.utils.builder.TableNameBuilder", "author": "xiangfu0", "createdAt": "2020-08-02T11:39:34Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/Constants.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+private[pinot] object Constants {\n+  type PinotTableType = String\n+\n+  object PinotTableTypes {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MTUzNg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464071536", "bodyText": "If you can get schema name from the table config, then you can use method getSchema(host, port, schema) in org.apache.pinot.common.utils.SchemaUtils", "author": "xiangfu0", "createdAt": "2020-08-02T12:29:50Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotClusterClient.scala", "diffHunk": "@@ -0,0 +1,208 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.net.{URI, URLEncoder}\n+import java.util.regex.Pattern\n+\n+import org.apache.pinot.connector.spark.decodeTo\n+import org.apache.pinot.connector.spark.exceptions.{HttpStatusCodeException, PinotException}\n+import org.apache.pinot.connector.spark.utils.{HttpUtils, Logging}\n+\n+import scala.util.{Failure, Success, Try}\n+import io.circe.generic.auto._\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.connector.query.GeneratedSQLs\n+import org.apache.pinot.spi.data.Schema\n+\n+/**\n+ * Client that read/write/prepare required data from/to Pinot.\n+ */\n+private[pinot] object PinotClusterClient extends Logging {\n+\n+  def getTableSchema(controllerUrl: String, tableName: String): Schema = {\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      val uri = new URI(s\"http://$controllerUrl/tables/$rawTableName/schema\")\n+      val response = HttpUtils.sendGetRequest(uri)\n+      Schema.fromString(response)", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUyMjkxMA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464522910", "bodyText": "Yes, but i guess required one more http request? is this way outdated?  @fx19880617", "author": "mangrrua", "createdAt": "2020-08-03T16:27:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MTUzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MTk0Mw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464071943", "bodyText": "Suggest to make all those URI patterns to be constant.", "author": "xiangfu0", "createdAt": "2020-08-02T12:33:56Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotClusterClient.scala", "diffHunk": "@@ -0,0 +1,208 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.net.{URI, URLEncoder}\n+import java.util.regex.Pattern\n+\n+import org.apache.pinot.connector.spark.decodeTo\n+import org.apache.pinot.connector.spark.exceptions.{HttpStatusCodeException, PinotException}\n+import org.apache.pinot.connector.spark.utils.{HttpUtils, Logging}\n+\n+import scala.util.{Failure, Success, Try}\n+import io.circe.generic.auto._\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.connector.query.GeneratedSQLs\n+import org.apache.pinot.spi.data.Schema\n+\n+/**\n+ * Client that read/write/prepare required data from/to Pinot.\n+ */\n+private[pinot] object PinotClusterClient extends Logging {\n+\n+  def getTableSchema(controllerUrl: String, tableName: String): Schema = {\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      val uri = new URI(s\"http://$controllerUrl/tables/$rawTableName/schema\")\n+      val response = HttpUtils.sendGetRequest(uri)\n+      Schema.fromString(response)\n+    } match {\n+      case Success(response) =>\n+        logDebug(s\"Pinot schema received successfully for table '$rawTableName'\")\n+        response\n+      case Failure(exception) =>\n+        throw PinotException(\n+          s\"An error occurred while getting Pinot schema for table '$rawTableName'\",\n+          exception\n+        )\n+    }\n+  }\n+\n+  /**\n+   * Get available broker urls(host:port) for given table.\n+   * This method is used when if broker instances not defined in the datasource options.\n+   */\n+  def getBrokerInstances(controllerUrl: String, tableName: String): List[String] = {\n+    val brokerPattern = Pattern.compile(\"Broker_(.*)_(\\\\d+)\")\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      val uri = new URI(s\"http://$controllerUrl/tables/$rawTableName/instances\")\n+      val response = HttpUtils.sendGetRequest(uri)\n+      val brokerUrls = decodeTo[PinotInstances](response).brokers\n+        .flatMap(_.instances)\n+        .distinct\n+        .map(brokerPattern.matcher)\n+        .filter(matcher => matcher.matches() && matcher.groupCount() == 2)\n+        .map { matcher =>\n+          val host = matcher.group(1)\n+          val port = matcher.group(2)\n+          s\"$host:$port\"\n+        }\n+\n+      if (brokerUrls.isEmpty) {\n+        throw new IllegalStateException(s\"Not found broker instance for table '$rawTableName'\")\n+      }\n+\n+      brokerUrls\n+    } match {\n+      case Success(result) =>\n+        logDebug(s\"Broker instances received successfully for table '$tableName'\")\n+        result\n+      case Failure(exception) =>\n+        throw PinotException(\n+          s\"An error occurred while getting broker instances for table '$rawTableName'\",\n+          exception\n+        )\n+    }\n+  }\n+\n+  /**\n+   * Get time boundary info of specified table.\n+   * This method is used when table is hybrid to ensure that the overlap\n+   * between realtime and offline segment data is queried exactly once.\n+   *\n+   * @return time boundary info if table exist and segments push type is 'append' or None otherwise\n+   */\n+  def getTimeBoundaryInfo(brokerUrl: String, tableName: String): Option[TimeBoundaryInfo] = {\n+    val rawTableName = PinotUtils.getRawTableName(tableName)\n+    Try {\n+      // pinot converts the given table name to the offline table name automatically\n+      val uri = new URI(s\"http://$brokerUrl/debug/timeBoundary/$rawTableName\")", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjI1OQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464072259", "bodyText": "possible to get more granular info about the executor?", "author": "xiangfu0", "createdAt": "2020-08-02T12:36:49Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotServerDataFetcher.scala", "diffHunk": "@@ -0,0 +1,134 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.util.{List => JList, Map => JMap}\n+\n+import com.yammer.metrics.core.MetricsRegistry\n+import org.apache.helix.model.InstanceConfig\n+import org.apache.pinot.common.metrics.BrokerMetrics\n+import org.apache.pinot.common.request.BrokerRequest\n+import org.apache.pinot.common.utils.DataTable\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.datasource.PinotDataSourceReadOptions\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.connector.spark.utils.Logging\n+import org.apache.pinot.core.transport.{AsyncQueryResponse, QueryRouter, ServerInstance}\n+import org.apache.pinot.sql.parsers.CalciteSqlCompiler\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * Fetch data from specified Pinot server.\n+ */\n+private[pinot] class PinotServerDataFetcher(\n+    partitionId: Int,\n+    pinotSplit: PinotSplit,\n+    dataSourceOptions: PinotDataSourceReadOptions)\n+  extends Logging {\n+  private val sqlCompiler = new CalciteSqlCompiler()\n+  private val brokerId = \"apache_spark\"", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU0NDE2NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464544165", "bodyText": "i'll look for more info about executor", "author": "mangrrua", "createdAt": "2020-08-03T17:06:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjI1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODU1NDI0Nw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r468554247", "bodyText": "I could not find any information about that for now. I'll look it in the next release", "author": "mangrrua", "createdAt": "2020-08-11T12:50:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjM3Mw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464072373", "bodyText": "Since we are using CalciteSqlCompiler here, I assume we are always using new SQL endpoint, could you change all the PQL references  to SQL?", "author": "xiangfu0", "createdAt": "2020-08-02T12:38:24Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotServerDataFetcher.scala", "diffHunk": "@@ -0,0 +1,134 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.util.{List => JList, Map => JMap}\n+\n+import com.yammer.metrics.core.MetricsRegistry\n+import org.apache.helix.model.InstanceConfig\n+import org.apache.pinot.common.metrics.BrokerMetrics\n+import org.apache.pinot.common.request.BrokerRequest\n+import org.apache.pinot.common.utils.DataTable\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.datasource.PinotDataSourceReadOptions\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.connector.spark.utils.Logging\n+import org.apache.pinot.core.transport.{AsyncQueryResponse, QueryRouter, ServerInstance}\n+import org.apache.pinot.sql.parsers.CalciteSqlCompiler\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * Fetch data from specified Pinot server.\n+ */\n+private[pinot] class PinotServerDataFetcher(\n+    partitionId: Int,\n+    pinotSplit: PinotSplit,\n+    dataSourceOptions: PinotDataSourceReadOptions)\n+  extends Logging {\n+  private val sqlCompiler = new CalciteSqlCompiler()", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNjc5Mg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464326792", "bodyText": "Yeah. I catch one thing. Connector uses broker debug/routingTable?pql API to get routing table for a query(for segment pruning). But in connector, i've changed pql to sql. I'll add sql endpoint to get routing table.", "author": "mangrrua", "createdAt": "2020-08-03T10:23:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjM3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU0MzE0Nw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464543147", "bodyText": "I've created PR related this, can you check? @fx19880617\n#5791\nI've changed routing table endpoint(in PinotClusterClient) with new endpoint in PR #5791", "author": "mangrrua", "createdAt": "2020-08-03T17:04:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjM3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjY0OA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464072648", "bodyText": "Why we need this ? Ideally it should just be inside the GeneratedSQLs? So we will send generated query to the given server for some segments.", "author": "xiangfu0", "createdAt": "2020-08-02T12:41:12Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotSplitter.scala", "diffHunk": "@@ -0,0 +1,85 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import java.util.regex.{Matcher, Pattern}\n+\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableType\n+import org.apache.pinot.connector.spark.connector.query.GeneratedSQLs\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.connector.spark.utils.Logging\n+\n+private[pinot] object PinotSplitter extends Logging {\n+  private val PINOT_SERVER_PATTERN = Pattern.compile(\"Server_(.*)_(\\\\d+)\")\n+\n+  def generatePinotSplits(\n+      generatedSQLs: GeneratedSQLs,\n+      routingTable: Map[String, Map[String, List[String]]],\n+      segmentsPerSplit: Int): List[PinotSplit] = {\n+    routingTable.flatMap {\n+      case (tableType, serversToSegments) =>\n+        serversToSegments\n+          .map { case (server, segments) => parseServerInput(server, segments) }\n+          .flatMap {\n+            case (matcher, segments) =>\n+              createPinotSplitsFromSubSplits(\n+                tableType,\n+                generatedSQLs,\n+                matcher,\n+                segments,\n+                segmentsPerSplit\n+              )\n+          }\n+    }.toList\n+  }\n+\n+  private def parseServerInput(server: String, segments: List[String]): (Matcher, List[String]) = {\n+    val matcher = PINOT_SERVER_PATTERN.matcher(server)\n+    if (matcher.matches() && matcher.groupCount() == 2) matcher -> segments\n+    else throw PinotException(s\"'$server' did not match!?\")\n+  }\n+\n+  private def createPinotSplitsFromSubSplits(\n+      tableType: PinotTableType,\n+      generatedSQLs: GeneratedSQLs,\n+      serverMatcher: Matcher,\n+      segments: List[String],\n+      segmentsPerSplit: Int): Iterator[PinotSplit] = {\n+    val serverHost = serverMatcher.group(1)\n+    val serverPort = serverMatcher.group(2)\n+    val maxSegmentCount = Math.min(segments.size, segmentsPerSplit)\n+    segments.grouped(maxSegmentCount).map { subSegments =>\n+      val serverAndSegments =\n+        PinotServerAndSegments(serverHost, serverPort, subSegments, tableType)\n+      PinotSplit(generatedSQLs, serverAndSegments)\n+    }\n+  }\n+}\n+\n+private[pinot] case class PinotSplit(\n+    generatedSQLs: GeneratedSQLs,\n+    serverAndSegments: PinotServerAndSegments)\n+\n+private[pinot] case class PinotServerAndSegments(\n+    serverHost: String,\n+    serverPort: String,\n+    segments: List[String],\n+    serverType: PinotTableType) {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxNzA3NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464317075", "bodyText": "GeneratedSQLs contains the realtime and offline queries. PinotSplit contains the generated queries with the server and segment informations. In the executor side, we are send query to the specific server. Connector checks the serverType to learn which query will be converted, and sent to the which server. Thus we need to the serverType in the current design. This is the usage;\nval pinotServerAsyncQueryResponse = pinotSplit.serverAndSegments.serverType match {\n      case TableType.REALTIME =>\n        val realtimeBrokerRequest =\n          sqlCompiler.compileToBrokerRequest(pinotSplit.generatedSQLs.realtimeSelectQuery)\n        submitRequestToPinotServer(null, null, realtimeBrokerRequest, routingTableForRequest)\n      case TableType.OFFLINE =>\n        val offlineBrokerRequest =\n          sqlCompiler.compileToBrokerRequest(pinotSplit.generatedSQLs.offlineSelectQuery)\n        submitRequestToPinotServer(offlineBrokerRequest, routingTableForRequest, null, null)\n    }", "author": "mangrrua", "createdAt": "2020-08-03T10:02:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MjY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MzA2NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464073065", "bodyText": "Try to see if you can reuse the methods in org.apache.pinot.spi.utils.builder.TableNameBuilder", "author": "xiangfu0", "createdAt": "2020-08-02T12:45:16Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotUtils.scala", "diffHunk": "@@ -0,0 +1,140 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import org.apache.pinot.common.utils.DataSchema.ColumnDataType\n+import org.apache.pinot.common.utils.DataTable\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.spi.data.{FieldSpec, Schema}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+\n+private[pinot] object PinotUtils {\n+  private val OFFLINE_TABLE_SUFFIX = s\"_${PinotTableTypes.OFFLINE}\"\n+  private val REALTIME_TABLE_SUFFIX = s\"_${PinotTableTypes.REALTIME}\"\n+\n+  /** Extract raw pinot table name. */\n+  def getRawTableName(tableName: String): String = {\n+    if (tableName.endsWith(OFFLINE_TABLE_SUFFIX)) {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MzE2OA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464073168", "bodyText": "This method is already implemented:\nTableNameBuilder.extractRawTableName()", "author": "xiangfu0", "createdAt": "2020-08-02T12:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MzA2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDA3MzIwNQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464073205", "bodyText": "TableNameBuilder .getTableTypeFromTableName()", "author": "xiangfu0", "createdAt": "2020-08-02T12:46:45Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotUtils.scala", "diffHunk": "@@ -0,0 +1,140 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import org.apache.pinot.common.utils.DataSchema.ColumnDataType\n+import org.apache.pinot.common.utils.DataTable\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.spi.data.{FieldSpec, Schema}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+\n+private[pinot] object PinotUtils {\n+  private val OFFLINE_TABLE_SUFFIX = s\"_${PinotTableTypes.OFFLINE}\"\n+  private val REALTIME_TABLE_SUFFIX = s\"_${PinotTableTypes.REALTIME}\"\n+\n+  /** Extract raw pinot table name. */\n+  def getRawTableName(tableName: String): String = {\n+    if (tableName.endsWith(OFFLINE_TABLE_SUFFIX)) {\n+      tableName.substring(0, tableName.length - OFFLINE_TABLE_SUFFIX.length)\n+    } else if (tableName.endsWith(REALTIME_TABLE_SUFFIX)) {\n+      tableName.substring(0, tableName.length - REALTIME_TABLE_SUFFIX.length)\n+    } else {\n+      tableName\n+    }\n+  }\n+\n+  /** Return offline/realtime table type, or None if table is hybrid. */\n+  def getTableType(tableName: String): Option[PinotTableType] = {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNjY5MA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464126690", "bodyText": "what about BYTES?", "author": "xiangfu0", "createdAt": "2020-08-02T21:41:16Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/PinotUtils.scala", "diffHunk": "@@ -0,0 +1,140 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import org.apache.pinot.common.utils.DataSchema.ColumnDataType\n+import org.apache.pinot.common.utils.DataTable\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+import org.apache.pinot.spi.data.{FieldSpec, Schema}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+\n+private[pinot] object PinotUtils {\n+  private val OFFLINE_TABLE_SUFFIX = s\"_${PinotTableTypes.OFFLINE}\"\n+  private val REALTIME_TABLE_SUFFIX = s\"_${PinotTableTypes.REALTIME}\"\n+\n+  /** Extract raw pinot table name. */\n+  def getRawTableName(tableName: String): String = {\n+    if (tableName.endsWith(OFFLINE_TABLE_SUFFIX)) {\n+      tableName.substring(0, tableName.length - OFFLINE_TABLE_SUFFIX.length)\n+    } else if (tableName.endsWith(REALTIME_TABLE_SUFFIX)) {\n+      tableName.substring(0, tableName.length - REALTIME_TABLE_SUFFIX.length)\n+    } else {\n+      tableName\n+    }\n+  }\n+\n+  /** Return offline/realtime table type, or None if table is hybrid. */\n+  def getTableType(tableName: String): Option[PinotTableType] = {\n+    if (tableName.endsWith(OFFLINE_TABLE_SUFFIX)) {\n+      Some(PinotTableTypes.OFFLINE)\n+    } else if (tableName.endsWith(REALTIME_TABLE_SUFFIX)) {\n+      Some(PinotTableTypes.REALTIME)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /** Convert a Pinot schema to Spark schema. */\n+  def pinotSchemaToSparkSchema(schema: Schema): StructType = {\n+    val structFields = schema.getAllFieldSpecs.asScala.map { field =>\n+      val sparkDataType = pinotDataTypeToSparkDataType(field.getDataType)\n+      if (field.isSingleValueField) {\n+        StructField(field.getName, sparkDataType)\n+      } else {\n+        StructField(field.getName, ArrayType(sparkDataType))\n+      }\n+    }\n+    StructType(structFields.toList)\n+  }\n+\n+  private def pinotDataTypeToSparkDataType(dataType: FieldSpec.DataType): DataType =\n+    dataType match {\n+      case FieldSpec.DataType.INT => IntegerType\n+      case FieldSpec.DataType.LONG => LongType\n+      case FieldSpec.DataType.FLOAT => FloatType\n+      case FieldSpec.DataType.DOUBLE => DoubleType\n+      case FieldSpec.DataType.STRING => StringType\n+      case _ =>\n+        throw PinotException(s\"Unsupported pinot data type '$dataType\")\n+    }\n+\n+  /** Convert Pinot DataTable to Seq of InternalRow */\n+  def pinotDataTableToInternalRows(\n+      dataTable: DataTable,\n+      sparkSchema: StructType): Seq[InternalRow] = {\n+    val dataTableColumnNames = dataTable.getDataSchema.getColumnNames\n+    (0 until dataTable.getNumberOfRows).map { rowIndex =>\n+      // spark schema is used to ensure columns order\n+      val columns = sparkSchema.fields.map { field =>\n+        val colIndex = dataTableColumnNames.indexOf(field.name)\n+        if (colIndex < 0) {\n+          throw PinotException(s\"'${field.name}' not found in Pinot server response\")\n+        } else {\n+          // pinot column data type can be used directly,\n+          // because all of them is supported in spark schema\n+          val columnDataType = dataTable.getDataSchema.getColumnDataType(colIndex)\n+          readPinotColumnData(dataTable, columnDataType, rowIndex, colIndex)\n+        }\n+      }\n+      InternalRow.fromSeq(columns)\n+    }\n+  }\n+\n+  private def readPinotColumnData(\n+      dataTable: DataTable,\n+      columnDataType: ColumnDataType,\n+      rowIndex: Int,\n+      colIndex: Int): Any = columnDataType match {\n+    // single column types\n+    case ColumnDataType.STRING =>", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNzA2NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464127065", "bodyText": "generateSQLs?", "author": "xiangfu0", "createdAt": "2020-08-02T21:45:41Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/query/SQLSelectionQueryGenerator.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector.query\n+\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+import org.apache.pinot.connector.spark.connector.{PinotUtils, TimeBoundaryInfo}\n+\n+/**\n+ * Generate realtime and offline SQL queries for specified table with given columns and filters.\n+ */\n+private[pinot] class SQLSelectionQueryGenerator(\n+    tableNameWithType: String,\n+    timeBoundaryInfo: Option[TimeBoundaryInfo],\n+    columns: Array[String],\n+    whereClause: Option[String]) {\n+  private val columnsExpression = columnsAsExpression()\n+  private val rawTableName = PinotUtils.getRawTableName(tableNameWithType)\n+  private val tableType = PinotUtils.getTableType(tableNameWithType)\n+\n+  def generatePQLs(): GeneratedSQLs = {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNzY3MA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464127670", "bodyText": "shall we check if there is existing LIMIT?", "author": "xiangfu0", "createdAt": "2020-08-02T21:51:05Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/connector/query/SQLSelectionQueryGenerator.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector.query\n+\n+import org.apache.pinot.connector.spark.connector.Constants.{PinotTableType, PinotTableTypes}\n+import org.apache.pinot.connector.spark.connector.{PinotUtils, TimeBoundaryInfo}\n+\n+/**\n+ * Generate realtime and offline SQL queries for specified table with given columns and filters.\n+ */\n+private[pinot] class SQLSelectionQueryGenerator(\n+    tableNameWithType: String,\n+    timeBoundaryInfo: Option[TimeBoundaryInfo],\n+    columns: Array[String],\n+    whereClause: Option[String]) {\n+  private val columnsExpression = columnsAsExpression()\n+  private val rawTableName = PinotUtils.getRawTableName(tableNameWithType)\n+  private val tableType = PinotUtils.getTableType(tableNameWithType)\n+\n+  def generatePQLs(): GeneratedSQLs = {\n+    val offlineSelectQuery = buildSelectQuery(PinotTableTypes.OFFLINE)\n+    val realtimeSelectQuery = buildSelectQuery(PinotTableTypes.REALTIME)\n+    GeneratedSQLs(\n+      rawTableName,\n+      tableType,\n+      offlineSelectQuery,\n+      realtimeSelectQuery\n+    )\n+  }\n+\n+  /**\n+   * Get all columns if selecting columns empty(eg: resultDataFrame.count())\n+   */\n+  private def columnsAsExpression(): String = {\n+    if (columns.isEmpty) \"*\" else columns.mkString(\",\")\n+  }\n+\n+  /**\n+   * Build realtime or offline PQL selection query.\n+   */\n+  private def buildSelectQuery(tableType: PinotTableType): String = {\n+    val tableNameWithType = s\"${rawTableName}_$tableType\"\n+    val queryBuilder = new StringBuilder(s\"SELECT $columnsExpression FROM $tableNameWithType\")\n+\n+    // add where clause if exists\n+    whereClause.foreach { x =>\n+      queryBuilder.append(s\" WHERE $x\")\n+    }\n+\n+    // add time boundary filter if exists\n+    timeBoundaryInfo.foreach { tbi =>\n+      val timeBoundaryFilter =\n+        if (tableType == PinotTableTypes.OFFLINE) {\n+          tbi.getOfflinePredicate\n+        } else {\n+          tbi.getRealtimePredicate\n+        }\n+\n+      if (whereClause.isEmpty) {\n+        queryBuilder.append(s\" WHERE $timeBoundaryFilter\")\n+      } else {\n+        queryBuilder.append(s\" AND $timeBoundaryFilter\")\n+      }\n+    }\n+\n+    // query will be converted to Pinot 'BrokerRequest' with PQL compiler\n+    // pinot set limit to 10 automatically\n+    // to prevent this add limit to query\n+    queryBuilder.append(s\" LIMIT ${Int.MaxValue}\")", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNDcyNQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464324725", "bodyText": "I do not know. Connector supports only selection queries to read and re-index data etc. In this case, LIMIT is unnecessary i think except one case. If we will support order by in selection query, limit can be used. But this is not good practice while working with spark.", "author": "mangrrua", "createdAt": "2020-08-03T10:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNzY3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUxMjU4NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r467512585", "bodyText": "Got it.", "author": "xiangfu0", "createdAt": "2020-08-08T22:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNzY3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3NDE1NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464474155", "bodyText": "Perhaps we should have a PinotSchema <-> SparkSchema converter?", "author": "mayankshriv", "createdAt": "2020-08-03T15:05:56Z", "path": "pinot-connectors/pinot-spark-connector/src/test/resources/schema/pinot-schema.json", "diffHunk": "@@ -0,0 +1,57 @@\n+{", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUxMzA0Mw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464513043", "bodyText": "yes", "author": "mangrrua", "createdAt": "2020-08-03T16:09:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3NDE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3NjAwMQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464476001", "bodyText": "Could we have docs on all classes?", "author": "mayankshriv", "createdAt": "2020-08-03T15:08:53Z", "path": "pinot-connectors/pinot-spark-connector/src/test/scala/org/apache/pinot/connector/spark/connector/PinotSplitterTest.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.connector\n+\n+import org.apache.pinot.connector.spark.BaseTest\n+import org.apache.pinot.connector.spark.connector.Constants.PinotTableTypes\n+import org.apache.pinot.connector.spark.connector.query.GeneratedSQLs\n+import org.apache.pinot.connector.spark.exceptions.PinotException\n+\n+class PinotSplitterTest extends BaseTest {", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3ODU1Mg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464478552", "bodyText": "Should this also go into https://docs.pinot.apache.org, after the PR is committed?", "author": "mayankshriv", "createdAt": "2020-08-03T15:13:04Z", "path": "pinot-connectors/pinot-spark-connector/documentation/read_model.md", "diffHunk": "@@ -0,0 +1,145 @@\n+<!--\n+\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+\n+-->\n+# Read Model", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4NzQ5MA==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464487490", "bodyText": "Connecting to pinot server directly leads to querying routing-table / time-boundary, which the broker does. Wondering if there is plan to connect via the broker to avoid this? It may have the following advantages:\n\nNo need to query routing-table / time-boundary, and unlike in this approach.\nFilter push down\n\nOne issue I see though, it may not be feasible to stream data out of broker with the current code. I am trying to see what the general direction/approach is with these connectors.", "author": "mayankshriv", "createdAt": "2020-08-03T15:27:18Z", "path": "pinot-connectors/pinot-spark-connector/src/main/scala/org/apache/pinot/connector/spark/datasource/PinotDataSourceReader.scala", "diffHunk": "@@ -0,0 +1,124 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.connector.spark.datasource\n+\n+import java.util.{List => JList}\n+\n+import org.apache.pinot.connector.spark.connector.query.SQLSelectionQueryGenerator\n+import org.apache.pinot.connector.spark.connector.{\n+  FilterPushDown,\n+  PinotClusterClient,\n+  PinotSplitter,\n+  PinotUtils\n+}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+import org.apache.spark.sql.sources.v2.reader.{\n+  DataSourceReader,\n+  InputPartition,\n+  SupportsPushDownFilters,\n+  SupportsPushDownRequiredColumns\n+}\n+import org.apache.spark.sql.types._\n+\n+import scala.collection.JavaConverters._\n+\n+class PinotDataSourceReader(options: DataSourceOptions, userSchema: Option[StructType] = None)\n+  extends DataSourceReader\n+  with SupportsPushDownFilters\n+  with SupportsPushDownRequiredColumns {\n+\n+  private val pinotDataSourceOptions = PinotDataSourceReadOptions.from(options)\n+  private var acceptedFilters: Array[Filter] = Array.empty\n+  private var currentSchema: StructType = _\n+\n+  override def readSchema(): StructType = {\n+    if (currentSchema == null) {\n+      currentSchema = userSchema.getOrElse {\n+        val pinotTableSchema = PinotClusterClient.getTableSchema(\n+          pinotDataSourceOptions.controller,\n+          pinotDataSourceOptions.tableName\n+        )\n+        PinotUtils.pinotSchemaToSparkSchema(pinotTableSchema)\n+      }\n+    }\n+    currentSchema\n+  }\n+\n+  override def planInputPartitions(): JList[InputPartition[InternalRow]] = {\n+    val schema = readSchema()\n+    val tableType = PinotUtils.getTableType(pinotDataSourceOptions.tableName)\n+\n+    // Time boundary is used when table is hybrid to ensure that the overlap\n+    // between realtime and offline segment data is queried exactly once\n+    val timeBoundaryInfo =\n+      if (tableType.isDefined) {\n+        None\n+      } else {\n+        PinotClusterClient.getTimeBoundaryInfo(\n+          pinotDataSourceOptions.broker,\n+          pinotDataSourceOptions.tableName\n+        )\n+      }\n+\n+    val whereCondition = FilterPushDown.compileFiltersToSqlWhereClause(this.acceptedFilters)\n+    val generatedSQLs = SQLSelectionQueryGenerator.generate(\n+      pinotDataSourceOptions.tableName,\n+      timeBoundaryInfo,\n+      schema.fieldNames,\n+      whereCondition\n+    )\n+\n+    val routingTable =\n+      PinotClusterClient.getRoutingTable(pinotDataSourceOptions.broker, generatedSQLs)", "originalCommit": "504a63b9bc3ff9affffd87a4e97dadd2a1c890e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwNDI0Mg==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r464504242", "bodyText": "Routing table is required. Spark reads data in parallel like broker. Each server-segments represented as spark partition. This is most efficient way to receive data from Pinot. Also Presto uses this architecture in pinot-connector.\nIf broker not available, connector throws exception. To avoid this, we can create a related API in controller side, and use it like metadata manager in other systems.", "author": "mangrrua", "createdAt": "2020-08-03T15:54:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4NzQ5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3NTY3Mw==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r468275673", "bodyText": "Connecting to pinot server directly leads to querying routing-table / time-boundary, which the broker does. Wondering if there is plan to connect via the broker to avoid this? It may have the following advantages:\n\nNo need to query routing-table / time-boundary, and unlike in this approach.\nFilter push down\n\nOne issue I see though, it may not be feasible to stream data out of broker with the current code. I am trying to see what the general direction/approach is with these connectors.\n\nWe are opening the gate on server with streaming api which can be used by presto for more scalable solution. This is in general the same usage pattern: make pinot data queryable in warehouse.", "author": "xiangfu0", "createdAt": "2020-08-11T01:32:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4NzQ5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODQxNzA3NQ==", "url": "https://github.com/apache/pinot/pull/5787#discussion_r568417075", "bodyText": "ok", "author": "tannghia025", "createdAt": "2021-02-02T08:43:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4NzQ5MA=="}], "type": "inlineReview"}, {"oid": "2388d968c8a0aeec8837e0dc496aa703d7b1627d", "url": "https://github.com/apache/pinot/commit/2388d968c8a0aeec8837e0dc496aa703d7b1627d", "message": "Add pinot-spark-connector", "committedDate": "2020-08-09T12:37:59Z", "type": "commit"}, {"oid": "2388d968c8a0aeec8837e0dc496aa703d7b1627d", "url": "https://github.com/apache/pinot/commit/2388d968c8a0aeec8837e0dc496aa703d7b1627d", "message": "Add pinot-spark-connector", "committedDate": "2020-08-09T12:37:59Z", "type": "forcePushed"}]}