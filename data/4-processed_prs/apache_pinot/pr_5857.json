{"pr_number": 5857, "pr_title": "[Deepstore by-pass]Add a Deepstore bypass integration test with minor bug fixes.", "pr_createdAt": "2020-08-13T06:35:42Z", "pr_url": "https://github.com/apache/pinot/pull/5857", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM0ODYyOQ==", "url": "https://github.com/apache/pinot/pull/5857#discussion_r481348629", "bodyText": "Can you use LLCSegmentName for this logic?", "author": "mcvsubbu", "createdAt": "2020-09-01T18:32:23Z", "path": "pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/PeerDownloadLLCRealtimeClusterIntegrationTest.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.integration.tests;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import org.apache.avro.reflect.Nullable;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.helix.ZNRecord;\n+import org.apache.helix.model.ExternalView;\n+import org.apache.pinot.common.utils.CommonConstants;\n+import org.apache.pinot.common.utils.helix.HelixHelper;\n+import org.apache.pinot.controller.ControllerConf;\n+import org.apache.pinot.spi.config.table.CompletionConfig;\n+import org.apache.pinot.spi.config.table.SegmentsValidationAndRetentionConfig;\n+import org.apache.pinot.spi.config.table.TableConfig;\n+import org.apache.pinot.spi.config.table.TableType;\n+import org.apache.pinot.spi.env.PinotConfiguration;\n+import org.apache.pinot.spi.filesystem.LocalPinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.utils.builder.TableConfigBuilder;\n+import org.apache.pinot.spi.utils.builder.TableNameBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.testng.Assert;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import static org.apache.pinot.controller.ControllerConf.ALLOW_HLC_TABLES;\n+import static org.apache.pinot.controller.ControllerConf.ENABLE_SPLIT_COMMIT;\n+import static org.testng.Assert.assertEquals;\n+\n+\n+/**\n+ * Integration test that extends RealtimeClusterIntegrationTest but uses low-level Kafka consumer and a fake PinotFS as\n+ * the deep store for segments. This test enables the peer to peer segment download scheme to test Pinot servers can\n+ * download segments from peer servers even the deep store is down. This is done by injection of failures in\n+ * the fake PinotFS segment upload api (i.e., copyFromLocal) for all segments whose seq number mod 5 is 0.\n+ *\n+ * Besides standard tests, it also verifies that\n+ * (1) All the segments on all servers are in either ONLINE or CONSUMING states\n+ * (2) For segments failed during deep store upload, the corresponding segment download url string is empty in Zk.\n+ */\n+public class PeerDownloadLLCRealtimeClusterIntegrationTest extends RealtimeClusterIntegrationTest {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PeerDownloadLLCRealtimeClusterIntegrationTest.class);\n+\n+  private static final String CONSUMER_DIRECTORY = \"/tmp/consumer-test\";\n+  private static final long RANDOM_SEED = System.currentTimeMillis();\n+  private static final Random RANDOM = new Random(RANDOM_SEED);\n+  private static final int NUM_SERVERS = 2;\n+  public static final int UPLOAD_FAILURE_MOD = 5;\n+\n+  private final boolean _isDirectAlloc = true; //Set as true; otherwise trigger indexing exception.\n+  private final boolean _isConsumerDirConfigured = true;\n+  private final boolean _enableSplitCommit = true;\n+  private final boolean _enableLeadControllerResource = RANDOM.nextBoolean();\n+  private static File PINOT_FS_ROOT_DIR;\n+\n+  @BeforeClass\n+  @Override\n+  public void setUp()\n+      throws Exception {\n+    System.out.println(String.format(\n+        \"Using random seed: %s, isDirectAlloc: %s, isConsumerDirConfigured: %s, enableSplitCommit: %s, enableLeadControllerResource: %s\",\n+        RANDOM_SEED, _isDirectAlloc, _isConsumerDirConfigured, _enableSplitCommit, _enableLeadControllerResource));\n+\n+    PINOT_FS_ROOT_DIR = new File(FileUtils.getTempDirectoryPath() + File.separator + System.currentTimeMillis() + \"/\");\n+    Preconditions.checkState(PINOT_FS_ROOT_DIR.mkdir(), \"Failed to make a dir for \" + PINOT_FS_ROOT_DIR.getPath());\n+\n+    // Remove the consumer directory\n+    File consumerDirectory = new File(CONSUMER_DIRECTORY);\n+    if (consumerDirectory.exists()) {\n+      FileUtils.deleteDirectory(consumerDirectory);\n+    }\n+    super.setUp();\n+  }\n+\n+\n+  @Override\n+  public void startServer() {\n+    startServers(NUM_SERVERS);\n+  }\n+\n+  @Override\n+  public void addTableConfig(TableConfig tableConfig)\n+      throws IOException {\n+    SegmentsValidationAndRetentionConfig segmentsValidationAndRetentionConfig =\n+        new SegmentsValidationAndRetentionConfig();\n+    CompletionConfig completionConfig = new CompletionConfig(\"DOWNLOAD\");\n+    segmentsValidationAndRetentionConfig.setCompletionConfig(completionConfig);\n+    segmentsValidationAndRetentionConfig.setReplicasPerPartition(String.valueOf(NUM_SERVERS));\n+    // Important: enable peer to peer download.\n+    segmentsValidationAndRetentionConfig.setPeerSegmentDownloadScheme(\"http\");\n+    tableConfig.setValidationConfig(segmentsValidationAndRetentionConfig);\n+    tableConfig.getValidationConfig().setTimeColumnName(this.getTimeColumnName());\n+\n+    sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfig.toJsonString());\n+  }\n+\n+\n+  @Override\n+  public void startController() {\n+    Map<String, Object> controllerConfig = getDefaultControllerConfiguration();\n+    controllerConfig.put(ALLOW_HLC_TABLES, false);\n+    controllerConfig.put(ENABLE_SPLIT_COMMIT, _enableSplitCommit);\n+    // Override the data dir config.\n+    controllerConfig.put(ControllerConf.DATA_DIR, \"mockfs://\" + getHelixClusterName());\n+    controllerConfig.put(ControllerConf.LOCAL_TEMP_DIR, FileUtils.getTempDirectory().getAbsolutePath());\n+    // Use the mock PinotFS as the PinotFS.\n+    controllerConfig.put(\"pinot.controller.storage.factory.class.mockfs\",\n+        \"org.apache.pinot.integration.tests.PeerDownloadLLCRealtimeClusterIntegrationTest$MockPinotFS\");\n+    startController(controllerConfig);\n+    enableResourceConfigForLeadControllerResource(_enableLeadControllerResource);\n+  }\n+\n+  @Override\n+  protected boolean useLlc() {\n+    return true;\n+  }\n+\n+  @Nullable\n+  @Override\n+  protected String getLoadMode() {\n+    return \"MMAP\";\n+  }\n+\n+  @Override\n+  protected void overrideServerConf(PinotConfiguration configuration) {\n+    configuration.setProperty(CommonConstants.Server.CONFIG_OF_REALTIME_OFFHEAP_ALLOCATION, true);\n+    configuration.setProperty(CommonConstants.Server.CONFIG_OF_REALTIME_OFFHEAP_DIRECT_ALLOCATION, _isDirectAlloc);\n+    configuration.setProperty(CommonConstants.Server.PREFIX_OF_CONFIG_OF_PINOT_FS_FACTORY + \".class.mockfs\",\n+        \"org.apache.pinot.integration.tests.PeerDownloadLLCRealtimeClusterIntegrationTest$MockPinotFS\");\n+    // Set the segment deep store uri.\n+    configuration.setProperty(\"pinot.server.instance.segment.store.uri\", \"mockfs://\" + getHelixClusterName());\n+    // For setting the HDFS segment fetcher.\n+    configuration.setProperty(CommonConstants.Server.PREFIX_OF_CONFIG_OF_SEGMENT_FETCHER_FACTORY + \".protocols\", \"file,http\");\n+    if (_isConsumerDirConfigured) {\n+      configuration.setProperty(CommonConstants.Server.CONFIG_OF_CONSUMER_DIR, CONSUMER_DIRECTORY);\n+    }\n+    if (_enableSplitCommit) {\n+      configuration.setProperty(CommonConstants.Server.CONFIG_OF_ENABLE_SPLIT_COMMIT, true);\n+      configuration.setProperty(CommonConstants.Server.CONFIG_OF_ENABLE_COMMIT_END_WITH_METADATA, true);\n+    }\n+  }\n+\n+  @Test\n+  public void testConsumerDirectoryExists() {\n+    File consumerDirectory = new File(CONSUMER_DIRECTORY, \"mytable_REALTIME\");\n+    assertEquals(consumerDirectory.exists(), _isConsumerDirConfigured,\n+        \"The off heap consumer directory does not exist\");\n+  }\n+\n+  @Test\n+  public void testSegmentFlushSize() {\n+    String zkSegmentsPath = \"/SEGMENTS/\" + TableNameBuilder.REALTIME.tableNameWithType(getTableName());\n+    List<String> segmentNames = _propertyStore.getChildNames(zkSegmentsPath, 0);\n+    for (String segmentName : segmentNames) {\n+      ZNRecord znRecord = _propertyStore.get(zkSegmentsPath + \"/\" + segmentName, null, 0);\n+      assertEquals(znRecord.getSimpleField(CommonConstants.Segment.FLUSH_THRESHOLD_SIZE),\n+          Integer.toString(getRealtimeSegmentFlushSize() / getNumKafkaPartitions()),\n+          \"Segment: \" + segmentName + \" does not have the expected flush size\");\n+    }\n+  }\n+\n+  @Test\n+  public void testSegmentDownloadURLs() {\n+    // Verify that all segments of even partition number have empty download url in zk.\n+    String zkSegmentsPath = \"/SEGMENTS/\" + TableNameBuilder.REALTIME.tableNameWithType(getTableName());\n+    List<String> segmentNames = _propertyStore.getChildNames(zkSegmentsPath, 0);\n+    for (String segmentName : segmentNames) {\n+      ZNRecord znRecord = _propertyStore.get(zkSegmentsPath + \"/\" + segmentName, null, 0);\n+      String downloadURL = znRecord.getSimpleField(\"segment.realtime.download.url\");\n+      String numberOfDoc = znRecord.getSimpleField(\"segment.total.docs\");\n+      if (numberOfDoc.equals(\"-1\")) {\n+        // This is a consuming segment so the download url is null.\n+        Assert.assertNull(downloadURL);\n+        continue;\n+      }\n+      int seqNum = Integer.parseInt(segmentName.split(\"__\")[2]);\n+      if (seqNum % UPLOAD_FAILURE_MOD == 0) {\n+        Assert.assertEquals(\"\", downloadURL);\n+      } else {\n+        Assert.assertTrue(downloadURL.startsWith(\"mockfs://\"));\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testAllSegmentsAreOnlineOrConsuming() {\n+    ExternalView externalView =\n+        HelixHelper.getExternalViewForResource(_helixAdmin, getHelixClusterName(),\n+            TableNameBuilder.REALTIME.tableNameWithType(getTableName()));\n+    Assert.assertEquals(\"2\", externalView.getReplicas());\n+    // Verify for each segment e, the state of e in its 2 hosting servers is either ONLINE or CONSUMING\n+    for(String segment : externalView.getPartitionSet()) {\n+      Map<String, String> instanceToStateMap = externalView.getStateMap(segment);\n+      Assert.assertEquals(2, instanceToStateMap.size());\n+      for (Map.Entry<String, String> instanceState : instanceToStateMap.entrySet()) {\n+        Assert.assertTrue(\"ONLINE\".equalsIgnoreCase(instanceState.getValue()) || \"CONSUMING\"\n+            .equalsIgnoreCase(instanceState.getValue()));\n+      }\n+    }\n+  }\n+\n+  @Test(expectedExceptions = IOException.class)\n+  public void testAddHLCTableShouldFail()\n+      throws IOException {\n+    TableConfig tableConfig = new TableConfigBuilder(TableType.REALTIME).setTableName(\"testTable\")\n+        .setStreamConfigs(Collections.singletonMap(\"stream.kafka.consumer.type\", \"HIGHLEVEL\")).build();\n+    sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfig.toJsonString());\n+  }\n+\n+  // MockPinotFS is a localPinotFS whose root directory is configured as _basePath;\n+  public static class MockPinotFS extends PinotFS {\n+    LocalPinotFS _localPinotFS = new LocalPinotFS();\n+    File _basePath;\n+    @Override\n+    public void init(PinotConfiguration config) {\n+      _localPinotFS.init(config);\n+      _basePath = PeerDownloadLLCRealtimeClusterIntegrationTest.PINOT_FS_ROOT_DIR;\n+    }\n+\n+    @Override\n+    public boolean mkdir(URI uri)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.mkdir(new URI(_basePath + uri.getPath()));\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public boolean delete(URI segmentUri, boolean forceDelete)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.delete(new URI(_basePath + segmentUri.getPath()), forceDelete);\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public boolean doMove(URI srcUri, URI dstUri)\n+        throws IOException {\n+      try {\n+        LOGGER.warn(\"Moving from {} to {}\", srcUri, dstUri);\n+        return _localPinotFS.doMove(new URI(_basePath + srcUri.getPath()), new URI(_basePath + dstUri.getPath()));\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public boolean copy(URI srcUri, URI dstUri)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.copy(new URI(_basePath + srcUri.getPath()), new URI(_basePath + dstUri.getPath()));\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public boolean exists(URI fileUri)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.exists(new URI(_basePath + fileUri.getPath()));\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public long length(URI fileUri)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.length(new URI(_basePath + fileUri.getPath()));\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public String[] listFiles(URI fileUri, boolean recursive)\n+        throws IOException {\n+      try {\n+        return _localPinotFS.listFiles(new URI(_basePath + fileUri.getPath()), recursive);\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e.getMessage());\n+      }\n+    }\n+\n+    @Override\n+    public void copyToLocalFile(URI srcUri, File dstFile)\n+        throws Exception {\n+      _localPinotFS.copyToLocalFile(new URI(_basePath + srcUri.getPath()), dstFile);\n+    }\n+\n+    @Override\n+    public void copyFromLocalFile(File srcFile, URI dstUri)\n+        throws Exception {\n+      // Inject failures for segments whose seq number mod 5 is 0.\n+      String[] parts = srcFile.getName().split(\"__\");\n+      if (parts.length > 3 && (Integer.parseInt(parts[2]) % UPLOAD_FAILURE_MOD == 0)) {", "originalCommit": "964942f480af58268cd1fde696f7e6eed3818535", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcxOTg3Mg==", "url": "https://github.com/apache/pinot/pull/5857#discussion_r481719872", "bodyText": "Done. Thanks for the good idea.", "author": "chenboat", "createdAt": "2020-09-02T05:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM0ODYyOQ=="}], "type": "inlineReview"}, {"oid": "f7b145666d443f5dbb9def4c7a01872eebe2cd7d", "url": "https://github.com/apache/pinot/commit/f7b145666d443f5dbb9def4c7a01872eebe2cd7d", "message": "Add a new integration test.", "committedDate": "2020-09-02T05:05:36Z", "type": "commit"}, {"oid": "0d4fcf5cc0241f2d513bfca44e377c0d584ebdcb", "url": "https://github.com/apache/pinot/commit/0d4fcf5cc0241f2d513bfca44e377c0d584ebdcb", "message": "Add an integration test for peer segment download for LLC. Serveral minor fixes.", "committedDate": "2020-09-02T05:05:36Z", "type": "commit"}, {"oid": "adf85b7ced6e031a80707a13971ab57ff1cf57d0", "url": "https://github.com/apache/pinot/commit/adf85b7ced6e031a80707a13971ab57ff1cf57d0", "message": "Fix broken unit tests and flaky integration tests.", "committedDate": "2020-09-02T05:05:36Z", "type": "commit"}, {"oid": "7d0cce697f04061cd815c481655b6076b96f90bb", "url": "https://github.com/apache/pinot/commit/7d0cce697f04061cd815c481655b6076b96f90bb", "message": "Use the LLCSegmentName to extract segment sequence number.", "committedDate": "2020-09-02T05:05:36Z", "type": "commit"}, {"oid": "7d0cce697f04061cd815c481655b6076b96f90bb", "url": "https://github.com/apache/pinot/commit/7d0cce697f04061cd815c481655b6076b96f90bb", "message": "Use the LLCSegmentName to extract segment sequence number.", "committedDate": "2020-09-02T05:05:36Z", "type": "forcePushed"}]}