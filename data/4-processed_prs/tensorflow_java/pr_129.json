{"pr_number": 129, "pr_title": "Add Losses", "pr_createdAt": "2020-10-08T22:57:20Z", "pr_url": "https://github.com/tensorflow/java/pull/129", "timeline": [{"oid": "c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "url": "https://github.com/tensorflow/java/commit/c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "message": "Merge pull request #3 from tensorflow/master\n\nSync with master tensorflow on upstream", "committedDate": "2020-10-08T17:19:37Z", "type": "commit"}, {"oid": "9cc26757f102688789b58d32f18d6fd7e4941fc2", "url": "https://github.com/tensorflow/java/commit/9cc26757f102688789b58d32f18d6fd7e4941fc2", "message": "Initial checkin to rebase to Initialziers to pick up changes to ndarry Shape", "committedDate": "2020-10-08T18:07:11Z", "type": "commit"}, {"oid": "2508f5e58b59e18d3537d845491ce1e3f7afbd85", "url": "https://github.com/tensorflow/java/commit/2508f5e58b59e18d3537d845491ce1e3f7afbd85", "message": "Initial Checkin for losses", "committedDate": "2020-10-08T18:07:11Z", "type": "commit"}, {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "url": "https://github.com/tensorflow/java/commit/17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "message": "Fix reshape in sparseCategoricalCrossentropy()", "committedDate": "2020-10-08T22:25:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NjE5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502686196", "bodyText": "perhaps \"of the predictions and result\"?", "author": "deansher", "createdAt": "2020-10-09T21:54:41Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjM0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842340", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-10-10T23:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NjE5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NzIwMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502687203", "bodyText": "The Tuple class name is uncomfortably vanilla for me. Perhaps LossTuple?", "author": "deansher", "createdAt": "2020-10-09T21:59:37Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjUzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842530", "bodyText": "This object will also be used in Metrics as many metrics are built using loss classes or Losses methods. I have changed it to LossTuple.", "author": "JimClarke5", "createdAt": "2020-10-10T23:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NzIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODAxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688018", "bodyText": "The Javadocs in this file are still partly in markdown.", "author": "deansher", "createdAt": "2020-10-09T22:03:44Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MzMyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502843327", "bodyText": "OK, I thought I caught them all, I will fix.", "author": "JimClarke5", "createdAt": "2020-10-10T23:24:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688796", "bodyText": "For this method, the returned sampleWeight is always null.", "author": "deansher", "createdAt": "2020-10-09T22:07:20Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NDQwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502784409", "bodyText": "That is not always the case when we do Metrics.", "author": "JimClarke5", "createdAt": "2020-10-10T12:15:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgyMzc5MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502823790", "bodyText": "I'm just thinking our documentation for this method might take into account that the returned sampleWeight is always null.", "author": "deansher", "createdAt": "2020-10-10T19:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MzU3OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502843579", "bodyText": "Now I see what you  are talking about. I added a comment in the @return that sampleWeight will be null for this particular method signature.", "author": "JimClarke5", "createdAt": "2020-10-10T23:27:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689064", "bodyText": "Is \"match\" the right way to describe the precondition relationship between predictions and labels?", "author": "deansher", "createdAt": "2020-10-09T22:08:36Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0NDYyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502844627", "bodyText": "It is definitely not the same Shape.  I was thinking of compatible, but that has specific meaning in Shape.isCompatibleWIth.  The description is saying the ranks must be equal or differ by one. I am not sure of one word that describes that.  match was the word used in the Python version of this method.", "author": "JimClarke5", "createdAt": "2020-10-10T23:43:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxNTYwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502915600", "bodyText": "Hmm, here's a suggestion:\n\nWe could decide what we want the convention to be, in terms of squeeze-or-expand plus maybe broadcasting.\nWrite this up carefully in the class javadoc for either Loss or Losses.\nMention that documentation in the class javadoc for every other loss class.\nAlso mention it in Loss#call.\nAnd be silent about it in the individual methods of Losses and LossesImpl.\nPerhaps?", "author": "deansher", "createdAt": "2020-10-11T13:26:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyMDQ2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502920467", "bodyText": "That said, it just occurred to me that we have another gap, and that filling that gap might help this issue.\nWe don't specify the behavior of these methods when labels and predictions don't have a permitted shape relationship. Nor do we make sure our behavior is consistent in that case.\nPerhaps we should\n\nspell out that there's an IllegalArgumentException for that in the statically-known-dimensions case,\nrename squeezeOrExpandDimensions into something like validateAndAdjustLossDimensions,\nhave that method throw IllegalArgumentException when appropriate,\nand then link to a fuller explanation in the documentation of the IllegalArgumentException?\n\nAlthough I have never been in the habit of subclassing IllegalArgumentException, I see Oracle does that sometimes. That could be an alternative way of pointing people to the fuller explanation.", "author": "deansher", "createdAt": "2020-10-11T14:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzMzc4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502933789", "bodyText": "match must mean that the shapes of the input operands are capable of being molded into the relationships defined for the result of this method. Again LossesImpl is intended to be marked as module private (JDK 11) and only should be accessible from the losses or metrics package. It is not intended to be a general use API.", "author": "JimClarke5", "createdAt": "2020-10-11T15:59:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDUxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500512", "bodyText": "We should probably note in the javadoc for the class that this is an internal implementation class and subject to change (and being locked off under the module system).", "author": "Craigacp", "createdAt": "2020-10-24T18:29:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjMzMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652331", "bodyText": "Added this comment for the LossesImpl class\n/**\n * These are helper methods for Losses and will be module private when\n * Java modularity is applied to TensorFlow Java.\n * These methods should not be used outside of the Loss package.\n */", "author": "JimClarke5", "createdAt": "2020-10-25T21:44:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTU4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689589", "bodyText": "For consistency, labelsRank.", "author": "deansher", "createdAt": "2020-10-09T22:11:07Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjc4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502926789", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-10-11T15:01:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTU4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjI2MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502776261", "bodyText": "Extraneous *", "author": "deansher", "createdAt": "2020-10-10T10:40:51Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTQxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839418", "bodyText": "Deleted", "author": "JimClarke5", "createdAt": "2020-10-10T22:33:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NjcwMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502786701", "bodyText": "Extraneous *", "author": "deansher", "createdAt": "2020-10-10T12:42:15Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTgyNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839825", "bodyText": "Removed all Extraneous @link *", "author": "JimClarke5", "createdAt": "2020-10-10T22:38:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NjcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787006", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "author": "deansher", "createdAt": "2020-10-10T12:45:30Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzMzk2OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502933968", "bodyText": "Why would someone want to pass null, when there are other CTORs that handle that condition?", "author": "JimClarke5", "createdAt": "2020-10-11T16:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1OTYyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503259628", "bodyText": "For APIs that will get enough use to be worth some polish, I tend toward carefully documenting edge cases. I don't know whether we want to invest in that now.", "author": "deansher", "createdAt": "2020-10-12T12:25:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjIxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492216", "bodyText": "I think it's worth documenting it in case users build their own losses.", "author": "Craigacp", "createdAt": "2020-10-24T16:59:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjYxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652618", "bodyText": "OK, added this to name param, if null the name will be {@link Class#getSimpleName()}.", "author": "JimClarke5", "createdAt": "2020-10-25T21:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787035", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "author": "deansher", "createdAt": "2020-10-10T12:45:46Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjY5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652698", "bodyText": "OK, added this to all  name param, if null the name will be {@link Class#getSimpleName()}.", "author": "JimClarke5", "createdAt": "2020-10-25T21:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzA4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787086", "bodyText": "Actually, there's a separate <U> for the labels.", "author": "deansher", "createdAt": "2020-10-10T12:46:27Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MTgwOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502841808", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-10T23:03:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808082", "bodyText": "How would you feel about mnemonic/indicative type names like L for labels? Or even LabelsT?", "author": "deansher", "createdAt": "2020-10-10T16:35:25Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjE3OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842178", "bodyText": "The standard convention is:\n\nE - Element (used extensively by the Java Collections Framework)\nK - Key\nN - Number\nT - Type\nV - Value\nS,U,V etc. - 2nd, 3rd, 4th types\n\nGeneric Types", "author": "JimClarke5", "createdAt": "2020-10-10T23:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI2MzU0Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503263542", "bodyText": "I don't think of that list as being especially prescriptive. In the Oracle documentation you link above, the list is introduced as \"The most commonly used type parameter names are: ...\".\nGoogle's Java style guide says:\n\nEach type variable is named in one of two styles:\n\nA single capital letter, optionally followed by a single numeral (such as E, T, X, T2)\nA name in the form used for classes (see Section 5.2.2, Class names), followed by the capital letter T (examples: RequestT, FooBarT).\n\n\nPersonally, I'd lean toward using some of our own single-letter conventions for situations that are common in our own code, including L as the labels type.", "author": "deansher", "createdAt": "2020-10-12T12:33:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjUxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492519", "bodyText": "My vote would be to stick to the Java conventions Jim described.\nI particularly dislike the Google style form where the type name is a word that isn't all caps, but I tend to find type variables that are longer than a single character tricky to read anyway.", "author": "Craigacp", "createdAt": "2020-10-24T17:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwMjg4OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521102888", "bodyText": "Personally, I'd lean toward using some of our own single-letter conventions for situations that are common in our own code, including L as the labels type.\n\nThis may be hard to follow consistently once several letters have been used e.g. 'L' might be needed for something other than label type. Seems a tad more confusing than the standard type names", "author": "KartikChugh", "createdAt": "2020-11-11T04:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNDIyMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522204222", "bodyText": "Ok, sticking with the original plan! Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODY5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808698", "bodyText": "Inconsistency between accessing the superclass's tf directly and accessing its reduction via getReduction.", "author": "deansher", "createdAt": "2020-10-10T16:42:33Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}\n+   *\n+   *\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link #REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+          Ops tf,  boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+      this(tf, null, fromLogits, labelSmoothing, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+      Ops tf, String name, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+      Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.binaryCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTUzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839536", "bodyText": "Changed to getTF()", "author": "JimClarke5", "createdAt": "2020-10-10T22:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODY5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODgwNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808807", "bodyText": "tf versus getReduction (but I'll stop mentioning these)", "author": "deansher", "createdAt": "2020-10-10T16:43:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link * Reduction#AUTO},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   * @param axis The channels axis. <code>axis=-1</code> corresponds to data format `Channels Last'\n+   *     and <code>axis=1</code> corresponds to data format 'Channels First'.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf,\n+      String name,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      Reduction reduction,\n+      int axis) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+    this.axis = axis;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+          Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.categoricalCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing, axis);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTg4NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839884", "bodyText": "Changed to getTF()", "author": "JimClarke5", "createdAt": "2020-10-10T22:39:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODgwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502809972", "bodyText": "How would you feel about a line break after the whole tf.math.abs(...), to make it easier to scan the parameters of tf.math.mean?", "author": "deansher", "createdAt": "2020-10-10T16:56:11Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjc1Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206753", "bodyText": "minor -- we'll call it Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMTM4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502811386", "bodyText": "In current invocations of these constructors, the target argument always comes from a variable called predictions.", "author": "deansher", "createdAt": "2020-10-10T17:12:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {\n+  private final Operand<T> labels;\n+  private final Operand<T> target;\n+  private final Operand<T> sampleWeights;\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param target the losses or target\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> target) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812240", "bodyText": "This is the first of these methods where we used (target, output) instead of (labels, predictions).", "author": "deansher", "createdAt": "2020-10-10T17:22:42Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjk3Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206976", "bodyText": "minor -- we'll call it Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812533", "bodyText": "sub will do broadcasting if needed. Do we feel good about applying squeezeOrExpandDimensions and then subsequent broadcasting? If so, is there a succinct description we could provide for overall treatment of dimensions?", "author": "deansher", "createdAt": "2020-10-10T17:25:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjM0OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206348", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813233", "bodyText": "Seems like we should document the required relationships between labels and predictions and the resulting transformations? (Given our use of squeezeOrExpandDimensions followed by broadcasting.)", "author": "deansher", "createdAt": "2020-10-10T17:34:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNTk5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522205999", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:44:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813406", "bodyText": "This is the first case in Losses where we haven't followed squeezeOrExpandDimensions with broadcasting. Do we want to add broadcasting here for consistency, or document the difference?", "author": "deansher", "createdAt": "2020-10-10T17:35:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzE0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207146", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815067", "bodyText": "Although the documentation of softmaxCrossEntropyWithLogits doesn't specify, I imagine it doesn't do broadcasting. So this would be another method in Losses that does squeezeOrExpandDimensions but does not broadcast.", "author": "deansher", "createdAt": "2020-10-10T17:53:59Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzMyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207327", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815385", "bodyText": "Although in this internal case of this method, we do broadcast. I'll stop commenting on this issue.", "author": "deansher", "createdAt": "2020-10-10T17:57:21Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzQ5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207496", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815866", "bodyText": "Do we want to avoid this cast in the case where labels already has the same data type?", "author": "deansher", "createdAt": "2020-10-10T18:03:36Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzNDEzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502934136", "bodyText": "I guess the question is what is the overhead of casting onto oneself vs the overhead of checking?  I would hope that tf.dtypes.cast already handles this, but I could be mistaken.", "author": "JimClarke5", "createdAt": "2020-10-11T16:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI3MTM0OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503271348", "bodyText": "The code for checking could be something like this:\n@SuppressWarnings(\"unchecked\")\nprivate static <T extends TNumber, U extends TNumber> Operand<T> castIfNecessary(\n    Operand<U> value, DataType<T> requiredType) {\n  return (value.asOutput().dataType() == requiredType) \n      ? (Operand<T>) value\n      : tf.dtypes.cast(value, requiredType);\n}\nSo the overhead of checking would be the function call plus value.asOutput().dataType() == requiredType.\nLooking at the code for tf.dtypes.cast, unless we think a cast is almost always needed, it would be cheaper to do the check to sometimes avoid it.\n  public <U extends TType, T extends TType> Cast<U> cast(Operand<T> x, DataType<U> DstT,\n      Cast.Options... options) {\n    return Cast.create(scope, x, DstT, options);\n  }\n\n  @Endpoint(describeByClass = true)\n  public static <U extends TType, T extends TType> Cast<U> create(Scope scope, Operand<T> x, DataType<U> DstT, Options... options) {\n    OperationBuilder opBuilder = scope.env().opBuilder(\"Cast\", scope.makeOpName(\"Cast\"));\n    opBuilder.addInput(x.asOutput());\n    opBuilder = scope.applyControlDependencies(opBuilder);\n    opBuilder.setAttr(\"DstT\", DstT);\n    if (options != null) {\n      for (Options opts : options) {\n        if (opts.Truncate != null) {\n          opBuilder.setAttr(\"Truncate\", opts.Truncate);\n        }\n      }\n    }\n    return new Cast<U>(opBuilder.build());\n  }", "author": "deansher", "createdAt": "2020-10-12T12:46:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Mjc2Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492766", "bodyText": "In graph construction mode the overhead is probably irrelevant because it's only called once during construction. In eager mode it could be faster as it could sidestep a JNI call in each step, but I suspect we've got other issues to get speed in eager mode.", "author": "Craigacp", "createdAt": "2020-10-24T17:05:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MzE1MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511653151", "bodyText": "I like castIfNecessary as a general util method. It would be used almost everywhere, so it would be a huge change.\nPerhaps create a new PR for castIfNecessary, then once that is merged we can start retrofitting all packages under framework.", "author": "JimClarke5", "createdAt": "2020-10-25T21:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg5MjYyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511892627", "bodyText": "In graph construction mode, an unnecessary call to cast creates an unnecessary graph operation.", "author": "deansher", "createdAt": "2020-10-26T11:31:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3NDcxNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512074714", "bodyText": "shrug it'll be a no-op most of the time and compiled away if we get XLA working. Given the relative size of the computation around it I suspect it won't be an issue.", "author": "Craigacp", "createdAt": "2020-10-26T15:59:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2Mzc0OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513863749", "bodyText": "I also vote for a explicit check in the code to avoid adding an extra operation to the graph when it is not required", "author": "karllessard", "createdAt": "2020-10-29T01:34:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMxNzY3OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r514317679", "bodyText": "OK, I will add a helper class in org.tensorflow.framework.utils, then retrofit the Loss classes.", "author": "JimClarke5", "createdAt": "2020-10-29T14:48:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMzMDA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r514330067", "bodyText": "Just a comment on @deansher proposed method here, the datatypes for <U> and <T> should not be restricted to TNumber because it is valid to cast to/from TNumber and TBool .", "author": "JimClarke5", "createdAt": "2020-10-29T15:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjE5Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502816197", "bodyText": "Can just use dataType.", "author": "deansher", "createdAt": "2020-10-10T18:06:57Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzNDE0OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502934149", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-11T16:02:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjE5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502818616", "bodyText": "I tripped over this private method having the usual naming of a loss method, since I didn't notice that it was private and so expected it to follow the conventions of public loss methods, such as invoking squeezeOrExpandDimensions.  Also (if I'm navigating accurately through unfamiliar territory), this method doesn't compute a binaryCrossentropy since it depends on its caller to compute the mean at the end.", "author": "deansher", "createdAt": "2020-10-10T18:31:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTM4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951386", "bodyText": "This method does the grunt work for the binaryCrossentropy after the operands have had their shapes and types manipulated and after smoothing the labels. Perhaps a new name would remove some of the confusion.", "author": "JimClarke5", "createdAt": "2020-10-11T18:45:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI3MTg0NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503271845", "bodyText": "Yes, I wonder if we want to call it something like binaryCrossentropyHelper?", "author": "deansher", "createdAt": "2020-10-12T12:47:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNDQ5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504134493", "bodyText": "OK, Changed", "author": "JimClarke5", "createdAt": "2020-10-13T17:30:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502819700", "bodyText": "Should we follow the Python in documenting that labels are expected to be 0 or 1?", "author": "deansher", "createdAt": "2020-10-10T18:43:05Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MDk5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502840999", "bodyText": "Yes, The Python CategporicalHinge class does not mention that at all, but it is mentioned in the categorical_hinge method.\nI have added an entry to the class JavaDoc and to the Losses.categoricalHinge method.", "author": "JimClarke5", "createdAt": "2020-10-10T22:53:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjcyODgxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512728819", "bodyText": "Actually the values can be [-1, 0, 1]. [0,1] is converted to [-1,1]. I have added a value check to make sure the values are wholly contained in the allowed values set  [-1, 0, 1]. This will either throw TFInvalidArgumentException if run in Graph mode via a control dependency, and throw IllegalArgumentException if created in Eager mode with the call method.", "author": "JimClarke5", "createdAt": "2020-10-27T14:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwMTYxMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522201610", "bodyText": "Cool -- Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3NzUwNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522577504", "bodyText": "What does it do if there are all three of [-1 0 1] present? That's probably an invalid input, does it throw?", "author": "Craigacp", "createdAt": "2020-11-13T02:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}], "type": "inlineReview"}, {"oid": "ee1c48a443810260be7319caab94bde8a3dae529", "url": "https://github.com/tensorflow/java/commit/ee1c48a443810260be7319caab94bde8a3dae529", "message": "Apply various fixes to JavaDoc", "committedDate": "2020-10-11T16:05:56Z", "type": "commit"}, {"oid": "287c96e34eea177303716e6a2b72509c2c749333", "url": "https://github.com/tensorflow/java/commit/287c96e34eea177303716e6a2b72509c2c749333", "message": "Change Tuple to LossTuple", "committedDate": "2020-10-11T16:46:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODI5Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918292", "bodyText": "Could just use dataType. I'll stop mentioning this.", "author": "deansher", "createdAt": "2020-10-11T13:50:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTY0NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951644", "bodyText": "Fixed, hopefully I have fixed them all.", "author": "JimClarke5", "createdAt": "2020-10-11T18:48:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODI5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODQ1Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918452", "bodyText": "I think \"percentage\" is extraneous here.", "author": "deansher", "createdAt": "2020-10-11T13:52:06Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTc5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951793", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-11T18:49:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODQ1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNDUyNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502924525", "bodyText": "If the rank is unknown, then the size of the last dimension is guaranteed to be unknown, so isCompatible is guaranteed true. (But there may be some idiomatic reason for writing it this way, of which I am blissfully unaware.)", "author": "deansher", "createdAt": "2020-10-11T14:42:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MzUxMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502953511", "bodyText": "Correct, it should have been or not and.", "author": "JimClarke5", "createdAt": "2020-10-11T19:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNDUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjYwMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502926601", "bodyText": "I'm pretty sure this logic is wrong. Perhaps either\n\ndocument preconditions of removeSqueezableDimensions and check exactly those,\nor (my leaning) just invoke removeSqueezableDimensions and make it however smart it needs to be.", "author": "deansher", "createdAt": "2020-10-11T14:59:53Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzODMyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504138328", "bodyText": "This logic is checking to see if both objects ranks are known (not Shape.unknown()). If both ranks are known, then it checks to see if the shapes are already in the right relationship or not. If not in the right relationship, then call removeSqueezableDimensions. It is basically an optimization to avoid doing the work in removeSqueezableDimensions if it does not need to be done.", "author": "JimClarke5", "createdAt": "2020-10-13T17:37:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzEwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927109", "bodyText": "Specify dimension -1L. Also, I'd advocate doing our own check that the last dimension of sampleWeight has size 1. The Python documentation for tf.squeeze says that, if axes are specified, then \"it is an error to squeeze a dimension that is not 1.\"", "author": "deansher", "createdAt": "2020-10-11T15:03:28Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzgzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927830", "bodyText": "Do we also have to go dynamic in the case where the ranks are both known but the size of the last weight dimension is unknown?", "author": "deansher", "createdAt": "2020-10-11T15:09:18Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODE4NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928184", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of sampleWeight may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "author": "deansher", "createdAt": "2020-10-11T15:11:55Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODUzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928535", "bodyText": "Need to specify squeezed axis -1L. But in that case:\nWhat, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "author": "deansher", "createdAt": "2020-10-11T15:15:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODYwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928609", "bodyText": "Same comments as for predictions above.", "author": "deansher", "createdAt": "2020-10-11T15:15:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODg5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928899", "bodyText": "Do we need to also go fully dynamic in the case where the size of a last dimension is unknown?", "author": "deansher", "createdAt": "2020-10-11T15:17:17Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTE5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929196", "bodyText": "We do have to verify tf.math.equal(tf.constant(expectedRankDiff+1), rankDiff), right?", "author": "deansher", "createdAt": "2020-10-11T15:19:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTM4MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929381", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1?", "author": "deansher", "createdAt": "2020-10-11T15:21:01Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTQxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929418", "bodyText": "Same comments as above for predictions.", "author": "deansher", "createdAt": "2020-10-11T15:21:24Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzODIwNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502938204", "bodyText": "To use squeezeOrExpandDimensions here, we might want to generalize that method so it doesn't think in terms of predictions when here we instead pass loss.", "author": "deansher", "createdAt": "2020-10-11T16:40:34Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MDM0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502940346", "bodyText": "This is another case where the combination of squeezeOrExpandDimensions and broadcasting yields a complex relationship between shapes of sampleWeight, loss, and the return value. In particular, due to broadcasting in mul, in the case of reduction == NONE and a surprising shape of sampleWeight, the return value may have a very different shape than loss.", "author": "deansher", "createdAt": "2020-10-11T17:01:15Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU2OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941569", "bodyText": "rank could be -1 at this point.", "author": "deansher", "createdAt": "2020-10-11T17:12:44Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] axes = new int[rank];", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5ODAyNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504198026", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-13T19:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTcyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941721", "bodyText": "allAxes?", "author": "deansher", "createdAt": "2020-10-11T17:14:06Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1Njc1NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502956755", "bodyText": "Changed name to allAxes", "author": "JimClarke5", "createdAt": "2020-10-11T19:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502943791", "bodyText": "What should happen if weightsRank is UNKNOWN?", "author": "deansher", "createdAt": "2020-10-11T17:32:41Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1Njk5Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502956997", "bodyText": "It falls through and executes the last part of the method after the // Use dynamic rank. comment.", "author": "JimClarke5", "createdAt": "2020-10-11T19:40:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4MDM4Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503280383", "bodyText": ":-) Oh yeah, that.", "author": "deansher", "createdAt": "2020-10-12T13:01:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944132", "bodyText": "Here we are working with the original predictionsRank, when we wanted to be working with the new rank.", "author": "deansher", "createdAt": "2020-10-11T17:35:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1NzgxNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502957817", "bodyText": "This matches the original Python code, but when you think about it, the predictions rank would never change from UNKNOWN to KNOWN and vice versa in a static context.", "author": "JimClarke5", "createdAt": "2020-10-11T19:47:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4NTc1NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503285754", "bodyText": "I was thinking perhaps predictions changed rank through our squeezing it to match labels earlier in the method. But I think there's a more pernicious problem. Here's an elided version of some of the method's code. Notice that we may squeeze predictions, but we only store the result in tuple. If we then also work with sampleWeight, we neither reference the squeezed version of predictions nor return it.\n    if (labels != null) {\n        . . . \n        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n          tuple = removeSqueezableDimensions(tf, labels, predictions);\n        }\n      } else { // use dynamic rank\n        tuple = removeSqueezableDimensions(tf, labels, predictions);\n      }\n    }\n    . . .\n    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n\n      if (weightsRank - predictionsRank == 1) {\n        sampleWeight = tf.squeeze(sampleWeight);\n        . . .\n      }\n      return new Tuple<>(labels, predictions, sampleWeight);\n    }", "author": "deansher", "createdAt": "2020-10-12T13:11:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5OTU5MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504199591", "bodyText": "Yes, we should probably fetch the labels and predictions from tuple first. I'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-13T19:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944276", "bodyText": "This method has a myriad of complex cases, so I think it deserves its own direct unit test.", "author": "deansher", "createdAt": "2020-10-11T17:37:25Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1ODUxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502958516", "bodyText": "I could not find direct test cases for this method in Python. It's defined in tensorflow/tensorflow/python/ops/losses/utils.py. You want to take a stab at it?", "author": "JimClarke5", "createdAt": "2020-10-11T19:54:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4NjQ5NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503286495", "bodyText": ":-) Totally. I want to do some work on #92 first, so I'll open an issue for myself.", "author": "deansher", "createdAt": "2020-10-12T13:12:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng=="}], "type": "inlineReview"}, {"oid": "642069c34d9e6b6c3df92cab4672c315029555de", "url": "https://github.com/tensorflow/java/commit/642069c34d9e6b6c3df92cab4672c315029555de", "message": "Repair JavaDOx", "committedDate": "2020-10-11T19:29:37Z", "type": "commit"}, {"oid": "249b65194bb055decf02d61f56378e7771e6d05f", "url": "https://github.com/tensorflow/java/commit/249b65194bb055decf02d61f56378e7771e6d05f", "message": "Fixed AllAxis to hanlde dynamic shape when static shape rank is unknown.", "committedDate": "2020-10-11T19:30:19Z", "type": "commit"}, {"oid": "794cfdca096223e521c8c45138fc872cc2a3ec75", "url": "https://github.com/tensorflow/java/commit/794cfdca096223e521c8c45138fc872cc2a3ec75", "message": "change method name allAxis to allAxes", "committedDate": "2020-10-11T19:36:18Z", "type": "commit"}, {"oid": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "url": "https://github.com/tensorflow/java/commit/fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "message": "change private method binaryCrossentropy to binaryCrossentropyHelper", "committedDate": "2020-10-13T17:12:40Z", "type": "commit"}, {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "url": "https://github.com/tensorflow/java/commit/928ef066f8d250b4ae41799eea40ab03fe3ecd23", "message": "Fixed squeezeOrExpandDimensions to make sure the updated labels, predictions and weights are returned in LossTuple", "committedDate": "2020-10-13T19:25:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDkzNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511490934", "bodyText": "This javadoc can get out of date as it links to Reduction#AUTO rather than REDUCTION_DEFAULT. If REDUCTION_DEFAULT changed we'd need to update all the docs.", "author": "Craigacp", "createdAt": "2020-10-24T16:45:18Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NDY3Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511654677", "bodyText": "I fixed all instances across all classes  to use Loss#REDUCTION_DEFAULT  which is set to Reduction.AUTO in the Loss class.", "author": "JimClarke5", "createdAt": "2020-10-25T22:05:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTMxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491316", "bodyText": "Does labelSmoothing = 1.0 mean the true label distribution is set to 1/n? I'm not sure what \"squeezing the values towards 0.5\" means, because it would only be 0.5 in a binary problem.", "author": "Craigacp", "createdAt": "2020-10-24T16:49:17Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NTYzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511655636", "bodyText": "Actually this is the comment for BinaryCrossentropy. It should be:\nFloat in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\nconfidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\nvalue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n\nI'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:14:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTMxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTU4MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491581", "bodyText": "What happens if multiple classes are set to 1.0? Does it throw some exception, or compute a different function? If it's the former we should document that.", "author": "Craigacp", "createdAt": "2020-10-24T16:51:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,93 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.\n+ *\n+ * <p><code>loss = maximum(neg - pos + 1, 0)</code> where <code>neg=maximum((1-labels)*predictions)\n+ * </code> and <code>pos=sum(labels*predictions)</code>\n+ *\n+ * <p><code>labels</code> values are expected to be 0 or 1.</p>", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NjkzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515166933", "bodyText": "It doesn't throw an exception and the Python test case uses multiple values set to one. There is a doc discrepancy between the documentation for CategorialHinge and the method for categorical_hinge wrt to expecting the values to be either 0 and 1, in both PythonTF and standalone Keras.", "author": "JimClarke5", "createdAt": "2020-10-30T15:07:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjE0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492140", "bodyText": "Doc says KL when it should say logcosh.", "author": "Craigacp", "createdAt": "2020-10-24T16:58:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes Computes the logarithm of the hyperbolic cosine of the prediction error.\n+ *\n+ * <p><code>logcosh = log((exp(x) + exp(-x))/2)</code>, where <code>x</code> is the error <code>\n+ * predictions - y_true</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{1.f, 1.f}, {0.f, 0.f}});\n+ *    LogCosh logcosh = new LogCosh(tf);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.108\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.8f, 0.2f});\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions, sampleWeight);\n+ *    // produces 0.087f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.217f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces [0.217f, 0f]\n+ * </pre>\n+ */\n+public class LogCosh extends Loss {\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name and a Loss\n+   * Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public LogCosh(Ops tf, Reduction reduction) {\n+    this(tf, null, reduction);\n+  }\n+\n+  /**\n+   * Creates a Kullback Leibler Divergence Loss", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NjM0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511656340", "bodyText": "Copy & paste error, I fixed it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:19:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjE0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDIxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494219", "bodyText": "This mentions smoothing the labels towards 0.5, but I think it's really towards 1/n.", "author": "Craigacp", "createdAt": "2020-10-24T17:20:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1ODA0Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511658047", "bodyText": "Actually this documentation belongs to BinaryCrossentropy,\nHere is what it should be:\nFloat in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\nconfidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\nvalue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n\nI'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494723", "bodyText": "Is this properly backtracking? It looks like it's going to pull out the softmax output not the input.", "author": "Craigacp", "createdAt": "2020-10-24T17:26:37Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MDU2OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511660569", "bodyText": "This was a TODO and the output() what just  a holder until I figured out how to get the inputs to the\nSoftMax operation. If you don't have a suggestion, maybe remove this logic for now.\nHere is the logic from Python.\n    # When softmax activation function is used for output operation, we\n    # use logits from the softmax function directly to compute loss in order\n    # to prevent collapsing zero when training.\n    # See b/117284466\n    assert len(output.op.inputs) == 1\n    output = output.op.inputs[0]\n    return nn.softmax_cross_entropy_with_logits_v2(\n        labels=target, logits=output, axis=axis)", "author": "JimClarke5", "createdAt": "2020-10-25T22:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDYzOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244639", "bodyText": "I commented out this code and marked it as a TODO.", "author": "JimClarke5", "createdAt": "2020-10-26T20:23:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511496862", "bodyText": "The CosineSimilarity doesn't mention that the value can be positive, and doesn't seem to restrict the output of this function so it is non-positive.", "author": "Craigacp", "createdAt": "2020-10-24T17:49:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI1MTgyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512251821", "bodyText": "The JavaDoc on the class should have matched the JavaDoc on Losses.cosineSimilarity method which returns values between -1 and 1. I have fixed the JavaDoc on the class CosineSimilarity to match the method JavaDoc.", "author": "JimClarke5", "createdAt": "2020-10-26T20:36:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMyMDA4NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512320085", "bodyText": "There is a discrepancy between the Python doc for CosineSimilarity  and the loss function cosine_similarity. Though the doc on CosineSimilarity indicates a value in the range of [-1 to  0], the actual method indicates a range of [-1 to  1].\nThere is nothing in the code between the two, that acts differently as CosineSimilarity just calls cosine_similarity.\nThe original Keras documentation on CosineSimilarity, agrees with the [-1  to  1] range. I am guessing the TensorFlow CosineSimilarity documentation is wrong.\nAnother weirdness creeps in in metrics.CosineSimilarity, which ends up calling another function called cosine_proximity. While the two different functions are in a similar category, they actually compute differently.\nWe'll address that issue in the metrics PR, as perhaps the metrics class should be renamed to CosineProximity.", "author": "JimClarke5", "createdAt": "2020-10-26T23:02:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM1ODE2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515358167", "bodyText": "We should probably open an issue on the main TF repo flagging the docs issue, but that can wait.", "author": "Craigacp", "createdAt": "2020-10-30T20:24:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NTEyOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522585129", "bodyText": "This javadoc is better, but I think it should mention that this function is inverted from the regular cosine similarity, as that's 1 when the values are most similar and -1 when they point in opposite directions. It makes sense that it is inverted because then you can minimise it sensibly, but it is confusing if you're just browsing through.", "author": "Craigacp", "createdAt": "2020-11-13T03:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499143", "bodyText": "But it didn't extract the logits, so won't this perform the wrong calculation?", "author": "Craigacp", "createdAt": "2020-10-24T18:14:31Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk0MDkzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511940935", "bodyText": "Correct, I really need the inputs to the Softmax Operation. This is the same issue as mentioned before,\nbut I correctly marked these as TODO's", "author": "JimClarke5", "createdAt": "2020-10-26T13:01:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MjQ2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512072467", "bodyText": "Yeah, but I think it would be best to just remove all this and leave the TODO comment in, rather than half put it in and have it break in the common case.", "author": "Craigacp", "createdAt": "2020-10-26T15:56:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDQzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244430", "bodyText": "I commented out this code and marked it as a TODO.", "author": "JimClarke5", "createdAt": "2020-10-26T20:22:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTQxMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499413", "bodyText": "The doc is wrong here, it scales towards 1/n.", "author": "Craigacp", "createdAt": "2020-10-24T18:17:34Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = tf.dtypes.cast(labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank-1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = tf.dtypes.cast(tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = tf.dtypes.cast(tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MTA4OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511661088", "bodyText": "Fixed the doc to:\n* @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n  *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n  *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>", "author": "JimClarke5", "createdAt": "2020-10-25T23:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTQxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTk1NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499954", "bodyText": "This links to Reduction#AUTO instead of REDUCTION_DEFAULT.", "author": "Craigacp", "createdAt": "2020-10-24T18:23:49Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. The labels are\n+ * expected to be provided as integers. If you want to provide labels using <code>one-hot</code>\n+ * representation, please use {@link CategoricalCrossentropy} loss. There should be <code># classes\n+ * </code> floating point values per feature for <code>predictions</code> and a single floating\n+ * point value per feature for <code>label</code>.\n+ *\n+ * <p>In the snippet below, there is a single floating point value per example for <code>labels\n+ * </code> and <code># classes</code> floating pointing values per example for <code>predictions\n+ * </code>. The shape of <code>labels</code> is <code>[batch_size]</code> and the shape of <code>\n+ * predictions</code> is <code>[batch_size, num_classes]</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[] {1, 2});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 1.177f\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class SparseCategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final int AXIS_DEFAULT = -1;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final int axis;\n+\n+  /**\n+   * Creates a SparseCategoricalCrossentropy loss using {@link Class#getSimpleName()} as the loss\n+   * name, a Loss Reduction of {@link Reduction#AUTO}, and fromLogits={@link #FROM_LOGITS_DEFAULT}.", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MTExNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511661115", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-25T23:00:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTk1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDc5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500793", "bodyText": "The last f is in caps. Is that intentional? It's not consistent throughout the file if so.", "author": "Craigacp", "createdAt": "2020-10-24T18:32:55Z", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.junit.jupiter.api.Test;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+\n+public class CategoricalCrossentropyTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  /** Test of call method, of class CategoricalCrossentropy. */\n+  @Test\n+  public void testAllCorrectUnweighted() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession testSession = TestSession.createTestSession(tfMode)) {\n+        Ops tf = testSession.getTF();\n+\n+        long[] trueArray = {\n+          1L, 0L, 0L,\n+          0L, 1L, 0L,\n+          0L, 0L, 1L\n+        };\n+        float[] predArray = {\n+          1.f, 0.f, 0.f,\n+          0.f, 1.f, 0.f,\n+          0.f, 0.f, 1.F", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDA5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244098", "bodyText": "Changed all to F", "author": "JimClarke5", "createdAt": "2020-10-26T20:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDc5Mw=="}], "type": "inlineReview"}, {"oid": "2bc54dd821b01c368914efdae87e503c3a61d989", "url": "https://github.com/tensorflow/java/commit/2bc54dd821b01c368914efdae87e503c3a61d989", "message": "Fix JavaDoc,\nAdd in rangeCheck and valueCheck\nMisc fixes based on review", "committedDate": "2020-10-27T16:24:22Z", "type": "commit"}, {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229", "url": "https://github.com/tensorflow/java/commit/951443b6cba9e42911ca2cfae05bee920d5ff229", "message": "Fix unused imports and add @SuppressWarnings(\"unchecked\") for casts.", "committedDate": "2020-10-27T16:31:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513868067", "bodyText": "What is the reason for having a seperate Losses class that implements all individual loss algorithms instead of implementing them directly in their respective class?\nI guess is to offer the choice to the user to choose an object-oriented API or a functional one? Then if we offer this, should we do the same for all other concepts in the framework?\nI wondering if we shouldn't take that decision for our users and only present a single API to accomplish a given task, for simplicity and consistency. I personally have a preference for the OO approach.", "author": "karllessard", "createdAt": "2020-10-29T01:44:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,711 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {", "originalCommit": "951443b6cba9e42911ca2cfae05bee920d5ff229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEwNTUyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515105523", "bodyText": "Metrics class will have to call these methods directly. The handling of the reduction is different in metrics from losses.", "author": "JimClarke5", "createdAt": "2020-10-30T13:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM4MzI5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515383299", "bodyText": "Also, the methods in Losses, do not apply a reduction. Loss and Metric handle the reduction differently, so the raw loss needs to be fetched first. If we didn't have the loss methods , the the metric classes would have to duplicate that functionality. Also, I can only assume that Keras has this method split from the class for some reason. Of course, we could move the static method to its corresponding class and leave it static so that it can be still invoked independent of any reduction logic.\nIt is worth noting, that not all Metrics are based on Loss algorithms, such as AUC (Area under the curve).\nAlso, there is one class,\nCosineSimilarity, that uses a different algorithm between the Loss class (uses cosineSimilarity) as opposed to the corresponding Metric class (uses cosineProximity). While these two methods are in the same category, they do not compute the exact same result. I tried to reasearch why there is a difference, maybe @Craigacp can shed some light on this. One of the questions in Metrics is should the metric class be renamed to CosineProximity.", "author": "JimClarke5", "createdAt": "2020-10-30T21:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NzUwMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515397502", "bodyText": "That's because the losses version of cosine similarity isn't actually cosine similarity, it's the negative of it. Because the cosine of 0 is 1, but here when the two vectors are pointing in the same direction it emits -1. The metrics one actually computes cosine similarity.\nI assume it's to make the minimize function work consistently.", "author": "Craigacp", "createdAt": "2020-10-30T21:47:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513870946", "bodyText": "For me, a *Impl should be the implementation of an interface, this one looks more like a LossesHelper with all its static methods (and the class should probably be final).\nI did not went through the whole thing but it looks like these helpers could also be moved directly to Loss as protected methods?", "author": "karllessard", "createdAt": "2020-10-29T01:51:22Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,402 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.AssertThat;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.SetDiff1d;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * These are helper methods for Losses and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the Loss package.\n+ */\n+public class LossesImpl {", "originalCommit": "951443b6cba9e42911ca2cfae05bee920d5ff229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEwOTQxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515109416", "bodyText": "The split really comes for module visibility. Losses should be publicly accessible, while LossesImpl should be module private. Some LossesImpl methods may be used by metrics. Whether we call it LossesImpl of LossesHelper is a matter of preference. The current methods in LossesImpl should not be restricted to Loss classes as metrics classes may also make use of them, therefore protected is not the right semantic, .", "author": "JimClarke5", "createdAt": "2020-10-30T13:48:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyNDI0MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515224241", "bodyText": "It feels uncomfortable to me that we plan to use the LossesImpl methods from other parts of our framework while restricting them from public use. When a system's built-ins rely on privileged capabilities that aren't available to 3rd-party code, I think it is commonly a big problem for the system's extensibility. In this case, I do see room to argue that these methods aren't \"capabilities\", but are just \"implementation\" which can safely be hidden. But given that it is important to us to reuse them for our own metrics, I lean toward thinking of them as capabilities that we should expose.", "author": "deansher", "createdAt": "2020-10-30T16:30:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxNDA3Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r520014072", "bodyText": "There is a tight symmetry between Losses and Metrics as many (but not all) metrics rely on the methods in Losses.\nDon't think other packages will have this close of a relationship.", "author": "JimClarke5", "createdAt": "2020-11-09T18:06:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTcxMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105711", "bodyText": "Is there a potential use case justifying exposing these to the public? Seeing as they are utilities needed to implement Losses/Metrics.\nAgree with a rename to LossesHelper or LossesUtility to differentiate from interface implementation, however.", "author": "KartikChugh", "createdAt": "2020-11-11T04:31:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}], "type": "inlineReview"}, {"oid": "ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "url": "https://github.com/tensorflow/java/commit/ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "message": "Add copyright", "committedDate": "2020-10-29T17:54:49Z", "type": "commit"}, {"oid": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "url": "https://github.com/tensorflow/java/commit/d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "message": "Add CastHelper and used that for all casts", "committedDate": "2020-10-29T18:04:39Z", "type": "commit"}, {"oid": "02573b594ca552371b8f42fa9e53c019143e6931", "url": "https://github.com/tensorflow/java/commit/02573b594ca552371b8f42fa9e53c019143e6931", "message": "Fix JavaDoc, change snake case to camel case.", "committedDate": "2020-11-09T18:18:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105860", "bodyText": "Class should have Javadoc description, no?", "author": "KartikChugh", "createdAt": "2020-11-11T04:32:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,303 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTUwODAxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521508012", "bodyText": "I have changed the class name to LossesHelper.\nI don't understand your comment on Class JavaDoc. This is what I have in my copy.\n/**\n * These are helper methods for Losses and Metrics and will be module private when Java modularity is applied to\n * TensorFlow Java. These methods should not be used outside of the losses and metrics packages.\n */\n\nThe basic comment was put in a while a ago, and I just updated it to mention metrics.", "author": "JimClarke5", "createdAt": "2020-11-11T17:08:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU1NTE0Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521555142", "bodyText": "Rendering issue I think. Looks good, thanks.", "author": "KartikChugh", "createdAt": "2020-11-11T18:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA=="}], "type": "inlineReview"}, {"oid": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "url": "https://github.com/tensorflow/java/commit/0bf49fe3203eb5f810ea09e0322fd36b6945856c", "message": "Change class LossesImpl to LossesHelper", "committedDate": "2020-11-11T17:01:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc5NDQyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521794423", "bodyText": "Can we remove this commented-out documentation?", "author": "karllessard", "createdAt": "2020-11-12T03:20:01Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,728 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods/**\n+  //   * Calculates the loss\n+  //   *\n+  //   * @param labels the truth values or labels\n+  //   * @param predictions the predictions\n+  //   * @param sampleWeights Optional SampleWeights acts as a coefficient for the loss. If a scalar\n+  // is\n+  //   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a\n+  // tensor\n+  //   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by\n+  // the\n+  //   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n+  //   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element\n+  // of\n+  //   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all\n+  // loss\n+  //   *     functions reduce by 1 dimension, usually axis=-1.)\n+  //   * @param <T> The data type of the predictions, sampleWeights and loss.\n+  //   * @param <U> The data type of the labels.\n+  //   * @return the loss\n+  //   *", "originalCommit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE5MjQyOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522192429", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-12T15:27:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc5NDQyMw=="}], "type": "inlineReview"}, {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "url": "https://github.com/tensorflow/java/commit/0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "message": "Remove commented out JavaDoc", "committedDate": "2020-11-12T15:09:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522579190", "bodyText": "We should open an issue to track inserting these cast checks into the optimizers for uniformity.", "author": "Craigacp", "createdAt": "2020-11-13T02:55:12Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/utils/CastHelper.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** A helper class for casting an Operand */\n+public class CastHelper {\n+\n+  /**\n+   * Casts an operand to the desired type.\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param value the value to be cast\n+   * @param requiredType the required data type\n+   * @param <T> the required data type\n+   * @param <U> the original data type of the value\n+   * @return the value cast to the required data type.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T extends TType, U extends TType> Operand<T> cast(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk3NjkxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522976912", "bodyText": "I could do it in the #106  Learning Rate PR if that works.", "author": "JimClarke5", "createdAt": "2020-11-13T14:17:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAzNDIyMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523034222", "bodyText": "No, let's not hold anything up for it, it's just something to clean up later.", "author": "Craigacp", "createdAt": "2020-11-13T15:45:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MDkzNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522580934", "bodyText": "label_smoothing -> labelSmoothing, here and elsewhere in this file.", "author": "Craigacp", "createdAt": "2020-11-13T03:01:28Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link Loss#REDUCTION_DEFAULT}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link\n+   * Loss#REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk3NzUxNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522977515", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MDkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581400", "bodyText": "This one's still got the doc from BinaryCrossEntropy wrt label_smoothing. And it's snake_case.", "author": "Craigacp", "createdAt": "2020-11-13T03:03:20Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NDUyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522984528", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:29:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581428", "bodyText": "Incorrect doc.", "author": "Craigacp", "createdAt": "2020-11-13T03:03:30Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NDgzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522984836", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:29:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQ3MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581470", "bodyText": "Incorrect doc.", "author": "Craigacp", "createdAt": "2020-11-13T03:03:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NjAxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522986016", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:31:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjU1OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586559", "bodyText": "I think this would be better called smoothBinaryLabels as it's not specific to the binary cross entropy as far as I can tell. But it's a private method so it's not too much of an issue.", "author": "Craigacp", "createdAt": "2020-11-13T03:11:08Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk5ODc4Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522998782", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:51:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjY0Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586647", "bodyText": "Similar comment to above, but smoothCategoricalLabels. Also I think the doc should explicitly state that it's smoothing the labels towards 1/n where n is the number of classes.", "author": "Craigacp", "createdAt": "2020-11-13T03:11:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = cast(tf,  tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param <T> the data type of the labels\n+   * @return the smoothed categorical labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsCatX(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk5OTA2MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522999060", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjY0Nw=="}], "type": "inlineReview"}, {"oid": "b211937c946a67c6f3830e70bdccf97a54cd8051", "url": "https://github.com/tensorflow/java/commit/b211937c946a67c6f3830e70bdccf97a54cd8051", "message": "Changed method name from smoothLabelsBinaryX to smoothBinaryLabels,\nsmoothLabelsCatX to smoothCategoricalLabels.\n\nAdded clarification oin JavaDoc for cosineSimilarity to describe the difference between the mathematical definition for cosine similarity and the loss definition.", "committedDate": "2020-11-13T14:56:15Z", "type": "commit"}, {"oid": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "url": "https://github.com/tensorflow/java/commit/3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "message": "Fixed JavaDoc for labelSmoothing", "committedDate": "2020-11-13T14:56:54Z", "type": "commit"}, {"oid": "914f16f4473512c8b5ef9df8ca43074b82d3edd0", "url": "https://github.com/tensorflow/java/commit/914f16f4473512c8b5ef9df8ca43074b82d3edd0", "message": "Fixed JavaDoc to change label_smoothing to labelSmoothing.", "committedDate": "2020-11-13T14:57:43Z", "type": "commit"}, {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "url": "https://github.com/tensorflow/java/commit/7eefbb7f197c731a7d304d055fd242d1acd9835f", "message": "Fix formatting", "committedDate": "2020-11-13T14:58:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzUxMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047510", "bodyText": "alue -> value", "author": "Craigacp", "createdAt": "2020-11-13T16:05:52Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -184,10 +184,9 @@ public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float la\n    *\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>", "originalCommit": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEzMzU5Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523133592", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T18:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzUxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzgzOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047838", "bodyText": "label_smoothing -> labelSmoothing, and in the docs below.", "author": "Craigacp", "createdAt": "2020-11-13T16:06:24Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -169,10 +170,9 @@ public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing)\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a", "originalCommit": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEzNDIyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523134221", "bodyText": "Ugh, I thought I got them all. I just did a global replace across the package.", "author": "JimClarke5", "createdAt": "2020-11-13T18:10:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzgzOA=="}], "type": "inlineReview"}, {"oid": "b87ad16118442643b845bb4e24a0145eea0056fb", "url": "https://github.com/tensorflow/java/commit/b87ad16118442643b845bb4e24a0145eea0056fb", "message": "replace label_smoothing with labelSmoothing.\nfix typo error in JavaDoc comment", "committedDate": "2020-11-13T18:11:21Z", "type": "commit"}, {"oid": "c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "url": "https://github.com/tensorflow/java/commit/c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "message": "Add copyright to test cases", "committedDate": "2020-11-16T23:17:39Z", "type": "commit"}, {"oid": "4d9fd24b809fd4141e61ca504b32b251a993cf8c", "url": "https://github.com/tensorflow/java/commit/4d9fd24b809fd4141e61ca504b32b251a993cf8c", "message": "Fix copyright to attribute TensorFlow Authors.", "committedDate": "2020-11-16T23:36:54Z", "type": "commit"}, {"oid": "d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "url": "https://github.com/tensorflow/java/commit/d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "message": "Fix typo on broadcast in JavaDoc", "committedDate": "2020-11-16T23:45:07Z", "type": "commit"}, {"oid": "744e32463c4aa8def4456fac4bcec53536a04fa4", "url": "https://github.com/tensorflow/java/commit/744e32463c4aa8def4456fac4bcec53536a04fa4", "message": "Fix typo on broadcast in JavaDoc", "committedDate": "2020-11-16T23:46:12Z", "type": "commit"}]}