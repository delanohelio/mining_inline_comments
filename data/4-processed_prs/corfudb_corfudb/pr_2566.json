{"pr_number": 2566, "pr_title": "Introduce Parallel Transfer", "pr_createdAt": "2020-06-04T01:20:19Z", "pr_url": "https://github.com/CorfuDB/CorfuDB/pull/2566", "timeline": [{"oid": "9d6a78f8964e582067d7ce388348cb6e2961625a", "url": "https://github.com/CorfuDB/CorfuDB/commit/9d6a78f8964e582067d7ce388348cb6e2961625a", "message": "Introduce Parallel Transfer", "committedDate": "2020-06-05T20:02:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ2MDE1Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436460152", "bodyText": "Is addressesToTransfer  always sorted?", "author": "zhangn49", "createdAt": "2020-06-08T05:12:48Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/StateTransferBatchProcessor.java", "diffHunk": "@@ -31,6 +42,17 @@\n      */\n     CompletableFuture<TransferBatchResponse> transfer(TransferBatchRequest transferBatchRequest);\n \n+    default Set<Long> getNonTransferredAddresses(LogUnitClient client, ImmutableList<Long> addressesToTransfer) {", "originalCommit": "9d6a78f8964e582067d7ce388348cb6e2961625a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAxNjI3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437016279", "bodyText": "We have a checkReadRecords method that should take care of that but I agree that there should be a check for that in this method too. I'll also add the javadoc.", "author": "PavelZaytsev", "createdAt": "2020-06-08T21:38:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ2MDE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ2MDU2NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436460564", "bodyText": "For OverwriteException, is it possible for us to check its cause?", "author": "zhangn49", "createdAt": "2020-06-08T05:14:48Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/StateTransferBatchProcessor.java", "diffHunk": "@@ -43,53 +65,99 @@\n      */\n     default TransferBatchResponse writeRecords(ReadBatch readBatch, LogUnitClient logUnitClient,\n                                                int writeRetriesAllowed, Duration writeSleepDuration) {\n-        List<Long> totalAddressesInRequest = readBatch.getAddresses();\n-        Optional<String> destination = readBatch.getDestination();\n+        ImmutableList<LogData> dataToTransfer = ImmutableList.copyOf(readBatch.getData());\n+        ImmutableList<Long> addressesToTransfer = ImmutableList.copyOf(readBatch.getAddresses());\n         Optional<Exception> lastWriteException = Optional.empty();\n         Optional<TransferBatchResponse> transferBatchResponse = Optional.empty();\n \n         for (int i = 0; i < writeRetriesAllowed; i++) {\n             List<LogData> remainingDataToWrite = readBatch.getData();\n             try {\n-                boolean writeSucceeded = logUnitClient.writeRange(remainingDataToWrite).join();\n-                if (!writeSucceeded) {\n-                    throw new IllegalStateException(\"Failed to write to a log unit server.\");\n-                } else {\n-                    lastWriteException = Optional.empty();\n+                CFUtils.getUninterruptibly(logUnitClient.writeRange(remainingDataToWrite),\n+                        OverwriteException.class,\n+                        TimeoutException.class,\n+                        NetworkException.class);\n+                transferBatchResponse = Optional.of(TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(new TransferBatchRequest(addressesToTransfer,\n+                                readBatch.getDestinationNode().map(ImmutableList::of), DATA))\n+                        .status(SUCCEEDED)\n+                        .build());\n+                break;\n+            } catch (OverwriteException | TimeoutException | NetworkException e) {", "originalCommit": "9d6a78f8964e582067d7ce388348cb6e2961625a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2NDMwOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437064308", "bodyText": "This is rangeWrite to the wiped out part of the log; the only cause for OverwirteException that it throws is SAME_DATA.", "author": "PavelZaytsev", "createdAt": "2020-06-08T23:59:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ2MDU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyMzI4MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437023280", "bodyText": "AutoCommitService is disabled in default now, so we need to call runtime.getAddressSpaceView().commit(); manually to have a committedTail. Sorry I forgot to update remote IT branch.", "author": "zhangn49", "createdAt": "2020-06-08T21:55:13Z", "path": "test/src/test/java/org/corfudb/integration/StateTransferIT.java", "diffHunk": "@@ -0,0 +1,334 @@\n+package org.corfudb.integration;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.Range;\n+import org.corfudb.protocols.wireprotocol.LogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.protocols.wireprotocol.TokenResponse;\n+import org.corfudb.runtime.BootstrapUtil;\n+import org.corfudb.runtime.CorfuRuntime;\n+import org.corfudb.runtime.view.Layout;\n+import org.corfudb.util.Sleep;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatCode;\n+\n+public class StateTransferIT extends AbstractIT {\n+\n+    private static String corfuSingleNodeHost;\n+    private final int basePort = 9000;\n+    private final int retries = 10;\n+    private CorfuRuntime firstRuntime;\n+    private CorfuRuntime secondRuntime;\n+    private CorfuRuntime writerRuntime;\n+    private String getServerEndpoint(int port) {\n+        return corfuSingleNodeHost + \":\" + port;\n+    }\n+\n+    private Layout getLayout(int numNodes) {\n+        List<String> servers = new ArrayList<>();\n+\n+        for (int x = 0; x < numNodes; x++) {\n+            String serverAddress = getServerEndpoint(basePort + x);\n+            servers.add(serverAddress);\n+        }\n+\n+        return new Layout(\n+                new ArrayList<>(servers),\n+                new ArrayList<>(servers),\n+                Collections.singletonList(new Layout.LayoutSegment(\n+                        Layout.ReplicationMode.CHAIN_REPLICATION,\n+                        0L,\n+                        -1L,\n+                        Collections.singletonList(new Layout.LayoutStripe(servers)))),\n+                0L,\n+                UUID.randomUUID());\n+    }\n+\n+    @Before\n+    public void loadProperties() {\n+        corfuSingleNodeHost = (String) PROPERTIES.get(\"corfuSingleNodeHost\");\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        if (firstRuntime != null) {\n+            firstRuntime.shutdown();\n+        }\n+        if (secondRuntime != null) {\n+            secondRuntime.shutdown();\n+        }\n+        if (writerRuntime != null) {\n+            writerRuntime.shutdown();\n+        }\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainHeadFailure() throws Exception {\n+        verifyStateTransferWithNodeFailure(0);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainTailFailure() throws Exception {\n+        verifyStateTransferWithNodeFailure(1);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainHeadRestart() throws Exception {\n+        verifyStateTransferWithNodeRestart(0);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainTailRestart() throws Exception {\n+        verifyStateTransferWithNodeRestart(1);\n+    }\n+\n+\n+    /**\n+     * A cluster of two nodes is started - 9000, 9001.\n+     * Then a block of data entries is written to the cluster.\n+     * 1 node - 9002 is added to the cluster, and triggers parallel transfer from two nodes.\n+     * Fail a node during transfer to verify it does not fail the transfer process.\n+     * Finally the addition of node 9002 in the layout is verified.\n+     *\n+     * @throws Exception\n+     */\n+    private void verifyStateTransferWithNodeFailure(int killNode) throws Exception {\n+        final int PORT_0 = 9000;\n+        final int PORT_1 = 9001;\n+        final int PORT_2 = 9002;\n+        final Duration timeout = Duration.ofMinutes(5);\n+        final Duration pollPeriod = Duration.ofSeconds(5);\n+        final int workflowNumRetry = 3;\n+        final int nodesCount = 3;\n+\n+        Process corfuServer_1 = runPersistentServer(corfuSingleNodeHost, PORT_0, false);\n+        Process corfuServer_2 = runPersistentServer(corfuSingleNodeHost, PORT_1, false);\n+        Process corfuServer_3 = runPersistentServer(corfuSingleNodeHost, PORT_2, false);\n+        List<Process> corfuServers = Arrays.asList(corfuServer_1, corfuServer_2, corfuServer_3);\n+\n+        // bootstrap cluster with 2 nodes\n+        final Layout twoNodeLayout = getLayout(2);\n+        BootstrapUtil.bootstrap(twoNodeLayout, retries, PARAMETERS.TIMEOUT_SHORT);\n+\n+        firstRuntime = createDefaultRuntime();\n+\n+        writerRuntime = createDefaultRuntime();\n+\n+        secondRuntime = createDefaultRuntime();\n+\n+        waitForLayoutChange(layout -> layout.getAllServers().size() == 2\n+                        && layout.getSegments().size() == 1,\n+                firstRuntime);\n+\n+\n+        // write records to the 2 node cluster\n+        final String data = createStringOfSize(100);\n+        for (int i = 0; i < PARAMETERS.NUM_ITERATIONS_MODERATE; i++) {\n+            TokenResponse token = writerRuntime.getSequencerView().next();\n+            writerRuntime.getAddressSpaceView().write(token, data.getBytes());\n+        }\n+", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYzMDExNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437630114", "bodyText": "Done", "author": "PavelZaytsev", "createdAt": "2020-06-09T18:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyMzI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyNDU1OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437024559", "bodyText": "Same here. This test might timeout in Travis since numEntries is too large. You can change it to PARAMETERS.NUM_ITERATIONS_MODERATE since we don't have delta transfer now.", "author": "zhangn49", "createdAt": "2020-06-08T21:58:17Z", "path": "test/src/test/java/org/corfudb/integration/StateTransferIT.java", "diffHunk": "@@ -0,0 +1,334 @@\n+package org.corfudb.integration;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.Range;\n+import org.corfudb.protocols.wireprotocol.LogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.protocols.wireprotocol.TokenResponse;\n+import org.corfudb.runtime.BootstrapUtil;\n+import org.corfudb.runtime.CorfuRuntime;\n+import org.corfudb.runtime.view.Layout;\n+import org.corfudb.util.Sleep;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatCode;\n+\n+public class StateTransferIT extends AbstractIT {\n+\n+    private static String corfuSingleNodeHost;\n+    private final int basePort = 9000;\n+    private final int retries = 10;\n+    private CorfuRuntime firstRuntime;\n+    private CorfuRuntime secondRuntime;\n+    private CorfuRuntime writerRuntime;\n+    private String getServerEndpoint(int port) {\n+        return corfuSingleNodeHost + \":\" + port;\n+    }\n+\n+    private Layout getLayout(int numNodes) {\n+        List<String> servers = new ArrayList<>();\n+\n+        for (int x = 0; x < numNodes; x++) {\n+            String serverAddress = getServerEndpoint(basePort + x);\n+            servers.add(serverAddress);\n+        }\n+\n+        return new Layout(\n+                new ArrayList<>(servers),\n+                new ArrayList<>(servers),\n+                Collections.singletonList(new Layout.LayoutSegment(\n+                        Layout.ReplicationMode.CHAIN_REPLICATION,\n+                        0L,\n+                        -1L,\n+                        Collections.singletonList(new Layout.LayoutStripe(servers)))),\n+                0L,\n+                UUID.randomUUID());\n+    }\n+\n+    @Before\n+    public void loadProperties() {\n+        corfuSingleNodeHost = (String) PROPERTIES.get(\"corfuSingleNodeHost\");\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        if (firstRuntime != null) {\n+            firstRuntime.shutdown();\n+        }\n+        if (secondRuntime != null) {\n+            secondRuntime.shutdown();\n+        }\n+        if (writerRuntime != null) {\n+            writerRuntime.shutdown();\n+        }\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainHeadFailure() throws Exception {\n+        verifyStateTransferWithNodeFailure(0);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainTailFailure() throws Exception {\n+        verifyStateTransferWithNodeFailure(1);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainHeadRestart() throws Exception {\n+        verifyStateTransferWithNodeRestart(0);\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"checkstyle:magicnumber\")\n+    public void verifyStateTransferWithChainTailRestart() throws Exception {\n+        verifyStateTransferWithNodeRestart(1);\n+    }\n+\n+\n+    /**\n+     * A cluster of two nodes is started - 9000, 9001.\n+     * Then a block of data entries is written to the cluster.\n+     * 1 node - 9002 is added to the cluster, and triggers parallel transfer from two nodes.\n+     * Fail a node during transfer to verify it does not fail the transfer process.\n+     * Finally the addition of node 9002 in the layout is verified.\n+     *\n+     * @throws Exception\n+     */\n+    private void verifyStateTransferWithNodeFailure(int killNode) throws Exception {\n+        final int PORT_0 = 9000;\n+        final int PORT_1 = 9001;\n+        final int PORT_2 = 9002;\n+        final Duration timeout = Duration.ofMinutes(5);\n+        final Duration pollPeriod = Duration.ofSeconds(5);\n+        final int workflowNumRetry = 3;\n+        final int nodesCount = 3;\n+\n+        Process corfuServer_1 = runPersistentServer(corfuSingleNodeHost, PORT_0, false);\n+        Process corfuServer_2 = runPersistentServer(corfuSingleNodeHost, PORT_1, false);\n+        Process corfuServer_3 = runPersistentServer(corfuSingleNodeHost, PORT_2, false);\n+        List<Process> corfuServers = Arrays.asList(corfuServer_1, corfuServer_2, corfuServer_3);\n+\n+        // bootstrap cluster with 2 nodes\n+        final Layout twoNodeLayout = getLayout(2);\n+        BootstrapUtil.bootstrap(twoNodeLayout, retries, PARAMETERS.TIMEOUT_SHORT);\n+\n+        firstRuntime = createDefaultRuntime();\n+\n+        writerRuntime = createDefaultRuntime();\n+\n+        secondRuntime = createDefaultRuntime();\n+\n+        waitForLayoutChange(layout -> layout.getAllServers().size() == 2\n+                        && layout.getSegments().size() == 1,\n+                firstRuntime);\n+\n+\n+        // write records to the 2 node cluster\n+        final String data = createStringOfSize(100);\n+        for (int i = 0; i < PARAMETERS.NUM_ITERATIONS_MODERATE; i++) {\n+            TokenResponse token = writerRuntime.getSequencerView().next();\n+            writerRuntime.getAddressSpaceView().write(token, data.getBytes());\n+        }\n+\n+        // start a writer future\n+        final AtomicBoolean moreDataToBeWritten = new AtomicBoolean(true);\n+        CompletableFuture<Void> writerFuture = startWriter(moreDataToBeWritten);\n+\n+        // use another thread to wait for layout change and fail node\n+        final Process killedServer = corfuServers.get(killNode);\n+\n+        CompletableFuture<Void> killerFuture = CompletableFuture.runAsync(() -> {\n+            try {\n+                secondRuntime.invalidateLayout();\n+                waitForLayoutChange(layout -> layout.getAllServers().size() == nodesCount\n+                        && layout.getSegments().size() == 2, secondRuntime);\n+                assertThat(shutdownCorfuServer(killedServer)).isTrue();\n+            } catch (Exception e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+\n+        // add node 9002\n+        firstRuntime.getManagementView().addNode(\"localhost:9002\", workflowNumRetry,\n+                timeout, pollPeriod);\n+\n+        assertThatCode(killerFuture::join).doesNotThrowAnyException();\n+\n+        // wait for killed node becomes unresponsive and state transfer completes\n+        String killedNode = getServerEndpoint(basePort + killNode);\n+        waitForLayoutChange(layout -> layout.getAllServers().size() == nodesCount\n+                && layout.getUnresponsiveServers().contains(killedNode)\n+                && layout.getSegments().size() == 1, firstRuntime);\n+\n+        // bring killed node back and verify data\n+        Process resumedServer = runPersistentServer(corfuSingleNodeHost, basePort + killNode, false);\n+        waitForLayoutChange(layout -> layout.getAllActiveServers().size() == nodesCount\n+                && layout.getUnresponsiveServers().isEmpty()\n+                && layout.getSegments().size() == 1, firstRuntime);\n+\n+        moreDataToBeWritten.set(false);\n+\n+        assertThatCode(writerFuture::join).doesNotThrowAnyException();\n+\n+        verifyData(firstRuntime);\n+\n+        shutdownCorfuServer(resumedServer);\n+        for (Process server : corfuServers) {\n+            shutdownCorfuServer(server);\n+        }\n+    }\n+\n+    /**\n+     * A cluster of three nodes is started - 9000, 9001, 9002\n+     * Then a block of data of 15,000 entries is written to the cluster.\n+     * This is to ensure we have at least 1.5 data log files.\n+     * 1 node - 9002 is shutdown for a while, and triggers state transfer from two nodes.\n+     * Restart a node during transfer to verify it does not fail the transfer process.\n+     * Finally verify two rounds of transfer completes.\n+     *\n+     * @throws Exception\n+     */\n+    private void verifyStateTransferWithNodeRestart(int restartNode) throws Exception {\n+        final int PORT_0 = 9000;\n+        final int PORT_1 = 9001;\n+        final int PORT_2 = 9002;\n+        final int nodesCount = 3;\n+        final int numEntries = 15_000;\n+\n+        Process corfuServer_1 = runPersistentServer(corfuSingleNodeHost, PORT_0, false);\n+        Process corfuServer_2 = runPersistentServer(corfuSingleNodeHost, PORT_1, false);\n+        Process corfuServer_3 = runPersistentServer(corfuSingleNodeHost, PORT_2, false);\n+\n+        // bootstrap cluster with 3 nodes\n+        final Layout twoNodeLayout = getLayout(3);\n+        BootstrapUtil.bootstrap(twoNodeLayout, retries, PARAMETERS.TIMEOUT_SHORT);\n+\n+        firstRuntime = createDefaultRuntime();\n+        writerRuntime = createDefaultRuntime();\n+        secondRuntime = createDefaultRuntime();\n+\n+        waitForLayoutChange(layout -> layout.getAllServers().size() == nodesCount\n+                && layout.getSegments().size() == 1, firstRuntime);\n+\n+        // write 15,000 records to the 3 node cluster\n+        final String data = createStringOfSize(100);\n+        for (int i = 0; i < numEntries; i++) {\n+            TokenResponse token = writerRuntime.getSequencerView().next();\n+            writerRuntime.getAddressSpaceView().write(token, data.getBytes());\n+        }\n+", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYzMDIxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437630210", "bodyText": "Done", "author": "PavelZaytsev", "createdAt": "2020-06-09T18:22:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyNDU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk0OTg1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r434949858", "bodyText": "private", "author": "WenbinZhu", "createdAt": "2020-06-04T01:59:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -170,22 +157,123 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n                 }\n             } catch (TransferSegmentException e) {\n                 throw new RetryExhaustedException(\"Transfer segment exception occurred.\", e);\n+            } catch (IllegalStateException e) {\n+                throw new RetryExhaustedException(\"The current node is in the unresponsive list.\", e);\n             }\n-\n         }).setOptions(retrySettings).run();\n+    }\n \n+    /**\n+     * Merge the state transfer segments and propose the new cluster layout.\n+     *\n+     * @param baseClient             Base client to the current node.\n+     * @param layoutManagementView   Layout management view.\n+     * @param layoutTransferSegments Layout and the transferred segments.\n+     */\n+    private void mergeSegments(BaseClient baseClient, LayoutManagementView layoutManagementView,\n+                               LayoutTransferSegments layoutTransferSegments) {\n+        Layout currentLayout = layoutTransferSegments.getLayout();\n+        ImmutableList<TransferSegment> transferredSegments =\n+                layoutTransferSegments.getTransferSegments();\n+\n+        // Seal the next epoch for the current server.\n+        // This is done in order to prevent the unnecessary OutRankedExceptions during the\n+        // layout reconfiguration that can happen if the min server set is sealed but the current node\n+        // was not part of this set.\n+        CFUtils.getUninterruptibly(baseClient.sealRemoteServer(currentLayout.getEpoch() + 1));\n+\n+        // State transfer did not happen. Try merging segments if possible.\n+        if (transferredSegments.isEmpty()) {\n+            log.info(\"State transfer on: {}: No transfer occurred, \" +\n+                    \"try merging the segments.\", currentNode);\n+            layoutManagementView.mergeSegments(currentLayout);\n+        }\n+        // State transfer happened.\n+        else {\n+            log.info(\"State transfer on {}: Transferred segments: {}.\", currentNode,\n+                    transferredSegments);\n+            // Create a new layout after the segments were transferred.\n+            // After this action is performed a current node will be present\n+            // in all the segments that previously had a status 'TRANSFERRED'.\n+            Layout newLayout = redundancyCalculator.updateLayoutAfterRedundancyRestoration(\n+                    transferredSegments, currentLayout);\n+\n+            log.info(\"State transfer on {}: New layout: {}.\", currentNode, newLayout);\n+\n+            // Merge the segments of the new layout if possible.\n+            if (RedundancyCalculator.canMergeSegments(newLayout)) {\n+                layoutManagementView.mergeSegments(newLayout);\n+            }\n+            // If the segments can't be merged, just propose a new layout.\n+            else {\n+                // Since we seal with a new epoch,\n+                // we also need to bump the epoch of the new layout.\n+                LayoutBuilder builder = new LayoutBuilder(newLayout);\n+                newLayout = builder.setEpoch(currentLayout.getEpoch() + 1).build();\n+                layoutManagementView\n+                        .runLayoutReconfiguration(currentLayout, newLayout,\n+                                false);\n+            }\n+        }\n     }\n \n+    /**\n+     * Perform a state transfer on a current node, if needed, and then\n+     * propose a new layout based on a transfer result.\n+     * If a state transfer was not needed, try merging the segments\n+     * of a current layout and then proposing it.\n+     *\n+     * @param runtime         A corfu runtime.\n+     * @param transferManager A transfer manager that runs the state transfer.\n+     * @return A new layout, if a redundancy restoration occurred; a current layout otherwise.\n+     */\n+    Layout restore(CorfuRuntime runtime, StateTransferManager transferManager)", "originalCommit": "40960cdd47b2f58144d7dbc14c06479c54b89174", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4OTY1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439889658", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:14:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk0OTg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk1MDk3NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r434950974", "bodyText": "Add javadoc for public method.", "author": "WenbinZhu", "createdAt": "2020-06-04T02:04:01Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/redundancy/RedundancyCalculator.java", "diffHunk": "@@ -192,31 +194,38 @@ public static boolean canRestoreRedundancyOrMergeSegments(Layout layout, String\n                         TransferSegmentStatus restored = TransferSegmentStatus\n                                 .builder()\n                                 .segmentState(RESTORED)\n-                                .totalTransferred(segmentEnd - segmentStart + 1L)\n                                 .build();\n \n                         return TransferSegment\n                                 .builder()\n                                 .startAddress(segmentStart)\n                                 .endAddress(segmentEnd)\n                                 .status(restored)\n+                                .logUnitServers(ImmutableList.copyOf(segment.getAllLogServers()))\n                                 .build();\n                     } else {\n                         TransferSegmentStatus notTransferred = TransferSegmentStatus\n                                 .builder()\n                                 .segmentState(NOT_TRANSFERRED)\n-                                .totalTransferred(0L)\n                                 .build();\n \n                         return TransferSegment\n                                 .builder()\n                                 .startAddress(segmentStart)\n                                 .endAddress(segmentEnd)\n                                 .status(notTransferred)\n+                                .logUnitServers(ImmutableList.copyOf(segment.getAllLogServers()))\n                                 .build();\n                     }\n \n                 })\n                 .collect(ImmutableList.toImmutableList());\n     }\n+\n+    public ImmutableList<TransferSegmentRange> prepareTransferWorkload(ImmutableList<TransferSegment> transferSegmentList,", "originalCommit": "40960cdd47b2f58144d7dbc14c06479c54b89174", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4OTgwMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439889801", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:15:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk1MDk3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk1MTA1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r434951051", "bodyText": "Line exceeds 120 characters.", "author": "WenbinZhu", "createdAt": "2020-06-04T02:04:22Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/redundancy/RedundancyCalculator.java", "diffHunk": "@@ -192,31 +194,38 @@ public static boolean canRestoreRedundancyOrMergeSegments(Layout layout, String\n                         TransferSegmentStatus restored = TransferSegmentStatus\n                                 .builder()\n                                 .segmentState(RESTORED)\n-                                .totalTransferred(segmentEnd - segmentStart + 1L)\n                                 .build();\n \n                         return TransferSegment\n                                 .builder()\n                                 .startAddress(segmentStart)\n                                 .endAddress(segmentEnd)\n                                 .status(restored)\n+                                .logUnitServers(ImmutableList.copyOf(segment.getAllLogServers()))\n                                 .build();\n                     } else {\n                         TransferSegmentStatus notTransferred = TransferSegmentStatus\n                                 .builder()\n                                 .segmentState(NOT_TRANSFERRED)\n-                                .totalTransferred(0L)\n                                 .build();\n \n                         return TransferSegment\n                                 .builder()\n                                 .startAddress(segmentStart)\n                                 .endAddress(segmentEnd)\n                                 .status(notTransferred)\n+                                .logUnitServers(ImmutableList.copyOf(segment.getAllLogServers()))\n                                 .build();\n                     }\n \n                 })\n                 .collect(ImmutableList.toImmutableList());\n     }\n+\n+    public ImmutableList<TransferSegmentRange> prepareTransferWorkload(ImmutableList<TransferSegment> transferSegmentList,", "originalCommit": "40960cdd47b2f58144d7dbc14c06479c54b89174", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4OTgzMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439889832", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:15:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk1MTA1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyNTA2Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436225062", "bodyText": "This can be removed, the caller already did invalidateLayout() and will also do it every retry.", "author": "WenbinZhu", "createdAt": "2020-06-06T01:29:47Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -270,30 +383,49 @@ public void impl(@Nonnull CorfuRuntime runtime) throws Exception {\n         runtime.invalidateLayout();", "originalCommit": "9d6a78f8964e582067d7ce388348cb6e2961625a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4OTkzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439889935", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:16:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyNTA2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyNzk1Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436227952", "bodyText": "Maybe add \"throw OutRankedException\" here and also in restore()?", "author": "WenbinZhu", "createdAt": "2020-06-06T02:06:27Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -170,22 +157,123 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n                 }\n             } catch (TransferSegmentException e) {\n                 throw new RetryExhaustedException(\"Transfer segment exception occurred.\", e);\n+            } catch (IllegalStateException e) {\n+                throw new RetryExhaustedException(\"The current node is in the unresponsive list.\", e);\n             }\n-\n         }).setOptions(retrySettings).run();\n+    }\n \n+    /**\n+     * Merge the state transfer segments and propose the new cluster layout.\n+     *\n+     * @param baseClient             Base client to the current node.\n+     * @param layoutManagementView   Layout management view.\n+     * @param layoutTransferSegments Layout and the transferred segments.\n+     */\n+    private void mergeSegments(BaseClient baseClient, LayoutManagementView layoutManagementView,", "originalCommit": "bea98d6ea379e3165a0cee48799249ef50cfe7a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4OTk3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439889979", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyNzk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODA0OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436228049", "bodyText": "I think this logic should be put inside mergeSegments(), can then you can also remove the BaseClient parameter of it, that will be much cleaner.", "author": "WenbinZhu", "createdAt": "2020-06-06T02:07:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -170,22 +157,123 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n                 }\n             } catch (TransferSegmentException e) {\n                 throw new RetryExhaustedException(\"Transfer segment exception occurred.\", e);\n+            } catch (IllegalStateException e) {\n+                throw new RetryExhaustedException(\"The current node is in the unresponsive list.\", e);\n             }\n-\n         }).setOptions(retrySettings).run();\n+    }\n \n+    /**\n+     * Merge the state transfer segments and propose the new cluster layout.\n+     *\n+     * @param baseClient             Base client to the current node.\n+     * @param layoutManagementView   Layout management view.\n+     * @param layoutTransferSegments Layout and the transferred segments.\n+     */\n+    private void mergeSegments(BaseClient baseClient, LayoutManagementView layoutManagementView,\n+                               LayoutTransferSegments layoutTransferSegments) {\n+        Layout currentLayout = layoutTransferSegments.getLayout();\n+        ImmutableList<TransferSegment> transferredSegments =\n+                layoutTransferSegments.getTransferSegments();\n+\n+        // Seal the next epoch for the current server.\n+        // This is done in order to prevent the unnecessary OutRankedExceptions during the\n+        // layout reconfiguration that can happen if the min server set is sealed but the current node\n+        // was not part of this set.\n+        CFUtils.getUninterruptibly(baseClient.sealRemoteServer(currentLayout.getEpoch() + 1));\n+\n+        // State transfer did not happen. Try merging segments if possible.\n+        if (transferredSegments.isEmpty()) {\n+            log.info(\"State transfer on: {}: No transfer occurred, \" +\n+                    \"try merging the segments.\", currentNode);\n+            layoutManagementView.mergeSegments(currentLayout);\n+        }\n+        // State transfer happened.\n+        else {\n+            log.info(\"State transfer on {}: Transferred segments: {}.\", currentNode,\n+                    transferredSegments);\n+            // Create a new layout after the segments were transferred.\n+            // After this action is performed a current node will be present\n+            // in all the segments that previously had a status 'TRANSFERRED'.\n+            Layout newLayout = redundancyCalculator.updateLayoutAfterRedundancyRestoration(\n+                    transferredSegments, currentLayout);\n+\n+            log.info(\"State transfer on {}: New layout: {}.\", currentNode, newLayout);\n+\n+            // Merge the segments of the new layout if possible.\n+            if (RedundancyCalculator.canMergeSegments(newLayout)) {\n+                layoutManagementView.mergeSegments(newLayout);\n+            }\n+            // If the segments can't be merged, just propose a new layout.\n+            else {\n+                // Since we seal with a new epoch,\n+                // we also need to bump the epoch of the new layout.\n+                LayoutBuilder builder = new LayoutBuilder(newLayout);\n+                newLayout = builder.setEpoch(currentLayout.getEpoch() + 1).build();\n+                layoutManagementView\n+                        .runLayoutReconfiguration(currentLayout, newLayout,\n+                                false);\n+            }\n+        }\n     }\n \n+    /**\n+     * Perform a state transfer on a current node, if needed, and then\n+     * propose a new layout based on a transfer result.\n+     * If a state transfer was not needed, try merging the segments\n+     * of a current layout and then proposing it.\n+     *\n+     * @param runtime         A corfu runtime.\n+     * @param transferManager A transfer manager that runs the state transfer.\n+     * @return A new layout, if a redundancy restoration occurred; a current layout otherwise.\n+     */\n+    Layout restore(CorfuRuntime runtime, StateTransferManager transferManager)\n+            throws InterruptedException {\n+\n+        LayoutTransferSegments layoutTransferSegments = performStateTransfer(runtime, transferManager);\n+        BaseClient baseClient = runtime.getLayoutView()\n+                .getRuntimeLayout(layoutTransferSegments.getLayout())\n+                .getBaseClient(currentNode);", "originalCommit": "bea98d6ea379e3165a0cee48799249ef50cfe7a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MDA2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439890069", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r436228142", "bodyText": "Why not put this right before the layoutManagementView.runLayoutReconfiguration() call ?", "author": "WenbinZhu", "createdAt": "2020-06-06T02:09:16Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -170,22 +157,123 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n                 }\n             } catch (TransferSegmentException e) {\n                 throw new RetryExhaustedException(\"Transfer segment exception occurred.\", e);\n+            } catch (IllegalStateException e) {\n+                throw new RetryExhaustedException(\"The current node is in the unresponsive list.\", e);\n             }\n-\n         }).setOptions(retrySettings).run();\n+    }\n \n+    /**\n+     * Merge the state transfer segments and propose the new cluster layout.\n+     *\n+     * @param baseClient             Base client to the current node.\n+     * @param layoutManagementView   Layout management view.\n+     * @param layoutTransferSegments Layout and the transferred segments.\n+     */\n+    private void mergeSegments(BaseClient baseClient, LayoutManagementView layoutManagementView,\n+                               LayoutTransferSegments layoutTransferSegments) {\n+        Layout currentLayout = layoutTransferSegments.getLayout();\n+        ImmutableList<TransferSegment> transferredSegments =\n+                layoutTransferSegments.getTransferSegments();\n+\n+        // Seal the next epoch for the current server.\n+        // This is done in order to prevent the unnecessary OutRankedExceptions during the\n+        // layout reconfiguration that can happen if the min server set is sealed but the current node\n+        // was not part of this set.\n+        CFUtils.getUninterruptibly(baseClient.sealRemoteServer(currentLayout.getEpoch() + 1));", "originalCommit": "bea98d6ea379e3165a0cee48799249ef50cfe7a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM4ODY1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438388658", "bodyText": "Because we need it for all of the following cases because in all of them we attempt consensus:\n\nS/T did not happen, so we don't create a new layout and just try to merge the segments of the current layout\nS/T happened and we create a new layout to merge segments\nS/T happened and we create a new layout without merging the segments.\nThis statement is put in front of all of these cases. I did not put the sealing of the current node inside the actual runLayoutReconfiguration and merge segments methods because this issue appears in the state transfer related workloads only.", "author": "PavelZaytsev", "createdAt": "2020-06-10T20:27:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNTIzMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438405230", "bodyText": "@PavelZaytsev One question, in your code comment, it says seal here is to prevent OutRankedException, but I think out rank happens when others are proposing new layout at the same time, it's not related to whether this server is sealed right?", "author": "WenbinZhu", "createdAt": "2020-06-10T20:59:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMTg1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438421858", "bodyText": "What we've seen happening is basically this kind of scenario:\n\nA, B, C at epoch 0, A has finished state transfer and is about to run this method.\nA invokes either layoutManagementView.mergeSegments or layoutManagementView.runLayoutReconfiguration does not matter.\nIn both of these methods, it calls sealEpoch.\nB & C seal immediately, now they are at epoch 1, and we only need a majority for this method to return. A is still at 0 as its layout server is still processing it.\nA calls attemptConsensus on the new layout.\nIt sends prepare to A, B, C, gets a quorum responses from B and C. So continues to propose phase.\nWhen the layout_prepare request hits A's layout server and it it's still not sealed, we reject the prepare request with a WEE.\nA runs the second phase, since the majority is sealed, and finds out that the phase1Rank is null (cause it has been previously rejected to the WEE), so it throws OutRankedException.\n\nThis issue does not appear in the healNode btw because there we make sure we seal the current node before the reset.", "author": "PavelZaytsev", "createdAt": "2020-06-10T21:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Mzg2OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438463868", "bodyText": "I see, I think this problem is because the propose() method in LayoutView is not implemented in the best way. In this example, the propose should work because the majority can succeed, however in our propose() method, we uses CompleteableFuture.anyOf() and throws OutRankedException if we see any, which could be a false negative, when a majority can  succeed.", "author": "WenbinZhu", "createdAt": "2020-06-10T23:37:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjc4Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439002783", "bodyText": "Yeah, it makes sense but I don't want to touch Paxos code in this PR. I will create an issue for this:\n#2574", "author": "PavelZaytsev", "createdAt": "2020-06-11T18:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIyODE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2MjkyMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437062923", "bodyText": "Using getTrimMark(false) will not retry and throw a RuntimeException, however your caller is not catching RuntimeException, it only catches specific eceptions like TimeoutException, NetworkException, WrongEpochException. So if getTrimMark failed, it won't be retried in performStateTransfer(), right? I think this needw to be taken care of. Maybe using plain version of CFUtils that throws RuntimeEception in other occurances, and catch RuntimeException as what getTrimMark does.", "author": "WenbinZhu", "createdAt": "2020-06-08T23:54:32Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -236,10 +320,9 @@ private void transferCommittedTail(CorfuRuntime runtime, Layout layout, long com\n      * @param runtime A current runtime.\n      * @return A retrieved trim mark.\n      */\n-    @VisibleForTesting\n-    long trimLog(CorfuRuntime runtime) {\n+    private long trimLog(CorfuRuntime runtime) throws TimeoutException {\n \n-        long trimMark = runtime.getAddressSpaceView().getTrimMark().getSequence();\n+        long trimMark = runtime.getAddressSpaceView().getTrimMark(false).getSequence();", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ0MjQ5OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438442498", "bodyText": "We have other exceptions that subclass the RuntimeExceptions that don't require retries. I guess I can catch it and then check if the cause is the TimeOutException or its the instance of WrongEpochException  or a network exception and then just wrap it in its own exception - TrimMarkRetrievalException if these conditions are true, otherwise just throw the original exception.", "author": "PavelZaytsev", "createdAt": "2020-06-10T22:29:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2MjkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2MTI1NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438461254", "bodyText": "@PavelZaytsev  What I'm thinking of is we need to catch RuntimeException in the outermost catch block inperformStateTransfer's backoff retry, instead of catching individual TimeoutException | NetworkException | WrongEpochException | UnknownAddressInRangeException. If you have exceptions that you do not want to retry, catch them first and then RuntimeTimeException. Also one question is why we do not retry TransferSegmentException? This exception happens if a transfer batch fails because of Timeout, WrongEpoch, etc, why don't we retry the workflow in performStateTransfer?", "author": "WenbinZhu", "createdAt": "2020-06-10T23:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2MjkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MjQwMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439892401", "bodyText": "Ok, fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2MjkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2Mzg0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437063843", "bodyText": "private", "author": "WenbinZhu", "createdAt": "2020-06-08T23:58:01Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -207,27 +295,23 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n     }\n \n     /**\n-     * Send the last transferred address as the committed tail to the target log unit.\n-     * This is required to prevent loss of committed tail after state transfer finishes\n-     * and then all other log units failed and auto commit service is paused.\n+     * Tries to retrieve a committed tail. If this fails, returns an empty option.\n+     * State transfer will still finish but all the addresses\n+     * will be transferred via replication protocol.\n+     *\n+     * @param runtime Corfu runtime\n+     * @return Maybe a committed tail.\n      */\n-    private void transferCommittedTail(CorfuRuntime runtime, Layout layout, long committedTail) {\n-        final int maxRetry = 3;\n-        LogUnitClient logUnitClient = runtime.getLayoutView()\n-                .getRuntimeLayout(layout)\n-                .getLogUnitClient(currentNode);\n-\n-        for (int i = 0; i < maxRetry; i++) {\n-            try {\n-                CFUtils.getUninterruptibly(logUnitClient.updateCommittedTail(committedTail),\n-                        TimeoutException.class, NetworkException.class);\n-                return;\n-            } catch (TimeoutException | NetworkException e) {\n-                log.error(\"Encountered network issue when transferring new committed tail.\");\n-            }\n+    Optional<Long> tryGetCommittedTail(CorfuRuntime runtime) {", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MjU0OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439892549", "bodyText": "fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:33:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2Mzg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2NDgxMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437064811", "bodyText": "getCommittedTail() needs to read from all log units, which has higher chance of failure, should be at least do some retry here before we default to use protocol transfer, like transferCommittedTail() ?", "author": "WenbinZhu", "createdAt": "2020-06-09T00:01:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -207,27 +295,23 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n     }\n \n     /**\n-     * Send the last transferred address as the committed tail to the target log unit.\n-     * This is required to prevent loss of committed tail after state transfer finishes\n-     * and then all other log units failed and auto commit service is paused.\n+     * Tries to retrieve a committed tail. If this fails, returns an empty option.\n+     * State transfer will still finish but all the addresses\n+     * will be transferred via replication protocol.\n+     *\n+     * @param runtime Corfu runtime\n+     * @return Maybe a committed tail.\n      */\n-    private void transferCommittedTail(CorfuRuntime runtime, Layout layout, long committedTail) {\n-        final int maxRetry = 3;\n-        LogUnitClient logUnitClient = runtime.getLayoutView()\n-                .getRuntimeLayout(layout)\n-                .getLogUnitClient(currentNode);\n-\n-        for (int i = 0; i < maxRetry; i++) {\n-            try {\n-                CFUtils.getUninterruptibly(logUnitClient.updateCommittedTail(committedTail),\n-                        TimeoutException.class, NetworkException.class);\n-                return;\n-            } catch (TimeoutException | NetworkException e) {\n-                log.error(\"Encountered network issue when transferring new committed tail.\");\n-            }\n+    Optional<Long> tryGetCommittedTail(CorfuRuntime runtime) {\n+        try {\n+            long committedTail = runtime.getAddressSpaceView().getCommittedTail();", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MDQ1MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439890450", "bodyText": "I made it retry on network and timeout exceptions and fail on all other exceptions.", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:20:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2NDgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2NjAxNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437066016", "bodyText": "Only catching RuntimeException here might not be a good idea, it might be a WrongEpochException, WrongClusterExecption, etc, which needs to abort and retry the entire transfer workflow. As mentioned in previous comment, maybe we can copy from transferCommittedTail() which does retry only on TimeoutException and NetworkException.", "author": "WenbinZhu", "createdAt": "2020-06-09T00:06:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -207,27 +295,23 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n     }\n \n     /**\n-     * Send the last transferred address as the committed tail to the target log unit.\n-     * This is required to prevent loss of committed tail after state transfer finishes\n-     * and then all other log units failed and auto commit service is paused.\n+     * Tries to retrieve a committed tail. If this fails, returns an empty option.\n+     * State transfer will still finish but all the addresses\n+     * will be transferred via replication protocol.\n+     *\n+     * @param runtime Corfu runtime\n+     * @return Maybe a committed tail.\n      */\n-    private void transferCommittedTail(CorfuRuntime runtime, Layout layout, long committedTail) {\n-        final int maxRetry = 3;\n-        LogUnitClient logUnitClient = runtime.getLayoutView()\n-                .getRuntimeLayout(layout)\n-                .getLogUnitClient(currentNode);\n-\n-        for (int i = 0; i < maxRetry; i++) {\n-            try {\n-                CFUtils.getUninterruptibly(logUnitClient.updateCommittedTail(committedTail),\n-                        TimeoutException.class, NetworkException.class);\n-                return;\n-            } catch (TimeoutException | NetworkException e) {\n-                log.error(\"Encountered network issue when transferring new committed tail.\");\n-            }\n+    Optional<Long> tryGetCommittedTail(CorfuRuntime runtime) {\n+        try {\n+            long committedTail = runtime.getAddressSpaceView().getCommittedTail();\n+            log.debug(\"tryGetCommittedTail: retrieved a committed tail: {}.\", committedTail);\n+            return Optional.of(committedTail);\n+        } catch (RuntimeException re) {", "originalCommit": "419a4a18f67ec74d906bea64feb07cf8fc4d4bad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MjYxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439892612", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:34:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA2NjAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA3OTMzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437079337", "bodyText": "Can we give this log more details? like \"performStateTransfer: Got exception {}. Retrying {} times.\"", "author": "WenbinZhu", "createdAt": "2020-06-09T00:58:17Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -74,94 +81,74 @@\n     private final int restoreRetries = 3;\n \n     /**\n-     * Perform a state transfer on a current node, if needed, and then\n-     * propose a new layout based on a transfer result.\n-     * If a state transfer was not needed, try merging the segments\n-     * of a current layout and then proposing it.\n-     * We utilize an exponential backoff since there can be cases\n-     * when multiple nodes are proposing a new layout simultaneously.\n+     * A data class that stores both the layout and the transferred segments.\n+     */\n+    @AllArgsConstructor\n+    private static class LayoutTransferSegments {\n+        @Getter\n+        @NonNull\n+        private final Layout layout;\n+\n+        @Getter\n+        @NonNull\n+        private final ImmutableList<TransferSegment> transferSegments;\n+    }\n+\n+    /**\n+     * Perform state transfer and return all the transferred segments along with the latest layout.\n      *\n-     * @param runtime         A corfu runtime.\n-     * @param transferManager A transfer manager that runs the state transfer.\n-     * @return A new layout, if a redundancy restoration occurred; a current layout otherwise.\n+     * @param runtime         Current runtime.\n+     * @param transferManager Transfer manager instance to perform a state transfer.\n+     * @return Current layout and the list of transferred segments.\n      */\n-    @VisibleForTesting\n-    Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferManager)\n+    private LayoutTransferSegments performStateTransfer(CorfuRuntime runtime,\n+                                                        StateTransferManager transferManager)\n             throws InterruptedException {\n-\n-        // Set up retry settings.\n+        // Settings for the retries\n         Consumer<ExponentialBackoffRetry> retrySettings = settings -> {\n             settings.setBase(retryBase);\n             settings.setExtraWait(extraWait.toMillis());\n             settings.setBackoffDuration(backoffDuration);\n             settings.setRandomPortion(randomPart);\n         };\n \n-        // Configure a number of retries.\n-        // Atomic is used here to overcome a restriction on the mutation of a local variable\n-        // within a lambda expression block.\n         AtomicInteger retries = new AtomicInteger(restoreRetries);\n         return IRetry.build(ExponentialBackoffRetry.class, RetryExhaustedException.class, () -> {\n             try {\n                 // Retrieve a current layout.\n                 runtime.invalidateLayout();\n                 Layout currentLayout = runtime.getLayoutView().getLayout();\n-\n+                if (currentLayout.getUnresponsiveServers().contains(currentNode)) {\n+                    throw new IllegalStateException(\"Node is in the unresponsive list.\");\n+                }\n                 log.info(\"State transfer on {}: Layout before transfer: {}\",\n                         currentNode, currentLayout);\n-\n-                // Trim a current stream log and retrieve a global trim mark.\n+                // Retrieve a current cluster trim mark.\n                 long trimMark = trimLog(runtime);\n-                List<TransferSegment> transferredSegments = getTransferSegments(transferManager, currentLayout, trimMark);\n+\n+                // Retrieve a current cluster committed tail.\n+                Optional<Long> committedTail = tryGetCommittedTail(runtime);\n+\n+                // Execute state transfer.\n+                ImmutableList<TransferSegment> transferSegments = ImmutableList.copyOf(\n+                        getTransferSegments(transferManager, currentLayout,\n+                                trimMark, committedTail));\n+\n+                // Get all the transferred segments as well as the current layout.\n+                LayoutTransferSegments layoutTransferSegments =\n+                        new LayoutTransferSegments(currentLayout, transferSegments);\n \n                 // Transfer a new committed tail after all segments are transferred.\n                 // The new committed tail is the last transferred address.\n-                transferredSegments\n+                layoutTransferSegments.getTransferSegments()\n                         .stream()\n                         .map(TransferSegment::getEndAddress)\n                         .max(Long::compare)\n                         .ifPresent(addr -> transferCommittedTail(runtime, currentLayout, addr));\n \n-                LayoutManagementView layoutManagementView = runtime.getLayoutManagementView();\n-\n-                // State transfer did not happen. Try merging segments if possible.\n-                if (transferredSegments.isEmpty()) {\n-                    log.info(\"State transfer on: {}: No transfer occurred, \" +\n-                            \"try merging the segments.\", currentNode);\n-                    layoutManagementView.mergeSegments(currentLayout);\n-                }\n-                // State transfer happened.\n-                else {\n-                    log.info(\"State transfer on {}: Transferred segments: {}.\", currentNode,\n-                            transferredSegments);\n-                    // Create a new layout after the segments were transferred.\n-                    // After this action is performed a current node will be present\n-                    // in all the segments that previously had a status 'TRANSFERRED'.\n-                    Layout newLayout = redundancyCalculator.updateLayoutAfterRedundancyRestoration(\n-                            transferredSegments, currentLayout);\n-\n-                    log.info(\"State transfer on {}: New layout: {}.\", currentNode, newLayout);\n-\n-                    // Merge the segments of the new layout if possible.\n-                    if (RedundancyCalculator.canMergeSegments(newLayout)) {\n-                        layoutManagementView.mergeSegments(newLayout);\n-                    }\n-                    // If the segments can't be merged, just propose a new layout.\n-                    else {\n-                        // Since we seal with a new epoch,\n-                        // we also need to bump the epoch of the new layout.\n-                        LayoutBuilder builder = new LayoutBuilder(newLayout);\n-                        newLayout = builder.setEpoch(currentLayout.getEpoch() + 1).build();\n-                        layoutManagementView\n-                                .runLayoutReconfiguration(currentLayout, newLayout,\n-                                        false);\n-                    }\n-                }\n-                // Return the latest layout.\n-                runtime.invalidateLayout();\n-                return runtime.getLayoutView().getLayout();\n-\n-            } catch (WrongEpochException | QuorumUnreachableException | OutrankedException e) {\n+                return layoutTransferSegments;\n+            } catch (TimeoutException | NetworkException |\n+                    WrongEpochException | UnknownAddressInRangeException e) {\n                 log.warn(\"Got: {}. Retrying: {} times.\", e.getMessage(), retries.get());", "originalCommit": "d48b384686c323c65810351f5a2de1758c8cd69b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5MjgzMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439892833", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA3OTMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4OTQxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437089410", "bodyText": "Can you move all these TransferSegment* classes into separate files? They are not only used in StateTransferManger, and are quite long that makes StateTransferManger cumbersome and hard to read.", "author": "WenbinZhu", "createdAt": "2020-06-09T01:37:23Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/StateTransferManager.java", "diffHunk": "@@ -71,140 +91,385 @@\n      */\n     @VisibleForTesting\n     ImmutableList<Long> getUnknownAddressesInRange(long rangeStart, long rangeEnd) {\n-        Set<Long> knownAddresses = logUnitClient\n-                .requestKnownAddresses(rangeStart, rangeEnd).join().getKnownAddresses();\n+        try {\n+            Set<Long> knownAddresses = CFUtils.getUninterruptibly(logUnitClient\n+                            .requestKnownAddresses(rangeStart, rangeEnd),\n+                    TimeoutException.class,\n+                    NetworkException.class,\n+                    WrongEpochException.class)\n+                    .getKnownAddresses();\n+\n+            return LongStream.range(rangeStart, rangeEnd + 1L)\n+                    .filter(address -> !knownAddresses.contains(address))\n+                    .boxed()\n+                    .collect(ImmutableList.toImmutableList());\n+        } catch (TimeoutException | NetworkException | WrongEpochException e) {\n+            // This function is called within a lambda expression, which does\n+            // not allow checked exceptions. So we wrap all the exceptions that can occur\n+            // here it the UnknownAddressInRangeException.\n+            throw new UnknownAddressInRangeException(e);\n+        }\n+    }\n+\n+    /**\n+     * Return a range with the updated list of unknown addresses.\n+     *\n+     * @param range A transfer segment range.\n+     * @return An updated transfer segment range.\n+     */\n+    TransferSegmentRangeSingle getUnknownAddressesInRangeForRange(TransferSegmentRangeSingle range) {\n+        long startAddress = range.getStartAddress();\n+        long endAddress = range.getEndAddress();\n+        ImmutableList<Long> unknownAddressesInRange =\n+                getUnknownAddressesInRange(startAddress, endAddress);\n+        return range.toBuilder()\n+                .unknownAddressesInRange(unknownAddressesInRange)\n+                .build();\n+    }\n+\n+    /**\n+     * Transform the given range into a stream of batch requests.\n+     *\n+     * @param range A transfer segment range that contains unknown addresses and maybe available\n+     *              log unit servers.\n+     * @return A stream of transfer batch requests.\n+     */\n+    Stream<TransferBatchRequest> rangeToBatchRequestStream(TransferSegmentRangeSingle range) {\n+        ImmutableList<Long> unknownAddressesInRange = range.unknownAddressesInRange;\n+        Optional<ImmutableList<String>> availableServers = range.availableServers;\n+        return Lists.partition(unknownAddressesInRange, batchSize).stream()\n+                .map(partition -> new TransferBatchRequest(partition, availableServers, DATA));\n+    }\n+\n+    /**\n+     * Go over all the single transfer segment ranges, filter them by the provided type,\n+     * get all the unknown addresses in range, and then turn the unknown addresses in range into\n+     * the transfer batch request stream - ready to be consumed by the appropriate\n+     * state transfer processor.\n+     *\n+     * @param ranges         A list of single transfer segment ranges.\n+     * @param typeOfTransfer A provided type of transfer - protocol read or consistent read.\n+     * @return A stream of transfer batch requests.\n+     */\n+    Stream<TransferBatchRequest> createBatchWorkload(List<TransferSegmentRangeSingle> ranges,\n+                                                     StateTransferType typeOfTransfer) {\n+        return ranges.stream()\n+                .filter(range -> range.typeOfTransfer == typeOfTransfer)\n+                .map(this::getUnknownAddressesInRangeForRange)\n+                .flatMap(this::rangeToBatchRequestStream);\n+    }\n \n-        return LongStream.range(rangeStart, rangeEnd + 1L)\n-                .filter(address -> !knownAddresses.contains(address))\n-                .boxed()\n+    /**\n+     * Transform the transfer segment ranges into the single ones and filter all the non transferred\n+     * ones.\n+     *\n+     * @param beforeTransferRanges Ranges before the transfer, some single and some split.\n+     * @return Ranges before the transfer, not transferred and single.\n+     */\n+    ImmutableList<TransferSegmentRangeSingle> toSingleNotTransferredRanges(\n+            List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream()\n+                .flatMap(range -> range.toSingle().stream())\n+                .filter(range -> range.getStatus().getSegmentState() == NOT_TRANSFERRED)\n                 .collect(ImmutableList.toImmutableList());\n     }\n \n     /**\n-     * Performs the state transfer for the current transfer segments and also\n+     * Transform all the transferred ranges back into the transfer segments. This data is used\n+     * later for cluster reconfiguration.\n+     *\n+     * @param transferRanges A list of transfer segment ranges\n+     * @return A list of transfer segments.\n+     */\n+    ImmutableList<TransferSegment> toSegments(List<TransferSegmentRange> transferRanges) {\n+        return transferRanges.stream().map(TransferSegmentRange::toTransferSegment)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n+\n+    /**\n+     * For all the segment ranges that were not transferred, updated their status.\n+     *\n+     * @param newStatus            A new status.\n+     * @param beforeTransferRanges Ranges before transfer.\n+     * @return Ranges after transfer.\n+     */\n+    ImmutableList<TransferSegmentRange> updateNotTransferredSegmentRangeStatus(TransferSegmentStatus newStatus,\n+                                                                               List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream().map(range -> {\n+            if (range.getStatus().getSegmentState() == NOT_TRANSFERRED) {\n+                return range.updateStatus(newStatus);\n+            }\n+            return range;\n+        }).collect(ImmutableList.toImmutableList());\n+    }\n+\n+    /**\n+     * Performs the state transfer for the current non-transferred transfer segments and also\n      * updates their state as a result.\n      *\n-     * @param beforeTransferSegments A list of segments before a transfer.\n+     * @param beforeTransferRanges A list of ranges before a transfer.\n      * @return A list of segments after a transfer.\n      */\n-    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegment> beforeTransferSegments) {\n+    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegmentRange> beforeTransferRanges) {\n+        // Transform all ranges into single ranges and filter all the not transferred ranges.\n+        ImmutableList<TransferSegmentRangeSingle> singleNotTransferredRanges =\n+                toSingleNotTransferredRanges(beforeTransferRanges);\n+\n+        // If none are NOT_TRANSFERRED, there is nothing to transfer, return the list as is.\n+        if (singleNotTransferredRanges.isEmpty()) {\n+            return toSegments(beforeTransferRanges);\n+        }\n \n-        List<TransferSegment> afterTransferSegments = new ArrayList<>();\n+        // Split into the protocol and committed workloads.\n+        Stream<TransferBatchRequest> consistentBatchStream =\n+                createBatchWorkload(singleNotTransferredRanges, CONSISTENT_READ);\n+\n+        Stream<TransferBatchRequest> protocolBatchStream =\n+                createBatchWorkload(singleNotTransferredRanges, PROTOCOL_READ);\n+\n+        // Execute a parallel transfer first, and then if it succeeds, execute a regular transfer.\n+        TransferProcessorResult result = parallelTransferProcessor.runStateTransfer(consistentBatchStream)\n+                .thenCompose(res -> {\n+                    if (res.getTransferState() == TRANSFER_SUCCEEDED) {\n+                        return basicTransferProcessor.runStateTransfer(protocolBatchStream);\n+                    } else {\n+                        return CompletableFuture.completedFuture(res);\n+                    }\n+                }).join();\n+\n+        log.info(\"handleTransfer: overall transfer result: {}\", result);\n+        // Update the segment status. If either of the transfers failed the status is failed\n+        // and if none failed, the status is transferred.\n+        TransferSegmentStatus newTransferSegmentStatus;\n+\n+        if (result.getTransferState() == TRANSFER_SUCCEEDED) {\n+            newTransferSegmentStatus = new TransferSegmentStatus(TRANSFERRED, Optional.empty());\n+        } else {\n+            newTransferSegmentStatus = new TransferSegmentStatus(FAILED, result.getCauseOfFailure());\n+        }\n \n-        for (TransferSegment segment : beforeTransferSegments) {\n-            TransferSegmentStatus newStatus = segment.getStatus();\n+        // Update the status of the not transferred segment ranges.\n+        ImmutableList<TransferSegmentRange> transferredSegmentRanges =\n+                updateNotTransferredSegmentRangeStatus(newTransferSegmentStatus,\n+                        beforeTransferRanges);\n+        // Transform the segment ranges back into the transfer segments.\n+        return toSegments(transferredSegmentRanges);\n+    }\n \n-            // If a segment is not NOT_TRANSFERRED -> return it as is.\n-            if (newStatus.getSegmentState() != NOT_TRANSFERRED) {\n-                afterTransferSegments.add(segment);\n-                continue;\n-            }\n \n-            // Get all the unknown addresses for this segment.\n-            List<Long> unknownAddressesInRange =\n-                    getUnknownAddressesInRange(segment.getStartAddress(),\n-                            segment.getEndAddress());\n-\n-            // If no addresses to transfer - mark a segment as transferred.\n-            if (unknownAddressesInRange.isEmpty()) {\n-                log.debug(\"All addresses are present in the range: [{}, {}], skipping transfer.\",\n-                        segment.getStartAddress(), segment.getEndAddress());\n-                long totalTransferred = segment.getTotal();\n-\n-                newStatus = TransferSegmentStatus\n-                        .builder()\n-                        .segmentState(TRANSFERRED)\n-                        .totalTransferred(totalTransferred)\n-                        .build();\n-            } else {\n-                // Get total number of addresses needed to transfer.\n-                long numAddressesToTransfer = unknownAddressesInRange.size();\n-                // Create a transferBatchRequest stream.\n-                Stream<TransferBatchRequest> batchStream = Lists\n-                        .partition(unknownAddressesInRange, batchSize)\n-                        .stream()\n-                        .map(groupedAddresses ->\n-                                new TransferBatchRequest(groupedAddresses, Optional.empty())\n-                        );\n-                // Execute state transfer synchronously.\n-                newStatus = synchronousStateTransfer(batchStream, numAddressesToTransfer);\n-            }\n+    public interface TransferSegmentRange {\n+        /**\n+         * Turn this range into a segment.\n+         *\n+         * @return A transfer segment\n+         */\n+        TransferSegment toTransferSegment();\n+\n+        /**\n+         * Turn this range into a single range.\n+         *\n+         * @return A list of one or two single ranges.\n+         */\n+        List<TransferSegmentRangeSingle> toSingle();\n+\n+        /**\n+         * Update the current transfer segment range with a provided status.\n+         *\n+         * @param newStatus A new status\n+         * @return An updated transfer segment range.\n+         */\n+        TransferSegmentRange updateStatus(TransferSegmentStatus newStatus);\n+\n+        /**\n+         * Get the status of this transfer segment range.\n+         *\n+         * @return A transfer status.\n+         */\n+        TransferSegmentStatus getStatus();\n+    }\n \n-            TransferSegment currentSegment = TransferSegment\n+    /**\n+     * A data class that represents a range split of a bounded transfer segment.\n+     */\n+    @EqualsAndHashCode\n+    @Getter\n+    @ToString\n+    @Builder(toBuilder = true)\n+    @AllArgsConstructor(access = AccessLevel.PRIVATE)\n+    public static class TransferSegmentRangeSplit implements TransferSegmentRange {\n+        @NonNull\n+        private final Tuple<TransferSegmentRangeSingle, TransferSegmentRangeSingle> splitSegments;\n+\n+        @Override\n+        public TransferSegment toTransferSegment() {\n+            return TransferSegment\n                     .builder()\n-                    .startAddress(segment.getStartAddress())\n-                    .endAddress(segment.getEndAddress())\n-                    .status(newStatus)\n+                    .startAddress(splitSegments.first.getStartAddress())\n+                    .endAddress(splitSegments.second.getEndAddress())\n+                    .status(splitSegments.first.status.toBuilder().build())\n+                    .logUnitServers(splitSegments.first.availableServers.orElse(ImmutableList.of()))\n                     .build();\n+        }\n \n-            afterTransferSegments.add(currentSegment);\n+        @Override\n+        public List<TransferSegmentRangeSingle> toSingle() {\n+            return ImmutableList.of(splitSegments.first, splitSegments.second);\n+        }\n+\n+        @Override\n+        public TransferSegmentRange updateStatus(TransferSegmentStatus newStatus) {\n+            TransferSegmentRangeSingle first = splitSegments.first.toBuilder()\n+                    .status(newStatus).build();\n+            TransferSegmentRangeSingle second = splitSegments.second.toBuilder()\n+                    .status(newStatus).build();\n+            return this.toBuilder()\n+                    .splitSegments(new Tuple<>(first, second)).build();\n+        }\n \n-            if (currentSegment.getStatus().getSegmentState() == FAILED) {\n-                // A segment failed -> short circuit.\n-                break;\n+        @Override\n+        public TransferSegmentStatus getStatus() {\n+            return splitSegments.first.status;\n+        }\n+\n+        public static class TransferSegmentRangeSplitBuilder {\n+\n+            public void verify() {\n+                if (splitSegments.first == null || splitSegments.second == null) {\n+                    throw new IllegalStateException(\"Both ranges should be defined.\");\n+                }\n+                TransferSegmentRangeSingle firstRange = splitSegments.first;\n+                TransferSegmentRangeSingle secondRange = splitSegments.second;\n+\n+                if (!firstRange.split) {\n+                    throw new IllegalStateException(\"First range should be split.\");\n+                }\n+\n+                if (!secondRange.split) {\n+                    throw new IllegalStateException(\"Second range should be split.\");\n+                }\n+\n+                if (firstRange.getEndAddress() + 1 != secondRange.getStartAddress()) {\n+                    throw new IllegalStateException(String.format(\"End address of a first: %s and \" +\n+                                    \"start address of a second: %s \" +\n+                                    \"are not consecutive.\", firstRange.getEndAddress(),\n+                            secondRange.getStartAddress()));\n+                }\n+                if (!firstRange.getStatus().equals(secondRange.getStatus())) {\n+                    throw new IllegalStateException(String.format(\"Status of a first: %s and \" +\n+                                    \"status of a second: %s \" +\n+                                    \"are not the same.\", firstRange.getStatus(),\n+                            secondRange.getStatus()));\n+                }\n+\n+                if (firstRange.getTypeOfTransfer() != CONSISTENT_READ) {\n+                    throw new IllegalStateException(\"First range should be of a type \" +\n+                            \"consistent read.\");\n+                }\n+\n+                if (secondRange.getTypeOfTransfer() != PROTOCOL_READ) {\n+                    throw new IllegalStateException(\"Second range type should be \" +\n+                            \"of a type protocol read.\");\n+                }\n+            }\n+\n+            public TransferSegmentRangeSplit build() {\n+                verify();\n+                return new TransferSegmentRangeSplit(splitSegments);\n             }\n         }\n-        return ImmutableList.copyOf(afterTransferSegments);\n     }\n \n     /**\n-     * Given a stream of batch requests, and a total number\n-     * of transferred addresses needed, execute a state transfer synchronously.\n-     *\n-     * @param batchStream A stream of batch requests.\n-     * @param totalNeeded A total number of addresses needed for transfer.\n-     * @return A status representing a final status of a transferred segment.\n+     * A data class that represents a single part of a bounded transfer segment.\n      */\n-    @VisibleForTesting\n-    TransferSegmentStatus synchronousStateTransfer(\n-            Stream<TransferBatchRequest> batchStream, long totalNeeded) {\n-        long accTransferred = 0L;\n-\n-        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n-\n-        while (iterator.hasNext()) {\n-            TransferBatchRequest nextBatch = iterator.next();\n-            TransferBatchResponse response =\n-                    batchProcessor.transfer(nextBatch).join();\n-            // In case of an error that is not handled by a batch processor, e.g. WrongEpochException,\n-            // we return a failed segment status to the caller and the exception. This exception is\n-            // thrown in a retry block of a RestoreRedundancyMergeSegments' restoreWithBackOff\n-            // method, which results in a retry.\n-            // The layout will be invalidated and only the non-transferred addresses of the\n-            // segment will be considered for the subsequent transfer.\n-            if (response.getStatus() == TransferStatus.FAILED) {\n-                Optional<TransferSegmentException> causeOfFailure =\n-                        Optional.of(response.getCauseOfFailure()\n-                                .map(TransferSegmentException::new)\n-                                .orElse(new TransferSegmentException(\"Failed to transfer.\")));\n-\n-                return TransferSegmentStatus\n-                        .builder()\n-                        .totalTransferred(accTransferred)\n-                        .segmentState(FAILED)\n-                        .causeOfFailure(causeOfFailure)\n-                        .build();\n-            }\n-            accTransferred += response.getTransferBatchRequest().getAddresses().size();\n-        }\n+    @EqualsAndHashCode\n+    @Getter\n+    @ToString\n+    @Builder(toBuilder = true)\n+    @AllArgsConstructor(access = AccessLevel.PRIVATE)\n+    public static class TransferSegmentRangeSingle implements TransferSegmentRange {", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5Mjg2Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439892863", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4OTQxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzY2Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437647667", "bodyText": "As mentioned in previous comment, if using plain CFUtils, which throws RuntimeExecption, then you don't need to worry about this.", "author": "WenbinZhu", "createdAt": "2020-06-09T18:52:47Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/StateTransferManager.java", "diffHunk": "@@ -71,140 +91,385 @@\n      */\n     @VisibleForTesting\n     ImmutableList<Long> getUnknownAddressesInRange(long rangeStart, long rangeEnd) {\n-        Set<Long> knownAddresses = logUnitClient\n-                .requestKnownAddresses(rangeStart, rangeEnd).join().getKnownAddresses();\n+        try {\n+            Set<Long> knownAddresses = CFUtils.getUninterruptibly(logUnitClient\n+                            .requestKnownAddresses(rangeStart, rangeEnd),\n+                    TimeoutException.class,\n+                    NetworkException.class,\n+                    WrongEpochException.class)\n+                    .getKnownAddresses();\n+\n+            return LongStream.range(rangeStart, rangeEnd + 1L)\n+                    .filter(address -> !knownAddresses.contains(address))\n+                    .boxed()\n+                    .collect(ImmutableList.toImmutableList());\n+        } catch (TimeoutException | NetworkException | WrongEpochException e) {\n+            // This function is called within a lambda expression, which does\n+            // not allow checked exceptions. So we wrap all the exceptions that can occur\n+            // here it the UnknownAddressInRangeException.\n+            throw new UnknownAddressInRangeException(e);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NDA2MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439894060", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzY2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NDE1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437664158", "bodyText": "It looks unnecessary to first transform them to single and split ranges separately in prepareTransferWorkload(), and then transform them all into single range in handleTransfer(). I think you can transform them all into single ranges (for the split ones, directly transform into two single ranges, not a split range) in the first step, then you can remove the split range class and the extra transform step.", "author": "WenbinZhu", "createdAt": "2020-06-09T19:23:06Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -170,22 +157,123 @@ Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferMan\n                 }\n             } catch (TransferSegmentException e) {\n                 throw new RetryExhaustedException(\"Transfer segment exception occurred.\", e);\n+            } catch (IllegalStateException e) {\n+                throw new RetryExhaustedException(\"The current node is in the unresponsive list.\", e);\n             }\n-\n         }).setOptions(retrySettings).run();\n+    }\n \n+    /**\n+     * Merge the state transfer segments and propose the new cluster layout.\n+     *\n+     * @param baseClient             Base client to the current node.\n+     * @param layoutManagementView   Layout management view.\n+     * @param layoutTransferSegments Layout and the transferred segments.\n+     */\n+    private void mergeSegments(BaseClient baseClient, LayoutManagementView layoutManagementView,\n+                               LayoutTransferSegments layoutTransferSegments) {\n+        Layout currentLayout = layoutTransferSegments.getLayout();\n+        ImmutableList<TransferSegment> transferredSegments =\n+                layoutTransferSegments.getTransferSegments();\n+\n+        // Seal the next epoch for the current server.\n+        // This is done in order to prevent the unnecessary OutRankedExceptions during the\n+        // layout reconfiguration that can happen if the min server set is sealed but the current node\n+        // was not part of this set.\n+        CFUtils.getUninterruptibly(baseClient.sealRemoteServer(currentLayout.getEpoch() + 1));\n+\n+        // State transfer did not happen. Try merging segments if possible.\n+        if (transferredSegments.isEmpty()) {\n+            log.info(\"State transfer on: {}: No transfer occurred, \" +\n+                    \"try merging the segments.\", currentNode);\n+            layoutManagementView.mergeSegments(currentLayout);\n+        }\n+        // State transfer happened.\n+        else {\n+            log.info(\"State transfer on {}: Transferred segments: {}.\", currentNode,\n+                    transferredSegments);\n+            // Create a new layout after the segments were transferred.\n+            // After this action is performed a current node will be present\n+            // in all the segments that previously had a status 'TRANSFERRED'.\n+            Layout newLayout = redundancyCalculator.updateLayoutAfterRedundancyRestoration(\n+                    transferredSegments, currentLayout);\n+\n+            log.info(\"State transfer on {}: New layout: {}.\", currentNode, newLayout);\n+\n+            // Merge the segments of the new layout if possible.\n+            if (RedundancyCalculator.canMergeSegments(newLayout)) {\n+                layoutManagementView.mergeSegments(newLayout);\n+            }\n+            // If the segments can't be merged, just propose a new layout.\n+            else {\n+                // Since we seal with a new epoch,\n+                // we also need to bump the epoch of the new layout.\n+                LayoutBuilder builder = new LayoutBuilder(newLayout);\n+                newLayout = builder.setEpoch(currentLayout.getEpoch() + 1).build();\n+                layoutManagementView\n+                        .runLayoutReconfiguration(currentLayout, newLayout,\n+                                false);\n+            }\n+        }\n     }\n \n+    /**\n+     * Perform a state transfer on a current node, if needed, and then\n+     * propose a new layout based on a transfer result.\n+     * If a state transfer was not needed, try merging the segments\n+     * of a current layout and then proposing it.\n+     *\n+     * @param runtime         A corfu runtime.\n+     * @param transferManager A transfer manager that runs the state transfer.\n+     * @return A new layout, if a redundancy restoration occurred; a current layout otherwise.\n+     */\n+    Layout restore(CorfuRuntime runtime, StateTransferManager transferManager)\n+            throws InterruptedException {\n+\n+        LayoutTransferSegments layoutTransferSegments = performStateTransfer(runtime, transferManager);\n+        BaseClient baseClient = runtime.getLayoutView()\n+                .getRuntimeLayout(layoutTransferSegments.getLayout())\n+                .getBaseClient(currentNode);\n+        mergeSegments(baseClient, runtime.getLayoutManagementView(), layoutTransferSegments);\n+        runtime.invalidateLayout();\n+        return runtime.getLayoutView().getLayout();\n+    }\n+\n+\n+    /**\n+     * Perform a state transfer for the current node.\n+     * Return all the segments that are transferred.\n+     * Throw an exception if at least one of the segments have failed.\n+     *\n+     * @param transferManager Transfer manager.\n+     * @param currentLayout   Current layout.\n+     * @param trimMark        Current cluster trim mark.\n+     * @param committedTail   An optional cluster committed tail.\n+     * @return List of transferred segments.\n+     */\n     private List<TransferSegment> getTransferSegments(\n-            StateTransferManager transferManager, Layout currentLayout, long trimMark) {\n-        // Create a pre transfer state list.\n+            StateTransferManager transferManager, Layout currentLayout, long trimMark,\n+            Optional<Long> committedTail) {\n+        // Create a pre transfer segment list.\n         ImmutableList<TransferSegment> preTransferList =\n                 redundancyCalculator.createStateList(currentLayout, trimMark);\n \n-        // Perform a state transfer for each segment synchronously and update the state list.\n+        log.info(\"getTransferSegments: pre-transfer list: {}\", preTransferList);\n+        // Create a transfer workload. This will turn the list of segments into the list of\n+        // ranges. The reason for this is the fact that a present committed tail can split the segment\n+        // into the committed and non-committed halves that should be handled differently\n+        // during a state transfer.\n+        ImmutableList<TransferSegmentRange> transferSegmentRanges = redundancyCalculator\n+                .prepareTransferWorkload(preTransferList, committedTail);\n+\n+        log.info(\"getTransferSegments: transfer workload: {}\", transferSegmentRanges);\n+\n+        // Perform a state transfer on the transfer segment ranges and return the updated\n+        // transferred transfer segment list.\n         ImmutableList<TransferSegment> transferList = transferManager\n-                .handleTransfer(preTransferList);\n+                .handleTransfer(transferSegmentRanges);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0OTExNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439649116", "bodyText": "This interface specifically defines the data classes that fit the domain in order to do two things:\n\nTransform from segments to ranges in order to perform state transfer.\nTransform back from ranges to segments (whether they are split or not) to allow for merging these segments later into the layout.\n\nThe solution that you are proposing will result in creating the lists of lists of single transfer segments that will require extra error-prone logic in both cases of handling the actual transfer and merging them back.", "author": "PavelZaytsev", "createdAt": "2020-06-12T21:26:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NDE1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5NzkyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437797922", "bodyText": "This log does not contain any information about the batch that are consuming, consider remove it or add more batch information.", "author": "WenbinZhu", "createdAt": "2020-06-10T00:42:17Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();\n+                        if (response.getStatus() == FAILED) {\n+                            String errorMsg = String.format(\"Failed request %s\",\n+                                    response.getTransferBatchRequest());\n+                            StateTransferBatchProcessorException ex = response\n+                                    .getCauseOfFailure()\n+                                    .orElse(new StateTransferBatchProcessorException(errorMsg));\n+                            log.error(\"runConsumer: consumer task finished with error:\", ex);\n+                            throw ex;\n+                        }\n+                    } else if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == POISON_PILL) {\n+                        // Got a poison pill message, finish gracefully.\n+                        log.debug(\"runConsumer: consumer task finished successfully.\");\n+                        transferInProgress = false;\n+                    } else if (maybeRequest.isPresent()) {\n+                        log.error(\"runConsumer: Unrecognized message: {}.\",\n+                                maybeRequest.get().getTransferBatchType());\n+                        throw new IllegalStateException(\"Message \" + maybeRequest.get() +\n+                                \" is unrecognized.\");\n+                    } else {\n+                        throw new TimeoutException(\"runConsumer: consumer timed out \" +\n+                                \"polling a message.\");\n+                    }\n+\n+                } catch (InterruptedException ie) {\n+                    log.error(\"runConsumer: consumer task was interrupted.\");\n+                    Thread.currentThread().interrupt();\n+                    throw new TransferConsumerException(ie);\n+                } catch (TimeoutException | IllegalStateException e) {\n+                    throw new TransferConsumerException(e);\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Send numConsumers poison pill messages to the queue to shutdown the consumers gracefully.\n+     *\n+     * @param numConsumers The number of consumers waiting on the other end of the queue.\n+     * @throws InterruptedException If the current thread is interrupted.\n+     * @throws TimeoutException     If times out while offering a pill to the queue.\n+     */\n+    private void finishConsumers(int numConsumers) throws InterruptedException, TimeoutException {\n+        log.debug(\"finishConsumers: producer finishing consumer tasks.\");", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NDcxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439894715", "bodyText": "Was removed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:47:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5NzkyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgxMTkzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437811935", "bodyText": "I think the offerTimeout here is probably making the code fragile and problematic. You have a 10 seconds producer offer timeout, which means if the consumers cannot finish the operation in 10 seconds, it will fail. However the transfer logic in consumer includes retry, from what I've seen you have 3 read retries in the lower level batch processor, which can adds up to 15 seconds timout (remember our RPC request timeout is 5 seconds). So the problem here is your lower level retries numbers are not respected, you basically have 2 sets of timeout mechanism over the same thing. I think there is no need to have an offer timeout here, the producer can simply block on consumers, until there is a free slot, since consumers are bounded  by their RPC timeout.", "author": "WenbinZhu", "createdAt": "2020-06-10T01:37:49Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();\n+                        if (response.getStatus() == FAILED) {\n+                            String errorMsg = String.format(\"Failed request %s\",\n+                                    response.getTransferBatchRequest());\n+                            StateTransferBatchProcessorException ex = response\n+                                    .getCauseOfFailure()\n+                                    .orElse(new StateTransferBatchProcessorException(errorMsg));\n+                            log.error(\"runConsumer: consumer task finished with error:\", ex);\n+                            throw ex;\n+                        }\n+                    } else if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == POISON_PILL) {\n+                        // Got a poison pill message, finish gracefully.\n+                        log.debug(\"runConsumer: consumer task finished successfully.\");\n+                        transferInProgress = false;\n+                    } else if (maybeRequest.isPresent()) {\n+                        log.error(\"runConsumer: Unrecognized message: {}.\",\n+                                maybeRequest.get().getTransferBatchType());\n+                        throw new IllegalStateException(\"Message \" + maybeRequest.get() +\n+                                \" is unrecognized.\");\n+                    } else {\n+                        throw new TimeoutException(\"runConsumer: consumer timed out \" +\n+                                \"polling a message.\");\n+                    }\n+\n+                } catch (InterruptedException ie) {\n+                    log.error(\"runConsumer: consumer task was interrupted.\");\n+                    Thread.currentThread().interrupt();\n+                    throw new TransferConsumerException(ie);\n+                } catch (TimeoutException | IllegalStateException e) {\n+                    throw new TransferConsumerException(e);\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Send numConsumers poison pill messages to the queue to shutdown the consumers gracefully.\n+     *\n+     * @param numConsumers The number of consumers waiting on the other end of the queue.\n+     * @throws InterruptedException If the current thread is interrupted.\n+     * @throws TimeoutException     If times out while offering a pill to the queue.\n+     */\n+    private void finishConsumers(int numConsumers) throws InterruptedException, TimeoutException {\n+        log.debug(\"finishConsumers: producer finishing consumer tasks.\");\n+        for (int i = 0; i < numConsumers; i++) {\n+            TransferBatchRequest poisonPill =\n+                    TransferBatchRequest.builder().transferBatchType(POISON_PILL).build();\n+            if (!queue.offer(poisonPill, offerTimeOutInSeconds, units)) {\n+                throw new TimeoutException(\"finishConsumers: producer timed\" +\n+                        \" out waiting to offer a poison pill message.\");\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Get the next transfer batch request and offer it to the blocking queue.\n+     * Once done, finish the transfer by sending the poison pill messages.\n+     *\n+     * @param batchStreamIterator A stream of transfer batch requests.\n+     * @param numConsumers        A number of consumers on the other end of the queue.\n+     * @return A completable future that completes successfully if the stream is processed\n+     * and all the poison pill messages are sent, an exceptionally completed future otherwise.\n+     */\n+    CompletableFuture<Void> runProducer(Iterator<TransferBatchRequest> batchStreamIterator,\n+                                        int numConsumers) {\n+        return CompletableFuture.runAsync(() -> {\n+            try {\n+                while (batchStreamIterator.hasNext()) {\n+                    TransferBatchRequest request = batchStreamIterator.next();\n+                    if (!queue.offer(request, offerTimeOutInSeconds, units)) {\n+                        throw new TimeoutException(\"runProducer: producer timed\" +\n+                                \" out waiting to offer a batch.\");", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NDczOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439894739", "bodyText": "Was removed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:47:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgxMTkzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyMjE4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437822188", "bodyText": "Do you need this parameter? You can use while(true) and return here when you encounter POINSON_PILL.", "author": "WenbinZhu", "createdAt": "2020-06-10T02:18:21Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();\n+                        if (response.getStatus() == FAILED) {\n+                            String errorMsg = String.format(\"Failed request %s\",\n+                                    response.getTransferBatchRequest());\n+                            StateTransferBatchProcessorException ex = response\n+                                    .getCauseOfFailure()\n+                                    .orElse(new StateTransferBatchProcessorException(errorMsg));\n+                            log.error(\"runConsumer: consumer task finished with error:\", ex);\n+                            throw ex;\n+                        }\n+                    } else if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == POISON_PILL) {\n+                        // Got a poison pill message, finish gracefully.\n+                        log.debug(\"runConsumer: consumer task finished successfully.\");\n+                        transferInProgress = false;", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NDc3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439894779", "bodyText": "Was removed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:48:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyMjE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyOTIxNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437829216", "bodyText": "This is a blocking call and the number of consumer is fixed to be 2. This means you only have at most 2 in flight transfer requests, which seems too conservative in terms of throughput. Our log unit server can process multiple requests at a time, if you have 2 request at a time, each send to one logunit (random selection), then on average a log unit processes only one state transfer request at a time, which is too conservative. You can increase the number of consumers, or,  each consumer can send out K non-blocking requests and wait for these K futures to succeed and then poll next K requests. Then you can have 2K requests in flight.\nBy the way, I think it will make your code much simpler and if you have only one consumer that keeps iterating and no producer at all, while still allow only K in-flight requests. Basically you have a counter (a semaphore), the consumer take a request from the iterator and then issue a non-blocking call, get a future for the request and move on. By using the counter, the consumer will be blocked if number of in-flight requests == K, but once any future completes. It will be unblocked, and be able to consume on more request. Basically it looks like following:\nSemaphore semaphore = new Semaphore(K);\nOptional<Exception> exp = Optional.empty(); \nCompletableFuture<Void> allFutures = CompletableFuture.completedFuture(null);\n\nwhile (it.hasNext() && !exp.isPresent()) {\n    semaphore.acquire();\n    request = it.next();\n\n    CompletableFuture cf = CompletableFuture.runAsync(() -> {\n        batchProcessor.transfer(request);\n    }).exceptionally(ex -> {\n        exp = Optional.of(ex);\n        throw new SomeException();\n    }).thenRunAsync(() -> {\n        semaphore.release();\n    });\n\n    allFutures = CompletableFuture.allOf(allFurures, cf);\n}\n\n// Probably not needed as allFutures will capture the exception.\nif (exp.isPresent) {\n    reuturn ...;\n}\n\n// if any sub-furture completes exceptionally, allFuture will stop waiting and finish.\nallFutures.handle((ex, v) -> {\n    ....;\n}); \nWith only one consumer and no producer, this looks much simplier right? You can take a look and consider it as an alternative.", "author": "WenbinZhu", "createdAt": "2020-06-10T02:45:01Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NDk5NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439894994", "bodyText": "I agree this approach looks better. The reason I introduced a blocking queue is to have a fixed number of inflight messages. I did not think about using more lightweight concurrency primitives for this.", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyOTIxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjE5NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437832195", "bodyText": "This code seems redundant, allTasks is returned from allof(), which ensures if enither future fails, allTasks also fails.\nFrom allOf() javadoc:\n/**\n* Returns a new CompletableFuture that is completed when all of\n* the given CompletableFutures complete.  If any of the given\n* CompletableFutures complete exceptionally, then the returned\n* CompletableFuture also does so, with a CompletionException\n* holding this exception as its cause.\n**/", "author": "WenbinZhu", "createdAt": "2020-06-10T02:56:26Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();\n+                        if (response.getStatus() == FAILED) {\n+                            String errorMsg = String.format(\"Failed request %s\",\n+                                    response.getTransferBatchRequest());\n+                            StateTransferBatchProcessorException ex = response\n+                                    .getCauseOfFailure()\n+                                    .orElse(new StateTransferBatchProcessorException(errorMsg));\n+                            log.error(\"runConsumer: consumer task finished with error:\", ex);\n+                            throw ex;\n+                        }\n+                    } else if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == POISON_PILL) {\n+                        // Got a poison pill message, finish gracefully.\n+                        log.debug(\"runConsumer: consumer task finished successfully.\");\n+                        transferInProgress = false;\n+                    } else if (maybeRequest.isPresent()) {\n+                        log.error(\"runConsumer: Unrecognized message: {}.\",\n+                                maybeRequest.get().getTransferBatchType());\n+                        throw new IllegalStateException(\"Message \" + maybeRequest.get() +\n+                                \" is unrecognized.\");\n+                    } else {\n+                        throw new TimeoutException(\"runConsumer: consumer timed out \" +\n+                                \"polling a message.\");\n+                    }\n+\n+                } catch (InterruptedException ie) {\n+                    log.error(\"runConsumer: consumer task was interrupted.\");\n+                    Thread.currentThread().interrupt();\n+                    throw new TransferConsumerException(ie);\n+                } catch (TimeoutException | IllegalStateException e) {\n+                    throw new TransferConsumerException(e);\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Send numConsumers poison pill messages to the queue to shutdown the consumers gracefully.\n+     *\n+     * @param numConsumers The number of consumers waiting on the other end of the queue.\n+     * @throws InterruptedException If the current thread is interrupted.\n+     * @throws TimeoutException     If times out while offering a pill to the queue.\n+     */\n+    private void finishConsumers(int numConsumers) throws InterruptedException, TimeoutException {\n+        log.debug(\"finishConsumers: producer finishing consumer tasks.\");\n+        for (int i = 0; i < numConsumers; i++) {\n+            TransferBatchRequest poisonPill =\n+                    TransferBatchRequest.builder().transferBatchType(POISON_PILL).build();\n+            if (!queue.offer(poisonPill, offerTimeOutInSeconds, units)) {\n+                throw new TimeoutException(\"finishConsumers: producer timed\" +\n+                        \" out waiting to offer a poison pill message.\");\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Get the next transfer batch request and offer it to the blocking queue.\n+     * Once done, finish the transfer by sending the poison pill messages.\n+     *\n+     * @param batchStreamIterator A stream of transfer batch requests.\n+     * @param numConsumers        A number of consumers on the other end of the queue.\n+     * @return A completable future that completes successfully if the stream is processed\n+     * and all the poison pill messages are sent, an exceptionally completed future otherwise.\n+     */\n+    CompletableFuture<Void> runProducer(Iterator<TransferBatchRequest> batchStreamIterator,\n+                                        int numConsumers) {\n+        return CompletableFuture.runAsync(() -> {\n+            try {\n+                while (batchStreamIterator.hasNext()) {\n+                    TransferBatchRequest request = batchStreamIterator.next();\n+                    if (!queue.offer(request, offerTimeOutInSeconds, units)) {\n+                        throw new TimeoutException(\"runProducer: producer timed\" +\n+                                \" out waiting to offer a batch.\");\n+                    }\n+                }\n+                finishConsumers(numConsumers);\n+\n+            } catch (InterruptedException ie) {\n+                log.error(\"runProducer: producer interrupted.\");\n+                Thread.currentThread().interrupt();\n+                throw new TransferProducerException(ie);\n+            } catch (TimeoutException toe) {\n+                log.error(\"runProducer: producer could not offer a payload within \" +\n+                        \"a given time range.\");\n+                throw new TransferProducerException(toe);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Run multiple consumers and one producer tasks, performing the batch transfer\n+     * for each of the requests\n+     * in the stream. Only when all the tasks are completed successfully,\n+     * return a TransferProcessorResult of type TRANSFER_SUCCEEDED,\n+     * otherwise return a TransferProcessorResult of type TRANSFER_FAILED.\n+     *\n+     * @param batchStream A stream of batch requests.\n+     * @return A transfer processor result with a state TRANSFER_SUCCEEDED if\n+     * all tasks finish successfully,\n+     * a transfer processor with a state TRANSFER_FAILED and the cause of failure otherwise.\n+     */\n+    @Override\n+    public CompletableFuture<TransferProcessorResult> runStateTransfer(\n+            Stream<TransferBatchRequest> batchStream) {\n+\n+        // Get a transfer batch request stream iterator\n+        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n+\n+        // No need to spawn tasks if there is no work to do.\n+        if (!iterator.hasNext()) {\n+            return CompletableFuture.completedFuture(TransferProcessorResult.builder()\n+                    .transferState(TRANSFER_SUCCEEDED).build());\n+        }\n+\n+        // Run number of consumer tasks\n+        List<CompletableFuture<Void>> consumerTasksToFutureList = IntStream.range(0, numConsumers)\n+                .boxed()\n+                .map(i -> runConsumer()).collect(Collectors.toList());\n+\n+        // Run a producer task\n+        CompletableFuture<Void> producerTaskFuture\n+                = runProducer(iterator, numConsumers);\n+\n+        // Sequence the consumer tasks\n+        CompletableFuture<List<Void>> consumersTaskFuture =\n+                CFUtils.sequence(consumerTasksToFutureList);\n+\n+        // Aggregate all the tasks\n+        CompletableFuture<Void> allTasks =\n+                CompletableFuture.allOf(producerTaskFuture, consumersTaskFuture);\n+\n+        // If either the producer or one or more consumer tasks fail -> fail all the tasks.\n+        Stream.of(producerTaskFuture, consumersTaskFuture)\n+                .forEach(future -> future.exceptionally(e -> {\n+                    allTasks.completeExceptionally(e);\n+                    return null;\n+                }));", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTA4Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895087", "bodyText": "Was removed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:49:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjgxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437832810", "bodyText": "Instead of converting to CFUtils.sequence() and then use allOf(), you can have an array of all individual consumer and producer futures and simply use a allOf() call. And as mentioned in previous comment, the aggregated future will fail if any of the consumer of producer future fails.", "author": "WenbinZhu", "createdAt": "2020-06-10T02:59:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,265 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferConsumerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferProducerException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.POISON_PILL;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by distributing\n+ * the workload among the multiple batch processor consumer tasks via a blocking queue.\n+ */\n+@Slf4j\n+@Getter\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A blocking queue for the producer-consumer interaction.\n+     */\n+    private final BlockingQueue<TransferBatchRequest> queue;\n+    /**\n+     * A time in seconds it takes for a producer to timeout while waiting to offer a next data batch\n+     * or a poison pill message if the queue is full.\n+     */\n+    private final long offerTimeOutInSeconds;\n+    /**\n+     * A time in seconds it takes for a consumer to timeout after waiting to poll the next data batch\n+     * or a poison pill message if the queue is empty.\n+     */\n+    private final long pollTimeOutInSeconds;\n+    /**\n+     * Units for the timeouts.\n+     */\n+    private final TimeUnit units = TimeUnit.SECONDS;\n+    /**\n+     * A configured batch processor that handles the transfer of a batch of addresses\n+     * (via direct read or a protocol), as well as all the required retry and error handling logic\n+     * during it.\n+     */\n+    private final StateTransferBatchProcessor batchProcessor;\n+    /**\n+     * A number of competing consumers that perform the batch state transfer in parallel.\n+     */\n+    private final int numConsumers;\n+\n+    public ParallelTransferProcessor(int queueSize, StateTransferBatchProcessor committedBatchProcessor) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        offerTimeOutInSeconds = 10L;\n+        pollTimeOutInSeconds = 2L;\n+    }\n+\n+    public ParallelTransferProcessor(int queueSize,\n+                                     StateTransferBatchProcessor committedBatchProcessor,\n+                                     long offerTimeOutInSeconds, long pollTimeOutInSeconds) {\n+        numConsumers = queueSize;\n+        queue = new LinkedBlockingDeque<>(queueSize);\n+        batchProcessor = committedBatchProcessor;\n+        this.offerTimeOutInSeconds = offerTimeOutInSeconds;\n+        this.pollTimeOutInSeconds = pollTimeOutInSeconds;\n+    }\n+\n+    /**\n+     * Poll the blocking queue to get the next transfer batch request.\n+     * If the poll times out, finish the current task with a TimeoutException.\n+     * If the request is retrieved, and its of type DATA - transfer it with a batch processor.\n+     * If this batch processor fails to transfer a batch, fail the current task with the\n+     * offending exception.\n+     * If the request is retrieved and its of a type POISON_PILL - finish this\n+     * task gracefully.\n+     * Otherwise - throw an exception.\n+     *\n+     * @return A completed future if all the DATA requests has been processed\n+     * and the POISON_PILL message is received, an exceptionally completed\n+     * future otherwise.\n+     */\n+    private CompletableFuture<Void> runConsumer() {\n+        return CompletableFuture.runAsync(() -> {\n+            boolean transferInProgress = true;\n+            while (transferInProgress) {\n+                try {\n+                    // Wait up to pollTimeOutInSeconds seconds to get the next message.\n+                    Optional<TransferBatchRequest> maybeRequest =\n+                            Optional.ofNullable(queue.poll(pollTimeOutInSeconds, units));\n+                    if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == DATA) {\n+                        // Got a data request.\n+                        // Perform a transfer with a given batch processor.\n+                        // If transfer fails, complete exceptionally.\n+                        TransferBatchResponse response = batchProcessor.transfer(maybeRequest.get())\n+                                .join();\n+                        if (response.getStatus() == FAILED) {\n+                            String errorMsg = String.format(\"Failed request %s\",\n+                                    response.getTransferBatchRequest());\n+                            StateTransferBatchProcessorException ex = response\n+                                    .getCauseOfFailure()\n+                                    .orElse(new StateTransferBatchProcessorException(errorMsg));\n+                            log.error(\"runConsumer: consumer task finished with error:\", ex);\n+                            throw ex;\n+                        }\n+                    } else if (maybeRequest.isPresent() &&\n+                            maybeRequest.get().getTransferBatchType() == POISON_PILL) {\n+                        // Got a poison pill message, finish gracefully.\n+                        log.debug(\"runConsumer: consumer task finished successfully.\");\n+                        transferInProgress = false;\n+                    } else if (maybeRequest.isPresent()) {\n+                        log.error(\"runConsumer: Unrecognized message: {}.\",\n+                                maybeRequest.get().getTransferBatchType());\n+                        throw new IllegalStateException(\"Message \" + maybeRequest.get() +\n+                                \" is unrecognized.\");\n+                    } else {\n+                        throw new TimeoutException(\"runConsumer: consumer timed out \" +\n+                                \"polling a message.\");\n+                    }\n+\n+                } catch (InterruptedException ie) {\n+                    log.error(\"runConsumer: consumer task was interrupted.\");\n+                    Thread.currentThread().interrupt();\n+                    throw new TransferConsumerException(ie);\n+                } catch (TimeoutException | IllegalStateException e) {\n+                    throw new TransferConsumerException(e);\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Send numConsumers poison pill messages to the queue to shutdown the consumers gracefully.\n+     *\n+     * @param numConsumers The number of consumers waiting on the other end of the queue.\n+     * @throws InterruptedException If the current thread is interrupted.\n+     * @throws TimeoutException     If times out while offering a pill to the queue.\n+     */\n+    private void finishConsumers(int numConsumers) throws InterruptedException, TimeoutException {\n+        log.debug(\"finishConsumers: producer finishing consumer tasks.\");\n+        for (int i = 0; i < numConsumers; i++) {\n+            TransferBatchRequest poisonPill =\n+                    TransferBatchRequest.builder().transferBatchType(POISON_PILL).build();\n+            if (!queue.offer(poisonPill, offerTimeOutInSeconds, units)) {\n+                throw new TimeoutException(\"finishConsumers: producer timed\" +\n+                        \" out waiting to offer a poison pill message.\");\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Get the next transfer batch request and offer it to the blocking queue.\n+     * Once done, finish the transfer by sending the poison pill messages.\n+     *\n+     * @param batchStreamIterator A stream of transfer batch requests.\n+     * @param numConsumers        A number of consumers on the other end of the queue.\n+     * @return A completable future that completes successfully if the stream is processed\n+     * and all the poison pill messages are sent, an exceptionally completed future otherwise.\n+     */\n+    CompletableFuture<Void> runProducer(Iterator<TransferBatchRequest> batchStreamIterator,\n+                                        int numConsumers) {\n+        return CompletableFuture.runAsync(() -> {\n+            try {\n+                while (batchStreamIterator.hasNext()) {\n+                    TransferBatchRequest request = batchStreamIterator.next();\n+                    if (!queue.offer(request, offerTimeOutInSeconds, units)) {\n+                        throw new TimeoutException(\"runProducer: producer timed\" +\n+                                \" out waiting to offer a batch.\");\n+                    }\n+                }\n+                finishConsumers(numConsumers);\n+\n+            } catch (InterruptedException ie) {\n+                log.error(\"runProducer: producer interrupted.\");\n+                Thread.currentThread().interrupt();\n+                throw new TransferProducerException(ie);\n+            } catch (TimeoutException toe) {\n+                log.error(\"runProducer: producer could not offer a payload within \" +\n+                        \"a given time range.\");\n+                throw new TransferProducerException(toe);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Run multiple consumers and one producer tasks, performing the batch transfer\n+     * for each of the requests\n+     * in the stream. Only when all the tasks are completed successfully,\n+     * return a TransferProcessorResult of type TRANSFER_SUCCEEDED,\n+     * otherwise return a TransferProcessorResult of type TRANSFER_FAILED.\n+     *\n+     * @param batchStream A stream of batch requests.\n+     * @return A transfer processor result with a state TRANSFER_SUCCEEDED if\n+     * all tasks finish successfully,\n+     * a transfer processor with a state TRANSFER_FAILED and the cause of failure otherwise.\n+     */\n+    @Override\n+    public CompletableFuture<TransferProcessorResult> runStateTransfer(\n+            Stream<TransferBatchRequest> batchStream) {\n+\n+        // Get a transfer batch request stream iterator\n+        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n+\n+        // No need to spawn tasks if there is no work to do.\n+        if (!iterator.hasNext()) {\n+            return CompletableFuture.completedFuture(TransferProcessorResult.builder()\n+                    .transferState(TRANSFER_SUCCEEDED).build());\n+        }\n+\n+        // Run number of consumer tasks\n+        List<CompletableFuture<Void>> consumerTasksToFutureList = IntStream.range(0, numConsumers)\n+                .boxed()\n+                .map(i -> runConsumer()).collect(Collectors.toList());\n+\n+        // Run a producer task\n+        CompletableFuture<Void> producerTaskFuture\n+                = runProducer(iterator, numConsumers);\n+\n+        // Sequence the consumer tasks\n+        CompletableFuture<List<Void>> consumersTaskFuture =\n+                CFUtils.sequence(consumerTasksToFutureList);\n+\n+        // Aggregate all the tasks\n+        CompletableFuture<Void> allTasks =\n+                CompletableFuture.allOf(producerTaskFuture, consumersTaskFuture);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTAzOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895039", "bodyText": "Was removed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMzE0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r437833146", "bodyText": "Add javadoc for public interface.", "author": "WenbinZhu", "createdAt": "2020-06-10T03:00:18Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/TransferProcessor.java", "diffHunk": "@@ -0,0 +1,42 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.Builder;\n+import lombok.Builder.Default;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.ToString;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Stream;\n+\n+import static lombok.EqualsAndHashCode.Exclude;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * An interface that runs the actual state transfer on the stream of transfer batch requests.\n+ */\n+public interface TransferProcessor {\n+\n+    @Getter\n+    @ToString\n+    @EqualsAndHashCode\n+    @Builder(toBuilder = true)\n+    class TransferProcessorResult {\n+        public enum TransferProcessorStatus {\n+            TRANSFER_FAILED,\n+            TRANSFER_SUCCEEDED\n+        }\n+\n+        @Default\n+        private final TransferProcessorStatus transferState = TRANSFER_SUCCEEDED;\n+\n+        @Default\n+        @Exclude\n+        private final Optional<TransferSegmentException> causeOfFailure = Optional.empty();\n+    }\n+\n+    CompletableFuture<TransferProcessorResult> runStateTransfer(Stream<TransferBatchRequest> batchStream);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTIwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895202", "bodyText": "Added", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:50:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMzE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Mzk5Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438463993", "bodyText": "NIT: Move this annoatation to after the javadoc.", "author": "WenbinZhu", "createdAt": "2020-06-10T23:37:38Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/BasicTransferProcessor.java", "diffHunk": "@@ -0,0 +1,48 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.AllArgsConstructor;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+@AllArgsConstructor", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTI2MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895261", "bodyText": "Done", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:50:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Mzk5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Njk2Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438466963", "bodyText": "private", "author": "WenbinZhu", "createdAt": "2020-06-10T23:47:56Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(addresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    public CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTM1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895356", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:51:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Njk2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NzU5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438477596", "bodyText": "The scheduling design here is fine for now as we are only doing random scheuling, but this seems not a flexible design as the scheduling decision is only done inside a batchRequest, which is not aware of the others. A flexible design would be that the batch processor is given a node endpoint to read data, and the node is selected by the global scheduler, which is aware of all the batches and their designated node and all the failures and designation history. That way the scheduling decision can be made more optimal, and it also works for random approach: the scheduler designates a random node to a batch, if that batch fails, it is (added back to the queue and then) retried by assigning a new random node to it. Anyway, this is an alternative design if you're interested.", "author": "WenbinZhu", "createdAt": "2020-06-11T00:25:57Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTYxNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895616", "bodyText": "Yeah, I did not introduce any advanced scheduling algos here.", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:52:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NzU5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MTAyMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438481023", "bodyText": "3 retries here seems too many, we can do 2 or even 1, since if it fails again, you can try another random node. 3 retries usually takes 15 seconds, while remeber the higher level processor queue has a producer timeout with 10 seconds, if you have 3 retries, you basically do not have any chance to retry on another node. Of course as mentioned in preivous comments, I don't think we should have a queue offer timeout.", "author": "WenbinZhu", "createdAt": "2020-06-11T00:39:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(addresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    public CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new IllegalStateException(\"No target nodes left to select from\");\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferBatchResponse> transfer(\n+            TransferBatchRequest transferBatchRequest) {\n+        return tryTransferForRandomNodes(new RandomNodeIterator(transferBatchRequest))\n+                .exceptionally(error -> TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(transferBatchRequest)\n+                        .status(FAILED)\n+                        .causeOfFailure(Optional.of(new StateTransferBatchProcessorException(\n+                                \"Failed batch: \" + transferBatchRequest, error)))\n+                        .build()\n+                );\n+    }\n+\n+    /**\n+     * Read records directly from the randomly scheduled destination node (don't hole fill).\n+     *\n+     * @param addresses A batch of consecutive addresses.\n+     * @param destNode  An optional destination node.\n+     * @param client    A log unit client to the node.\n+     * @return A read batch if the read went fine, an exception otherwise.\n+     */\n+    public ReadBatch readRecords(List<Long> addresses,\n+                                 Optional<String> destNode,\n+                                 LogUnitClient client) {\n+        for (int i = 0; i < maxReadRetries; i++) {", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTc3NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895775", "bodyText": "I left 3 retries since I decided to go with a semaphore to control the number of inflight messages.", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MTAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MjU3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438482579", "bodyText": "Copying the addresses every time seems too expensive, you can have a immutableList field in this class instead of the TransferBatchRequest (originalRequest) since all you need is just the addresses.", "author": "WenbinZhu", "createdAt": "2020-06-11T00:45:42Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTY2MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895660", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:53:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MjU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MzcyMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438483720", "bodyText": "I think it's not appropriate to say illegal State, since you just exhausted your retryable nodes. ReadBatchException or RetryExhaustedException might be better.", "author": "WenbinZhu", "createdAt": "2020-06-11T00:50:04Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(addresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    public CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new IllegalStateException(\"No target nodes left to select from\");", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NTg1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439895851", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MzcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4NTA5Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r438485092", "bodyText": "If WrongEpochException happens, you stop retrying on this node, but you will conitnue retrying on a new random node, which is not good. Any WrongEpochException happens during read/write should be propogated immediately to the outmost layer, which is the backoff retry where we invalidate layout.", "author": "WenbinZhu", "createdAt": "2020-06-11T00:55:14Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(addresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    public CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new IllegalStateException(\"No target nodes left to select from\");\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferBatchResponse> transfer(\n+            TransferBatchRequest transferBatchRequest) {\n+        return tryTransferForRandomNodes(new RandomNodeIterator(transferBatchRequest))\n+                .exceptionally(error -> TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(transferBatchRequest)\n+                        .status(FAILED)\n+                        .causeOfFailure(Optional.of(new StateTransferBatchProcessorException(\n+                                \"Failed batch: \" + transferBatchRequest, error)))\n+                        .build()\n+                );\n+    }\n+\n+    /**\n+     * Read records directly from the randomly scheduled destination node (don't hole fill).\n+     *\n+     * @param addresses A batch of consecutive addresses.\n+     * @param destNode  An optional destination node.\n+     * @param client    A log unit client to the node.\n+     * @return A read batch if the read went fine, an exception otherwise.\n+     */\n+    public ReadBatch readRecords(List<Long> addresses,\n+                                 Optional<String> destNode,\n+                                 LogUnitClient client) {\n+        for (int i = 0; i < maxReadRetries; i++) {\n+            try {\n+                ReadResponse response = CFUtils\n+                        .getUninterruptibly(client.readAll(ImmutableList.copyOf(addresses)),\n+                                TimeoutException.class, NetworkException.class);\n+                Map<Long, ILogData> records = new HashMap<>(response.getAddresses());\n+                ReadBatch readBatch = checkReadRecords(ImmutableList.copyOf(addresses),\n+                        records, destNode);\n+                if (readBatch.getStatus() == ReadBatch.ReadStatus.FAILED) {\n+                    throw new IllegalStateException(\"Some addresses failed to transfer: \" +\n+                            readBatch.getFailedAddresses());\n+                }\n+                return readBatch;\n+\n+            } catch (NetworkException | TimeoutException | IllegalStateException e) {\n+                log.warn(\"readRecords: error occurred: {}, retry {} times.\", e, i);\n+\n+            } catch (RuntimeException re) {\n+                log.error(\"readRecords: can't retry, error:\", re);\n+                break;\n+            }", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NjQ5MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439896490", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:57:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4NTA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAyMTI4MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439021281", "bodyText": "I think we need to get rid of copying addresses every time for read and checkReadRecords", "author": "WenbinZhu", "createdAt": "2020-06-11T19:32:08Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/protocolbatchprocessor/ProtocolBatchProcessor.java", "diffHunk": "@@ -98,9 +95,12 @@\n             for (int i = 0; i < maxReadRetries; i++) {\n                 try {\n                     Map<Long, ILogData> records =\n-                            addressSpaceView.simpleProtocolRead(transferBatchRequest.getAddresses(), readOptions);\n-                    ReadBatch batch = checkReadRecords(transferBatchRequest.getAddresses(),\n-                            records, transferBatchRequest.getDestination());\n+                            addressSpaceView.simpleProtocolRead(\n+                                    ImmutableList.copyOf(transferBatchRequest.getAddresses()),\n+                                    readOptions);\n+                    ReadBatch batch = checkReadRecords(\n+                            ImmutableList.copyOf(transferBatchRequest.getAddresses()),", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NjUyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439896529", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:58:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAyMTI4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEyNTQzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439125435", "bodyText": "Not used.", "author": "WenbinZhu", "createdAt": "2020-06-11T23:32:10Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NjU2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439896569", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:58:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEyNTQzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzMTkyNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439131924", "bodyText": "Here we catch different exceptions explicitly, and then if other RuntimeException happens, we break out, but in ProtocolBatchProcessor, we only retry on all RuntimeExceptions (except WEE), and also sleep for sometime. Should these two class use the same exception handling logic?", "author": "WenbinZhu", "createdAt": "2020-06-11T23:55:57Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final TransferBatchRequest originalRequest;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalRequest = request;\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            List<Long> addresses = ImmutableList.copyOf(originalRequest.getAddresses());\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(addresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    public CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new IllegalStateException(\"No target nodes left to select from\");\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferBatchResponse> transfer(\n+            TransferBatchRequest transferBatchRequest) {\n+        return tryTransferForRandomNodes(new RandomNodeIterator(transferBatchRequest))\n+                .exceptionally(error -> TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(transferBatchRequest)\n+                        .status(FAILED)\n+                        .causeOfFailure(Optional.of(new StateTransferBatchProcessorException(\n+                                \"Failed batch: \" + transferBatchRequest, error)))\n+                        .build()\n+                );\n+    }\n+\n+    /**\n+     * Read records directly from the randomly scheduled destination node (don't hole fill).\n+     *\n+     * @param addresses A batch of consecutive addresses.\n+     * @param destNode  An optional destination node.\n+     * @param client    A log unit client to the node.\n+     * @return A read batch if the read went fine, an exception otherwise.\n+     */\n+    public ReadBatch readRecords(List<Long> addresses,\n+                                 Optional<String> destNode,\n+                                 LogUnitClient client) {\n+        for (int i = 0; i < maxReadRetries; i++) {\n+            try {\n+                ReadResponse response = CFUtils\n+                        .getUninterruptibly(client.readAll(ImmutableList.copyOf(addresses)),\n+                                TimeoutException.class, NetworkException.class);\n+                Map<Long, ILogData> records = new HashMap<>(response.getAddresses());\n+                ReadBatch readBatch = checkReadRecords(ImmutableList.copyOf(addresses),\n+                        records, destNode);\n+                if (readBatch.getStatus() == ReadBatch.ReadStatus.FAILED) {\n+                    throw new IllegalStateException(\"Some addresses failed to transfer: \" +\n+                            readBatch.getFailedAddresses());\n+                }\n+                return readBatch;\n+\n+            } catch (NetworkException | TimeoutException | IllegalStateException e) {\n+                log.warn(\"readRecords: error occurred: {}, retry {} times.\", e, i);\n+\n+            } catch (RuntimeException re) {\n+                log.error(\"readRecords: can't retry, error:\", re);\n+                break;", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5NjY1Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439896657", "bodyText": "Made the same error handling logic for both of them.", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzMTkyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0ODc0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439148743", "bodyText": "First, it's not clean to have a lot logic in the catch. Second, the code in the catch block, like this one, incurs an RPC call, which can also fail with the exception you're catching, but will not retried. I think it's better to move the logic here in the try block, instead of catch block.", "author": "WenbinZhu", "createdAt": "2020-06-12T00:43:33Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/StateTransferBatchProcessor.java", "diffHunk": "@@ -43,53 +78,104 @@\n      */\n     default TransferBatchResponse writeRecords(ReadBatch readBatch, LogUnitClient logUnitClient,\n                                                int writeRetriesAllowed, Duration writeSleepDuration) {\n-        List<Long> totalAddressesInRequest = readBatch.getAddresses();\n-        Optional<String> destination = readBatch.getDestination();\n+        ImmutableList<LogData> dataToTransfer = ImmutableList.copyOf(readBatch.getData());\n+        ImmutableList<Long> addressesToTransfer = ImmutableList.copyOf(readBatch.getAddresses());\n         Optional<Exception> lastWriteException = Optional.empty();\n         Optional<TransferBatchResponse> transferBatchResponse = Optional.empty();\n \n         for (int i = 0; i < writeRetriesAllowed; i++) {\n             List<LogData> remainingDataToWrite = readBatch.getData();\n             try {\n-                boolean writeSucceeded = logUnitClient.writeRange(remainingDataToWrite).join();\n-                if (!writeSucceeded) {\n-                    throw new IllegalStateException(\"Failed to write to a log unit server.\");\n-                } else {\n-                    lastWriteException = Optional.empty();\n+                CFUtils.getUninterruptibly(logUnitClient.writeRange(remainingDataToWrite),\n+                        OverwriteException.class,\n+                        TimeoutException.class,\n+                        NetworkException.class);\n+                transferBatchResponse = Optional.of(TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(new TransferBatchRequest(addressesToTransfer,\n+                                readBatch.getDestinationNode().map(ImmutableList::of), DATA))\n+                        .status(SUCCEEDED)\n+                        .build());\n+                break;\n+            } catch (OverwriteException | TimeoutException | NetworkException e) {\n+                lastWriteException = Optional.of(e);\n+                Sleep.sleepUninterruptibly(writeSleepDuration);\n+                // Get all the addresses that were not written.\n+                Set<Long> nonWrittenAddresses =\n+                        getNonTransferredAddresses(logUnitClient, addressesToTransfer);", "originalCommit": "73e74a15ecfb045c564748bf423660f5aa58a683", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg5Njc1MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r439896750", "bodyText": "Fixed", "author": "PavelZaytsev", "createdAt": "2020-06-15T01:59:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0ODc0Mw=="}], "type": "inlineReview"}, {"oid": "a03c9f0ae2468fc4370ffb1f3cdee821cf2a8d44", "url": "https://github.com/CorfuDB/CorfuDB/commit/a03c9f0ae2468fc4370ffb1f3cdee821cf2a8d44", "message": "Introduce parallel transfer", "committedDate": "2020-06-15T02:21:18Z", "type": "forcePushed"}, {"oid": "548bb81604a51cc601c78c508fb23623f3219e17", "url": "https://github.com/CorfuDB/CorfuDB/commit/548bb81604a51cc601c78c508fb23623f3219e17", "message": "Introduce parallel transfer", "committedDate": "2020-06-15T02:31:53Z", "type": "forcePushed"}, {"oid": "c7261525020eb379eff8923c5fda605c47196c61", "url": "https://github.com/CorfuDB/CorfuDB/commit/c7261525020eb379eff8923c5fda605c47196c61", "message": "Introduce parallel transfer", "committedDate": "2020-06-15T18:44:06Z", "type": "forcePushed"}, {"oid": "6d20edcc14e7e0c021c5a52c280bd3ce2849c5e9", "url": "https://github.com/CorfuDB/CorfuDB/commit/6d20edcc14e7e0c021c5a52c280bd3ce2849c5e9", "message": "Introduce parallel transfer", "committedDate": "2020-06-16T20:44:42Z", "type": "forcePushed"}, {"oid": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "url": "https://github.com/CorfuDB/CorfuDB/commit/27b6fc0f3551c7453b9eaf645cab699b8d210eae", "message": "Introduce parallel transfer", "committedDate": "2020-06-16T23:28:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE0MTEyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441141122", "bodyText": "Unused import.", "author": "WenbinZhu", "createdAt": "2020-06-16T21:04:39Z", "path": "infrastructure/src/test/java/org/corfudb/infrastructure/log/statetransfer/SuccessfulBatchProcessor.java", "diffHunk": "@@ -5,8 +5,8 @@\n import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n import org.corfudb.util.CFUtils;\n \n+import javax.swing.text.html.Option;", "originalCommit": "6d20edcc14e7e0c021c5a52c280bd3ce2849c5e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwMzY3OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441203678", "bodyText": "Why not use @slf4j annotation as used to do?", "author": "WenbinZhu", "createdAt": "2020-06-16T23:51:36Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/StateTransferBatchProcessor.java", "diffHunk": "@@ -1,28 +1,42 @@\n package org.corfudb.infrastructure.log.statetransfer.batchprocessor;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Ordering;\n import com.google.common.collect.Sets;\n import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.IMetadata;\n import org.corfudb.protocols.wireprotocol.LogData;\n import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.NetworkException;\n+import org.corfudb.runtime.exceptions.OverwriteException;\n+import org.corfudb.runtime.exceptions.RetryExhaustedException;\n+import org.corfudb.util.CFUtils;\n import org.corfudb.util.Sleep;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n+import java.util.Comparator;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n \n import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.SUCCEEDED;\n \n /**\n  * An interface that every state transfer batch processor should implement.\n  */\n public interface StateTransferBatchProcessor {\n-\n+    Logger log = LoggerFactory.getLogger(StateTransferBatchProcessor.class);", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgwODMzOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441808338", "bodyText": "This annotation is not allowed for the interfaces, it's only allowed for classes and enums.", "author": "PavelZaytsev", "createdAt": "2020-06-17T20:17:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwMzY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxNzc0MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441217740", "bodyText": "This kind of layered try catch block doesn't look good. What I meant in the initial comment was actually to put the write and the  getNonTransferredAddresses in the same try block, then you can have try catch block and also won't have exceptions in the catch block, basically something like this:\nfor () {\n    try {\n        if (lastException.isPresent()) {\n            getNonTransferredAddresses();\n        }\n        tryWrite();\n    } catch () {\n        lastException = ...;\n    }\n}", "author": "WenbinZhu", "createdAt": "2020-06-17T00:43:07Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/StateTransferBatchProcessor.java", "diffHunk": "@@ -43,53 +134,93 @@\n      */\n     default TransferBatchResponse writeRecords(ReadBatch readBatch, LogUnitClient logUnitClient,\n                                                int writeRetriesAllowed, Duration writeSleepDuration) {\n-        List<Long> totalAddressesInRequest = readBatch.getAddresses();\n-        Optional<String> destination = readBatch.getDestination();\n-        Optional<Exception> lastWriteException = Optional.empty();\n-        Optional<TransferBatchResponse> transferBatchResponse = Optional.empty();\n-\n+        ImmutableList<LogData> dataToTransfer = ImmutableList.copyOf(readBatch.getData());\n+        ImmutableList<Long> addressesToTransfer = ImmutableList.copyOf(readBatch.getAddresses());\n+        Optional<Exception> lastException = Optional.empty();\n         for (int i = 0; i < writeRetriesAllowed; i++) {\n-            List<LogData> remainingDataToWrite = readBatch.getData();\n             try {\n-                boolean writeSucceeded = logUnitClient.writeRange(remainingDataToWrite).join();\n-                if (!writeSucceeded) {\n-                    throw new IllegalStateException(\"Failed to write to a log unit server.\");\n-                } else {\n-                    lastWriteException = Optional.empty();\n-                    transferBatchResponse = Optional.of(TransferBatchResponse\n-                            .builder()\n-                            .transferBatchRequest(new TransferBatchRequest(totalAddressesInRequest, destination))\n-                            .status(SUCCEEDED)\n-                            .build());\n-                    break;\n+                try {\n+                    return tryWrite(readBatch, logUnitClient, addressesToTransfer);\n+                } catch (OverwriteException | TimeoutException | NetworkException e) {\n+                    // If the write fails with OverwriteException, TimeoutException\n+                    // or a NetworkException:\n+                    // Sleep then get all the non-written addresses.\n+                    log.warn(\"writeRecords: tryWrite failed. Retrying {}/{}.\", i,\n+                            writeRetriesAllowed, e);\n+                    lastException = Optional.of(e);\n+                    Sleep.sleepUninterruptibly(writeSleepDuration);\n+                    Set<Long> notWrittenAddresses =\n+                            getNonTransferredAddresses(logUnitClient, addressesToTransfer);\n+                    // If we've written all the addresses, return a successful TransferBatchResponse.\n+                    if (notWrittenAddresses.isEmpty()) {\n+                        return TransferBatchResponse\n+                                .builder()\n+                                .transferBatchRequest(new TransferBatchRequest(addressesToTransfer,\n+                                        readBatch.getDestinationNode().map(ImmutableList::of)))\n+                                .status(SUCCEEDED)\n+                                .build();\n+                    }\n+                    // Create a new ReadBatch from the notWrittenAddresses and retry the workflow.\n+                    readBatch = createReadBatchFromNotTransferredAddresses(dataToTransfer,\n+                            notWrittenAddresses);\n                 }\n-            } catch (Exception e) {\n-                lastWriteException = Optional.of(e);\n-                Sleep.sleepUninterruptibly(writeSleepDuration);\n-                List<Long> remainingAddressesToWrite = readBatch.getAddresses();\n-                // Get all the addresses that were supposed to be written to a stream log.\n-                long start = remainingAddressesToWrite.get(0);\n-                long end = remainingAddressesToWrite.get(remainingAddressesToWrite.size() - 1);\n-                Set<Long> knownAddresses =\n-                        logUnitClient.requestKnownAddresses(start, end).join().getKnownAddresses();\n-                // Get all the addresses that were not written.\n-                Set<Long> nonWrittenAddresses =\n-                        Sets.difference(new HashSet<>(remainingAddressesToWrite), knownAddresses);\n-                // Create a new read batch with the missing data and retry.\n-                ImmutableList<LogData> nonWrittenData = readBatch.getData()\n-                        .stream()\n-                        .filter(data -> nonWrittenAddresses.contains(data.getGlobalAddress()))\n-                        .collect(ImmutableList.toImmutableList());\n-                readBatch = readBatch.toBuilder().data(nonWrittenData).build();\n+            } catch (TimeoutException | NetworkException e) {", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MDI4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441840289", "bodyText": "Ok makes sense. Fixed.", "author": "PavelZaytsev", "createdAt": "2020-06-17T21:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxNzc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxOTkwNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441219907", "bodyText": "private", "author": "WenbinZhu", "createdAt": "2020-06-17T00:51:15Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/StateTransferManager.java", "diffHunk": "@@ -81,228 +88,151 @@\n     }\n \n     /**\n-     * Performs the state transfer for the current transfer segments and also\n-     * updates their state as a result.\n+     * Return a range with the updated list of unknown addresses.\n      *\n-     * @param beforeTransferSegments A list of segments before a transfer.\n-     * @return A list of segments after a transfer.\n+     * @param range A transfer segment range.\n+     * @return An updated transfer segment range.\n      */\n-    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegment> beforeTransferSegments) {\n-\n-        List<TransferSegment> afterTransferSegments = new ArrayList<>();\n-\n-        for (TransferSegment segment : beforeTransferSegments) {\n-            TransferSegmentStatus newStatus = segment.getStatus();\n-\n-            // If a segment is not NOT_TRANSFERRED -> return it as is.\n-            if (newStatus.getSegmentState() != NOT_TRANSFERRED) {\n-                afterTransferSegments.add(segment);\n-                continue;\n-            }\n-\n-            // Get all the unknown addresses for this segment.\n-            List<Long> unknownAddressesInRange =\n-                    getUnknownAddressesInRange(segment.getStartAddress(),\n-                            segment.getEndAddress());\n-\n-            // If no addresses to transfer - mark a segment as transferred.\n-            if (unknownAddressesInRange.isEmpty()) {\n-                log.debug(\"All addresses are present in the range: [{}, {}], skipping transfer.\",\n-                        segment.getStartAddress(), segment.getEndAddress());\n-                long totalTransferred = segment.getTotal();\n-\n-                newStatus = TransferSegmentStatus\n-                        .builder()\n-                        .segmentState(TRANSFERRED)\n-                        .totalTransferred(totalTransferred)\n-                        .build();\n-            } else {\n-                // Get total number of addresses needed to transfer.\n-                long numAddressesToTransfer = unknownAddressesInRange.size();\n-                // Create a transferBatchRequest stream.\n-                Stream<TransferBatchRequest> batchStream = Lists\n-                        .partition(unknownAddressesInRange, batchSize)\n-                        .stream()\n-                        .map(groupedAddresses ->\n-                                new TransferBatchRequest(groupedAddresses, Optional.empty())\n-                        );\n-                // Execute state transfer synchronously.\n-                newStatus = synchronousStateTransfer(batchStream, numAddressesToTransfer);\n-            }\n-\n-            TransferSegment currentSegment = TransferSegment\n-                    .builder()\n-                    .startAddress(segment.getStartAddress())\n-                    .endAddress(segment.getEndAddress())\n-                    .status(newStatus)\n-                    .build();\n-\n-            afterTransferSegments.add(currentSegment);\n-\n-            if (currentSegment.getStatus().getSegmentState() == FAILED) {\n-                // A segment failed -> short circuit.\n-                break;\n-            }\n-        }\n-        return ImmutableList.copyOf(afterTransferSegments);\n+    TransferSegmentRangeSingle getUnknownAddressesInRangeForRange(TransferSegmentRangeSingle range) {\n+        long startAddress = range.getStartAddress();\n+        long endAddress = range.getEndAddress();\n+        ImmutableList<Long> unknownAddressesInRange =\n+                getUnknownAddressesInRange(startAddress, endAddress);\n+        return range.toBuilder()\n+                .unknownAddressesInRange(unknownAddressesInRange)\n+                .build();\n     }\n \n     /**\n-     * Given a stream of batch requests, and a total number\n-     * of transferred addresses needed, execute a state transfer synchronously.\n+     * Transform the given range into a stream of batch requests.\n      *\n-     * @param batchStream A stream of batch requests.\n-     * @param totalNeeded A total number of addresses needed for transfer.\n-     * @return A status representing a final status of a transferred segment.\n+     * @param range A transfer segment range that contains unknown addresses and maybe available\n+     *              log unit servers.\n+     * @return A stream of transfer batch requests.\n      */\n-    @VisibleForTesting\n-    TransferSegmentStatus synchronousStateTransfer(\n-            Stream<TransferBatchRequest> batchStream, long totalNeeded) {\n-        long accTransferred = 0L;\n-\n-        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n-\n-        while (iterator.hasNext()) {\n-            TransferBatchRequest nextBatch = iterator.next();\n-            TransferBatchResponse response =\n-                    batchProcessor.transfer(nextBatch).join();\n-            // In case of an error that is not handled by a batch processor, e.g. WrongEpochException,\n-            // we return a failed segment status to the caller and the exception. This exception is\n-            // thrown in a retry block of a RestoreRedundancyMergeSegments' restoreWithBackOff\n-            // method, which results in a retry.\n-            // The layout will be invalidated and only the non-transferred addresses of the\n-            // segment will be considered for the subsequent transfer.\n-            if (response.getStatus() == TransferStatus.FAILED) {\n-                Optional<TransferSegmentException> causeOfFailure =\n-                        Optional.of(response.getCauseOfFailure()\n-                                .map(TransferSegmentException::new)\n-                                .orElse(new TransferSegmentException(\"Failed to transfer.\")));\n-\n-                return TransferSegmentStatus\n-                        .builder()\n-                        .totalTransferred(accTransferred)\n-                        .segmentState(FAILED)\n-                        .causeOfFailure(causeOfFailure)\n-                        .build();\n-            }\n-            accTransferred += response.getTransferBatchRequest().getAddresses().size();\n-        }\n-\n-        if (accTransferred == totalNeeded) {\n-            return TransferSegmentStatus\n-                    .builder()\n-                    .totalTransferred(accTransferred)\n-                    .segmentState(TRANSFERRED)\n-                    .build();\n-        }\n-\n-        String errorMsg = String.format(\"Needed: %s, but transferred: %s\",\n-                totalNeeded, accTransferred);\n-\n-        return TransferSegmentStatus\n-                .builder()\n-                .totalTransferred(0L)\n-                .segmentState(FAILED)\n-                .causeOfFailure(Optional.of(new TransferSegmentException(errorMsg)))\n-                .build();\n+    Stream<TransferBatchRequest> rangeToBatchRequestStream(TransferSegmentRangeSingle range) {\n+        ImmutableList<Long> unknownAddressesInRange = range.getUnknownAddressesInRange();\n+        Optional<ImmutableList<String>> availableServers = range.getAvailableServers();\n+        return Lists.partition(unknownAddressesInRange, batchSize).stream()\n+                .map(partition -> new TransferBatchRequest(partition, availableServers));\n     }\n \n     /**\n-     * A data class that represents a non-empty and bounded segment to be transferred.\n+     * Go over all the single transfer segment ranges, filter them by the provided type,\n+     * get all the unknown addresses in range, and then turn the unknown addresses in range into\n+     * the transfer batch request stream - ready to be consumed by the appropriate\n+     * state transfer processor.\n+     *\n+     * @param ranges         A list of single transfer segment ranges.\n+     * @param typeOfTransfer A provided type of transfer - protocol read or consistent read.\n+     * @return A stream of transfer batch requests.\n      */\n-    @EqualsAndHashCode\n-    @Getter\n-    @ToString\n-    @Builder\n-    @AllArgsConstructor(access = AccessLevel.PRIVATE)\n-    public static class TransferSegment {\n-        /**\n-         * Start address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long startAddress;\n-        /**\n-         * End address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long endAddress;\n-        /**\n-         * A status of a transfer of a segment.\n-         */\n-        private final TransferSegmentStatus status;\n-\n-        /**\n-         * Get the total number of addresses in range.\n-         * {@link #endAddress} and {@link #startAddress} can only be non-negative longs such that\n-         * {@link #endAddress} >= {@link #startAddress}.\n-         *\n-         * @return Total number of addresses in this segment.\n-         */\n-        public long getTotal() {\n-            return endAddress - startAddress + 1L;\n-        }\n-\n-        public static class TransferSegmentBuilder {\n+    Stream<TransferBatchRequest> createBatchWorkload(List<TransferSegmentRangeSingle> ranges,\n+                                                     StateTransferType typeOfTransfer) {\n+        return ranges.stream()\n+                .filter(range -> range.getTypeOfTransfer() == typeOfTransfer)\n+                .map(this::getUnknownAddressesInRangeForRange)\n+                .flatMap(this::rangeToBatchRequestStream);\n+    }\n \n-            public void verify() {\n-                if (startAddress < 0L || endAddress < 0L) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s or end: %s \" +\n-                                    \"can not be negative.\", startAddress, endAddress));\n-                }\n-                if (startAddress > endAddress) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s can not be \" +\n-                                    \"greater than end: %s.\", startAddress, endAddress));\n-                }\n+    /**\n+     * Transform the transfer segment ranges into the single ones and filter all the non transferred\n+     * ones.\n+     *\n+     * @param beforeTransferRanges Ranges before the transfer, some single and some split.\n+     * @return Ranges before the transfer, not transferred and single.\n+     */\n+    ImmutableList<TransferSegmentRangeSingle> toSingleNotTransferredRanges(\n+            List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream()\n+                .flatMap(range -> range.toSingle().stream())\n+                .filter(range -> range.getStatus().getSegmentState() == NOT_TRANSFERRED)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-                if (status == null) {\n-                    throw new IllegalStateException(\"Status should be defined.\");\n-                }\n-            }\n+    /**\n+     * Transform all the transferred ranges back into the transfer segments. This data is used\n+     * later for cluster reconfiguration.\n+     *\n+     * @param transferRanges A list of transfer segment ranges\n+     * @return A list of transfer segments.\n+     */\n+    ImmutableList<TransferSegment> toSegments(List<TransferSegmentRange> transferRanges) {\n+        return transferRanges.stream().map(TransferSegmentRange::toTransferSegment)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-            public TransferSegment build() {\n-                verify();\n-                return new TransferSegment(startAddress, endAddress, status);\n+    /**\n+     * For all the segment ranges that were not transferred, updated their status.\n+     *\n+     * @param newStatus            A new status.\n+     * @param beforeTransferRanges Ranges before transfer.\n+     * @return Ranges after transfer.\n+     */\n+    ImmutableList<TransferSegmentRange> updateNotTransferredSegmentRangeStatus(TransferSegmentStatus newStatus,", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyMDIwOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441220208", "bodyText": "Line exceeds limit.", "author": "WenbinZhu", "createdAt": "2020-06-17T00:52:27Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/StateTransferManager.java", "diffHunk": "@@ -81,228 +88,151 @@\n     }\n \n     /**\n-     * Performs the state transfer for the current transfer segments and also\n-     * updates their state as a result.\n+     * Return a range with the updated list of unknown addresses.\n      *\n-     * @param beforeTransferSegments A list of segments before a transfer.\n-     * @return A list of segments after a transfer.\n+     * @param range A transfer segment range.\n+     * @return An updated transfer segment range.\n      */\n-    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegment> beforeTransferSegments) {\n-\n-        List<TransferSegment> afterTransferSegments = new ArrayList<>();\n-\n-        for (TransferSegment segment : beforeTransferSegments) {\n-            TransferSegmentStatus newStatus = segment.getStatus();\n-\n-            // If a segment is not NOT_TRANSFERRED -> return it as is.\n-            if (newStatus.getSegmentState() != NOT_TRANSFERRED) {\n-                afterTransferSegments.add(segment);\n-                continue;\n-            }\n-\n-            // Get all the unknown addresses for this segment.\n-            List<Long> unknownAddressesInRange =\n-                    getUnknownAddressesInRange(segment.getStartAddress(),\n-                            segment.getEndAddress());\n-\n-            // If no addresses to transfer - mark a segment as transferred.\n-            if (unknownAddressesInRange.isEmpty()) {\n-                log.debug(\"All addresses are present in the range: [{}, {}], skipping transfer.\",\n-                        segment.getStartAddress(), segment.getEndAddress());\n-                long totalTransferred = segment.getTotal();\n-\n-                newStatus = TransferSegmentStatus\n-                        .builder()\n-                        .segmentState(TRANSFERRED)\n-                        .totalTransferred(totalTransferred)\n-                        .build();\n-            } else {\n-                // Get total number of addresses needed to transfer.\n-                long numAddressesToTransfer = unknownAddressesInRange.size();\n-                // Create a transferBatchRequest stream.\n-                Stream<TransferBatchRequest> batchStream = Lists\n-                        .partition(unknownAddressesInRange, batchSize)\n-                        .stream()\n-                        .map(groupedAddresses ->\n-                                new TransferBatchRequest(groupedAddresses, Optional.empty())\n-                        );\n-                // Execute state transfer synchronously.\n-                newStatus = synchronousStateTransfer(batchStream, numAddressesToTransfer);\n-            }\n-\n-            TransferSegment currentSegment = TransferSegment\n-                    .builder()\n-                    .startAddress(segment.getStartAddress())\n-                    .endAddress(segment.getEndAddress())\n-                    .status(newStatus)\n-                    .build();\n-\n-            afterTransferSegments.add(currentSegment);\n-\n-            if (currentSegment.getStatus().getSegmentState() == FAILED) {\n-                // A segment failed -> short circuit.\n-                break;\n-            }\n-        }\n-        return ImmutableList.copyOf(afterTransferSegments);\n+    TransferSegmentRangeSingle getUnknownAddressesInRangeForRange(TransferSegmentRangeSingle range) {\n+        long startAddress = range.getStartAddress();\n+        long endAddress = range.getEndAddress();\n+        ImmutableList<Long> unknownAddressesInRange =\n+                getUnknownAddressesInRange(startAddress, endAddress);\n+        return range.toBuilder()\n+                .unknownAddressesInRange(unknownAddressesInRange)\n+                .build();\n     }\n \n     /**\n-     * Given a stream of batch requests, and a total number\n-     * of transferred addresses needed, execute a state transfer synchronously.\n+     * Transform the given range into a stream of batch requests.\n      *\n-     * @param batchStream A stream of batch requests.\n-     * @param totalNeeded A total number of addresses needed for transfer.\n-     * @return A status representing a final status of a transferred segment.\n+     * @param range A transfer segment range that contains unknown addresses and maybe available\n+     *              log unit servers.\n+     * @return A stream of transfer batch requests.\n      */\n-    @VisibleForTesting\n-    TransferSegmentStatus synchronousStateTransfer(\n-            Stream<TransferBatchRequest> batchStream, long totalNeeded) {\n-        long accTransferred = 0L;\n-\n-        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n-\n-        while (iterator.hasNext()) {\n-            TransferBatchRequest nextBatch = iterator.next();\n-            TransferBatchResponse response =\n-                    batchProcessor.transfer(nextBatch).join();\n-            // In case of an error that is not handled by a batch processor, e.g. WrongEpochException,\n-            // we return a failed segment status to the caller and the exception. This exception is\n-            // thrown in a retry block of a RestoreRedundancyMergeSegments' restoreWithBackOff\n-            // method, which results in a retry.\n-            // The layout will be invalidated and only the non-transferred addresses of the\n-            // segment will be considered for the subsequent transfer.\n-            if (response.getStatus() == TransferStatus.FAILED) {\n-                Optional<TransferSegmentException> causeOfFailure =\n-                        Optional.of(response.getCauseOfFailure()\n-                                .map(TransferSegmentException::new)\n-                                .orElse(new TransferSegmentException(\"Failed to transfer.\")));\n-\n-                return TransferSegmentStatus\n-                        .builder()\n-                        .totalTransferred(accTransferred)\n-                        .segmentState(FAILED)\n-                        .causeOfFailure(causeOfFailure)\n-                        .build();\n-            }\n-            accTransferred += response.getTransferBatchRequest().getAddresses().size();\n-        }\n-\n-        if (accTransferred == totalNeeded) {\n-            return TransferSegmentStatus\n-                    .builder()\n-                    .totalTransferred(accTransferred)\n-                    .segmentState(TRANSFERRED)\n-                    .build();\n-        }\n-\n-        String errorMsg = String.format(\"Needed: %s, but transferred: %s\",\n-                totalNeeded, accTransferred);\n-\n-        return TransferSegmentStatus\n-                .builder()\n-                .totalTransferred(0L)\n-                .segmentState(FAILED)\n-                .causeOfFailure(Optional.of(new TransferSegmentException(errorMsg)))\n-                .build();\n+    Stream<TransferBatchRequest> rangeToBatchRequestStream(TransferSegmentRangeSingle range) {\n+        ImmutableList<Long> unknownAddressesInRange = range.getUnknownAddressesInRange();\n+        Optional<ImmutableList<String>> availableServers = range.getAvailableServers();\n+        return Lists.partition(unknownAddressesInRange, batchSize).stream()\n+                .map(partition -> new TransferBatchRequest(partition, availableServers));\n     }\n \n     /**\n-     * A data class that represents a non-empty and bounded segment to be transferred.\n+     * Go over all the single transfer segment ranges, filter them by the provided type,\n+     * get all the unknown addresses in range, and then turn the unknown addresses in range into\n+     * the transfer batch request stream - ready to be consumed by the appropriate\n+     * state transfer processor.\n+     *\n+     * @param ranges         A list of single transfer segment ranges.\n+     * @param typeOfTransfer A provided type of transfer - protocol read or consistent read.\n+     * @return A stream of transfer batch requests.\n      */\n-    @EqualsAndHashCode\n-    @Getter\n-    @ToString\n-    @Builder\n-    @AllArgsConstructor(access = AccessLevel.PRIVATE)\n-    public static class TransferSegment {\n-        /**\n-         * Start address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long startAddress;\n-        /**\n-         * End address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long endAddress;\n-        /**\n-         * A status of a transfer of a segment.\n-         */\n-        private final TransferSegmentStatus status;\n-\n-        /**\n-         * Get the total number of addresses in range.\n-         * {@link #endAddress} and {@link #startAddress} can only be non-negative longs such that\n-         * {@link #endAddress} >= {@link #startAddress}.\n-         *\n-         * @return Total number of addresses in this segment.\n-         */\n-        public long getTotal() {\n-            return endAddress - startAddress + 1L;\n-        }\n-\n-        public static class TransferSegmentBuilder {\n+    Stream<TransferBatchRequest> createBatchWorkload(List<TransferSegmentRangeSingle> ranges,\n+                                                     StateTransferType typeOfTransfer) {\n+        return ranges.stream()\n+                .filter(range -> range.getTypeOfTransfer() == typeOfTransfer)\n+                .map(this::getUnknownAddressesInRangeForRange)\n+                .flatMap(this::rangeToBatchRequestStream);\n+    }\n \n-            public void verify() {\n-                if (startAddress < 0L || endAddress < 0L) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s or end: %s \" +\n-                                    \"can not be negative.\", startAddress, endAddress));\n-                }\n-                if (startAddress > endAddress) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s can not be \" +\n-                                    \"greater than end: %s.\", startAddress, endAddress));\n-                }\n+    /**\n+     * Transform the transfer segment ranges into the single ones and filter all the non transferred\n+     * ones.\n+     *\n+     * @param beforeTransferRanges Ranges before the transfer, some single and some split.\n+     * @return Ranges before the transfer, not transferred and single.\n+     */\n+    ImmutableList<TransferSegmentRangeSingle> toSingleNotTransferredRanges(\n+            List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream()\n+                .flatMap(range -> range.toSingle().stream())\n+                .filter(range -> range.getStatus().getSegmentState() == NOT_TRANSFERRED)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-                if (status == null) {\n-                    throw new IllegalStateException(\"Status should be defined.\");\n-                }\n-            }\n+    /**\n+     * Transform all the transferred ranges back into the transfer segments. This data is used\n+     * later for cluster reconfiguration.\n+     *\n+     * @param transferRanges A list of transfer segment ranges\n+     * @return A list of transfer segments.\n+     */\n+    ImmutableList<TransferSegment> toSegments(List<TransferSegmentRange> transferRanges) {\n+        return transferRanges.stream().map(TransferSegmentRange::toTransferSegment)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-            public TransferSegment build() {\n-                verify();\n-                return new TransferSegment(startAddress, endAddress, status);\n+    /**\n+     * For all the segment ranges that were not transferred, updated their status.\n+     *\n+     * @param newStatus            A new status.\n+     * @param beforeTransferRanges Ranges before transfer.\n+     * @return Ranges after transfer.\n+     */\n+    ImmutableList<TransferSegmentRange> updateNotTransferredSegmentRangeStatus(TransferSegmentStatus newStatus,\n+                                                                               List<TransferSegmentRange> beforeTransferRanges) {", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyMDMwNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441220304", "bodyText": "NIT: remove extra blank lines.", "author": "WenbinZhu", "createdAt": "2020-06-17T00:52:50Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/StateTransferManager.java", "diffHunk": "@@ -81,228 +88,151 @@\n     }\n \n     /**\n-     * Performs the state transfer for the current transfer segments and also\n-     * updates their state as a result.\n+     * Return a range with the updated list of unknown addresses.\n      *\n-     * @param beforeTransferSegments A list of segments before a transfer.\n-     * @return A list of segments after a transfer.\n+     * @param range A transfer segment range.\n+     * @return An updated transfer segment range.\n      */\n-    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegment> beforeTransferSegments) {\n-\n-        List<TransferSegment> afterTransferSegments = new ArrayList<>();\n-\n-        for (TransferSegment segment : beforeTransferSegments) {\n-            TransferSegmentStatus newStatus = segment.getStatus();\n-\n-            // If a segment is not NOT_TRANSFERRED -> return it as is.\n-            if (newStatus.getSegmentState() != NOT_TRANSFERRED) {\n-                afterTransferSegments.add(segment);\n-                continue;\n-            }\n-\n-            // Get all the unknown addresses for this segment.\n-            List<Long> unknownAddressesInRange =\n-                    getUnknownAddressesInRange(segment.getStartAddress(),\n-                            segment.getEndAddress());\n-\n-            // If no addresses to transfer - mark a segment as transferred.\n-            if (unknownAddressesInRange.isEmpty()) {\n-                log.debug(\"All addresses are present in the range: [{}, {}], skipping transfer.\",\n-                        segment.getStartAddress(), segment.getEndAddress());\n-                long totalTransferred = segment.getTotal();\n-\n-                newStatus = TransferSegmentStatus\n-                        .builder()\n-                        .segmentState(TRANSFERRED)\n-                        .totalTransferred(totalTransferred)\n-                        .build();\n-            } else {\n-                // Get total number of addresses needed to transfer.\n-                long numAddressesToTransfer = unknownAddressesInRange.size();\n-                // Create a transferBatchRequest stream.\n-                Stream<TransferBatchRequest> batchStream = Lists\n-                        .partition(unknownAddressesInRange, batchSize)\n-                        .stream()\n-                        .map(groupedAddresses ->\n-                                new TransferBatchRequest(groupedAddresses, Optional.empty())\n-                        );\n-                // Execute state transfer synchronously.\n-                newStatus = synchronousStateTransfer(batchStream, numAddressesToTransfer);\n-            }\n-\n-            TransferSegment currentSegment = TransferSegment\n-                    .builder()\n-                    .startAddress(segment.getStartAddress())\n-                    .endAddress(segment.getEndAddress())\n-                    .status(newStatus)\n-                    .build();\n-\n-            afterTransferSegments.add(currentSegment);\n-\n-            if (currentSegment.getStatus().getSegmentState() == FAILED) {\n-                // A segment failed -> short circuit.\n-                break;\n-            }\n-        }\n-        return ImmutableList.copyOf(afterTransferSegments);\n+    TransferSegmentRangeSingle getUnknownAddressesInRangeForRange(TransferSegmentRangeSingle range) {\n+        long startAddress = range.getStartAddress();\n+        long endAddress = range.getEndAddress();\n+        ImmutableList<Long> unknownAddressesInRange =\n+                getUnknownAddressesInRange(startAddress, endAddress);\n+        return range.toBuilder()\n+                .unknownAddressesInRange(unknownAddressesInRange)\n+                .build();\n     }\n \n     /**\n-     * Given a stream of batch requests, and a total number\n-     * of transferred addresses needed, execute a state transfer synchronously.\n+     * Transform the given range into a stream of batch requests.\n      *\n-     * @param batchStream A stream of batch requests.\n-     * @param totalNeeded A total number of addresses needed for transfer.\n-     * @return A status representing a final status of a transferred segment.\n+     * @param range A transfer segment range that contains unknown addresses and maybe available\n+     *              log unit servers.\n+     * @return A stream of transfer batch requests.\n      */\n-    @VisibleForTesting\n-    TransferSegmentStatus synchronousStateTransfer(\n-            Stream<TransferBatchRequest> batchStream, long totalNeeded) {\n-        long accTransferred = 0L;\n-\n-        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n-\n-        while (iterator.hasNext()) {\n-            TransferBatchRequest nextBatch = iterator.next();\n-            TransferBatchResponse response =\n-                    batchProcessor.transfer(nextBatch).join();\n-            // In case of an error that is not handled by a batch processor, e.g. WrongEpochException,\n-            // we return a failed segment status to the caller and the exception. This exception is\n-            // thrown in a retry block of a RestoreRedundancyMergeSegments' restoreWithBackOff\n-            // method, which results in a retry.\n-            // The layout will be invalidated and only the non-transferred addresses of the\n-            // segment will be considered for the subsequent transfer.\n-            if (response.getStatus() == TransferStatus.FAILED) {\n-                Optional<TransferSegmentException> causeOfFailure =\n-                        Optional.of(response.getCauseOfFailure()\n-                                .map(TransferSegmentException::new)\n-                                .orElse(new TransferSegmentException(\"Failed to transfer.\")));\n-\n-                return TransferSegmentStatus\n-                        .builder()\n-                        .totalTransferred(accTransferred)\n-                        .segmentState(FAILED)\n-                        .causeOfFailure(causeOfFailure)\n-                        .build();\n-            }\n-            accTransferred += response.getTransferBatchRequest().getAddresses().size();\n-        }\n-\n-        if (accTransferred == totalNeeded) {\n-            return TransferSegmentStatus\n-                    .builder()\n-                    .totalTransferred(accTransferred)\n-                    .segmentState(TRANSFERRED)\n-                    .build();\n-        }\n-\n-        String errorMsg = String.format(\"Needed: %s, but transferred: %s\",\n-                totalNeeded, accTransferred);\n-\n-        return TransferSegmentStatus\n-                .builder()\n-                .totalTransferred(0L)\n-                .segmentState(FAILED)\n-                .causeOfFailure(Optional.of(new TransferSegmentException(errorMsg)))\n-                .build();\n+    Stream<TransferBatchRequest> rangeToBatchRequestStream(TransferSegmentRangeSingle range) {\n+        ImmutableList<Long> unknownAddressesInRange = range.getUnknownAddressesInRange();\n+        Optional<ImmutableList<String>> availableServers = range.getAvailableServers();\n+        return Lists.partition(unknownAddressesInRange, batchSize).stream()\n+                .map(partition -> new TransferBatchRequest(partition, availableServers));\n     }\n \n     /**\n-     * A data class that represents a non-empty and bounded segment to be transferred.\n+     * Go over all the single transfer segment ranges, filter them by the provided type,\n+     * get all the unknown addresses in range, and then turn the unknown addresses in range into\n+     * the transfer batch request stream - ready to be consumed by the appropriate\n+     * state transfer processor.\n+     *\n+     * @param ranges         A list of single transfer segment ranges.\n+     * @param typeOfTransfer A provided type of transfer - protocol read or consistent read.\n+     * @return A stream of transfer batch requests.\n      */\n-    @EqualsAndHashCode\n-    @Getter\n-    @ToString\n-    @Builder\n-    @AllArgsConstructor(access = AccessLevel.PRIVATE)\n-    public static class TransferSegment {\n-        /**\n-         * Start address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long startAddress;\n-        /**\n-         * End address of a segment range to transfer, inclusive and non-negative.\n-         */\n-        private final long endAddress;\n-        /**\n-         * A status of a transfer of a segment.\n-         */\n-        private final TransferSegmentStatus status;\n-\n-        /**\n-         * Get the total number of addresses in range.\n-         * {@link #endAddress} and {@link #startAddress} can only be non-negative longs such that\n-         * {@link #endAddress} >= {@link #startAddress}.\n-         *\n-         * @return Total number of addresses in this segment.\n-         */\n-        public long getTotal() {\n-            return endAddress - startAddress + 1L;\n-        }\n-\n-        public static class TransferSegmentBuilder {\n+    Stream<TransferBatchRequest> createBatchWorkload(List<TransferSegmentRangeSingle> ranges,\n+                                                     StateTransferType typeOfTransfer) {\n+        return ranges.stream()\n+                .filter(range -> range.getTypeOfTransfer() == typeOfTransfer)\n+                .map(this::getUnknownAddressesInRangeForRange)\n+                .flatMap(this::rangeToBatchRequestStream);\n+    }\n \n-            public void verify() {\n-                if (startAddress < 0L || endAddress < 0L) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s or end: %s \" +\n-                                    \"can not be negative.\", startAddress, endAddress));\n-                }\n-                if (startAddress > endAddress) {\n-                    throw new IllegalStateException(\n-                            String.format(\"Start: %s can not be \" +\n-                                    \"greater than end: %s.\", startAddress, endAddress));\n-                }\n+    /**\n+     * Transform the transfer segment ranges into the single ones and filter all the non transferred\n+     * ones.\n+     *\n+     * @param beforeTransferRanges Ranges before the transfer, some single and some split.\n+     * @return Ranges before the transfer, not transferred and single.\n+     */\n+    ImmutableList<TransferSegmentRangeSingle> toSingleNotTransferredRanges(\n+            List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream()\n+                .flatMap(range -> range.toSingle().stream())\n+                .filter(range -> range.getStatus().getSegmentState() == NOT_TRANSFERRED)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-                if (status == null) {\n-                    throw new IllegalStateException(\"Status should be defined.\");\n-                }\n-            }\n+    /**\n+     * Transform all the transferred ranges back into the transfer segments. This data is used\n+     * later for cluster reconfiguration.\n+     *\n+     * @param transferRanges A list of transfer segment ranges\n+     * @return A list of transfer segments.\n+     */\n+    ImmutableList<TransferSegment> toSegments(List<TransferSegmentRange> transferRanges) {\n+        return transferRanges.stream().map(TransferSegmentRange::toTransferSegment)\n+                .collect(ImmutableList.toImmutableList());\n+    }\n \n-            public TransferSegment build() {\n-                verify();\n-                return new TransferSegment(startAddress, endAddress, status);\n+    /**\n+     * For all the segment ranges that were not transferred, updated their status.\n+     *\n+     * @param newStatus            A new status.\n+     * @param beforeTransferRanges Ranges before transfer.\n+     * @return Ranges after transfer.\n+     */\n+    ImmutableList<TransferSegmentRange> updateNotTransferredSegmentRangeStatus(TransferSegmentStatus newStatus,\n+                                                                               List<TransferSegmentRange> beforeTransferRanges) {\n+        return beforeTransferRanges.stream().map(range -> {\n+            if (range.getStatus().getSegmentState() == NOT_TRANSFERRED) {\n+                return range.updateStatus(newStatus);\n             }\n-        }\n+            return range;\n+        }).collect(ImmutableList.toImmutableList());\n     }\n \n     /**\n-     * A data class that represents a status of a segment to be transferred.\n+     * Performs the state transfer for the current non-transferred transfer segments and also\n+     * updates their state as a result.\n+     *\n+     * @param beforeTransferRanges A list of ranges before a transfer.\n+     * @return A list of segments after a transfer.\n      */\n-    @Getter\n-    @ToString\n-    @EqualsAndHashCode\n-    @Builder\n-    public static class TransferSegmentStatus {\n-        /**\n-         * States of the segment:\n-         * - {@link SegmentState#NOT_TRANSFERRED}: Segment is not transferred.\n-         * - {@link SegmentState#TRANSFERRED}: Segment was transferred fully.\n-         * - {@link SegmentState#RESTORED}: Segment was restored, and is present in the current layout.\n-         * - {@link SegmentState#FAILED}: The state transfer of a segment has failed.\n-         */\n-        public enum SegmentState {\n-            NOT_TRANSFERRED,\n-            TRANSFERRED,\n-            RESTORED,\n-            FAILED\n+    public ImmutableList<TransferSegment> handleTransfer(List<TransferSegmentRange> beforeTransferRanges) {\n+        // Transform all ranges into single ranges and filter all the not transferred ranges.\n+        ImmutableList<TransferSegmentRangeSingle> singleNotTransferredRanges =\n+                toSingleNotTransferredRanges(beforeTransferRanges);\n+\n+        // If none are NOT_TRANSFERRED, there is nothing to transfer, return the list as is.\n+        if (singleNotTransferredRanges.isEmpty()) {\n+            return toSegments(beforeTransferRanges);\n         }\n \n-        /**\n-         * A state of a segment.\n-         */\n-        @Default\n-        private final SegmentState segmentState = NOT_TRANSFERRED;\n-        /**\n-         * Total number of records transferred for this segment.\n-         */\n-        @Default\n-        private final long totalTransferred = 0L;\n-        /**\n-         * An optional cause of failure for this segment.\n-         */\n-        @Default\n-        @Exclude\n-        private final Optional<TransferSegmentException> causeOfFailure = Optional.empty();\n+        // Split into the protocol and committed workloads.\n+        Stream<TransferBatchRequest> consistentBatchStream =\n+                createBatchWorkload(singleNotTransferredRanges, CONSISTENT_READ);\n+\n+        Stream<TransferBatchRequest> protocolBatchStream =\n+                createBatchWorkload(singleNotTransferredRanges, PROTOCOL_READ);\n+\n+        // Execute a parallel transfer first, and then if it succeeds, execute a regular transfer.\n+        TransferProcessorResult result = parallelTransferProcessor.runStateTransfer(consistentBatchStream)\n+                .thenCompose(res -> {\n+                    if (res.getTransferState() == TRANSFER_SUCCEEDED) {\n+                        return basicTransferProcessor.runStateTransfer(protocolBatchStream);\n+                    } else {\n+                        return CompletableFuture.completedFuture(res);\n+                    }\n+                }).join();\n+\n+        log.info(\"handleTransfer: overall transfer result: {}\", result);\n+        // Update the segment status. If either of the transfers failed the status is failed\n+        // and if none failed, the status is transferred.\n+        TransferSegmentStatus newTransferSegmentStatus;\n+\n+        if (result.getTransferState() == TRANSFER_SUCCEEDED) {\n+            newTransferSegmentStatus = TransferSegmentStatus.builder().segmentState(TRANSFERRED)\n+                    .causeOfFailure(Optional.empty()).build();\n+        } else {\n+            newTransferSegmentStatus = TransferSegmentStatus.builder().segmentState(FAILED)\n+                    .causeOfFailure(result.getCauseOfFailure()).build();\n+        }\n+\n+        // Update the status of the not transferred segment ranges.\n+        ImmutableList<TransferSegmentRange> transferredSegmentRanges =\n+                updateNotTransferredSegmentRangeStatus(newTransferSegmentStatus,\n+                        beforeTransferRanges);\n+        // Transform the segment ranges back into the transfer segments.\n+        return toSegments(transferredSegmentRanges);\n     }\n+", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441225685", "bodyText": "How did we come up with 25? Is it too many considering we have at most 2 source nodes? Should this number take into account number of source nodes?", "author": "WenbinZhu", "createdAt": "2020-06-17T01:14:21Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -58,6 +65,9 @@\n     @NonNull\n     private final RedundancyCalculator redundancyCalculator;\n \n+    @Getter\n+    private final int committedProcessorNumInFlightMessages = 25;", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgxNDA0MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441814040", "bodyText": "@WenbinZhu\nI am not sure about the number. The thing is we are transferring all the segments at once, so within one transfer we can transfer a segment that has 2 source nodes and another segment that has only 1. So this number ideally should be dynamically configured but even if its the case, it is still unclear what to use for one node. I think we should settle on some number and then see how it affects the performance before introducing any dynamic configurations. I am not sure about any other approach than to eyeball it.", "author": "PavelZaytsev", "createdAt": "2020-06-17T20:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTgxODU1MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441818550", "bodyText": "@WenbinZhu I think ideally this number should not even be configured. The consumer (log unit server) must have a backpressure mechanism in place that would balance the incoming traffic. There are frameworks that achieve this easily, for example, RxJava, we don't use any of these though.", "author": "PavelZaytsev", "createdAt": "2020-06-17T20:36:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MTM3OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441841378", "bodyText": "@PavelZaytsev This number is for parallel transfer only, I think there can be a rough calculation on this number, for a layout with 2 available source log units, under the random algorithem, 25 in flight messages means every log unit will have approximately 12.5 requests at the same time, which seems too many considering we have normal workloads other than state transfer. And since the number of source log units is known, it can be configured based on it, but the problem you have now, is that every batch has it's own number of available log units, which is actually the same for parallel workloads. Ideally only the prallel batch processor should know the number of available processors, and this number can vary on every retry where layout is invalidated.", "author": "WenbinZhu", "createdAt": "2020-06-17T21:21:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0Mjg4MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441842881", "bodyText": "@WenbinZhu Ok I can configure that. But what do you suggest we use as the number of in-flight requests for one source log unit server? Is 5 enough?", "author": "PavelZaytsev", "createdAt": "2020-06-17T21:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0NTc0MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441845740", "bodyText": "Yea, we can start with 5, and later if performance is good, we can increase to somthing like 10 mabye.", "author": "WenbinZhu", "createdAt": "2020-06-17T21:30:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNTY4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441237012", "bodyText": "I think we both misunderstood how allOf works. I just did an experiment (see code below), even if one of the future throws an exception, the aggreatetd future, i.e. allFutures, will not complete immediately, until every one finishes. So we cannot use isCompletedExceptionally() here to check. I think the way to fix it is to have anther variable to see if an exception is thrown, and then check this variable here and also exceptioinally compelte allFurures.\nExperiment code:\n    @Test\n    public void ttt() throws Exception {\n        CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n\n        for (int i = 0; i < 50; i++) {\n            // Doesn't work\n            if (f.isCompletedExceptionally()) {\n                return;\n            }\n\n            final int j = i;\n\n            if (i == 5) {\n                f = CompletableFuture.allOf(f, CompletableFuture.runAsync(() -> {\n                    System.out.println(\"**\" + j);\n                    throw new WrongEpochException(1);\n                }));\n            } else {\n                f = CompletableFuture.allOf(f, CompletableFuture.runAsync(() -> {\n                    Sleep.sleepUninterruptibly(Duration.ofSeconds(2));\n                    System.out.println(j);\n                }));\n\n                Sleep.sleepUninterruptibly(Duration.ofSeconds(1));\n            }\n        }\n\n        f.get();  // finishes with exception only after everyone finishes.\n    }", "author": "WenbinZhu", "createdAt": "2020-06-17T01:58:24Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,90 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Semaphore;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.SUCCEEDED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by allowing only numInFlightRequests at a time.\n+ */\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A semaphore to control the number of in-flight messages.\n+     */\n+    private final Semaphore semaphore;\n+    /**\n+     * A state transfer batch processor that performs the batch transfer.\n+     */\n+    private final StateTransferBatchProcessor stateTransferBatchProcessor;\n+\n+    public ParallelTransferProcessor(int numInFlightRequests,\n+                                     StateTransferBatchProcessor stateTransferBatchProcessor) {\n+        this.semaphore = new Semaphore(numInFlightRequests);\n+        this.stateTransferBatchProcessor = stateTransferBatchProcessor;\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferProcessorResult> runStateTransfer\n+            (Stream<TransferBatchRequest> batchStream) {\n+        // Get a transfer batch request stream iterator\n+        Iterator<TransferBatchRequest> iterator = batchStream.iterator();\n+\n+        // No need to spawn tasks if there is no work to do.\n+        if (!iterator.hasNext()) {\n+            return CompletableFuture.completedFuture(TransferProcessorResult.builder()\n+                    .transferState(TRANSFER_SUCCEEDED).build());\n+        }\n+\n+        CompletableFuture<Void> allFutures = CompletableFuture.completedFuture(null);\n+\n+        while (iterator.hasNext() && !allFutures.isCompletedExceptionally()) {", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI0MTE3Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441241173", "bodyText": "@PavelZaytsev  After you fix this, please write a test for this, I think the existing test cases are not covering this.", "author": "WenbinZhu", "createdAt": "2020-06-17T02:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkwMzczNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441903734", "bodyText": "@WenbinZhu  I understand that this check is trying to fast fail to avoid more task being spawned. For those not completed in allFutures , it seems like calling completeExceptionally does not  cancel tasks behind CF, which means those tasks still consuming resources silently for a while.\nAccording to official doc:\n\nIf not already completed, causes invocations of {@link #get()}\n* and related methods to throw the given exception.\n\nIs it possible to assign an executor when creating tasks by cf? And once we got an exception, we can shutdownNow that executor.", "author": "zhangn49", "createdAt": "2020-06-18T00:29:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkxNDMyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441914325", "bodyText": "@zhangn49 We probably need to  change two things: 1) have a variable to record if an exception is thrown in any sub-furture and break out of loop, 2) call allFutures. completeExceptionally() after we break out of loop with exception and before we return allFutures.\nI'm not sure if your question is on the change above or on the usage of compleExceptionally(), if it's on completeExceptionally(), what I was referring to is calling allFutures. completeExceptionally(), after this, whoever waits on allFutures will complete immediately with an exception. You can verify this behavior with following code that fails after 2 seconds rather than 10 sec.\n    @Test\n    public void ttt() {\n        CompletableFuture<Void> f = CompletableFuture.runAsync(() -> {\n            Sleep.sleepUninterruptibly(Duration.ofSeconds(10));\n        });\n\n        Sleep.sleepUninterruptibly(Duration.ofSeconds(3));\n        f.completeExceptionally(new WrongEpochException(1));\n\n        f.join();\n    }", "author": "WenbinZhu", "createdAt": "2020-06-18T01:12:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzMTE1Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441931157", "bodyText": "@WenbinZhu Yes, my question is about usage of compleExceptionally(). I am saying that call compleExceptionally() does not cancel the real task. In this case, the real task is parallel read and write range.\nIn below test case, the real task is set flag. And it turns out that even if we call compleExceptionally() for all futures, those read and write still occurs..\n@Test\n    public void testCompleteExceptionally() {\n        AtomicBoolean flag = new AtomicBoolean(false);\n        CompletableFuture<Void> f = CompletableFuture.runAsync(() -> {\n            Sleep.sleepUninterruptibly(Duration.ofSeconds(10));\n            System.out.println(\"After 10s, set flag to true\");\n            flag.set(true);\n        });\n\n        Sleep.sleepUninterruptibly(Duration.ofSeconds(3));\n        f.completeExceptionally(new WrongEpochException(1));\n\n        try {\n            f.join();\n        } catch (CompletionException e) {\n            assertThat(e).hasCauseExactlyInstanceOf(WrongEpochException.class);\n        }\n        System.out.println(\"Let's wait 10 more seconds after join\");\n        Sleep.sleepUninterruptibly(Duration.ofSeconds(10));\n        assertThat(flag).isTrue();\n    }\nI am not sure keep those tasks running is good or bad. It silently transfer some batches, but I don't know how it handles exception in background.", "author": "zhangn49", "createdAt": "2020-06-18T02:20:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4MjU3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441982579", "bodyText": "@zhangn49 Yea, this can not be entriely avoid, but here what we care about is WrongEpochException, but since we are reading/writing only committed data, it should be fine for now.", "author": "WenbinZhu", "createdAt": "2020-06-18T05:50:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNzAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkwODgyNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441908826", "bodyText": "Retrieve trim mark does not have retry and it will rely on the whole workflow's retry mechanism.\nSince it will send getTrimMark request to all log unit severs, and TimeoutException/NetworkException will consume one time workflow retry. Is it a little dangerous?", "author": "zhangn49", "createdAt": "2020-06-18T00:50:02Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/orchestrator/actions/RestoreRedundancyMergeSegments.java", "diffHunk": "@@ -74,118 +84,201 @@\n     private final int restoreRetries = 3;\n \n     /**\n-     * Perform a state transfer on a current node, if needed, and then\n-     * propose a new layout based on a transfer result.\n-     * If a state transfer was not needed, try merging the segments\n-     * of a current layout and then proposing it.\n-     * We utilize an exponential backoff since there can be cases\n-     * when multiple nodes are proposing a new layout simultaneously.\n+     * A data class that stores both the layout and the transferred segments.\n+     */\n+    @AllArgsConstructor\n+    private static class LayoutTransferSegments {\n+        @Getter\n+        @NonNull\n+        private final Layout layout;\n+\n+        @Getter\n+        @NonNull\n+        private final ImmutableList<TransferSegment> transferSegments;\n+    }\n+\n+    /**\n+     * Perform state transfer and return all the transferred segments along with the latest layout.\n      *\n-     * @param runtime         A corfu runtime.\n-     * @param transferManager A transfer manager that runs the state transfer.\n-     * @return A new layout, if a redundancy restoration occurred; a current layout otherwise.\n+     * @param runtime         Current runtime.\n+     * @param transferManager Transfer manager instance to perform a state transfer.\n+     * @return Current layout and the list of transferred segments.\n      */\n-    @VisibleForTesting\n-    Layout restoreWithBackOff(CorfuRuntime runtime, StateTransferManager transferManager)\n+    private LayoutTransferSegments performStateTransfer(CorfuRuntime runtime,\n+                                                        StateTransferManager transferManager)\n             throws InterruptedException {\n-\n-        // Set up retry settings.\n+        // Settings for the retries\n         Consumer<ExponentialBackoffRetry> retrySettings = settings -> {\n             settings.setBase(retryBase);\n             settings.setExtraWait(extraWait.toMillis());\n             settings.setBackoffDuration(backoffDuration);\n             settings.setRandomPortion(randomPart);\n         };\n \n-        // Configure a number of retries.\n-        // Atomic is used here to overcome a restriction on the mutation of a local variable\n-        // within a lambda expression block.\n         AtomicInteger retries = new AtomicInteger(restoreRetries);\n         return IRetry.build(ExponentialBackoffRetry.class, RetryExhaustedException.class, () -> {\n             try {\n                 // Retrieve a current layout.\n                 runtime.invalidateLayout();\n                 Layout currentLayout = runtime.getLayoutView().getLayout();\n-\n+                if (currentLayout.getUnresponsiveServers().contains(currentNode)) {\n+                    throw new IllegalStateException(\"Node is in the unresponsive list.\");\n+                }\n                 log.info(\"State transfer on {}: Layout before transfer: {}\",\n                         currentNode, currentLayout);\n+                // Retrieve a current cluster trim mark.\n+                long trimMark = trimLog(runtime, currentLayout);", "originalCommit": "27b6fc0f3551c7453b9eaf645cab699b8d210eae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk2Mjc4NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r441962785", "bodyText": "Sure I can add some retries.", "author": "PavelZaytsev", "createdAt": "2020-06-18T04:33:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkwODgyNg=="}], "type": "inlineReview"}, {"oid": "894c79689e75bfbc9174738ff23bff0527319b7a", "url": "https://github.com/CorfuDB/CorfuDB/commit/894c79689e75bfbc9174738ff23bff0527319b7a", "message": "Introduce Parallel Transfer", "committedDate": "2020-06-18T21:49:27Z", "type": "commit"}, {"oid": "894c79689e75bfbc9174738ff23bff0527319b7a", "url": "https://github.com/CorfuDB/CorfuDB/commit/894c79689e75bfbc9174738ff23bff0527319b7a", "message": "Introduce Parallel Transfer", "committedDate": "2020-06-18T21:49:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2NzM1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442567351", "bodyText": "I think this still does not solve the problem we have.\nHere is your impl of CFUtils.allOfOrTerminateExceptionally:\nCompletableFuture<Void> result = CompletableFuture.allOf(futures);\nfor (CompletableFuture<?> future : futures) {\n    future.handle((res, ex) -> ex == null || result.completeExceptionally(ex));\n}\nLet's suppose this is the current allFutures structure:\na3<a2<a1<f0, f1>, f2>, f3>\nwhere a_i means allFuture, f_i means the individual transfer future.\nIf f1 fails at this moment, it will cancel a1, however f2 and f3 are not finished yet, so even though a1 is exceptionally completedly, the curerrnt allFuture, i.e. a3 still waits until f2 and f3 finishes. So if there are more futures, we won't be able to jump out of the loop, right?", "author": "WenbinZhu", "createdAt": "2020-06-19T00:18:57Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,139 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.AbstractCollection;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Semaphore;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.SEGMENT_INIT;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.SUCCEEDED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by dynamically distributing and parallelizing\n+ * the workload among the source log unit servers of each segment.\n+ */\n+@Slf4j\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A state transfer batch processor that performs the batch transfer.\n+     */\n+    private final StateTransferBatchProcessor stateTransferBatchProcessor;\n+\n+    /**\n+     * A number of in-flight requests per one node.\n+     */\n+    private static final int NUM_REQUESTS_PER_NODE = 5;\n+\n+    public ParallelTransferProcessor(StateTransferBatchProcessor stateTransferBatchProcessor) {\n+        this.stateTransferBatchProcessor = stateTransferBatchProcessor;\n+    }\n+\n+    private CompletableFuture<Void> handleNonTransferException(CompletableFuture<Void> futures,\n+                                                               Throwable ex) {\n+        CompletableFuture<Void> failedFuture = new CompletableFuture<>();\n+        failedFuture.completeExceptionally(ex);\n+        return CFUtils.allOfOrTerminateExceptionally(futures, failedFuture);\n+    }\n+\n+    private CompletableFuture<Void> handleBatchRequest(TransferBatchRequest request,\n+                                                       CompletableFuture<Void> allFutures,\n+                                                       Semaphore semaphore) throws InterruptedException {\n+        // If the request is of type SEGMENT_INIT - it signals to the batch processor that the\n+        // consecutive DATA requests all belong to the same segment. Since all the DATA requests\n+        // that belong to the same segment have the same number of source nodes, we can dynamically\n+        // configure the semaphore to support the average parallelism level for the entire segment\n+        // which is equal to the NUM_REQUEST_PER_NODE * number of source nodes in the segment.\n+        if (request.getBatchType() == SEGMENT_INIT) {\n+            int numPermits = request.getDestinationNodes()\n+                    .map(AbstractCollection::size).orElse(1) *\n+                    NUM_REQUESTS_PER_NODE;\n+            if (numPermits == 0) {\n+                throw new IllegalStateException(\"Number of permits should be equal to\" +\n+                        \" at least one.\");\n+            }\n+            log.trace(\"Starting a new segment transfer. Parallelism level: {}\", numPermits);\n+            // Acquire all the permits if any.\n+            semaphore.drainPermits();\n+            // Release numPermits for the batch transfer futures to acquire.\n+            semaphore.release(numPermits);\n+        }\n+        else if (request.getBatchType() == DATA) {\n+            semaphore.acquire();\n+            CompletableFuture<Void> batchTransferResult =\n+                    stateTransferBatchProcessor\n+                            .transfer(request)\n+                            .thenApply(response -> {\n+                                semaphore.release();\n+                                if (response.getStatus() == SUCCEEDED) {\n+                                    return null;\n+                                } else if (response.getStatus() == FAILED &&\n+                                        response.getCauseOfFailure().isPresent()) {\n+                                    throw response.getCauseOfFailure().get();\n+                                } else {\n+                                    throw new StateTransferBatchProcessorException();\n+                                }\n+                            });\n+\n+            allFutures = CFUtils.allOfOrTerminateExceptionally(allFutures, batchTransferResult);", "originalCommit": "894c79689e75bfbc9174738ff23bff0527319b7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3MzAyMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442573020", "bodyText": "@WenbinZhu I am pretty sure it works fine and I also wrote a test for it. You can check the test testRunParallelTransferShortCircuitsOnFailure. You can replace the CFUtils.allOfOrTerminateExceptionally with CompletableFuture.allOf and see that the test does not go through.", "author": "PavelZaytsev", "createdAt": "2020-06-19T00:42:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2NzM1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3MzIwMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442573200", "bodyText": "@WenbinZhu Let me know if you think the test does not capture it.", "author": "PavelZaytsev", "createdAt": "2020-06-19T00:43:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2NzM1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3OTI3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442579277", "bodyText": "Ok I see, I didn't notice both futures in allOf are attached a handler, so it can exit. But the problem is when exception happens in the lowest level future, the exception needs to propagate multiple levels all the way to the outermost future, while we could have a variable to jump out and only fail the outmost future once. But I think it's ok for now.", "author": "WenbinZhu", "createdAt": "2020-06-19T01:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2NzM1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2OTQ1NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442569455", "bodyText": "This is quite hacky and probably incorrect. drainPermits() only drains currently \"available\" permits, after that previously acquired permits can still be released, reusulting in more permits than numPermits here. I think we need to transfer segments one by one and within each segment, you can parallelize it with a fixed number of permits.", "author": "WenbinZhu", "createdAt": "2020-06-19T00:27:05Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/transferprocessor/ParallelTransferProcessor.java", "diffHunk": "@@ -0,0 +1,139 @@\n+package org.corfudb.infrastructure.log.statetransfer.transferprocessor;\n+\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.TransferSegmentException;\n+import org.corfudb.util.CFUtils;\n+\n+import java.util.AbstractCollection;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Semaphore;\n+import java.util.stream.Stream;\n+\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.DATA;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest.TransferBatchType.SEGMENT_INIT;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.SUCCEEDED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_FAILED;\n+import static org.corfudb.infrastructure.log.statetransfer.transferprocessor.TransferProcessor.TransferProcessorResult.TransferProcessorStatus.TRANSFER_SUCCEEDED;\n+\n+/**\n+ * A transfer processor that performs state transfer by dynamically distributing and parallelizing\n+ * the workload among the source log unit servers of each segment.\n+ */\n+@Slf4j\n+public class ParallelTransferProcessor implements TransferProcessor {\n+    /**\n+     * A state transfer batch processor that performs the batch transfer.\n+     */\n+    private final StateTransferBatchProcessor stateTransferBatchProcessor;\n+\n+    /**\n+     * A number of in-flight requests per one node.\n+     */\n+    private static final int NUM_REQUESTS_PER_NODE = 5;\n+\n+    public ParallelTransferProcessor(StateTransferBatchProcessor stateTransferBatchProcessor) {\n+        this.stateTransferBatchProcessor = stateTransferBatchProcessor;\n+    }\n+\n+    private CompletableFuture<Void> handleNonTransferException(CompletableFuture<Void> futures,\n+                                                               Throwable ex) {\n+        CompletableFuture<Void> failedFuture = new CompletableFuture<>();\n+        failedFuture.completeExceptionally(ex);\n+        return CFUtils.allOfOrTerminateExceptionally(futures, failedFuture);\n+    }\n+\n+    private CompletableFuture<Void> handleBatchRequest(TransferBatchRequest request,\n+                                                       CompletableFuture<Void> allFutures,\n+                                                       Semaphore semaphore) throws InterruptedException {\n+        // If the request is of type SEGMENT_INIT - it signals to the batch processor that the\n+        // consecutive DATA requests all belong to the same segment. Since all the DATA requests\n+        // that belong to the same segment have the same number of source nodes, we can dynamically\n+        // configure the semaphore to support the average parallelism level for the entire segment\n+        // which is equal to the NUM_REQUEST_PER_NODE * number of source nodes in the segment.\n+        if (request.getBatchType() == SEGMENT_INIT) {\n+            int numPermits = request.getDestinationNodes()\n+                    .map(AbstractCollection::size).orElse(1) *\n+                    NUM_REQUESTS_PER_NODE;\n+            if (numPermits == 0) {\n+                throw new IllegalStateException(\"Number of permits should be equal to\" +\n+                        \" at least one.\");\n+            }\n+            log.trace(\"Starting a new segment transfer. Parallelism level: {}\", numPermits);\n+            // Acquire all the permits if any.\n+            semaphore.drainPermits();\n+            // Release numPermits for the batch transfer futures to acquire.\n+            semaphore.release(numPermits);", "originalCommit": "894c79689e75bfbc9174738ff23bff0527319b7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTE0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r442619146", "bodyText": "@WenbinZhu Ok, it makes sense. I split the workload by segments. Please see if the code makes sense.", "author": "PavelZaytsev", "createdAt": "2020-06-19T04:03:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2OTQ1NQ=="}], "type": "inlineReview"}, {"oid": "7032caeb4e0191abcafd14252609a19adef9a3d6", "url": "https://github.com/CorfuDB/CorfuDB/commit/7032caeb4e0191abcafd14252609a19adef9a3d6", "message": "Fixes", "committedDate": "2020-06-19T04:00:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEwNjYwNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r443106607", "bodyText": "The second brace is not necessary.\nreadRecords: encountered a wrong epoch exception on try 1: {}. org.corfudb.runtime.exceptions.WrongEpochException: Wrong epoch. [expected=1]", "author": "zhangn49", "createdAt": "2020-06-20T06:31:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,192 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.WrongEpochException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+import org.corfudb.util.Sleep;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final List<Long> originalAddresses;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalAddresses = request.getAddresses();\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(originalAddresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    private CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new ReadBatchException(\"No target nodes left to select from\");\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferBatchResponse> transfer(\n+            TransferBatchRequest transferBatchRequest) {\n+        return tryTransferForRandomNodes(new RandomNodeIterator(transferBatchRequest))\n+                .exceptionally(error -> TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(transferBatchRequest)\n+                        .status(FAILED)\n+                        .causeOfFailure(Optional.of(new StateTransferBatchProcessorException(\n+                                \"Failed batch: \" + transferBatchRequest, error)))\n+                        .build()\n+                );\n+    }\n+\n+    /**\n+     * Read records directly from the randomly scheduled destination node (don't hole fill).\n+     *\n+     * @param addresses A batch of consecutive addresses.\n+     * @param destNode  An optional destination node.\n+     * @param client    A log unit client to the node.\n+     * @return A read batch if the read went fine, an exception otherwise.\n+     */\n+    public ReadBatch readRecords(List<Long> addresses,\n+                                 Optional<String> destNode,\n+                                 LogUnitClient client) {\n+        for (int i = 0; i < maxReadRetries; i++) {\n+            try {\n+                ReadResponse response = CFUtils\n+                        .getUninterruptibly(client.readAll(addresses));\n+                Map<Long, ILogData> records = new HashMap<>(response.getAddresses());\n+                ReadBatch readBatch = checkReadRecords(addresses,\n+                        records, destNode);\n+                if (readBatch.getStatus() == ReadBatch.ReadStatus.FAILED) {\n+                    throw new IllegalStateException(\"Some addresses failed to transfer: \" +\n+                            readBatch.getFailedAddresses());\n+                }\n+                return readBatch;\n+\n+            } catch (WrongEpochException e) {\n+                // If the WEE is hit, fail immediately.\n+                log.warn(\"readRecords: encountered a wrong epoch exception on try {}: {}.\",\n+                        i, e);\n+                throw e;\n+\n+            } catch (RuntimeException e) {\n+                log.warn(\"readRecords: encountered an exception on try {}: {}.\", i, e);", "originalCommit": "7032caeb4e0191abcafd14252609a19adef9a3d6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEwNjY3Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r443106676", "bodyText": "same here.", "author": "zhangn49", "createdAt": "2020-06-20T06:32:47Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/protocolbatchprocessor/ProtocolBatchProcessor.java", "diffHunk": "@@ -93,79 +90,32 @@\n     CompletableFuture<ReadBatch> readRecords(TransferBatchRequest transferBatchRequest) {\n         return CompletableFuture.supplyAsync(() -> {\n \n-            Optional<ReadBatchException> latestReadException = Optional.empty();\n-            Optional<ReadBatch> latestReadBatch = Optional.empty();\n             for (int i = 0; i < maxReadRetries; i++) {\n                 try {\n                     Map<Long, ILogData> records =\n-                            addressSpaceView.simpleProtocolRead(transferBatchRequest.getAddresses(), readOptions);\n-                    ReadBatch batch = checkReadRecords(transferBatchRequest.getAddresses(),\n-                            records, transferBatchRequest.getDestination());\n-                    latestReadBatch = Optional.of(batch);\n+                            addressSpaceView.simpleProtocolRead(\n+                                    transferBatchRequest.getAddresses(),\n+                                    readOptions);\n+                    ReadBatch batch = checkReadRecords(\n+                            transferBatchRequest.getAddresses(),\n+                            records, Optional.empty());\n                     if (batch.getStatus() == ReadBatch.ReadStatus.FAILED) {\n                         throw new IllegalStateException(\"Some addresses failed to transfer: \" +\n                                 batch.getFailedAddresses());\n-                    } else {\n-                        latestReadException = Optional.empty();\n-                        break;\n                     }\n+                    return batch;\n \n                 } catch (WrongEpochException e) {\n-                    log.warn(\"readRecords: encountered a wrong epoch exception on try {}: \", i, e);\n-                    latestReadException = Optional.of(new ReadBatchException(e));\n-                    break;\n+                    log.warn(\"readRecords: encountered a wrong epoch exception on try {}: {}.\",\n+                            i, e);\n+                    throw e;\n                 } catch (RuntimeException e) {\n-                    log.warn(\"readRecords: encountered an exception on try {}: \", i, e);\n-                    latestReadException = Optional.of(new ReadBatchException(e));\n+                    log.warn(\"readRecords: encountered an exception on try {}: {}.\", i, e);", "originalCommit": "7032caeb4e0191abcafd14252609a19adef9a3d6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEwNjgyMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2566#discussion_r443106820", "bodyText": "Is throw IllegalStateException here in order to retry read? Just want to confirm my understanding.", "author": "zhangn49", "createdAt": "2020-06-20T06:35:48Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/log/statetransfer/batchprocessor/committedbatchprocessor/CommittedBatchProcessor.java", "diffHunk": "@@ -0,0 +1,192 @@\n+package org.corfudb.infrastructure.log.statetransfer.batchprocessor.committedbatchprocessor;\n+\n+import com.google.common.collect.ImmutableList;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import org.corfudb.infrastructure.log.statetransfer.batch.ReadBatch;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequest;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchRequestForNode;\n+import org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse;\n+import org.corfudb.infrastructure.log.statetransfer.batchprocessor.StateTransferBatchProcessor;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.ReadBatchException;\n+import org.corfudb.infrastructure.log.statetransfer.exceptions.StateTransferBatchProcessorException;\n+import org.corfudb.protocols.wireprotocol.ILogData;\n+import org.corfudb.protocols.wireprotocol.ReadResponse;\n+import org.corfudb.runtime.clients.LogUnitClient;\n+import org.corfudb.runtime.exceptions.WrongEpochException;\n+import org.corfudb.runtime.view.RuntimeLayout;\n+import org.corfudb.util.CFUtils;\n+import org.corfudb.util.Sleep;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static lombok.Builder.Default;\n+import static org.corfudb.infrastructure.log.statetransfer.batch.TransferBatchResponse.TransferStatus.FAILED;\n+\n+/**\n+ * A transferBatchRequest processor that transfers committed addresses one transferBatchRequest\n+ * at a time via reading directly from the provided log unit servers in a random order.\n+ * A committed address is the one that belongs to the part of an address space that has been\n+ * previously committed by the auto-commit service or some previously run state transfer.\n+ */\n+@Slf4j\n+@Builder\n+@Getter\n+public class CommittedBatchProcessor implements StateTransferBatchProcessor {\n+\n+    /**\n+     * Configurations for the retry logic.\n+     */\n+    @Default\n+    private final int maxReadRetries = 3;\n+    @Default\n+    private final Duration readSleepDuration = Duration.ofMillis(300);\n+    @Default\n+    private final int maxWriteRetries = 3;\n+    @Default\n+    private final Duration writeSleepDuration = Duration.ofMillis(300);\n+\n+    /**\n+     * Current node.\n+     */\n+    private final String currentNode;\n+    /**\n+     * Current corfu runtime layout.\n+     */\n+    private final RuntimeLayout runtimeLayout;\n+\n+\n+    /**\n+     * An iterator over a provided transfer batch request. It iterates over the available\n+     * destination nodes in the random order producing the TransferBatchRequestForNode requests.\n+     * This allows to distribute the workload among the multiple consecutive transfer batch\n+     * requests destined for this batch processor somewhat evenly.\n+     */\n+    public static class RandomNodeIterator implements Iterator<TransferBatchRequestForNode> {\n+\n+        private final List<Long> originalAddresses;\n+        private final Iterator<String> iter;\n+\n+        public RandomNodeIterator(TransferBatchRequest request) {\n+            originalAddresses = request.getAddresses();\n+            ArrayList<String> nodes = new ArrayList<>(request.getDestinationNodes()\n+                    .orElse(ImmutableList.of()));\n+            Collections.shuffle(nodes);\n+            iter = nodes.iterator();\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return iter.hasNext();\n+        }\n+\n+        @Override\n+        public TransferBatchRequestForNode next() {\n+            String nextNode = iter.next();\n+            return new TransferBatchRequestForNode(originalAddresses, nextNode);\n+        }\n+    }\n+\n+    /**\n+     * Given a random node iterator created from the transfer batch request, try perform a\n+     * state transfer by reading the data from the randomly picked log unit and writing it to\n+     * the current log unit server. If the read failed for a node, pick the next available one\n+     * randomly and retry. If the transfer failed for some other reason (e.g. during the writes) or\n+     * if there are no more nodes left to distribute the workload over,\n+     * fail the transfer with an exception.\n+     *\n+     * @param nodeIterator A random node iterator produced from the transfer batch request.\n+     * @return A normally completed future if the transfer for this batch succeeds, an exceptionally\n+     * completed future, otherwise.\n+     */\n+    private CompletableFuture<TransferBatchResponse> tryTransferForRandomNodes(\n+            RandomNodeIterator nodeIterator) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            while (nodeIterator.hasNext()) {\n+                TransferBatchRequestForNode transferBatchRequestForNode = nodeIterator.next();\n+                List<Long> addresses = transferBatchRequestForNode.getAddresses();\n+                String destinationNode = transferBatchRequestForNode.getDestinationNode();\n+                LogUnitClient logUnitClientToTargetNode =\n+                        runtimeLayout.getLogUnitClient(destinationNode);\n+                LogUnitClient logUnitClientToCurrentNode =\n+                        runtimeLayout.getLogUnitClient(currentNode);\n+                try {\n+                    ReadBatch readBatch = readRecords(addresses,\n+                            Optional.of(destinationNode), logUnitClientToTargetNode);\n+                    return writeRecords(readBatch, logUnitClientToCurrentNode,\n+                            maxWriteRetries, writeSleepDuration);\n+                } catch (ReadBatchException re) {\n+                    log.warn(\"Read exception for node {} occurred. \" +\n+                            \"Retry transfer for the next available node.\", destinationNode, re);\n+                } catch (Exception e) {\n+                    log.error(\"Exception during batch transfer occurred. Failing the transfer.\", e);\n+                    throw e;\n+                }\n+            }\n+            throw new ReadBatchException(\"No target nodes left to select from\");\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<TransferBatchResponse> transfer(\n+            TransferBatchRequest transferBatchRequest) {\n+        return tryTransferForRandomNodes(new RandomNodeIterator(transferBatchRequest))\n+                .exceptionally(error -> TransferBatchResponse\n+                        .builder()\n+                        .transferBatchRequest(transferBatchRequest)\n+                        .status(FAILED)\n+                        .causeOfFailure(Optional.of(new StateTransferBatchProcessorException(\n+                                \"Failed batch: \" + transferBatchRequest, error)))\n+                        .build()\n+                );\n+    }\n+\n+    /**\n+     * Read records directly from the randomly scheduled destination node (don't hole fill).\n+     *\n+     * @param addresses A batch of consecutive addresses.\n+     * @param destNode  An optional destination node.\n+     * @param client    A log unit client to the node.\n+     * @return A read batch if the read went fine, an exception otherwise.\n+     */\n+    public ReadBatch readRecords(List<Long> addresses,\n+                                 Optional<String> destNode,\n+                                 LogUnitClient client) {\n+        for (int i = 0; i < maxReadRetries; i++) {\n+            try {\n+                ReadResponse response = CFUtils\n+                        .getUninterruptibly(client.readAll(addresses));\n+                Map<Long, ILogData> records = new HashMap<>(response.getAddresses());\n+                ReadBatch readBatch = checkReadRecords(addresses,\n+                        records, destNode);\n+                if (readBatch.getStatus() == ReadBatch.ReadStatus.FAILED) {\n+                    throw new IllegalStateException(\"Some addresses failed to transfer: \" +\n+                            readBatch.getFailedAddresses());\n+                }", "originalCommit": "7032caeb4e0191abcafd14252609a19adef9a3d6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}