{"pr_number": 2703, "pr_title": "Stream write path performance optimization.", "pr_createdAt": "2020-08-12T02:28:03Z", "pr_url": "https://github.com/CorfuDB/CorfuDB/pull/2703", "timeline": [{"oid": "190a4c0cd682c1dfe7340b572459349816686892", "url": "https://github.com/CorfuDB/CorfuDB/commit/190a4c0cd682c1dfe7340b572459349816686892", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-12T02:55:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5MjMxNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r468992317", "bodyText": "Would be great if you replace boolean by\nenum SerializationMetaData {\n        INCLUDE, EXCLUDE\n    }\n\nif possible", "author": "xnull", "createdAt": "2020-08-12T04:04:10Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/ILogData.java", "diffHunk": "@@ -51,11 +49,12 @@ public ILogData getSerialized() {\n          * Create a new serialized handle with a reference\n          * to the log data.\n          *\n-         * @param data The log data to manage.\n+         * @param data     the log data to manage\n+         * @param metadata whether metadata needs to be serialized\n          */\n-        public SerializationHandle(ILogData data) {\n+        public SerializationHandle(ILogData data, boolean metadata) {", "originalCommit": "190a4c0cd682c1dfe7340b572459349816686892", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5Mjg4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r468992889", "bodyText": "a better one getBackpointerMap().containsKey(stream)", "author": "xnull", "createdAt": "2020-08-12T04:06:44Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/IMetadata.java", "diffHunk": "@@ -54,11 +47,12 @@\n \n     /**\n      * Get whether or not this entry contains a given stream.\n-     * @param stream    The stream to check.\n-     * @return          True, if the entry contains the given stream.\n+     *\n+     * @param stream The stream to check.\n+     * @return True, if the entry contains the given stream.\n      */\n     default boolean containsStream(UUID stream) {\n-        return  getBackpointerMap().keySet().contains(stream);\n+        return getBackpointerMap().keySet().contains(stream);", "originalCommit": "190a4c0cd682c1dfe7340b572459349816686892", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NjI5MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469486290", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5Mjg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5NDMyNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r468994327", "bodyText": "it should be static\n\ntake into account how many instances of LogData we have when Corfu works it should be a huge overhead", "author": "xnull", "createdAt": "2020-08-12T04:13:16Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -32,14 +31,20 @@\n     @Getter\n     byte[] data;\n \n-    private ByteBuf serializedCache = null;\n+    private SerializedCache serializedCache = null;\n \n     private int lastKnownSize = NOT_KNOWN;\n \n     private final transient AtomicReference<Object> payload = new AtomicReference<>();\n \n     private final EnumMap<LogUnitMetadataType, Object> metadataMap;\n \n+    @RequiredArgsConstructor\n+    private class SerializedCache {", "originalCommit": "190a4c0cd682c1dfe7340b572459349816686892", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NDM1Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469484353", "bodyText": "What type of overhead ?", "author": "Maithem", "createdAt": "2020-08-12T19:17:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5NDMyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NjIyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469486229", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5NDMyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI3MDcwNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r482270706", "bodyText": "What type of overhead ?\n\nThis is what I remember from \"Effective Java\"\nhttps://stackoverflow.com/questions/43998428/why-to-favor-static-member-classes-over-nonstatic-joshua-bloch-item-22#:~:text=Answer%3A,reference%20to%20its%20enclosing%20instance.", "author": "xnull", "createdAt": "2020-09-02T18:15:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5NDMyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5NTEwOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r468995108", "bodyText": "I think serializedCache should be Optional it will make code more secure and less error-prone", "author": "xnull", "createdAt": "2020-08-12T04:16:08Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -32,14 +31,20 @@\n     @Getter\n     byte[] data;\n \n-    private ByteBuf serializedCache = null;\n+    private SerializedCache serializedCache = null;", "originalCommit": "190a4c0cd682c1dfe7340b572459349816686892", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDMzMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469004331", "bodyText": "if you invert the if condition it makes code a bit more straightforward\n    public synchronized void acquireBuffer(boolean metadata) {\n        if (serializedCache == null) {\n            ByteBuf buf = Unpooled.buffer();\n            int metadataOffset;\n            \n            if (metadata) {\n                metadataOffset = doSerializeInternal(buf);\n            } else {\n                doSerializePayloadInternal(buf);\n                metadataOffset = buf.writerIndex();\n            }\n            serializedCache = new SerializedCache(buf, metadataOffset);\n        } else {\n            if (metadata) {\n                serializedCache.buffer.resetReaderIndex();\n                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);\n                doSerializeMetadataInternal(serializedCache.buffer);\n            }\n            serializedCache.buffer.retain();\n        }\n    }", "author": "xnull", "createdAt": "2020-08-12T04:54:28Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,32 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n-        if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+    public synchronized void acquireBuffer(boolean metadata) {\n+        if (serializedCache != null) {", "originalCommit": "190a4c0cd682c1dfe7340b572459349816686892", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NjA5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469486096", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:20:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDMzMQ=="}], "type": "inlineReview"}, {"oid": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "url": "https://github.com/CorfuDB/CorfuDB/commit/eade3dd0fec53507e9e180d5965e92a6addc93fa", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-12T05:28:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNjA3NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469036075", "bodyText": "This is a perfect place to use a CompositeByteBuf. The logic of maintaining an offset and overwriting can be greatly simplified. You can also remove the SerializedCache class\n        CompositeByteBuf serializationCache = Unpooled.compositeBuffer();\n        ByteBuf metadata = Unpooled.buffer();\n        ByteBuf payload = Unpooled.buffer();\n        serializationCache.addComponents(metadata, payload);", "author": "Maithem", "createdAt": "2020-08-12T06:37:19Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,32 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n-        if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+    public synchronized void acquireBuffer(boolean metadata) {\n+        if (serializedCache != null) {\n+            if (metadata) {\n+                serializedCache.buffer.resetReaderIndex();\n+                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);\n+                doSerializeMetadataInternal(serializedCache.buffer);\n+            }\n+            serializedCache.buffer.retain();\n+            return;\n+        }\n+\n+        ByteBuf buf = Unpooled.buffer();\n+        if (metadata) {\n+            int metadataOffset = doSerializeInternal(buf);\n+            serializedCache = new SerializedCache(buf, metadataOffset);\n         } else {\n-            serializedCache.retain();\n+            doSerializePayloadInternal(buf);\n+            serializedCache = new SerializedCache(buf, buf.writerIndex());", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NDE2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469484169", "bodyText": "@Maithem Using CompositeByteBuf is exactly same idea that I was initially thinking, but later I changed to simply use one a buffer and offset, because the major purpose of CompositeByteBuf is to provide a single reader/writer index for multiple buffers, but in our case, we mostly want to overwrite the metadata part, which gives two options: 1) using the single reader/writer index that CompositeByteBuf provide, which is same as the current approach, and it's cleaner to just use the normal buffer; 2) maniputate the internal metadata sub-component. This would make confusion on the indexes, and also it's not the standard pattern of using CompositeByteBuf. So I changed to simply use a normal buffer and offset.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:16:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNjA3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUwMTgyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469501825", "bodyText": "I thought that was one of the main use cases of the CompositeByteBuf: allowing multiple buffers to be manipulated independently with the ability to append them to each other cheaply.\nhttps://livebook.manning.com/book/netty-in-action/chapter-5/47", "author": "Maithem", "createdAt": "2020-08-12T19:51:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNjA3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxNDA3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469514077", "bodyText": "So I assume you're referring to the second approahc that manipulates internal metadata sub-component without the offset variable? I that case, if we changed the metadata component, then you also need to change the unified writer index to payload componet writer index + new metadata size. I think this kind of manipulating writer index after write finishes seems error prone and would cause confusion, and it's not clean as having a normal buffer...", "author": "WenbinZhu", "createdAt": "2020-08-12T20:15:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNjA3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNzcxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469037712", "bodyText": "I think the metadata serialization is cheap enough to actually do it twice.\nI would just serialize both metadata + payload here. Then after getting a token just call doSerializeMetadataInternal with the last ByteBuf of the Composite buffer. It will overwrite it with the new token information.\nInstead of adding the boolean here, you can just introduce a method updateMetadata that will rewrite the metadata portion.", "author": "Maithem", "createdAt": "2020-08-12T06:41:22Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/ILogData.java", "diffHunk": "@@ -67,13 +66,28 @@ public void close() {\n         }\n     }\n \n-    void releaseBuffer();\n+    /**\n+     * Get the serialization handle of this entry that manages\n+     * the lifetime of the serialized copy.\n+     *\n+     * @param metadata whether metadata needs to be serialized\n+     * @return a serialization handle of this entry\n+     */\n+    default SerializationHandle getSerializedForm(boolean metadata) {\n+        return new SerializationHandle(this, metadata);\n+    }\n \n-    void acquireBuffer();\n+    /**\n+     * Release the serialization buffer.\n+     */\n+    void releaseBuffer();\n \n-    default SerializationHandle getSerializedForm() {\n-        return new SerializationHandle(this);\n-    }\n+    /**\n+     * Acquire the serialization buffer.\n+     *\n+     * @param metadata whether metadata needs to be serialized\n+     */\n+    void acquireBuffer(boolean metadata);", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NTk1NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469485954", "bodyText": "The reason I added this flag is that sometimes, metadata is even larger compared to the payload (e.g. logunit cache oom issue we used to fix), and also when the token is not being set, the metadata is in-complete, serializing the metadata makes the meaning of the serialized cache a bit unclear. What do you think?", "author": "WenbinZhu", "createdAt": "2020-08-12T19:20:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNzcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MDg1NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471040855", "bodyText": "I see.", "author": "Maithem", "createdAt": "2020-08-15T22:43:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzNzcxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzODc4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469038784", "bodyText": "Can remove the first clause if you change maxWriteSize to Integer.MAX_VALUE", "author": "Maithem", "createdAt": "2020-08-12T06:44:05Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -340,12 +359,19 @@ public String toString() {\n      * Verify that max payload is enforced for the specified limit.\n      *\n      * @param limit Max write limit.\n+     * @return the serialized size of the payload\n      */\n-    public void checkMaxWriteSize(int limit) {\n-        try (ILogData.SerializationHandle sh = this.getSerializedForm()) {\n-            if (limit != 0 && getSizeEstimate() > limit) {\n-                throw new WriteSizeException(getSizeEstimate(), limit);\n-            }\n+    public int checkMaxWriteSize(int limit) {\n+        if (serializedCache == null) {\n+            throw new IllegalStateException(\"checkMaxWriteSize requires serialized form\");\n         }\n+\n+        int payloadSize = getSizeEstimate();\n+\n+        if (limit != 0 && payloadSize > limit) {", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NjM4Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469486383", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:20:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAzODc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MjI5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469042299", "bodyText": "This doesn't look correct.\nSince the SerializationHandle is acquired before the for-loop retries that will end up calling preCommitCallback will change the payload, but it will not be re-serialized. Only the token portion will be. This can introduce correctness issues to the queue ordering.\nRIP. I don't remember seeing a PR that introduced this code. It seems like it went in #2498, but it has no reviews.", "author": "Maithem", "createdAt": "2020-08-12T06:51:47Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -132,90 +132,98 @@ public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictIn\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(false)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n+\n+                // Is our token a valid type?\n+                AbortCause abortCause = null;\n+                switch (tokenResponse.getRespType()) {\n+                    case TX_ABORT_CONFLICT:\n+                        abortCause = AbortCause.CONFLICT;\n+                        break;\n+                    case TX_ABORT_NEWSEQ:\n+                        abortCause = AbortCause.NEW_SEQUENCER;\n+                        break;\n+                    case TX_ABORT_SEQ_OVERFLOW:\n+                        abortCause = AbortCause.SEQUENCER_OVERFLOW;\n+                        break;\n+                    case TX_ABORT_SEQ_TRIM:\n+                        abortCause = AbortCause.SEQUENCER_TRIM;\n+                        break;\n                 }\n \n-                // Attempt to write to the log.\n-                runtime.getAddressSpaceView().write(tokenResponse, ld, cacheOption);\n-                // If we're here, we succeeded, return the acquired token.\n-                return tokenResponse.getSequence();\n-            } catch (OverwriteException oe) {\n-                // We were overwritten, get a new token and try again.\n-                log.warn(\"append[{}]: Overwritten after {} retries, streams {}\",\n-                        tokenResponse.getSequence(), x,\n-                        Arrays.stream(streamIDs).map(Utils::toReadableId).collect(Collectors.toSet()));\n-\n-                if (conflictInfo != null) {\n-                    // On retry, check for conflicts only from the previous attempt position,\n-                    // otherwise the transaction will always conflict with itself.\n-                    conflictInfo.setSnapshotTimestamp(tokenResponse.getToken());\n+                if (abortCause != null) {\n+                    throw new TransactionAbortedException(\n+                            conflictInfo,\n+                            tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n+                            tokenResponse.getToken().getSequence(), abortCause,\n+                            TransactionalContext.getCurrentContext());\n                 }\n \n-            } catch (StaleTokenException se) {\n-                // the epoch changed from when we grabbed the token from sequencer\n-                log.warn(\"append[{}]: StaleToken, streams {}\", tokenResponse.getSequence(),\n-                        Arrays.stream(streamIDs).map(Utils::toReadableId).collect(Collectors.toSet()));\n-\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(),\n-                        AbortCause.NEW_SEQUENCER, // in the future perhaps define a new AbortCause?\n-                        TransactionalContext.getCurrentContext());\n+                try {\n+                    if (TransactionalContext.isInTransaction()) {\n+                        // If this transaction has entries that wish to capture the committed address\n+                        // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n+                        // Note that we might invoke the same method multiple times on retries,\n+                        // which means the preCommitCallback must be idempotent.\n+                        TokenResponse finalTokenResponse = tokenResponse;\n+                        log.debug(\"append: Invoking {} preCommitListeners\",\n+                                TransactionalContext.getRootContext().getPreCommitListeners().size());\n+                        TransactionalContext.getRootContext()\n+                                .getPreCommitListeners()\n+                                .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+                    }\n+", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ4NzkxMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469487913", "bodyText": "#2498 is the backporting, the PR on master went through reviews.\nA quick fix can be putting the serialization in for-loop, but I think it's not the best way of doing it, because we really don't need to serialize payload on every retry just to make it work for the queue case.\n@Maithem @hisundar Let's have a discussion to see what is a better solution.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:23:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MjI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUwMzE4Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469503187", "bodyText": "I see.\nEven pre-this patch this call back can introduce a correctness issue. Imagine getting a commit token for a transaction, but then before replicating the call back changes the payload's contents such that the initial transaction resolution step is invalid.", "author": "Maithem", "createdAt": "2020-08-12T19:53:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MjI5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MzA4MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469043080", "bodyText": "Can you just set the token here instead of propegating it down and setting it later. Ideally, we should just send a LogData object to the write method", "author": "Maithem", "createdAt": "2020-08-12T06:53:36Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -132,90 +132,98 @@ public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictIn\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(false)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n+\n+                // Is our token a valid type?\n+                AbortCause abortCause = null;\n+                switch (tokenResponse.getRespType()) {\n+                    case TX_ABORT_CONFLICT:\n+                        abortCause = AbortCause.CONFLICT;\n+                        break;\n+                    case TX_ABORT_NEWSEQ:\n+                        abortCause = AbortCause.NEW_SEQUENCER;\n+                        break;\n+                    case TX_ABORT_SEQ_OVERFLOW:\n+                        abortCause = AbortCause.SEQUENCER_OVERFLOW;\n+                        break;\n+                    case TX_ABORT_SEQ_TRIM:\n+                        abortCause = AbortCause.SEQUENCER_TRIM;\n+                        break;\n                 }\n \n-                // Attempt to write to the log.\n-                runtime.getAddressSpaceView().write(tokenResponse, ld, cacheOption);", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ5MzgxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469493810", "bodyText": "Because when writing the logdata, writer also needs to set the clientId, thereadId, I think it's cleaner to let the writer do that as current impl does, putting that logic into LogData seems a bit unclean and we have to do that all over the places.", "author": "WenbinZhu", "createdAt": "2020-08-12T19:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MzA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0NDM2Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r469044366", "bodyText": "Instead of having checkMaxWriteSize return the size, you can directly access it from the serialization handle", "author": "Maithem", "createdAt": "2020-08-12T06:56:17Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/stream/AbstractQueuedStreamView.java", "diffHunk": "@@ -177,65 +177,72 @@ public long append(Object object,\n                        Function<TokenResponse, Boolean> deacquisitionCallback) {\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n \n-        // Validate if the  size of the log data is under max write size.\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n-        // First, we get a token from the sequencer.\n-        TokenResponse tokenResponse = runtime.getSequencerView()\n-                .next(id);\n-\n-        // We loop forever until we are interrupted, since we may have to\n-        // acquire an address several times until we are successful.\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Next, we call the acquisitionCallback, if present, informing\n-            // the client of the token that we acquired.\n-            if (acquisitionCallback != null) {\n-                if (!acquisitionCallback.apply(tokenResponse)) {\n-                    // The client did not like our token, so we end here.\n-                    // We'll leave the hole to be filled by the client or\n-                    // someone else.\n-                    log.debug(\"Acquisition rejected token={}\", tokenResponse);\n-                    return -1L;\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(false)) {\n+            // Validate if the  size of the log data is under max write size.\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());", "originalCommit": "eade3dd0fec53507e9e180d5965e92a6addc93fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a82780986fc1e6abd76367d2b7287e3ba88b57d6", "url": "https://github.com/CorfuDB/CorfuDB/commit/a82780986fc1e6abd76367d2b7287e3ba88b57d6", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-12T19:11:41Z", "type": "forcePushed"}, {"oid": "7392463f51fd244363745d6206c0cc111d461c2f", "url": "https://github.com/CorfuDB/CorfuDB/commit/7392463f51fd244363745d6206c0cc111d461c2f", "message": "Re-serialize payload after pre-commit listener runs.", "committedDate": "2020-08-13T03:14:09Z", "type": "forcePushed"}, {"oid": "9d965b3b93982ae565641a77c3efd6688fe64d06", "url": "https://github.com/CorfuDB/CorfuDB/commit/9d965b3b93982ae565641a77c3efd6688fe64d06", "message": "Re-serialize payload after pre-commit listener runs.", "committedDate": "2020-08-13T04:03:04Z", "type": "forcePushed"}, {"oid": "a3ec98e56ad43d60a902b60b2349100c08b144e1", "url": "https://github.com/CorfuDB/CorfuDB/commit/a3ec98e56ad43d60a902b60b2349100c08b144e1", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-13T19:43:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDcyODkwMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470728901", "bodyText": "please add @NonNull annotation to guarantee that buffers can't be null", "author": "xnull", "createdAt": "2020-08-14T16:29:10Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -32,14 +31,20 @@\n     @Getter\n     byte[] data;\n \n-    private ByteBuf serializedCache = null;\n+    private SerializedCache serializedCache = null;\n \n     private int lastKnownSize = NOT_KNOWN;\n \n     private final transient AtomicReference<Object> payload = new AtomicReference<>();\n \n     private final EnumMap<LogUnitMetadataType, Object> metadataMap;\n \n+    @RequiredArgsConstructor\n+    private static class SerializedCache {\n+        private final ByteBuf buffer;", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczMTQxOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470731419", "bodyText": "Doesn't release mean also reset refCnt?", "author": "xnull", "createdAt": "2020-08-14T16:34:09Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,42 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgzNjE4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470836189", "bodyText": "Release reduces refCnt by one.", "author": "WenbinZhu", "createdAt": "2020-08-14T19:52:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczMTQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc0NzI3NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470747274", "bodyText": "please make metadataMap static, otherwise, it will be a huge overhead\nLooks like code was changed earlier, and metadataTypeMap is not used anymore.", "author": "xnull", "createdAt": "2020-08-14T17:05:54Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -166,11 +193,7 @@ public LogData(ByteBuf buf) {\n             data = null;\n         }\n \n-        if (type.isMetadataAware()) {\n-            metadataMap = ICorfuPayload.enumMapFromBuffer(buf, IMetadata.LogUnitMetadataType.class);\n-        } else {\n-            metadataMap = new EnumMap<>(IMetadata.LogUnitMetadataType.class);\n-        }\n+        metadataMap = ICorfuPayload.enumMapFromBuffer(buf, IMetadata.LogUnitMetadataType.class);", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgwOTI1OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470809259", "bodyText": "shouldn't we create an instance of serializedCache?", "author": "xnull", "createdAt": "2020-08-14T18:49:31Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,42 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n+    public synchronized void acquireBuffer(boolean metadata) {\n         if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+            acquireBufferInternal(metadata);\n         } else {\n-            serializedCache.retain();\n+            if (metadata) {\n+                serializedCache.buffer.resetReaderIndex();\n+                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);\n+                doSerializeMetadataInternal(serializedCache.buffer);\n+            }\n+            serializedCache.buffer.retain();\n+        }\n+    }\n+\n+     public synchronized void updateAcquiredBuffer(boolean metadata) {\n+        if (serializedCache == null) {\n+            return;", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgzNTQxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470835412", "bodyText": "This should be called when serializedCache is already acquired", "author": "WenbinZhu", "createdAt": "2020-08-14T19:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgwOTI1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MjcyNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471042727", "bodyText": "If this is only expected to be called if acquireBuffer and that would create an instance of SerializedCache , then this can never be null, if it is then it should throw an exception.", "author": "Maithem", "createdAt": "2020-08-15T23:11:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgwOTI1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MzUyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471043522", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-15T23:24:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgwOTI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgxMDM4Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470810387", "bodyText": "when one small method throws two different exceptions it seems too much, can we throw the only WriteSizeException?", "author": "xnull", "createdAt": "2020-08-14T18:51:42Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -340,12 +369,19 @@ public String toString() {\n      * Verify that max payload is enforced for the specified limit.\n      *\n      * @param limit Max write limit.\n+     * @return the serialized size of the payload\n      */\n-    public void checkMaxWriteSize(int limit) {\n-        try (ILogData.SerializationHandle sh = this.getSerializedForm()) {\n-            if (limit != 0 && getSizeEstimate() > limit) {\n-                throw new WriteSizeException(getSizeEstimate(), limit);\n-            }\n+    public int checkMaxWriteSize(int limit) {\n+        if (serializedCache == null) {", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgzNTkzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470835937", "bodyText": "A precondition check is required to throw IllegalStateException", "author": "WenbinZhu", "createdAt": "2020-08-14T19:51:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgxMDM4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgxMDY3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470810679", "bodyText": "please rename x", "author": "xnull", "createdAt": "2020-08-14T18:52:21Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,105 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {", "originalCommit": "42e4fd156fdb7b26bf7e6404f9a8a13a4c84236a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgzNjAwNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470836006", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-14T19:52:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDgxMDY3OQ=="}], "type": "inlineReview"}, {"oid": "42747cc1eca8c035ed33bc2ad6b156c382cb80de", "url": "https://github.com/CorfuDB/CorfuDB/commit/42747cc1eca8c035ed33bc2ad6b156c382cb80de", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-14T19:44:51Z", "type": "forcePushed"}, {"oid": "90c9616470241e60f1d44db30fe216a574471ed5", "url": "https://github.com/CorfuDB/CorfuDB/commit/90c9616470241e60f1d44db30fe216a574471ed5", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-14T19:45:50Z", "type": "forcePushed"}, {"oid": "4b704b757d5571fa30ed9cff7c40a0ee30e58f44", "url": "https://github.com/CorfuDB/CorfuDB/commit/4b704b757d5571fa30ed9cff7c40a0ee30e58f44", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-15T00:26:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNDcxOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470934719", "bodyText": "You can slice the buffer at the metadataOffset offset", "author": "Maithem", "createdAt": "2020-08-15T04:09:06Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,42 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n+    public synchronized void acquireBuffer(boolean metadata) {\n         if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+            acquireBufferInternal(metadata);\n         } else {\n-            serializedCache.retain();\n+            if (metadata) {\n+                serializedCache.buffer.resetReaderIndex();\n+                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);\n+                doSerializeMetadataInternal(serializedCache.buffer);", "originalCommit": "8d23cfab651e4fe53a66073c7904a9077626ac5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODAxNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470938017", "bodyText": "Slice will share the same buffer and create a new buffer, I think just set the writerIndex might be simpler?", "author": "WenbinZhu", "createdAt": "2020-08-15T04:53:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNDcxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNTM5MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470935390", "bodyText": "acquireBufferInternsl is really serializing data, making the name misleading. Perhaps something like serializeToNewBuffer()", "author": "hisundar", "createdAt": "2020-08-15T04:17:02Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,42 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n+    public synchronized void acquireBuffer(boolean metadata) {\n         if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+            acquireBufferInternal(metadata);\n         } else {\n-            serializedCache.retain();\n+            if (metadata) {\n+                serializedCache.buffer.resetReaderIndex();\n+                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);\n+                doSerializeMetadataInternal(serializedCache.buffer);\n+            }\n+            serializedCache.buffer.retain();\n+        }\n+    }\n+\n+     public synchronized void updateAcquiredBuffer(boolean metadata) {\n+        if (serializedCache == null) {\n+            return;\n+        }\n+        acquireBufferInternal(metadata);\n+    }\n+\n+    private void acquireBufferInternal(boolean metadata) {", "originalCommit": "8d23cfab651e4fe53a66073c7904a9077626ac5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODI0OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r470938249", "bodyText": "Initially I also wanted to name it serailizeXXX, but we have other methods with this form of name, so to aviod confusion, I changed to use acquireBufferInternal.", "author": "WenbinZhu", "createdAt": "2020-08-15T04:56:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNTM5MA=="}], "type": "inlineReview"}, {"oid": "d5332163e88b32a943770e747de84d8d2402869c", "url": "https://github.com/CorfuDB/CorfuDB/commit/d5332163e88b32a943770e747de84d8d2402869c", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-15T04:47:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMDg2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471020869", "bodyText": "we don't break the cycle here, is it correct? Are we looking for the latest abortCause on the list?\nThat looks a bit unusual. Could you please explain what expected behavior here?\nI see there is an if condition", "author": "xnull", "createdAt": "2020-08-15T18:20:18Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,105 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n+\n+                // Is our token a valid type?\n+                AbortCause abortCause = null;", "originalCommit": "d5332163e88b32a943770e747de84d8d2402869c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTA1Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471021057", "bodyText": "the whole method is quite big, can you split it to a couple of smaller ones?", "author": "xnull", "createdAt": "2020-08-15T18:22:30Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,105 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {", "originalCommit": "d5332163e88b32a943770e747de84d8d2402869c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAzODgyMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471038823", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-15T22:13:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTA1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTUzOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471021539", "bodyText": "Alternatively, you can use a map\nMap<TokenType, AbortCause> dict = new HashMap<>();\n                dict.put(TX_ABORT_CONFLICT, AbortCause.CONFLICT);\n                dict.put(TX_ABORT_NEWSEQ, AbortCause.NEW_SEQUENCER);\n                dict.put(TX_ABORT_SEQ_OVERFLOW, AbortCause.SEQUENCER_OVERFLOW);\n                dict.put(TX_ABORT_SEQ_TRIM, AbortCause.TX_ABORT_SEQ_TRIM);\n\n                AbortCause abortCause = dict.get(tokenResponse.getRespType());\n...", "author": "xnull", "createdAt": "2020-08-15T18:28:38Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,105 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n+\n+                // Is our token a valid type?\n+                AbortCause abortCause = null;\n+                switch (tokenResponse.getRespType()) {", "originalCommit": "d5332163e88b32a943770e747de84d8d2402869c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTgxOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471021818", "bodyText": "everything inside for cycle above this point should be extracted into a method, let's say checkAbort();\nfor (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n                checkAbort();\n\n                try {\n                    if (TransactionalContext.isInTransaction()) {...", "author": "xnull", "createdAt": "2020-08-15T18:32:06Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,105 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n-\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n         TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n-\n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            for (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n+\n+                // Is our token a valid type?\n+                AbortCause abortCause = null;\n+                switch (tokenResponse.getRespType()) {\n+                    case TX_ABORT_CONFLICT:\n+                        abortCause = AbortCause.CONFLICT;\n+                        break;\n+                    case TX_ABORT_NEWSEQ:\n+                        abortCause = AbortCause.NEW_SEQUENCER;\n+                        break;\n+                    case TX_ABORT_SEQ_OVERFLOW:\n+                        abortCause = AbortCause.SEQUENCER_OVERFLOW;\n+                        break;\n+                    case TX_ABORT_SEQ_TRIM:\n+                        abortCause = AbortCause.SEQUENCER_TRIM;\n+                        break;\n                 }\n \n-                // Attempt to write to the log.\n-                runtime.getAddressSpaceView().write(tokenResponse, ld, cacheOption);\n-                // If we're here, we succeeded, return the acquired token.\n-                return tokenResponse.getSequence();\n-            } catch (OverwriteException oe) {\n-                // We were overwritten, get a new token and try again.\n-                log.warn(\"append[{}]: Overwritten after {} retries, streams {}\",\n-                        tokenResponse.getSequence(), x,\n-                        Arrays.stream(streamIDs).map(Utils::toReadableId).collect(Collectors.toSet()));\n-\n-                if (conflictInfo != null) {\n-                    // On retry, check for conflicts only from the previous attempt position,\n-                    // otherwise the transaction will always conflict with itself.\n-                    conflictInfo.setSnapshotTimestamp(tokenResponse.getToken());\n+                if (abortCause != null) {\n+                    throw new TransactionAbortedException(\n+                            conflictInfo,\n+                            tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n+                            tokenResponse.getToken().getSequence(), abortCause,\n+                            TransactionalContext.getCurrentContext());\n                 }\n \n-            } catch (StaleTokenException se) {\n-                // the epoch changed from when we grabbed the token from sequencer\n-                log.warn(\"append[{}]: StaleToken, streams {}\", tokenResponse.getSequence(),\n-                        Arrays.stream(streamIDs).map(Utils::toReadableId).collect(Collectors.toSet()));\n-\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(),\n-                        AbortCause.NEW_SEQUENCER, // in the future perhaps define a new AbortCause?\n-                        TransactionalContext.getCurrentContext());\n+                try {", "originalCommit": "d5332163e88b32a943770e747de84d8d2402869c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTkzNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471021934", "bodyText": "the whole for cycle would be good to have like this:\nfor (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n                checkAbort();\n                write();", "author": "xnull", "createdAt": "2020-08-15T18:33:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAzODgzMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471038831", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-15T22:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMTgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMjExNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471022116", "bodyText": "is it Address.NON_ADDRESS ?", "author": "xnull", "createdAt": "2020-08-15T18:35:35Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/stream/AbstractQueuedStreamView.java", "diffHunk": "@@ -177,65 +177,72 @@ public long append(Object object,\n                        Function<TokenResponse, Boolean> deacquisitionCallback) {\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n \n-        // Validate if the  size of the log data is under max write size.\n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n-\n-        // First, we get a token from the sequencer.\n-        TokenResponse tokenResponse = runtime.getSequencerView()\n-                .next(id);\n-\n-        // We loop forever until we are interrupted, since we may have to\n-        // acquire an address several times until we are successful.\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Next, we call the acquisitionCallback, if present, informing\n-            // the client of the token that we acquired.\n-            if (acquisitionCallback != null) {\n-                if (!acquisitionCallback.apply(tokenResponse)) {\n-                    // The client did not like our token, so we end here.\n-                    // We'll leave the hole to be filled by the client or\n-                    // someone else.\n-                    log.debug(\"Acquisition rejected token={}\", tokenResponse);\n-                    return -1L;\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(false)) {\n+            // Validate if the  size of the log data is under max write size.\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+\n+            // First, we get a token from the sequencer.\n+            TokenResponse tokenResponse = runtime.getSequencerView().next(id);\n+\n+            // We loop forever until we are interrupted, since we may have to\n+            // acquire an address several times until we are successful.\n+            for (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n+                // Next, we call the acquisitionCallback, if present, informing\n+                // the client of the token that we acquired.\n+                if (acquisitionCallback != null) {\n+                    if (!acquisitionCallback.apply(tokenResponse)) {\n+                        // The client did not like our token, so we end here.\n+                        // We'll leave the hole to be filled by the client or\n+                        // someone else.\n+                        log.warn(\"Acquisition rejected token={}\", tokenResponse);\n+                        return -1L;", "originalCommit": "d5332163e88b32a943770e747de84d8d2402869c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAzODgzOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471038838", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-15T22:13:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAyMjExNg=="}], "type": "inlineReview"}, {"oid": "acd49fc6faa8cfd4d458b623dca3e105611e6cc0", "url": "https://github.com/CorfuDB/CorfuDB/commit/acd49fc6faa8cfd4d458b623dca3e105611e6cc0", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-15T22:13:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MTQ3OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471041478", "bodyText": "Why do you need to reset the reader index? shouldn't you assert that its actually zero?\nWhen would you read the payload, then reseralize the metadata portion and then reread ?", "author": "Maithem", "createdAt": "2020-08-15T22:53:07Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -119,20 +124,42 @@ public Object getPayload(CorfuRuntime runtime) {\n     @Override\n     public synchronized void releaseBuffer() {\n         if (serializedCache != null) {\n-            serializedCache.release();\n-            if (serializedCache.refCnt() == 0) {\n+            serializedCache.buffer.release();\n+            if (serializedCache.buffer.refCnt() == 0) {\n                 serializedCache = null;\n             }\n         }\n     }\n \n     @Override\n-    public synchronized void acquireBuffer() {\n+    public synchronized void acquireBuffer(boolean metadata) {\n         if (serializedCache == null) {\n-            serializedCache = Unpooled.buffer();\n-            doSerializeInternal(serializedCache);\n+            acquireBufferInternal(metadata);\n         } else {\n-            serializedCache.retain();\n+            if (metadata) {\n+                serializedCache.buffer.resetReaderIndex();\n+                serializedCache.buffer.writerIndex(serializedCache.metadataOffset);", "originalCommit": "acd49fc6faa8cfd4d458b623dca3e105611e6cc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MzM2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471043365", "bodyText": "Because in case of retries, the same LogData is read into the netty buffer, which changes readerIndex, in the retries, we need to reset reader index.", "author": "WenbinZhu", "createdAt": "2020-08-15T23:21:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MTQ3OA=="}], "type": "inlineReview"}, {"oid": "f3672f7d0a31b805149c327f4e2a40a81484f380", "url": "https://github.com/CorfuDB/CorfuDB/commit/f3672f7d0a31b805149c327f4e2a40a81484f380", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-15T23:16:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MzEyNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471043124", "bodyText": "This doesn't seem right. When an object is serialized (not byte array), then this always return 1. Having serializedCache != null doesn't imply that LogData::data is populated with the serialized data.\nShouldn't this logic check the size against serializedCache.writerIndex() ?", "author": "Maithem", "createdAt": "2020-08-15T23:17:43Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -340,12 +369,17 @@ public String toString() {\n      * Verify that max payload is enforced for the specified limit.\n      *\n      * @param limit Max write limit.\n+     * @return the serialized size of the payload\n      */\n-    public void checkMaxWriteSize(int limit) {\n-        try (ILogData.SerializationHandle sh = this.getSerializedForm()) {\n-            if (limit != 0 && getSizeEstimate() > limit) {\n-                throw new WriteSizeException(getSizeEstimate(), limit);\n-            }\n+    public int checkMaxWriteSize(int limit) {\n+        Preconditions.checkState(serializedCache != null, \"checkMaxWriteSize requires serialized form\");\n+\n+        int payloadSize = getSizeEstimate();\n+\n+        if (payloadSize > limit) {\n+            throw new WriteSizeException(payloadSize, limit);\n         }", "originalCommit": "f3672f7d0a31b805149c327f4e2a40a81484f380", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MzQ0Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471043442", "bodyText": "@Maithem getSizeEstimate() will use lastKnownSize if that is available, which will be set to the correct payload size when the payload was first serialized, so it will not return 1.", "author": "WenbinZhu", "createdAt": "2020-08-15T23:23:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0MzEyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0NDA0OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471044048", "bodyText": "This doesn't cover the case of writes are issued directly to the AddressSpaceView::write method (i.e. the checkpointer writer).", "author": "Maithem", "createdAt": "2020-08-15T23:31:07Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,75 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n+        TokenResponse tokenResponse = null;\n \n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());", "originalCommit": "f3672f7d0a31b805149c327f4e2a40a81484f380", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0NDIzMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471044231", "bodyText": "It seems like the CheckpointWriter will use this path too.", "author": "Maithem", "createdAt": "2020-08-15T23:33:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0NDA0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0NDUzNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471044536", "bodyText": "Any  request directly from AddressSpaceView::write will be serialized only once in ChainReplicationProtocol::write, that path should be fine, right?", "author": "WenbinZhu", "createdAt": "2020-08-15T23:38:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0NDA0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0ODAyNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471048027", "bodyText": "Should we have a trace level logging printing the estimate size of our payloads? which could be used for tracking purposes.", "author": "annym", "createdAt": "2020-08-16T00:30:38Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java", "diffHunk": "@@ -340,12 +369,17 @@ public String toString() {\n      * Verify that max payload is enforced for the specified limit.\n      *\n      * @param limit Max write limit.\n+     * @return the serialized size of the payload\n      */\n-    public void checkMaxWriteSize(int limit) {\n-        try (ILogData.SerializationHandle sh = this.getSerializedForm()) {\n-            if (limit != 0 && getSizeEstimate() > limit) {\n-                throw new WriteSizeException(getSizeEstimate(), limit);\n-            }\n+    public int checkMaxWriteSize(int limit) {\n+        Preconditions.checkState(serializedCache != null, \"checkMaxWriteSize requires serialized form\");\n+\n+        int payloadSize = getSizeEstimate();\n+", "originalCommit": "f3672f7d0a31b805149c327f4e2a40a81484f380", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzMyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471063329", "bodyText": "Done.", "author": "WenbinZhu", "createdAt": "2020-08-16T04:23:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0ODAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0OTM0Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471049347", "bodyText": "By definition a pre-commit listener does not impose that the payload needs to change, right? so maybe in the future we could make the pre-commit listener return whether it modified or not the data, to decide if we need to update the serialized buffer or we can just skip..", "author": "annym", "createdAt": "2020-08-16T00:50:06Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -229,6 +213,47 @@ public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictIn\n         return append(object, conflictInfo, CacheOption.WRITE_THROUGH, streamIDs);\n     }\n \n+    private AbortCause getAbortCauseFromToken(TokenResponse tokenResponse) {\n+        AbortCause abortCause = null;\n+\n+        switch (tokenResponse.getRespType()) {\n+            case TX_ABORT_CONFLICT:\n+                abortCause = AbortCause.CONFLICT;\n+                break;\n+            case TX_ABORT_NEWSEQ:\n+                abortCause = AbortCause.NEW_SEQUENCER;\n+                break;\n+            case TX_ABORT_SEQ_OVERFLOW:\n+                abortCause = AbortCause.SEQUENCER_OVERFLOW;\n+                break;\n+            case TX_ABORT_SEQ_TRIM:\n+                abortCause = AbortCause.SEQUENCER_TRIM;\n+                break;\n+        }\n+\n+        return abortCause;\n+    }\n+\n+    private void runPreCommitListeners(TokenResponse tokenResponse,\n+                                       LogData ld, boolean serializeMetadata) {\n+        if (TransactionalContext.isInTransaction()) {\n+            // If this transaction has entries that wish to capture the committed address\n+            // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n+            // Note that we might invoke the same method multiple times on retries,\n+            // which means the preCommitCallback must be idempotent.\n+            log.debug(\"append: Invoking {} preCommitListeners\",\n+                    TransactionalContext.getRootContext().getPreCommitListeners().size());\n+            List<TransactionalContext.PreCommitListener> listeners =\n+                    TransactionalContext.getRootContext().getPreCommitListeners();\n+            // If there are pre-commit listeners, the payload will be changed,", "originalCommit": "f3672f7d0a31b805149c327f4e2a40a81484f380", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzM2Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471063362", "bodyText": "Yes, for now we have to assume it changes LogData...", "author": "WenbinZhu", "createdAt": "2020-08-16T04:23:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA0OTM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA1MTQwMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471051401", "bodyText": "question, in AddressSpaceView.Write we call useToken which internally sets the global address to the payload, but since, we've already serialized the payload and we'll just serialize the metadata, won't this be lost in the serialized form?\n\n  \n    \n      CorfuDB/runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java\n    \n    \n        Lines 243 to 245\n      in\n      4530118\n    \n    \n    \n    \n\n        \n          \n           if (payload.get() instanceof LogEntry) { \n        \n\n        \n          \n               ((LogEntry) payload.get()).setGlobalAddress(token.getSequence()); \n        \n\n        \n          \n           }", "author": "annym", "createdAt": "2020-08-16T01:21:48Z", "path": "runtime/src/main/java/org/corfudb/runtime/view/StreamsView.java", "diffHunk": "@@ -131,91 +131,75 @@ public void gc(long trimMark) {\n     public long append(@Nonnull Object object, @Nullable TxResolutionInfo conflictInfo,\n                        @Nonnull CacheOption cacheOption, @Nonnull UUID... streamIDs) {\n \n+        final boolean serializeMetadata = false;\n         final LogData ld = new LogData(DataType.DATA, object, runtime.getParameters().getCodecType());\n+        TokenResponse tokenResponse = null;\n \n-        ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n+        // Opening serialization handle before acquiring token, this way we prevent the\n+        // readers to wait for the possibly long serialization time in writer.\n+        // The serialization here only serializes the payload because the token is not\n+        // acquired yet, thus metadata is incomplete. Once a token is acquired, the\n+        // writer will append the serialized metadata to the buffer.\n+        try (ILogData.SerializationHandle sh = ld.getSerializedForm(serializeMetadata)) {\n+            int payloadSize = ld.checkMaxWriteSize(runtime.getParameters().getMaxWriteSize());\n \n-        TokenResponse tokenResponse = null;\n-        for (int x = 0; x < runtime.getParameters().getWriteRetry(); x++) {\n-            // Go to the sequencer, grab a token to write.\n-            tokenResponse = conflictInfo == null\n-                    ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n-                    : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n-\n-            // Is our token a valid type?\n-            AbortCause abortCause = null;\n-            switch (tokenResponse.getRespType()) {\n-                case TX_ABORT_CONFLICT:\n-                    abortCause = AbortCause.CONFLICT;\n-                    break;\n-                case TX_ABORT_NEWSEQ:\n-                    abortCause = AbortCause.NEW_SEQUENCER;\n-                    break;\n-                case TX_ABORT_SEQ_OVERFLOW:\n-                    abortCause = AbortCause.SEQUENCER_OVERFLOW;\n-                    break;\n-                case TX_ABORT_SEQ_TRIM:\n-                    abortCause = AbortCause.SEQUENCER_TRIM;\n-                    break;\n-            }\n+            for (int retry = 0; retry < runtime.getParameters().getWriteRetry(); retry++) {\n+                // Go to the sequencer, grab a token to write.\n+                tokenResponse = conflictInfo == null\n+                        ? runtime.getSequencerView().next(streamIDs) // Token w/o conflict info\n+                        : runtime.getSequencerView().next(conflictInfo, streamIDs); // Token w/ conflict info\n \n-            if (abortCause != null) {\n-                throw new TransactionAbortedException(\n-                        conflictInfo,\n-                        tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n-                        tokenResponse.getToken().getSequence(), abortCause,\n-                        TransactionalContext.getCurrentContext());\n-            }\n+                // Is our token a valid type?\n+                AbortCause abortCause = getAbortCauseFromToken(tokenResponse);\n \n-            try {\n-                if (TransactionalContext.isInTransaction()) {\n-                    // If this transaction has entries that wish to capture the committed address\n-                    // invoke its preCommitCallbacks with the tokenResponse from the sequencer.\n-                    // Note that we might invoke the same method multiple times on retries,\n-                    // which means the preCommitCallback must be idempotent.\n-                    TokenResponse finalTokenResponse = tokenResponse;\n-                    log.debug(\"append: Invoking {} preCommitListeners\",\n-                            TransactionalContext.getRootContext().getPreCommitListeners().size());\n-                    TransactionalContext.getRootContext()\n-                            .getPreCommitListeners()\n-                            .forEach(e -> e.preCommitCallback(finalTokenResponse));\n+                if (abortCause != null) {\n+                    throw new TransactionAbortedException(\n+                            conflictInfo,\n+                            tokenResponse.getConflictKey(), tokenResponse.getConflictStream(),\n+                            tokenResponse.getToken().getSequence(), abortCause,\n+                            TransactionalContext.getCurrentContext());\n                 }\n \n-                // Attempt to write to the log.\n-                runtime.getAddressSpaceView().write(tokenResponse, ld, cacheOption);\n-                // If we're here, we succeeded, return the acquired token.\n-                return tokenResponse.getSequence();\n-            } catch (OverwriteException oe) {\n-                // We were overwritten, get a new token and try again.\n-                log.warn(\"append[{}]: Overwritten after {} retries, streams {}\",\n-                        tokenResponse.getSequence(), x,\n-                        Arrays.stream(streamIDs).map(Utils::toReadableId).collect(Collectors.toSet()));\n-\n-                if (conflictInfo != null) {\n-                    // On retry, check for conflicts only from the previous attempt position,\n-                    // otherwise the transaction will always conflict with itself.\n-                    conflictInfo.setSnapshotTimestamp(tokenResponse.getToken());\n-                }\n+                try {\n+                    // Run pre-commit listeners if we are in transaction.\n+                    runPreCommitListeners(tokenResponse, ld, serializeMetadata);\n+                    // Attempt to write to the log.\n+                    runtime.getAddressSpaceView().write(tokenResponse, ld, cacheOption);", "originalCommit": "f3672f7d0a31b805149c327f4e2a40a81484f380", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2Mzc2Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471063763", "bodyText": "The address in the LogEntry is used when we sync object, for real workloads, this  block is actually redundant, because the address will be set to LogData's address when we call getPayload(), so even if we set here, it will be overwritten after getPayload() is called. However in unit test framework, on the write path, payload is not being serailized to data[], that means when getPayload() is called, it's always in object format, so it won't go to following code to set the address.\nThis block of code is used to workaround the unit test framework case. I moved this logic to getPayload and added comments to make it more clear.\n\n  \n    \n      CorfuDB/runtime/src/main/java/org/corfudb/protocols/wireprotocol/LogData.java\n    \n    \n        Lines 94 to 95\n      in\n      4530118\n    \n    \n    \n    \n\n        \n          \n           if (actualValue instanceof LogEntry) { \n        \n\n        \n          \n               ((LogEntry) actualValue).setGlobalAddress(getGlobalAddress());", "author": "WenbinZhu", "createdAt": "2020-08-16T04:29:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA1MTQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NzMyNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471067326", "bodyText": "Great, that makes it cleaner. Thanks!", "author": "annym", "createdAt": "2020-08-16T05:21:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA1MTQwMQ=="}], "type": "inlineReview"}, {"oid": "881aee8a6f354a22a6567ceb70c49b59a196dce2", "url": "https://github.com/CorfuDB/CorfuDB/commit/881aee8a6f354a22a6567ceb70c49b59a196dce2", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-16T04:18:12Z", "type": "forcePushed"}, {"oid": "f9690ae566e0fb121f8d29cceb18e5197b8929d1", "url": "https://github.com/CorfuDB/CorfuDB/commit/f9690ae566e0fb121f8d29cceb18e5197b8929d1", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-17T05:59:11Z", "type": "commit"}, {"oid": "f9690ae566e0fb121f8d29cceb18e5197b8929d1", "url": "https://github.com/CorfuDB/CorfuDB/commit/f9690ae566e0fb121f8d29cceb18e5197b8929d1", "message": "Stream write path performance optimization.\n\n- Reuse serialization handle, which reduces number of serializations.\n- Acquire token after payload serialized, which reduces reader wait time.", "committedDate": "2020-08-17T05:59:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI0ODc2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471248765", "bodyText": "Codacy found an issue: Use explicit scoping instead of the default package private level", "author": "corfudb-bot", "createdAt": "2020-08-17T06:04:27Z", "path": "runtime/src/main/java/org/corfudb/runtime/CorfuRuntime.java", "diffHunk": "@@ -85,13 +83,11 @@ public static CorfuRuntimeParametersBuilder builder() {\n         /*\n          * Max size for a write request.\n          */\n-\n-        int maxWriteSize = 0;\n+        int maxWriteSize = Integer.MAX_VALUE;", "originalCommit": "f9690ae566e0fb121f8d29cceb18e5197b8929d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI0ODc3Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2703#discussion_r471248772", "bodyText": "Codacy found an issue: Fields should be declared at the top of the class, before any method declarations, constructors, initializers or inner classes.", "author": "corfudb-bot", "createdAt": "2020-08-17T06:04:27Z", "path": "runtime/src/main/java/org/corfudb/runtime/CorfuRuntime.java", "diffHunk": "@@ -85,13 +83,11 @@ public static CorfuRuntimeParametersBuilder builder() {\n         /*\n          * Max size for a write request.\n          */\n-\n-        int maxWriteSize = 0;\n+        int maxWriteSize = Integer.MAX_VALUE;", "originalCommit": "f9690ae566e0fb121f8d29cceb18e5197b8929d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}