{"pr_number": 2665, "pr_title": "Improve SQL documentation", "pr_createdAt": "2020-11-16T10:59:19Z", "pr_url": "https://github.com/hazelcast/hazelcast-jet/pull/2665", "timeline": [{"oid": "9cc877d51c5517f1294d2e12cf8dc23d5be32247", "url": "https://github.com/hazelcast/hazelcast-jet/commit/9cc877d51c5517f1294d2e12cf8dc23d5be32247", "message": "Remove number from SQL chapters", "committedDate": "2020-11-16T10:49:23Z", "type": "commit"}, {"oid": "a4ddd26eaabe8d24005a07e0f899a57c46006eae", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a4ddd26eaabe8d24005a07e0f899a57c46006eae", "message": "Document job options\n\nFixes #2660", "committedDate": "2020-11-16T10:50:08Z", "type": "commit"}, {"oid": "92cdb8c7a8a839be28fa7709db150c5a6d6a6204", "url": "https://github.com/hazelcast/hazelcast-jet/commit/92cdb8c7a8a839be28fa7709db150c5a6d6a6204", "message": "Add missing key.serializer req'd by kafka", "committedDate": "2020-11-16T10:54:35Z", "type": "commit"}, {"oid": "48e904157b1210ab5d753c0e7107dc28c6668c69", "url": "https://github.com/hazelcast/hazelcast-jet/commit/48e904157b1210ab5d753c0e7107dc28c6668c69", "message": "Document objectName SQL MAPPING option", "committedDate": "2020-11-16T10:58:00Z", "type": "commit"}, {"oid": "1d1b9d4274c819e05a87f51ec573379f7348e6bf", "url": "https://github.com/hazelcast/hazelcast-jet/commit/1d1b9d4274c819e05a87f51ec573379f7348e6bf", "message": "value -> price", "committedDate": "2020-11-18T09:18:36Z", "type": "commit"}, {"oid": "338991192034f12bb4a34ce20af911713bced3ca", "url": "https://github.com/hazelcast/hazelcast-jet/commit/338991192034f12bb4a34ce20af911713bced3ca", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into docs", "committedDate": "2020-11-18T09:23:19Z", "type": "commit"}, {"oid": "552faf8ca2c4163000b7884414c6caf0a4d95029", "url": "https://github.com/hazelcast/hazelcast-jet/commit/552faf8ca2c4163000b7884414c6caf0a4d95029", "message": "Fix grammar", "committedDate": "2020-11-18T09:24:10Z", "type": "commit"}, {"oid": "aca72c1ac660019e64834a2709aaef1b82d8c19c", "url": "https://github.com/hazelcast/hazelcast-jet/commit/aca72c1ac660019e64834a2709aaef1b82d8c19c", "message": "Fix broken links", "committedDate": "2020-11-18T10:22:19Z", "type": "commit"}, {"oid": "c2692dd271185b0a30cc5b2e8cc830af8e854235", "url": "https://github.com/hazelcast/hazelcast-jet/commit/c2692dd271185b0a30cc5b2e8cc830af8e854235", "message": "Minor fixes", "committedDate": "2020-11-18T13:04:00Z", "type": "commit"}, {"oid": "1b3e2356a1604805a021db0db5a0657b0e21f9a0", "url": "https://github.com/hazelcast/hazelcast-jet/commit/1b3e2356a1604805a021db0db5a0657b0e21f9a0", "message": "Change scope for avro and jackson-databind", "committedDate": "2020-11-18T13:04:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjEwMzA4Mg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526103082", "bodyText": "Please replace __key and this with explicit references. It\u2019s a default that\u2019s not well known.", "author": "vladoschreiner", "createdAt": "2020-11-18T13:52:59Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -117,19 +124,19 @@ statement:\n )\n ```\n \n-The SINK INTO command will look like this:\n+The `SINK INTO` command will look like this:\n \n ```sql\n SINK INTO latest_trades(__key, this)", "originalCommit": "552faf8ca2c4163000b7884414c6caf0a4d95029", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjEwMzY3Ng==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526103676", "bodyText": "Extend the sample so that it includes Kafka key as well. People use keys in their Kafka topics.", "author": "vladoschreiner", "createdAt": "2020-11-18T13:53:46Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -49,14 +49,21 @@ data types:\n ```sql\n CREATE MAPPING trades (", "originalCommit": "552faf8ca2c4163000b7884414c6caf0a4d95029", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0aae1a5fb0ec5f9e96873ce1c7a61e5d04284e6a", "url": "https://github.com/hazelcast/hazelcast-jet/commit/0aae1a5fb0ec5f9e96873ce1c7a61e5d04284e6a", "message": "WIP", "committedDate": "2020-11-18T15:06:32Z", "type": "commit"}, {"oid": "d691ebc953fd29018e96a9e8853fc07aa32c6021", "url": "https://github.com/hazelcast/hazelcast-jet/commit/d691ebc953fd29018e96a9e8853fc07aa32c6021", "message": "Infer Kafka (de)serializers", "committedDate": "2020-11-18T15:17:27Z", "type": "commit"}, {"oid": "ba69af4e644717c893e1f40fbfba9b304dbac57d", "url": "https://github.com/hazelcast/hazelcast-jet/commit/ba69af4e644717c893e1f40fbfba9b304dbac57d", "message": "Merge remote-tracking branch 'remotes/gierlachg/kafka-serializers' into docs", "committedDate": "2020-11-18T15:47:10Z", "type": "commit"}, {"oid": "a5a3c2266d7f15f1cfbad11baee3f0ba315a0f20", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a5a3c2266d7f15f1cfbad11baee3f0ba315a0f20", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into docs\n\n# Conflicts:\n#\thazelcast-jet-sql/src/test/java/com/hazelcast/jet/sql/impl/connector/kafka/PropertiesResolverTest.java", "committedDate": "2020-11-18T16:17:37Z", "type": "commit"}, {"oid": "2dd88c566c737261dde9482fe12e9715e9c61fc4", "url": "https://github.com/hazelcast/hazelcast-jet/commit/2dd88c566c737261dde9482fe12e9715e9c61fc4", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into docs", "committedDate": "2020-11-19T09:03:49Z", "type": "commit"}, {"oid": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "url": "https://github.com/hazelcast/hazelcast-jet/commit/ef16c4d330d5d808acaf553a2bdb5f02beba056c", "message": "Finish the manual", "committedDate": "2020-11-19T11:21:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgzNTYwMg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526835602", "bodyText": "Mention client-server before embedded. Client-server is the main scenario.", "author": "vladoschreiner", "createdAt": "2020-11-19T12:27:44Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -0,0 +1,306 @@\n+---\n+title: Hazelcast Jet SQL\n+description: Introduction to Hazelcast Jet SQL features.\n+---\n+\n+Hazelcast Jet allows you to create a Jet processing job using the\n+familiar SQL language. It can execute distributed SQL statements over\n+Hazelcast IMaps and external data sets.\n+\n+**Note:** _The service is in beta state. Behavior and API might change\n+in future releases. Binary compatibility is not guaranteed between minor\n+or patch releases._\n+\n+## Overview\n+\n+In the first release, Jet SQL supports the following features:\n+\n+- SQL queries over [Apache Kafka topics](kafka-connector.md) and files\n+(local and remote) (TODO)\n+- Joining Kafka or file data with local IMaps (enrichment)\n+- Filtering and projection using [SQL\n+expressions](https://docs.hazelcast.org/docs/{imdg-version}/manual/html-single/index.html#expressions)\n+- Aggregating data from files using predefined [aggregate\n+functions](basic-commands#aggregation-functions)\n+- Receiving query results via Jet client (Java) or writing the results\n+to an [IMap](imap-connector.md) in the Jet cluster\n+- Running continuous (streaming) and batch queries, see\n+[Job Management](job-management.md)\n+\n+These are some of the features on our roadmap:\n+\n+- Joins with arbitrary external data sources\n+- Windowed aggregation of streaming data\n+- JDBC\n+\n+## Installation\n+\n+You need the `hazelcast-jet-sql` module on your classpath. For", "originalCommit": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgzNjY1NA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526836654", "bodyText": "Include commands to install Kafka. We need code that can be copy-pasted and works. Also, we want to control Kafka server versioning and ports.", "author": "vladoschreiner", "createdAt": "2020-11-19T12:29:34Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -0,0 +1,306 @@\n+---\n+title: Hazelcast Jet SQL\n+description: Introduction to Hazelcast Jet SQL features.\n+---\n+\n+Hazelcast Jet allows you to create a Jet processing job using the\n+familiar SQL language. It can execute distributed SQL statements over\n+Hazelcast IMaps and external data sets.\n+\n+**Note:** _The service is in beta state. Behavior and API might change\n+in future releases. Binary compatibility is not guaranteed between minor\n+or patch releases._\n+\n+## Overview\n+\n+In the first release, Jet SQL supports the following features:\n+\n+- SQL queries over [Apache Kafka topics](kafka-connector.md) and files\n+(local and remote) (TODO)\n+- Joining Kafka or file data with local IMaps (enrichment)\n+- Filtering and projection using [SQL\n+expressions](https://docs.hazelcast.org/docs/{imdg-version}/manual/html-single/index.html#expressions)\n+- Aggregating data from files using predefined [aggregate\n+functions](basic-commands#aggregation-functions)\n+- Receiving query results via Jet client (Java) or writing the results\n+to an [IMap](imap-connector.md) in the Jet cluster\n+- Running continuous (streaming) and batch queries, see\n+[Job Management](job-management.md)\n+\n+These are some of the features on our roadmap:\n+\n+- Joins with arbitrary external data sources\n+- Windowed aggregation of streaming data\n+- JDBC\n+\n+## Installation\n+\n+You need the `hazelcast-jet-sql` module on your classpath. For\n+Gradle or Maven, make sure to add the dependency:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-sql:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-sql</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+If you're using the distribution package, make sure to move the\n+`hazelcast-jet-sql-{jet-version}.jar` file from the `opt/` to the `lib/`\n+directory.\n+\n+If you use other features from SQL such as the Kafka or File connectors,\n+also add the `hazelcast-jet-kafka` or `hazelcast-jet-hadoop` modules in\n+the same way.\n+\n+## Example\n+\n+In this example we'll show how to query Apache Kafka using SQL and\n+stream the messages to an IMap.\n+\n+First, make sure you have Apache Kafka up and running. Please follow any", "originalCommit": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgzNzM5OA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526837398", "bodyText": "We plan CLI at the first place. Please mention it.", "author": "vladoschreiner", "createdAt": "2020-11-19T12:30:49Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -0,0 +1,306 @@\n+---\n+title: Hazelcast Jet SQL\n+description: Introduction to Hazelcast Jet SQL features.\n+---\n+\n+Hazelcast Jet allows you to create a Jet processing job using the\n+familiar SQL language. It can execute distributed SQL statements over\n+Hazelcast IMaps and external data sets.\n+\n+**Note:** _The service is in beta state. Behavior and API might change\n+in future releases. Binary compatibility is not guaranteed between minor\n+or patch releases._\n+\n+## Overview\n+\n+In the first release, Jet SQL supports the following features:\n+\n+- SQL queries over [Apache Kafka topics](kafka-connector.md) and files\n+(local and remote) (TODO)\n+- Joining Kafka or file data with local IMaps (enrichment)\n+- Filtering and projection using [SQL\n+expressions](https://docs.hazelcast.org/docs/{imdg-version}/manual/html-single/index.html#expressions)\n+- Aggregating data from files using predefined [aggregate\n+functions](basic-commands#aggregation-functions)\n+- Receiving query results via Jet client (Java) or writing the results\n+to an [IMap](imap-connector.md) in the Jet cluster\n+- Running continuous (streaming) and batch queries, see\n+[Job Management](job-management.md)\n+\n+These are some of the features on our roadmap:\n+\n+- Joins with arbitrary external data sources\n+- Windowed aggregation of streaming data\n+- JDBC\n+\n+## Installation\n+\n+You need the `hazelcast-jet-sql` module on your classpath. For\n+Gradle or Maven, make sure to add the dependency:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-sql:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-sql</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+If you're using the distribution package, make sure to move the\n+`hazelcast-jet-sql-{jet-version}.jar` file from the `opt/` to the `lib/`\n+directory.\n+\n+If you use other features from SQL such as the Kafka or File connectors,\n+also add the `hazelcast-jet-kafka` or `hazelcast-jet-hadoop` modules in\n+the same way.\n+\n+## Example\n+\n+In this example we'll show how to query Apache Kafka using SQL and\n+stream the messages to an IMap.\n+\n+First, make sure you have Apache Kafka up and running. Please follow any\n+of the Kafka tutorials. Start a broker and create a topic named\n+`trades`.\n+\n+The trades will be encoded as JSON messages:\n+\n+```plain\n+key: ABCD\n+value: {\n+    \"ticker\": \"ABCD\",\n+    \"price\": 5.5,\n+    \"amount\": 10\n+}\n+```\n+\n+The `ticker` is repeated in both the key and the value. We'll ignore the\n+key in the subsequent examples, it's only used by Kafka for\n+partitioning.\n+\n+### Creating the Mapping for Kafka Topic\n+\n+To use a remote topic as a table in Jet, first create an `EXTERNAL\n+MAPPING` for the topic. It maps the messages to a fixed list of columns\n+with data types:\n+\n+```sql\n+CREATE MAPPING trades (\n+    ticker VARCHAR,\n+    price DECIMAL,\n+    amount BIGINT)\n+TYPE Kafka\n+OPTIONS (\n+    valueFormat 'json',\n+    \"bootstrap.servers\" '1.2.3.4:9092'\n+    /* ... more configuration options for the Kafka consumer */\n+)\n+```\n+\n+The `valueFormat` option specifies the serialization format for the\n+value. For other possible values see the [Kafka\n+Connector](kafka-connector) page. The options not handled by Jet are all\n+passed to the Kafka consumer or producer, such as the\n+`bootstrap.servers` above.\n+\n+To submit the above query, use the Java API (we plan JDBC support and", "originalCommit": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjg4MzAzMg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526883032", "bodyText": "The current draft of CLI doesn't support multi-line commands. I'll add a todo to add it later.", "author": "viliam-durina", "createdAt": "2020-11-19T13:31:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgzNzM5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjgzOTY5OQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526839699", "bodyText": "I'd rather say \"Creating a long-running job\".", "author": "vladoschreiner", "createdAt": "2020-11-19T12:32:38Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -0,0 +1,306 @@\n+---\n+title: Hazelcast Jet SQL\n+description: Introduction to Hazelcast Jet SQL features.\n+---\n+\n+Hazelcast Jet allows you to create a Jet processing job using the\n+familiar SQL language. It can execute distributed SQL statements over\n+Hazelcast IMaps and external data sets.\n+\n+**Note:** _The service is in beta state. Behavior and API might change\n+in future releases. Binary compatibility is not guaranteed between minor\n+or patch releases._\n+\n+## Overview\n+\n+In the first release, Jet SQL supports the following features:\n+\n+- SQL queries over [Apache Kafka topics](kafka-connector.md) and files\n+(local and remote) (TODO)\n+- Joining Kafka or file data with local IMaps (enrichment)\n+- Filtering and projection using [SQL\n+expressions](https://docs.hazelcast.org/docs/{imdg-version}/manual/html-single/index.html#expressions)\n+- Aggregating data from files using predefined [aggregate\n+functions](basic-commands#aggregation-functions)\n+- Receiving query results via Jet client (Java) or writing the results\n+to an [IMap](imap-connector.md) in the Jet cluster\n+- Running continuous (streaming) and batch queries, see\n+[Job Management](job-management.md)\n+\n+These are some of the features on our roadmap:\n+\n+- Joins with arbitrary external data sources\n+- Windowed aggregation of streaming data\n+- JDBC\n+\n+## Installation\n+\n+You need the `hazelcast-jet-sql` module on your classpath. For\n+Gradle or Maven, make sure to add the dependency:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-sql:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-sql</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+If you're using the distribution package, make sure to move the\n+`hazelcast-jet-sql-{jet-version}.jar` file from the `opt/` to the `lib/`\n+directory.\n+\n+If you use other features from SQL such as the Kafka or File connectors,\n+also add the `hazelcast-jet-kafka` or `hazelcast-jet-hadoop` modules in\n+the same way.\n+\n+## Example\n+\n+In this example we'll show how to query Apache Kafka using SQL and\n+stream the messages to an IMap.\n+\n+First, make sure you have Apache Kafka up and running. Please follow any\n+of the Kafka tutorials. Start a broker and create a topic named\n+`trades`.\n+\n+The trades will be encoded as JSON messages:\n+\n+```plain\n+key: ABCD\n+value: {\n+    \"ticker\": \"ABCD\",\n+    \"price\": 5.5,\n+    \"amount\": 10\n+}\n+```\n+\n+The `ticker` is repeated in both the key and the value. We'll ignore the\n+key in the subsequent examples, it's only used by Kafka for\n+partitioning.\n+\n+### Creating the Mapping for Kafka Topic\n+\n+To use a remote topic as a table in Jet, first create an `EXTERNAL\n+MAPPING` for the topic. It maps the messages to a fixed list of columns\n+with data types:\n+\n+```sql\n+CREATE MAPPING trades (\n+    ticker VARCHAR,\n+    price DECIMAL,\n+    amount BIGINT)\n+TYPE Kafka\n+OPTIONS (\n+    valueFormat 'json',\n+    \"bootstrap.servers\" '1.2.3.4:9092'\n+    /* ... more configuration options for the Kafka consumer */\n+)\n+```\n+\n+The `valueFormat` option specifies the serialization format for the\n+value. For other possible values see the [Kafka\n+Connector](kafka-connector) page. The options not handled by Jet are all\n+passed to the Kafka consumer or producer, such as the\n+`bootstrap.servers` above.\n+\n+To submit the above query, use the Java API (we plan JDBC support and\n+support in non-Java clients in the future):\n+\n+```java\n+JetInstance inst = ...;\n+inst.getSql().execute( /* query text */ );\n+```\n+\n+### Querying the Kafka Topic\n+\n+A SQL query can now be used to read from the `trades` topic, as if it\n+was a table:\n+\n+```java\n+JetInstance inst = ...;\n+try (SqlResult result = inst.getSql().execute(\"SELECT * FROM trades\")) {\n+    for (SqlRow row : result) {\n+        // Process the row\n+        System.out.println(row);\n+    }\n+}\n+```\n+\n+The query now runs in the Jet cluster and streams the results to the Jet\n+client that started it. The iteration will never complete, a Kafka topic\n+is a _streaming source_, that is it has no end of data. The backing Jet\n+job will terminate when the client crashes, or you can add a\n+`result.close()` call to close it earlier. By default, the reading\n+starts at the tip of the topic, but you can modify it by adding\n+`auto.offset.reset` Kafka property to the mapping options.\n+\n+You can use the power of the SQL language and use expressions and the\n+`WHERE` clause, e.g.:\n+\n+```sql\n+SELECT ticker, ROUND(price * 100) AS price_cents, amount\n+FROM trades\n+WHERE price * amount > 100\n+```\n+\n+### Creating the Mapping for the IMap\n+\n+Jet can update an IMap using the `SINK INTO` command, which means you\n+can use Jet SQL as a simple API to ingest data and store it in IMDG. For\n+the sake of this tutorial it's enough to think about the `SINK INTO`\n+command as of a standard `INSERT INTO` command. If you're interested to\n+know why don't we use the standard command, [see\n+here](basic-commands#insertsink-statement).\n+\n+To be able to write to an IMap, Jet has to know what type of objects to\n+create for the map key and value. It can derive that automatically by\n+sampling an existing entry in the map, but if the map is empty<sup><a\n+name='footnote-1-backref'></a>[*](#footnote-1)</sup>, you have to create\n+a mapping for it first. We want to replicate the topic structure, so we\n+need to create a Java class for the value:\n+\n+```java\n+public class Trade implements Serializable {\n+    public String ticker;\n+    public BigDecimal price;\n+    public long amount;\n+}\n+```\n+\n+We used public fields, but of course you can use private fields and use\n+setters/getters. This class must be available to the cluster. You can\n+either add it to the members class paths by creating a JAR file and\n+adding to the `lib` folder, or you can use User Code Deployment. The\n+user code deployment has to be enabled on the members; add the following\n+section to the `config/hazelcast.yaml` file:\n+\n+```yaml\n+hazelcast:\n+  user-code-deployment:\n+    enabled: true\n+```\n+\n+Then use a client to upload the class:\n+\n+```java\n+ClientConfig clientConfig = new JetClientConfig();\n+clientConfig.getUserCodeDeploymentConfig()\n+            .setEnabled(true)\n+            .addClass(Trade.class);\n+JetInstance jet = Jet.newJetClient(clientConfig);\n+```\n+\n+After this, you can create the mapping for the IMap. The name of the\n+IMap is `latest_trades`:\n+\n+```sql\n+CREATE MAPPING latest_trades\n+TYPE IMap\n+OPTIONS (\n+    keyFormat 'java',\n+    keyJavaClass 'java.lang.String',\n+    valueFormat 'java',\n+    valueJavaClass 'com.example.Trade'\n+)\n+```\n+\n+Note that we omitted the column list in this query. It will be\n+determined automatically according to the OPTIONS. Since we use `String`\n+as the key class, the default column name for the key will be used:\n+`__key`. The properties of the `Trade` class will be used. So the\n+mapping will behave as if the following columns were specified in the\n+previous statement:\n+\n+```sql\n+(\n+    __key VARCHAR,\n+    ticker VARCHAR,\n+    price DECIMAL,\n+    amount BIGINT\n+)\n+```\n+\n+### Streaming Messages from the Kafka Topic to the IMap\n+\n+After creating the mapping for the `latest_trades` IMap, we can submit\n+the following statement:\n+\n+```sql\n+SINK INTO latest_trades(__key, ticker, price, amount)\n+SELECT ticker, ticker, price, amount\n+FROM trades\n+```\n+\n+It will put the fields from the Kafka topic into the IMap, replicating\n+the `ticker` twice: once into the value and once into the key. Since we\n+use the `ticker` as the key, every new trade for the same ticker will\n+overwrite the previous trade for that ticker, hence the name\n+`latest_trades`.\n+\n+### Creating a Streaming Job", "originalCommit": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjg0MjAxMA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2665#discussion_r526842010", "bodyText": "latest_trades map can be queried using SQL. Please provide a sample command and link to IMDG SQL. This is a marketing touch to present the big picture.", "author": "vladoschreiner", "createdAt": "2020-11-19T12:34:24Z", "path": "site/docs/sql/intro.md", "diffHunk": "@@ -0,0 +1,306 @@\n+---\n+title: Hazelcast Jet SQL\n+description: Introduction to Hazelcast Jet SQL features.\n+---\n+\n+Hazelcast Jet allows you to create a Jet processing job using the\n+familiar SQL language. It can execute distributed SQL statements over\n+Hazelcast IMaps and external data sets.\n+\n+**Note:** _The service is in beta state. Behavior and API might change\n+in future releases. Binary compatibility is not guaranteed between minor\n+or patch releases._\n+\n+## Overview\n+\n+In the first release, Jet SQL supports the following features:\n+\n+- SQL queries over [Apache Kafka topics](kafka-connector.md) and files\n+(local and remote) (TODO)\n+- Joining Kafka or file data with local IMaps (enrichment)\n+- Filtering and projection using [SQL\n+expressions](https://docs.hazelcast.org/docs/{imdg-version}/manual/html-single/index.html#expressions)\n+- Aggregating data from files using predefined [aggregate\n+functions](basic-commands#aggregation-functions)\n+- Receiving query results via Jet client (Java) or writing the results\n+to an [IMap](imap-connector.md) in the Jet cluster\n+- Running continuous (streaming) and batch queries, see\n+[Job Management](job-management.md)\n+\n+These are some of the features on our roadmap:\n+\n+- Joins with arbitrary external data sources\n+- Windowed aggregation of streaming data\n+- JDBC\n+\n+## Installation\n+\n+You need the `hazelcast-jet-sql` module on your classpath. For\n+Gradle or Maven, make sure to add the dependency:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-sql:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-sql</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+If you're using the distribution package, make sure to move the\n+`hazelcast-jet-sql-{jet-version}.jar` file from the `opt/` to the `lib/`\n+directory.\n+\n+If you use other features from SQL such as the Kafka or File connectors,\n+also add the `hazelcast-jet-kafka` or `hazelcast-jet-hadoop` modules in\n+the same way.\n+\n+## Example\n+\n+In this example we'll show how to query Apache Kafka using SQL and\n+stream the messages to an IMap.\n+\n+First, make sure you have Apache Kafka up and running. Please follow any\n+of the Kafka tutorials. Start a broker and create a topic named\n+`trades`.\n+\n+The trades will be encoded as JSON messages:\n+\n+```plain\n+key: ABCD\n+value: {\n+    \"ticker\": \"ABCD\",\n+    \"price\": 5.5,\n+    \"amount\": 10\n+}\n+```\n+\n+The `ticker` is repeated in both the key and the value. We'll ignore the\n+key in the subsequent examples, it's only used by Kafka for\n+partitioning.\n+\n+### Creating the Mapping for Kafka Topic\n+\n+To use a remote topic as a table in Jet, first create an `EXTERNAL\n+MAPPING` for the topic. It maps the messages to a fixed list of columns\n+with data types:\n+\n+```sql\n+CREATE MAPPING trades (\n+    ticker VARCHAR,\n+    price DECIMAL,\n+    amount BIGINT)\n+TYPE Kafka\n+OPTIONS (\n+    valueFormat 'json',\n+    \"bootstrap.servers\" '1.2.3.4:9092'\n+    /* ... more configuration options for the Kafka consumer */\n+)\n+```\n+\n+The `valueFormat` option specifies the serialization format for the\n+value. For other possible values see the [Kafka\n+Connector](kafka-connector) page. The options not handled by Jet are all\n+passed to the Kafka consumer or producer, such as the\n+`bootstrap.servers` above.\n+\n+To submit the above query, use the Java API (we plan JDBC support and\n+support in non-Java clients in the future):\n+\n+```java\n+JetInstance inst = ...;\n+inst.getSql().execute( /* query text */ );\n+```\n+\n+### Querying the Kafka Topic\n+\n+A SQL query can now be used to read from the `trades` topic, as if it\n+was a table:\n+\n+```java\n+JetInstance inst = ...;\n+try (SqlResult result = inst.getSql().execute(\"SELECT * FROM trades\")) {\n+    for (SqlRow row : result) {\n+        // Process the row\n+        System.out.println(row);\n+    }\n+}\n+```\n+\n+The query now runs in the Jet cluster and streams the results to the Jet\n+client that started it. The iteration will never complete, a Kafka topic\n+is a _streaming source_, that is it has no end of data. The backing Jet\n+job will terminate when the client crashes, or you can add a\n+`result.close()` call to close it earlier. By default, the reading\n+starts at the tip of the topic, but you can modify it by adding\n+`auto.offset.reset` Kafka property to the mapping options.\n+\n+You can use the power of the SQL language and use expressions and the\n+`WHERE` clause, e.g.:\n+\n+```sql\n+SELECT ticker, ROUND(price * 100) AS price_cents, amount\n+FROM trades\n+WHERE price * amount > 100\n+```\n+\n+### Creating the Mapping for the IMap\n+\n+Jet can update an IMap using the `SINK INTO` command, which means you\n+can use Jet SQL as a simple API to ingest data and store it in IMDG. For\n+the sake of this tutorial it's enough to think about the `SINK INTO`\n+command as of a standard `INSERT INTO` command. If you're interested to\n+know why don't we use the standard command, [see\n+here](basic-commands#insertsink-statement).\n+\n+To be able to write to an IMap, Jet has to know what type of objects to\n+create for the map key and value. It can derive that automatically by\n+sampling an existing entry in the map, but if the map is empty<sup><a\n+name='footnote-1-backref'></a>[*](#footnote-1)</sup>, you have to create\n+a mapping for it first. We want to replicate the topic structure, so we\n+need to create a Java class for the value:\n+\n+```java\n+public class Trade implements Serializable {\n+    public String ticker;\n+    public BigDecimal price;\n+    public long amount;\n+}\n+```\n+\n+We used public fields, but of course you can use private fields and use\n+setters/getters. This class must be available to the cluster. You can\n+either add it to the members class paths by creating a JAR file and\n+adding to the `lib` folder, or you can use User Code Deployment. The\n+user code deployment has to be enabled on the members; add the following\n+section to the `config/hazelcast.yaml` file:\n+\n+```yaml\n+hazelcast:\n+  user-code-deployment:\n+    enabled: true\n+```\n+\n+Then use a client to upload the class:\n+\n+```java\n+ClientConfig clientConfig = new JetClientConfig();\n+clientConfig.getUserCodeDeploymentConfig()\n+            .setEnabled(true)\n+            .addClass(Trade.class);\n+JetInstance jet = Jet.newJetClient(clientConfig);\n+```\n+\n+After this, you can create the mapping for the IMap. The name of the\n+IMap is `latest_trades`:\n+\n+```sql\n+CREATE MAPPING latest_trades\n+TYPE IMap\n+OPTIONS (\n+    keyFormat 'java',\n+    keyJavaClass 'java.lang.String',\n+    valueFormat 'java',\n+    valueJavaClass 'com.example.Trade'\n+)\n+```\n+\n+Note that we omitted the column list in this query. It will be\n+determined automatically according to the OPTIONS. Since we use `String`\n+as the key class, the default column name for the key will be used:\n+`__key`. The properties of the `Trade` class will be used. So the\n+mapping will behave as if the following columns were specified in the\n+previous statement:\n+\n+```sql\n+(\n+    __key VARCHAR,\n+    ticker VARCHAR,\n+    price DECIMAL,\n+    amount BIGINT\n+)\n+```\n+\n+### Streaming Messages from the Kafka Topic to the IMap\n+\n+After creating the mapping for the `latest_trades` IMap, we can submit\n+the following statement:\n+\n+```sql\n+SINK INTO latest_trades(__key, ticker, price, amount)\n+SELECT ticker, ticker, price, amount\n+FROM trades\n+```\n+\n+It will put the fields from the Kafka topic into the IMap, replicating\n+the `ticker` twice: once into the value and once into the key. Since we\n+use the `ticker` as the key, every new trade for the same ticker will\n+overwrite the previous trade for that ticker, hence the name\n+`latest_trades`.\n+\n+### Creating a Streaming Job\n+\n+However, you cannot directly execute the above command because it would\n+never complete. Remember, it's reading from a Kafka topic, which is a\n+stream of messages. Since it doesn't return any rows to the client, you\n+must create a job for it:\n+\n+```sql\n+CREATE JOB trades_ingestion AS\n+SINK INTO latest_trades(__key, ticker, price, amount)\n+SELECT ticker, ticker, price, amount\n+FROM trades\n+```\n+\n+The part after the `AS` keyword is same as the previous statement. The\n+command will create a job named `trades_ingestion`. Now, even if the\n+client disconnects, the cluster will continue running the job.\n+\n+### Cancelling the Job\n+\n+To cancel the job, use:\n+", "originalCommit": "ef16c4d330d5d808acaf553a2bdb5f02beba056c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "42782afdc149a0f2e69c86d8bed657762ddd01ea", "url": "https://github.com/hazelcast/hazelcast-jet/commit/42782afdc149a0f2e69c86d8bed657762ddd01ea", "message": "Address Vlado's review", "committedDate": "2020-11-19T14:13:09Z", "type": "commit"}, {"oid": "b27ce762a47189c9d0417a4f9cad2f55e085adbb", "url": "https://github.com/hazelcast/hazelcast-jet/commit/b27ce762a47189c9d0417a4f9cad2f55e085adbb", "message": "Fix", "committedDate": "2020-11-19T14:33:47Z", "type": "commit"}]}