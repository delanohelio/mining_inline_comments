{"pr_number": 2729, "pr_title": "Add SQL file support", "pr_createdAt": "2020-12-04T10:58:38Z", "pr_url": "https://github.com/hazelcast/hazelcast-jet/pull/2729", "timeline": [{"oid": "f11c05c57c9bb15c0a278086e363aa1fd4a88643", "url": "https://github.com/hazelcast/hazelcast-jet/commit/f11c05c57c9bb15c0a278086e363aa1fd4a88643", "message": "Add file connector (SQL)", "committedDate": "2020-11-05T14:12:52Z", "type": "commit"}, {"oid": "57e63c8ccff9282db2fc2030d5916853c1acc3bd", "url": "https://github.com/hazelcast/hazelcast-jet/commit/57e63c8ccff9282db2fc2030d5916853c1acc3bd", "message": "Merge branch 'master' into sql-file\n\n# Conflicts:\n#\thazelcast-jet-sql/pom.xml", "committedDate": "2020-11-06T15:28:59Z", "type": "commit"}, {"oid": "0e6f1fd651728dfaf433af1fe26dd0b38bd8d5d3", "url": "https://github.com/hazelcast/hazelcast-jet/commit/0e6f1fd651728dfaf433af1fe26dd0b38bd8d5d3", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-11-16T14:49:54Z", "type": "commit"}, {"oid": "f5d5da09c55535a9461920ab07dda014e29bbb57", "url": "https://github.com/hazelcast/hazelcast-jet/commit/f5d5da09c55535a9461920ab07dda014e29bbb57", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-11-24T11:37:07Z", "type": "commit"}, {"oid": "8077b57f3a270926adccb030c6e4726a4c1513cc", "url": "https://github.com/hazelcast/hazelcast-jet/commit/8077b57f3a270926adccb030c6e4726a4c1513cc", "message": "Follow changes from master", "committedDate": "2020-11-24T13:12:46Z", "type": "commit"}, {"oid": "3b0ace95bfba981d6eed59c628023635330b7d43", "url": "https://github.com/hazelcast/hazelcast-jet/commit/3b0ace95bfba981d6eed59c628023635330b7d43", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-11-26T08:54:08Z", "type": "commit"}, {"oid": "21c580558b81267ecac12e34a065cb158226ac1f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/21c580558b81267ecac12e34a065cb158226ac1f", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-11-26T12:09:03Z", "type": "commit"}, {"oid": "928151365ea2adec57a04b3e23ec045fba6e16c0", "url": "https://github.com/hazelcast/hazelcast-jet/commit/928151365ea2adec57a04b3e23ec045fba6e16c0", "message": "Fix checkstyle", "committedDate": "2020-11-26T14:44:00Z", "type": "commit"}, {"oid": "cf4fb6096b4415c18a0fae4554265ff0057a14ee", "url": "https://github.com/hazelcast/hazelcast-jet/commit/cf4fb6096b4415c18a0fae4554265ff0057a14ee", "message": "Fix dependency issue (Jetty conflict)", "committedDate": "2020-11-26T14:44:00Z", "type": "commit"}, {"oid": "7b5d3f70596398516f8fbdd5d41ed669740e80e5", "url": "https://github.com/hazelcast/hazelcast-jet/commit/7b5d3f70596398516f8fbdd5d41ed669740e80e5", "message": "Add buildMetaSupplier() to FileSourceBuilder", "committedDate": "2020-11-26T14:44:00Z", "type": "commit"}, {"oid": "f39fe5cc5d4ab305678176041d12844ce6012ff4", "url": "https://github.com/hazelcast/hazelcast-jet/commit/f39fe5cc5d4ab305678176041d12844ce6012ff4", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-11-26T15:26:01Z", "type": "commit"}, {"oid": "bf2c4b5bb864f6c0dd0e86a7c5792f44681c47db", "url": "https://github.com/hazelcast/hazelcast-jet/commit/bf2c4b5bb864f6c0dd0e86a7c5792f44681c47db", "message": "Switch to new File API", "committedDate": "2020-12-02T10:23:12Z", "type": "commit"}, {"oid": "6b740f40158111eec2d7d1b78c8199d8ad596bee", "url": "https://github.com/hazelcast/hazelcast-jet/commit/6b740f40158111eec2d7d1b78c8199d8ad596bee", "message": "Use testcontainers based Kafka Schema Registry for Kafka tests", "committedDate": "2020-12-02T13:17:32Z", "type": "commit"}, {"oid": "a856cb91e710e94e30487324a46a8a6ea9ee659a", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a856cb91e710e94e30487324a46a8a6ea9ee659a", "message": "Support file table function options", "committedDate": "2020-12-02T15:36:12Z", "type": "commit"}, {"oid": "b89b89728f3e7e68060ebf84f1e5b1d791aed416", "url": "https://github.com/hazelcast/hazelcast-jet/commit/b89b89728f3e7e68060ebf84f1e5b1d791aed416", "message": "Validation", "committedDate": "2020-12-03T11:36:16Z", "type": "commit"}, {"oid": "eb002cba1cbd0f210efe2be3893d1ed2537a6297", "url": "https://github.com/hazelcast/hazelcast-jet/commit/eb002cba1cbd0f210efe2be3893d1ed2537a6297", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-12-03T14:01:09Z", "type": "commit"}, {"oid": "a6104928346ca44c166940b2674d99dff01d378c", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a6104928346ca44c166940b2674d99dff01d378c", "message": "Cleanup", "committedDate": "2020-12-03T14:48:39Z", "type": "commit"}, {"oid": "86b092adb1ec3b17178d84215b9e6e88870de345", "url": "https://github.com/hazelcast/hazelcast-jet/commit/86b092adb1ec3b17178d84215b9e6e88870de345", "message": "Make glob optional", "committedDate": "2020-12-04T09:41:23Z", "type": "commit"}, {"oid": "d9cb06cc31f711612bce1e00dbd9be81db4b1b91", "url": "https://github.com/hazelcast/hazelcast-jet/commit/d9cb06cc31f711612bce1e00dbd9be81db4b1b91", "message": "Add Sql File connector docs", "committedDate": "2020-12-04T10:50:17Z", "type": "commit"}, {"oid": "a92d5e9881d8fc427803f7d8fa6e646bc14bcccc", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a92d5e9881d8fc427803f7d8fa6e646bc14bcccc", "message": "Fix Kafka docs", "committedDate": "2020-12-07T07:17:34Z", "type": "commit"}, {"oid": "8a32108b9bb866ca2c25907e286231a4fcd93f30", "url": "https://github.com/hazelcast/hazelcast-jet/commit/8a32108b9bb866ca2c25907e286231a4fcd93f30", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into sql-file", "committedDate": "2020-12-07T13:41:44Z", "type": "commit"}, {"oid": "be8d86842daac7e7586d541dffcdc6b2e68a934d", "url": "https://github.com/hazelcast/hazelcast-jet/commit/be8d86842daac7e7586d541dffcdc6b2e68a934d", "message": "Merge remote-tracking branch 'upstream/sql-file' into sql-file", "committedDate": "2020-12-07T14:14:24Z", "type": "commit"}, {"oid": "6a3852360edff9267d54b6a320b68180f9360844", "url": "https://github.com/hazelcast/hazelcast-jet/commit/6a3852360edff9267d54b6a320b68180f9360844", "message": "Wording touchups, test for empty column list", "committedDate": "2020-12-07T15:04:22Z", "type": "commit"}, {"oid": "13e762dc40402c9f054afc186814c7cdb7759694", "url": "https://github.com/hazelcast/hazelcast-jet/commit/13e762dc40402c9f054afc186814c7cdb7759694", "message": "Documentation updates", "committedDate": "2020-12-07T16:28:10Z", "type": "commit"}, {"oid": "696347824cb00088fc9585049ee7129bfb530f81", "url": "https://github.com/hazelcast/hazelcast-jet/commit/696347824cb00088fc9585049ee7129bfb530f81", "message": "Merge remote-tracking branch 'upstream/master' into sql-file", "committedDate": "2020-12-09T07:39:42Z", "type": "commit"}, {"oid": "89fde5bc970c9c0c5c560328bd6ab803891810bd", "url": "https://github.com/hazelcast/hazelcast-jet/commit/89fde5bc970c9c0c5c560328bd6ab803891810bd", "message": "Merge branch 'sql-file' into sql-file-upstream", "committedDate": "2020-12-09T07:49:21Z", "type": "commit"}, {"oid": "1ff2845ac238cda544c0c886429bacd344ff1235", "url": "https://github.com/hazelcast/hazelcast-jet/commit/1ff2845ac238cda544c0c886429bacd344ff1235", "message": "Minor changes", "committedDate": "2020-12-09T08:11:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTQ5OA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r537539498", "bodyText": "This probably has no effect.", "author": "viliam-durina", "createdAt": "2020-12-07T14:16:02Z", "path": "extensions/csv/src/main/java/com/hazelcast/jet/csv/impl/CsvReadFileFnProvider.java", "diffHunk": "@@ -51,20 +52,31 @@\n     public <T> FunctionEx<Path, Stream<T>> createReadFileFn(@Nonnull FileFormat<T> format) {\n         CsvFileFormat<T> csvFileFormat = (CsvFileFormat<T>) format;\n         Class<?> formatClazz = csvFileFormat.clazz(); // Format is not Serializable\n-        return path -> {\n-            CsvSchema schema = CsvSchema.emptySchema().withHeader();\n-            CsvMapper mapper = new CsvMapper();\n-            ObjectReader reader = mapper.readerFor(formatClazz)\n-                                        .withoutFeatures(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n-                                        .with(schema);\n+        boolean includesHeader = csvFileFormat.includesHeader();\n \n+        return path -> {\n+            ObjectReader reader = reader(formatClazz, includesHeader);\n             FileInputStream fis = new FileInputStream(path.toFile());\n-\n             return StreamSupport.<T>stream(Spliterators.spliteratorUnknownSize(reader.readValues(fis), ORDERED), false)\n                     .onClose(() -> uncheckRun(fis::close));\n         };\n     }\n \n+    private static <T> ObjectReader reader(Class<T> clazz, boolean includesHeader) {\n+        if (clazz == null) {\n+            return new CsvMapper().enable(Feature.WRAP_AS_ARRAY)\n+                                  .readerFor(String[].class)\n+                                  .withoutFeatures(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)", "originalCommit": "8a32108b9bb866ca2c25907e286231a4fcd93f30", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTY4MA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r537539680", "bodyText": "This probably has no effect.", "author": "viliam-durina", "createdAt": "2020-12-07T14:16:17Z", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "diffHunk": "@@ -101,4 +97,19 @@ public void close() throws IOException {\n             }\n         };\n     }\n+\n+    private static <T> ObjectReader reader(Class<T> clazz, boolean includesHeader) {\n+        if (clazz == null) {\n+            return new CsvMapper().enable(Feature.WRAP_AS_ARRAY)\n+                                  .readerFor(String[].class)\n+                                  .withoutFeatures(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)", "originalCommit": "8a32108b9bb866ca2c25907e286231a4fcd93f30", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwNjE1MQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r537606151", "bodyText": "We should add this dep everywhere where jackson-jr-objects i guess. For example, jackson-jr-objects is optional in hazelcast-jet-distribution", "author": "viliam-durina", "createdAt": "2020-12-07T15:40:52Z", "path": "hazelcast-jet-core/pom.xml", "diffHunk": "@@ -107,6 +107,11 @@\n             <artifactId>classgraph</artifactId>\n             <version>${classgraph.version}</version>\n         </dependency>\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.jr</groupId>\n+            <artifactId>jackson-jr-stree</artifactId>\n+            <version>${jackson.jr.version}</version>", "originalCommit": "6a3852360edff9267d54b6a320b68180f9360844", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyMDA4NQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538220085", "bodyText": "It's not great that we just ignore the logging. When one enables the logging to see which files are open, it's not good that files open through this traverser are not logged.", "author": "viliam-durina", "createdAt": "2020-12-08T10:30:08Z", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/connector/ReadFilesP.java", "diffHunk": "@@ -160,24 +129,118 @@ public void close() throws IOException {\n     ) {\n         checkSerializable(readFileFn, \"readFileFn\");\n \n-        return ProcessorMetaSupplier.of(DEFAULT_LOCAL_PARALLELISM, () -> new ReadFilesP<>(\n-                directory, glob, sharedFileSystem, readFileFn)\n-        );\n+        return new MetaSupplier<>(DEFAULT_LOCAL_PARALLELISM, directory, glob, sharedFileSystem, readFileFn);\n     }\n \n-    /**\n-     * Private API.\n-     */\n-    public static <T> Processor processor(\n-            @Nonnull String directory,\n-            @Nonnull String glob,\n-            boolean sharedFileSystem,\n-            @Nonnull FunctionEx<? super Path, ? extends Stream<T>> readFileFn\n-    ) {\n-        checkSerializable(readFileFn, \"readFileFn\");\n+    private static final class MetaSupplier<T> implements FileProcessorMetaSupplier<T> {\n+\n+        private final int localParallelism;\n+        private final String directory;\n+        private final String glob;\n+        private final boolean sharedFileSystem;\n+        private final FunctionEx<? super Path, ? extends Stream<T>> readFileFn;\n+\n+        private MetaSupplier(\n+                int localParallelism,\n+                String directory,\n+                String glob,\n+                boolean sharedFileSystem,\n+                FunctionEx<? super Path, ? extends Stream<T>> readFileFn\n+        ) {\n+            this.localParallelism = localParallelism;\n+            this.directory = directory;\n+            this.glob = glob;\n+            this.sharedFileSystem = sharedFileSystem;\n+            this.readFileFn = readFileFn;\n+        }\n \n-        return new ReadFilesP<>(\n-                directory, glob, sharedFileSystem, readFileFn\n-        );\n+        @Nonnull\n+        @Override\n+        public Function<? super Address, ? extends ProcessorSupplier> get(@Nonnull List<Address> addresses) {\n+            return address -> ProcessorSupplier.of(() -> new ReadFilesP<>(directory, glob, sharedFileSystem, readFileFn));\n+        }\n+\n+        @Override\n+        public int preferredLocalParallelism() {\n+            return localParallelism;\n+        }\n+\n+        @Override\n+        public FileTraverser<T> traverser() {\n+            return new LocalFileTraverser<>(DisabledLogger.INSTANCE, directory, glob, path -> true, readFileFn);", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyMDk0NQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538220945", "bodyText": "Javadoc needs updating. Class javadoc too.", "author": "viliam-durina", "createdAt": "2020-12-08T10:31:09Z", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/FileSourceFactory.java", "diffHunk": "@@ -36,5 +37,5 @@\n      * @param <T>           type of the item the source emits\n      */\n     @Nonnull\n-    <T> BatchSource<T> create(@Nonnull FileSourceConfiguration<T> configuration);\n+    <T> ProcessorMetaSupplier create(@Nonnull FileSourceConfiguration<T> configuration);", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyMzg1Ng==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538223856", "bodyText": "Might be useful to actually assert this, not just ignore the rest. For example like this:\n            // Only one implementation is expected to be present on classpath\n            Iterator<FileSourceFactory> iterator = loader.iterator();\n            if (!iterator.hasNext()) {\n                throw new JetException(\"No suitable FileSourceFactory found. \" +\n                        \"Do you have Jet's Hadoop module on classpath?\");\n            }\n            try {\n                return iterator.next().create(fsc);\n            } finally {\n                if (iterator.hasNext()) {\n                    throw new JetException(\"Multiple FileSourceFactory implementations found\");\n                }\n            }", "author": "viliam-durina", "createdAt": "2020-12-08T10:34:16Z", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -201,7 +214,7 @@ public FileSourceBuilder(@Nonnull String path) {\n                 path, glob, format, sharedFileSystem, options\n         );\n \n-        if (useHadoop || hasHadoopPrefix(path)) {\n+        if (shouldUseHadoop()) {\n             ServiceLoader<FileSourceFactory> loader = ServiceLoader.load(FileSourceFactory.class);\n             // Only one implementation is expected to be present on classpath", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI0MTA5Ng==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538241096", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        List<Processor> processors = new ArrayList<>(count);\n          \n          \n            \n                        for (int i = 0; i < count; i++) {\n          \n          \n            \n                            ResettableSingletonTraverser<Object[]> traverser = new ResettableSingletonTraverser<>();\n          \n          \n            \n                            RowProjector projector = projectorSupplier.get();\n          \n          \n            \n                            Processor processor = new TransformP<>(object -> {\n          \n          \n            \n                                traverser.accept(projector.project(object));\n          \n          \n            \n                                return traverser;\n          \n          \n            \n                            });\n          \n          \n            \n                            processors.add(processor);\n          \n          \n            \n                        }\n          \n          \n            \n                        return processors;\n          \n          \n            \n                        return Stream.generate(projectorSupplier)\n          \n          \n            \n                                     .limit(count)\n          \n          \n            \n                                     .map(projector -> mapP(projector::project).get())\n          \n          \n            \n                                     .collect(Collectors.toList());\n          \n      \n    \n    \n  \n\nWe also need to make RowProjector.Supplier implement SupplierEx<RowProjector> for the above to compile.", "author": "viliam-durina", "createdAt": "2020-12-08T10:56:39Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/Processors.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector;\n+\n+import com.hazelcast.function.SupplierEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.core.ResettableSingletonTraverser;\n+import com.hazelcast.jet.impl.processor.TransformP;\n+import com.hazelcast.nio.ObjectDataInput;\n+import com.hazelcast.nio.ObjectDataOutput;\n+import com.hazelcast.nio.serialization.DataSerializable;\n+import com.hazelcast.sql.impl.expression.Expression;\n+import com.hazelcast.sql.impl.extract.QueryTarget;\n+import com.hazelcast.sql.impl.type.QueryDataType;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public final class Processors {\n+\n+    private Processors() {\n+    }\n+\n+    public static ProcessorSupplier rowProjector(\n+            String[] paths,\n+            QueryDataType[] types,\n+            SupplierEx<QueryTarget> targetSupplier,\n+            Expression<Boolean> predicate,\n+            List<Expression<?>> projection\n+    ) {\n+        return new Processors.RowProjectorProcessorSupplier(\n+                RowProjector.supplier(paths, types, targetSupplier, predicate, projection));\n+    }\n+\n+    @SuppressFBWarnings(\n+            value = {\"SE_BAD_FIELD\", \"SE_NO_SERIALVERSIONID\"},\n+            justification = \"the class is never java-serialized\"\n+    )\n+    private static final class RowProjectorProcessorSupplier implements ProcessorSupplier, DataSerializable {\n+\n+        private RowProjector.Supplier projectorSupplier;\n+\n+        @SuppressWarnings(\"unused\")\n+        private RowProjectorProcessorSupplier() {\n+        }\n+\n+        RowProjectorProcessorSupplier(RowProjector.Supplier projectorSupplier) {\n+            this.projectorSupplier = projectorSupplier;\n+        }\n+\n+        @Nonnull\n+        @Override\n+        public Collection<? extends Processor> get(int count) {\n+            List<Processor> processors = new ArrayList<>(count);\n+            for (int i = 0; i < count; i++) {\n+                ResettableSingletonTraverser<Object[]> traverser = new ResettableSingletonTraverser<>();\n+                RowProjector projector = projectorSupplier.get();\n+                Processor processor = new TransformP<>(object -> {\n+                    traverser.accept(projector.project(object));\n+                    return traverser;\n+                });\n+                processors.add(processor);\n+            }\n+            return processors;", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI0MzE5MQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538243191", "bodyText": "You can use writeUtfArray.", "author": "viliam-durina", "createdAt": "2020-12-08T10:59:33Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/RowProjector.java", "diffHunk": "@@ -90,4 +95,77 @@ public RowProjector(\n     public int getColumnCount() {\n         return extractors.length;\n     }\n+\n+    public static Supplier supplier(\n+            String[] paths,\n+            QueryDataType[] types,\n+            SupplierEx<QueryTarget> targetSupplier,\n+            Expression<Boolean> predicate,\n+            List<Expression<?>> projections\n+    ) {\n+        return new Supplier(paths, types, targetSupplier, predicate, projections);\n+    }\n+\n+    public static class Supplier implements DataSerializable {\n+\n+        private String[] paths;\n+        private QueryDataType[] types;\n+\n+        private SupplierEx<QueryTarget> targetSupplier;\n+\n+        private Expression<Boolean> predicate;\n+        private List<Expression<?>> projections;\n+\n+        @SuppressWarnings(\"unused\")\n+        private Supplier() {\n+        }\n+\n+        Supplier(\n+                String[] paths,\n+                QueryDataType[] types,\n+                SupplierEx<QueryTarget> targetSupplier,\n+                Expression<Boolean> predicate,\n+                List<Expression<?>> projections\n+        ) {\n+            this.paths = paths;\n+            this.types = types;\n+            this.targetSupplier = targetSupplier;\n+            this.predicate = predicate;\n+            this.projections = projections;\n+        }\n+\n+        public RowProjector get() {\n+            return new RowProjector(paths, types, targetSupplier.get(), predicate, projections);\n+        }\n+\n+        @Override\n+        public void writeData(ObjectDataOutput out) throws IOException {\n+            out.writeInt(paths.length);\n+            for (String path : paths) {", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI1MTI5Ng==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538251296", "bodyText": "Jet already has Processors class, it might confuse users which one to import. What about SqlProcessors?", "author": "viliam-durina", "createdAt": "2020-12-08T11:12:04Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/Processors.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector;\n+\n+import com.hazelcast.function.SupplierEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.core.ResettableSingletonTraverser;\n+import com.hazelcast.jet.impl.processor.TransformP;\n+import com.hazelcast.nio.ObjectDataInput;\n+import com.hazelcast.nio.ObjectDataOutput;\n+import com.hazelcast.nio.serialization.DataSerializable;\n+import com.hazelcast.sql.impl.expression.Expression;\n+import com.hazelcast.sql.impl.extract.QueryTarget;\n+import com.hazelcast.sql.impl.type.QueryDataType;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public final class Processors {", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI1MjUwNg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538252506", "bodyText": "As far as I understand, this whole class is needed to support DataSerializable. But I don't think it's a big issue if we use java serialization, we do it everywhere else. The below code will do that, and you can delete RowProjectorProcessorSupplier and the RowProjector.Supplier:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new Processors.RowProjectorProcessorSupplier(\n          \n          \n            \n                            RowProjector.supplier(paths, types, targetSupplier, predicate, projection));\n          \n          \n            \n                    ServiceFactory<?, RowProjector> service =\n          \n          \n            \n                            nonSharedService(ctx -> new RowProjector(paths, types, targetSupplier.get(), predicate, projection));\n          \n          \n            \n                    return mapUsingServiceP(service, (projector, item) -> projector.project(item));", "author": "viliam-durina", "createdAt": "2020-12-08T11:14:13Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/Processors.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector;\n+\n+import com.hazelcast.function.SupplierEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.core.ResettableSingletonTraverser;\n+import com.hazelcast.jet.impl.processor.TransformP;\n+import com.hazelcast.nio.ObjectDataInput;\n+import com.hazelcast.nio.ObjectDataOutput;\n+import com.hazelcast.nio.serialization.DataSerializable;\n+import com.hazelcast.sql.impl.expression.Expression;\n+import com.hazelcast.sql.impl.extract.QueryTarget;\n+import com.hazelcast.sql.impl.type.QueryDataType;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public final class Processors {\n+\n+    private Processors() {\n+    }\n+\n+    public static ProcessorSupplier rowProjector(\n+            String[] paths,\n+            QueryDataType[] types,\n+            SupplierEx<QueryTarget> targetSupplier,\n+            Expression<Boolean> predicate,\n+            List<Expression<?>> projection\n+    ) {\n+        return new Processors.RowProjectorProcessorSupplier(\n+                RowProjector.supplier(paths, types, targetSupplier, predicate, projection));", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI1NzQ1Mg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r538257452", "bodyText": "Is there a reason it's not supported?", "author": "viliam-durina", "createdAt": "2020-12-08T11:22:34Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/file/CsvMetadataResolver.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.sql.impl.extract.CsvQueryTarget;\n+import com.hazelcast.jet.sql.impl.schema.MappingField;\n+import com.hazelcast.sql.impl.QueryException;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+final class CsvMetadataResolver extends MetadataResolver {\n+\n+    static final CsvMetadataResolver INSTANCE = new CsvMetadataResolver();\n+\n+    private CsvMetadataResolver() {\n+    }\n+\n+    @Override\n+    public String supportedFormat() {\n+        return CsvFileFormat.FORMAT_CSV;\n+    }\n+\n+    @Override\n+    public List<MappingField> resolveAndValidateFields(List<MappingField> userFields, Map<String, ?> options) {\n+        return !userFields.isEmpty() ? validateFields(userFields) : resolveFieldsFromSample(options);\n+    }\n+\n+    private List<MappingField> validateFields(List<MappingField> userFields) {\n+        for (MappingField userField : userFields) {\n+            if (userField.externalName() != null) {\n+                throw QueryException.error(\"EXTERNAL NAME not supported\");", "originalCommit": "13e762dc40402c9f054afc186814c7cdb7759694", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ2NDA5MA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r539464090", "bodyText": "The following test fails:\n    @Test\n    public void test_fieldsSubset() {\n        String name = randomName();\n        sqlService.execute(\"CREATE MAPPING \" + name + \" (\"\n                + \"\\\"int\\\" VARCHAR\"\n                + \", byte VARCHAR\"\n                + \") TYPE \" + FileSqlConnector.TYPE_NAME + ' '\n                + \"OPTIONS ( \"\n                + '\\'' + OPTION_FORMAT + \"'='\" + CSV_FORMAT + '\\''\n                + \", '\" + FileSqlConnector.OPTION_PATH + \"'='\" + RESOURCES_PATH + '\\''\n                + \", '\" + FileSqlConnector.OPTION_GLOB + \"'='\" + \"file.csv\" + '\\''\n                + \")\"\n        );\n\n        assertRowsAnyOrder(\n                \"SELECT * FROM \" + name,\n                singletonList(new Row(\n                        2147483647,\n                        (byte) 127\n                ))\n        );\n    }\nThe failure is:\njava.lang.AssertionError: \nExpecting:\n  <[Row{[string, true]}]>\nto contain exactly in any order:\n  <[Row{[2147483647, 127]}]>\nelements not found:\n  <[Row{[2147483647, 127]}]>\nand elements not expected:\n  <[Row{[string, true]}]>\n\nThe reason is that when we specify the columns for the CSV file mapping, it doesn't extract the specified columns from the CSV file, but it simply uses the first column in the CSV under the first name etc. This is surprising behavior IMO. When I cay create mapping t (my_field INT), i expect that the field my_field will be extracted from the CSV and not the first field.\nSecond, when I map a non-existing field, the query should either fail or return null for that field. I think the 2nd option is better, this will allow for schema evolution: newer files will miss some fields, but we can map them and process older and newer files together. The following test should pass too:\n    @Test\n    public void when_mapNonExistingColumn_then_isNull() {\n        String name = randomName();\n        sqlService.execute(\"CREATE MAPPING \" + name + \" (non_existent_field VARCHAR) \"\n                + \"TYPE \" + FileSqlConnector.TYPE_NAME + ' '\n                + \"OPTIONS ( \"\n                + '\\'' + OPTION_FORMAT + \"'='\" + CSV_FORMAT + '\\''\n                + \", '\" + FileSqlConnector.OPTION_PATH + \"'='\" + RESOURCES_PATH + '\\''\n                + \", '\" + FileSqlConnector.OPTION_GLOB + \"'='\" + \"file.csv\" + '\\''\n                + \")\"\n        );\n\n        assertRowsAnyOrder(\"select * from \" + name,\n                singletonList(new Row((Object) null)));\n    }", "author": "viliam-durina", "createdAt": "2020-12-09T16:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODI1NzQ1Mg=="}], "type": "inlineReview"}, {"oid": "86ba6f252acb2be279a5a9a2ce44f86fdbaff7db", "url": "https://github.com/hazelcast/hazelcast-jet/commit/86ba6f252acb2be279a5a9a2ce44f86fdbaff7db", "message": "Fix checkstyle", "committedDate": "2020-12-09T08:31:59Z", "type": "commit"}, {"oid": "b47e8f4cb6a317c94cf76f40f51d80ae88c5554a", "url": "https://github.com/hazelcast/hazelcast-jet/commit/b47e8f4cb6a317c94cf76f40f51d80ae88c5554a", "message": "Address PR comments", "committedDate": "2020-12-09T09:22:22Z", "type": "commit"}, {"oid": "2c897b050513dbf55f516f5cf1b886ac6531c7b6", "url": "https://github.com/hazelcast/hazelcast-jet/commit/2c897b050513dbf55f516f5cf1b886ac6531c7b6", "message": "Address PR comments", "committedDate": "2020-12-09T09:38:58Z", "type": "commit"}, {"oid": "926fa8c31dc5dfa079d8e314f3d2f871d0d0d97a", "url": "https://github.com/hazelcast/hazelcast-jet/commit/926fa8c31dc5dfa079d8e314f3d2f871d0d0d97a", "message": "Publish hadoop module without relocation\n\nWe need 3 different hadoop modules:\n- hazelcast-jet-hadoop-core - all our classes without relocation\nshould be used from other Jet modules, in combination with\nhazelcast-jet-core\n\n- hazelcast-jet-hadoop - for users, with relocated packages (Jackson),\nto be used together with hazelcast-jet\nusers provide own version of hadoop\n\n- hazelcast-jet-hadoop-all - same as previous, with all deps included", "committedDate": "2020-12-09T11:59:03Z", "type": "commit"}, {"oid": "c1997ecc87503c134c2e364a7bdbe62b047a7e6e", "url": "https://github.com/hazelcast/hazelcast-jet/commit/c1997ecc87503c134c2e364a7bdbe62b047a7e6e", "message": "Merge remote-tracking branch 'upstream/sql-file' into sql-file", "committedDate": "2020-12-09T12:23:52Z", "type": "commit"}, {"oid": "989c35e515c3cc22fdb7a4a074e1c387f7a6946b", "url": "https://github.com/hazelcast/hazelcast-jet/commit/989c35e515c3cc22fdb7a4a074e1c387f7a6946b", "message": "Merge branch 'master' into sql-file", "committedDate": "2020-12-09T13:14:28Z", "type": "commit"}, {"oid": "2136662e8d498e397d61973321a462e22b05aa42", "url": "https://github.com/hazelcast/hazelcast-jet/commit/2136662e8d498e397d61973321a462e22b05aa42", "message": "Shade sql module", "committedDate": "2020-12-09T13:44:29Z", "type": "commit"}, {"oid": "3d9723eac7d14a0150f455bca7ec9c77caed6578", "url": "https://github.com/hazelcast/hazelcast-jet/commit/3d9723eac7d14a0150f455bca7ec9c77caed6578", "message": "An attempt to fix Kafka Avro test", "committedDate": "2020-12-09T14:57:27Z", "type": "commit"}, {"oid": "d2bf7ffa2716e4c93f746ac665fd22a616c9e5da", "url": "https://github.com/hazelcast/hazelcast-jet/commit/d2bf7ffa2716e4c93f746ac665fd22a616c9e5da", "message": "Fix imports", "committedDate": "2020-12-09T15:01:53Z", "type": "commit"}, {"oid": "5cd6fac674c647f79a4460affd00ab733e7876c7", "url": "https://github.com/hazelcast/hazelcast-jet/commit/5cd6fac674c647f79a4460affd00ab733e7876c7", "message": "Remove unnecessary exception", "committedDate": "2020-12-09T15:02:26Z", "type": "commit"}, {"oid": "4b4ac59ca00eab4b8a8b20a3390483fedbdf071a", "url": "https://github.com/hazelcast/hazelcast-jet/commit/4b4ac59ca00eab4b8a8b20a3390483fedbdf071a", "message": "More tests", "committedDate": "2020-12-09T15:06:05Z", "type": "commit"}, {"oid": "bd99e7a5b41f8665b15e0e8961381ee0b0117c37", "url": "https://github.com/hazelcast/hazelcast-jet/commit/bd99e7a5b41f8665b15e0e8961381ee0b0117c37", "message": "Merge remote-tracking branch 'upstream/sql-file' into sql-file", "committedDate": "2020-12-09T15:08:04Z", "type": "commit"}, {"oid": "64681b5e39078b7de55bfa52ff3f7d2fdcd463af", "url": "https://github.com/hazelcast/hazelcast-jet/commit/64681b5e39078b7de55bfa52ff3f7d2fdcd463af", "message": "An attempt to fix Kafka Avro test", "committedDate": "2020-12-09T15:29:09Z", "type": "commit"}, {"oid": "32888ae7d629841e7f3f90364f3cc2773842d426", "url": "https://github.com/hazelcast/hazelcast-jet/commit/32888ae7d629841e7f3f90364f3cc2773842d426", "message": "An attempt to fix Kafka Avro test", "committedDate": "2020-12-10T10:17:48Z", "type": "commit"}, {"oid": "d418b43f270b94dbb5cef446df03f50c813249b8", "url": "https://github.com/hazelcast/hazelcast-jet/commit/d418b43f270b94dbb5cef446df03f50c813249b8", "message": "Remove never thrown exceptions", "committedDate": "2020-12-10T10:44:44Z", "type": "commit"}, {"oid": "02d757ce8f3ede648eccddf5478d4018de0bd592", "url": "https://github.com/hazelcast/hazelcast-jet/commit/02d757ce8f3ede648eccddf5478d4018de0bd592", "message": "Fix checkstyle", "committedDate": "2020-12-10T10:47:55Z", "type": "commit"}, {"oid": "72c825bb5a1da357589e626e5a20a230882f1bd6", "url": "https://github.com/hazelcast/hazelcast-jet/commit/72c825bb5a1da357589e626e5a20a230882f1bd6", "message": "An attempt to fix Kafka Avro test", "committedDate": "2020-12-10T11:35:23Z", "type": "commit"}, {"oid": "2863f4d3d577f140c78a13b3c01caeb6cd629383", "url": "https://github.com/hazelcast/hazelcast-jet/commit/2863f4d3d577f140c78a13b3c01caeb6cd629383", "message": "An attempt to fix Kafka Avro test", "committedDate": "2020-12-10T11:57:03Z", "type": "commit"}, {"oid": "c797f388177a2432ef74efdf41098e5682920d8d", "url": "https://github.com/hazelcast/hazelcast-jet/commit/c797f388177a2432ef74efdf41098e5682920d8d", "message": "Use actual logger for ReadFilesP", "committedDate": "2020-12-10T12:05:47Z", "type": "commit"}, {"oid": "06f2051f0cbfd9535d78afede89e7becb632b046", "url": "https://github.com/hazelcast/hazelcast-jet/commit/06f2051f0cbfd9535d78afede89e7becb632b046", "message": "Rework CSV file connector", "committedDate": "2020-12-10T14:11:11Z", "type": "commit"}, {"oid": "15df73f7d5732514fb1082530d4490e8b059f53f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/15df73f7d5732514fb1082530d4490e8b059f53f", "message": "Remove duplicate tests", "committedDate": "2020-12-10T15:04:09Z", "type": "commit"}, {"oid": "37e069c353bc9a771b0bce73ae75777665f93e6e", "url": "https://github.com/hazelcast/hazelcast-jet/commit/37e069c353bc9a771b0bce73ae75777665f93e6e", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into sql-file", "committedDate": "2020-12-10T15:47:19Z", "type": "commit"}, {"oid": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/c65fb479402bba445e4d8e5cd3419d51865b5a4f", "message": "Minor changes", "committedDate": "2020-12-10T18:22:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NzU5MA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540297590", "bodyText": "What about this more concise name?\nfilesHadoop(s3:///directory/*)\n\nfilesLocal(/directory/*.csv)\n\n\"files\" + (shouldUseHadoop() ? \"Hadoop\" : \"Local\") + '(' + path + '/' + glob + ')'", "author": "viliam-durina", "createdAt": "2020-12-10T16:12:58Z", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -190,6 +193,17 @@ public FileSourceBuilder(@Nonnull String path) {\n      */\n     @Nonnull\n     public BatchSource<T> build() {\n+        ProcessorMetaSupplier metaSupplier = buildMetaSupplier();\n+\n+        return Sources.batchFromProcessor(\"files(path=\" + path + \", glob=\" + glob + \", hadoop=\" + shouldUseHadoop(),", "originalCommit": "37e069c353bc9a771b0bce73ae75777665f93e6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM3OTAzNA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540379034", "bodyText": "Shouldn't we return null?", "author": "viliam-durina", "createdAt": "2020-12-10T17:56:23Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/schema/UnknownStatistic.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.schema;\n+\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelReferentialConstraint;\n+import org.apache.calcite.schema.Statistic;\n+import org.apache.calcite.util.ImmutableBitSet;\n+\n+import java.util.List;\n+\n+public final class UnknownStatistic implements Statistic {\n+\n+    public static final UnknownStatistic INSTANCE = new UnknownStatistic();\n+\n+    private UnknownStatistic() {\n+    }\n+\n+    @Override\n+    public Double getRowCount() {\n+        return 0.0;", "originalCommit": "37e069c353bc9a771b0bce73ae75777665f93e6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM5NzkyMQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540397921", "bodyText": "Is the only purpose of this class to be able to somehow pass the HazelcastTable object to place where the function is used? An explanatory comment would be good.", "author": "viliam-durina", "createdAt": "2020-12-10T18:24:48Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/schema/JetTableFunction.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.schema;\n+\n+import com.hazelcast.sql.impl.calcite.schema.HazelcastTable;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeComparability;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeFamily;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rel.type.RelDataTypePrecedenceList;\n+import org.apache.calcite.rel.type.StructKind;\n+import org.apache.calcite.schema.TableFunction;\n+import org.apache.calcite.sql.SqlCollation;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlIntervalQualifier;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+\n+import java.nio.charset.Charset;\n+import java.util.List;\n+\n+public abstract class JetTableFunction implements TableFunction {\n+\n+    @Override\n+    public final RelDataType getRowType(RelDataTypeFactory typeFactory, List<Object> arguments) {\n+        HazelcastTable table = toTable(arguments);\n+        RelDataType rowType = table.getRowType(typeFactory);\n+\n+        return new JetFunctionRelDataType(table, rowType);\n+    }\n+\n+    protected abstract HazelcastTable toTable(List<Object> arguments);\n+\n+    public HazelcastTable toTable(RelDataType rowType) {\n+        return ((JetFunctionRelDataType) rowType).table();\n+    }\n+\n+    private static final class JetFunctionRelDataType implements RelDataType {", "originalCommit": "37e069c353bc9a771b0bce73ae75777665f93e6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NDQwMw==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540294403", "bodyText": "Was there an issue with reading the csv as String[]? Reading each line as a Map seems memory heavy. I didn't do any benchmarks though so it might be just a feeling.", "author": "frant-hartm", "createdAt": "2020-12-10T16:08:51Z", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "diffHunk": "@@ -50,20 +51,15 @@\n \n             @Override\n             public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n-\n                 FileSplit fileSplit = (FileSplit) split;\n                 Configuration conf = context.getConfiguration();\n \n                 Configuration configuration = context.getConfiguration();\n                 String className = configuration.get(CSV_INPUT_FORMAT_BEAN_CLASS);\n-                Class<?> clazz = ReflectionUtils.loadClass(className);\n-\n-                CsvMapper mapper = new CsvMapper();\n-\n-                CsvSchema schema = CsvSchema.emptySchema().withHeader();\n-                ObjectReader reader = mapper.readerFor(clazz)\n-                                            .withoutFeatures(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n-                                            .with(schema);\n+                Class<?> clazz = className == null ? null : ReflectionUtils.loadClass(className);\n+                ObjectReader reader = new CsvMapper().readerFor(clazz != null ? clazz : Map.class)", "originalCommit": "37e069c353bc9a771b0bce73ae75777665f93e6e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1MTMzOQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540751339", "bodyText": "It was changed to support field subsets & different schemas per file. It's not much different from json where JrsObject stores a map internally.", "author": "gierlachg", "createdAt": "2020-12-11T07:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NDQwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDkwODAzNA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540908034", "bodyText": "For json we should ideally use streaming parser that would extract the fields we're interested in.", "author": "viliam-durina", "createdAt": "2020-12-11T12:19:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NDQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUwNzYyOQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540507629", "bodyText": "The FileFormat#json should also be able to read regular json files, not only jsonl. It doesn't work currently (bug reported by ondrej).\nIs the user expected to specify json or jsonl as format in SQL query? Maybe json would be more intuitive.", "author": "frant-hartm", "createdAt": "2020-12-10T21:25:07Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/file/FileTableFunction.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.hazelcast.internal.util.UuidUtil;\n+import com.hazelcast.jet.sql.impl.schema.JetTableFunction;\n+import com.hazelcast.jet.sql.impl.schema.JetTableFunctionParameter;\n+import com.hazelcast.jet.sql.impl.schema.MappingField;\n+import com.hazelcast.jet.sql.impl.schema.UnknownStatistic;\n+import com.hazelcast.sql.impl.calcite.schema.HazelcastTable;\n+import com.hazelcast.sql.impl.schema.Table;\n+import org.apache.calcite.schema.FunctionParameter;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+\n+import java.lang.reflect.Type;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.AVRO_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.CSV_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.JSONL_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.OPTION_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.PARQUET_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_GLOB;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_OPTIONS;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_PATH;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_SHARED_FILE_SYSTEM;\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptyList;\n+\n+public final class FileTableFunction extends JetTableFunction {\n+\n+    public static final FileTableFunction CSV = new FileTableFunction(CSV_FORMAT, asList(\n+            new JetTableFunctionParameter(0, OPTION_PATH, SqlTypeName.VARCHAR, true),\n+            new JetTableFunctionParameter(1, OPTION_GLOB, SqlTypeName.VARCHAR, false),\n+            new JetTableFunctionParameter(2, OPTION_SHARED_FILE_SYSTEM, SqlTypeName.VARCHAR, false),\n+            new JetTableFunctionParameter(3, OPTION_OPTIONS, SqlTypeName.MAP, false)\n+    ));\n+\n+    public static final FileTableFunction JSONL = new FileTableFunction(JSONL_FORMAT, asList(", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUwOTEyMQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540509121", "bodyText": "What happens when the first record you get for schema resolution has a field null, but other records have a int/double/string/ there.", "author": "frant-hartm", "createdAt": "2020-12-10T21:27:39Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/file/JsonResolver.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.fasterxml.jackson.jr.stree.JrsBoolean;\n+import com.fasterxml.jackson.jr.stree.JrsObject;\n+import com.fasterxml.jackson.jr.stree.JrsValue;\n+import com.hazelcast.jet.sql.impl.schema.MappingField;\n+import com.hazelcast.sql.impl.type.QueryDataType;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+final class JsonResolver {\n+\n+    private JsonResolver() {\n+    }\n+\n+    static List<MappingField> resolveFields(JrsObject object) {\n+        Map<String, MappingField> fields = new LinkedHashMap<>();\n+        Iterator<Entry<String, JrsValue>> iterator = object.fields();\n+        while (iterator.hasNext()) {\n+            Entry<String, JrsValue> entry = iterator.next();\n+\n+            String name = entry.getKey();\n+            QueryDataType type = resolveType(entry.getValue());\n+\n+            MappingField field = new MappingField(name, type);\n+            fields.putIfAbsent(field.name(), field);\n+        }\n+        return new ArrayList<>(fields.values());\n+    }\n+\n+    private static QueryDataType resolveType(JrsValue value) {\n+        if (value == null || value.isNull()) {", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1NDgzNg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540754836", "bodyText": "It's going to be resolved to OBJECT type and treated as such. Those are the downsides of automatic schema resolution.", "author": "gierlachg", "createdAt": "2020-12-11T07:55:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUwOTEyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDkwOTQ3NQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540909475", "bodyText": "We should mention it in the docs I think.", "author": "viliam-durina", "createdAt": "2020-12-11T12:22:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUwOTEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMDYzMA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540510630", "bodyText": "Does array fall under this? Are arrays supported by our SQL implementation?", "author": "frant-hartm", "createdAt": "2020-12-10T21:30:22Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/file/JsonResolver.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.fasterxml.jackson.jr.stree.JrsBoolean;\n+import com.fasterxml.jackson.jr.stree.JrsObject;\n+import com.fasterxml.jackson.jr.stree.JrsValue;\n+import com.hazelcast.jet.sql.impl.schema.MappingField;\n+import com.hazelcast.sql.impl.type.QueryDataType;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+final class JsonResolver {\n+\n+    private JsonResolver() {\n+    }\n+\n+    static List<MappingField> resolveFields(JrsObject object) {\n+        Map<String, MappingField> fields = new LinkedHashMap<>();\n+        Iterator<Entry<String, JrsValue>> iterator = object.fields();\n+        while (iterator.hasNext()) {\n+            Entry<String, JrsValue> entry = iterator.next();\n+\n+            String name = entry.getKey();\n+            QueryDataType type = resolveType(entry.getValue());\n+\n+            MappingField field = new MappingField(name, type);\n+            fields.putIfAbsent(field.name(), field);\n+        }\n+        return new ArrayList<>(fields.values());\n+    }\n+\n+    private static QueryDataType resolveType(JrsValue value) {\n+        if (value == null || value.isNull()) {\n+            return QueryDataType.OBJECT;\n+        } else if (value instanceof JrsBoolean) {\n+            return QueryDataType.BOOLEAN;\n+        } else if (value.isNumber()) {\n+            return QueryDataType.DOUBLE;\n+        } else if (value.isValueNode()) {\n+            return QueryDataType.VARCHAR;\n+        } else {\n+            return QueryDataType.OBJECT;", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1NTcyOA==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540755728", "bodyText": "Yes, array falls under this. We don't support ARRAY as type but an array is still an object and you can for example IS NULL it.", "author": "gierlachg", "createdAt": "2020-12-11T07:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMDYzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxNDEzNQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540514135", "bodyText": "Does this mean I can't query e.g. nested json fields?", "author": "frant-hartm", "createdAt": "2020-12-10T21:36:04Z", "path": "hazelcast-jet-sql/src/main/java/com/hazelcast/jet/sql/impl/connector/file/MetadataResolver.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.hazelcast.function.SupplierEx;\n+import com.hazelcast.jet.core.ProcessorMetaSupplier;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.impl.FileProcessorMetaSupplier;\n+import com.hazelcast.jet.pipeline.file.impl.FileTraverser;\n+import com.hazelcast.jet.sql.impl.schema.MappingField;\n+import com.hazelcast.sql.impl.QueryException;\n+import com.hazelcast.sql.impl.extract.QueryTarget;\n+import com.hazelcast.sql.impl.schema.TableField;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.sneakyThrow;\n+import static com.hazelcast.jet.impl.util.Util.toList;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_GLOB;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_PATH;\n+import static com.hazelcast.jet.sql.impl.connector.file.FileSqlConnector.OPTION_SHARED_FILE_SYSTEM;\n+import static java.util.Map.Entry;\n+\n+abstract class MetadataResolver<T> {\n+\n+    private final FileFormat<?> format;\n+    private final Function<T, List<MappingField>> fieldResolver;\n+    private final SupplierEx<QueryTarget> queryTargetSupplier;\n+\n+    protected MetadataResolver(\n+            FileFormat<?> format,\n+            Function<T, List<MappingField>> fieldResolver,\n+            SupplierEx<QueryTarget> queryTargetSupplier\n+    ) {\n+        this.format = format;\n+        this.fieldResolver = fieldResolver;\n+        this.queryTargetSupplier = queryTargetSupplier;\n+    }\n+\n+\n+    String supportedFormat() {\n+        return format.format();\n+    }\n+\n+    List<MappingField> resolveAndValidateFields(List<MappingField> userFields, Map<String, ?> options) {\n+        return !userFields.isEmpty() ? validateFields(userFields) : resolveFieldsFromSample(options);\n+    }\n+\n+    private List<MappingField> validateFields(List<MappingField> userFields) {\n+        for (MappingField userField : userFields) {\n+            String externalName = userField.externalName();\n+            if (externalName != null && externalName.indexOf('.') >= 0) {", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1NTk2Nw==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540755967", "bodyText": "Correct, nested fields are not supported for any connector at the moment.", "author": "gierlachg", "createdAt": "2020-12-11T07:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxNDEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNTYwNg==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540525606", "bodyText": "This is a binary file checked into the repo.\nIt would be easier to maintain to generate the file on the fly during the test - e.g. when someone needs to add a field to the file, create a similar file with a specific corner case etc. If the code is there it can be easily modified. Otherwise one would need to create the file from scratch or maybe somehow modify it.", "author": "frant-hartm", "createdAt": "2020-12-10T21:56:14Z", "path": "hazelcast-jet-sql/src/test/java/com/hazelcast/jet/sql/impl/connector/file/SqlAvroTest.java", "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.sql.impl.connector.file;\n+\n+import com.hazelcast.jet.sql.SqlTestSupport;\n+import com.hazelcast.sql.SqlService;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.math.BigDecimal;\n+import java.nio.file.Paths;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.AVRO_FORMAT;\n+import static com.hazelcast.jet.sql.impl.connector.SqlConnector.OPTION_FORMAT;\n+import static java.time.ZoneOffset.UTC;\n+import static java.util.Collections.singletonList;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+public class SqlAvroTest extends SqlTestSupport {\n+\n+    private static final String RESOURCES_PATH = Paths.get(\"src/test/resources\").toFile().getAbsolutePath();\n+\n+    private static SqlService sqlService;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+        initialize(1, null);\n+        sqlService = instance().getSql();\n+    }\n+\n+    @Test\n+    public void test_nulls() {\n+        String name = randomName();\n+        sqlService.execute(\"CREATE MAPPING \" + name + \" (\"\n+                + \"nonExistingField VARCHAR\"\n+                + \") TYPE \" + FileSqlConnector.TYPE_NAME + ' '\n+                + \"OPTIONS (\"\n+                + '\\'' + OPTION_FORMAT + \"'='\" + AVRO_FORMAT + '\\''\n+                + \", '\" + FileSqlConnector.OPTION_PATH + \"'='\" + RESOURCES_PATH + '\\''\n+                + \", '\" + FileSqlConnector.OPTION_GLOB + \"'='\" + \"file.avro\" + '\\''", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDA0MQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540534041", "bodyText": "This should be hazelcast-jet-hadoop-all.\nCloud storages need their respective jars, maybe just link to the section in sources and sinks.", "author": "frant-hartm", "createdAt": "2020-12-10T22:10:50Z", "path": "site/docs/sql/file-connector.md", "diffHunk": "@@ -0,0 +1,247 @@\n+---\n+title: File Connector\n+description: Description of the SQL File connector\n+---\n+\n+The File connector supports reading from both local and remote files.\n+\n+To work with files you must specify location and serialization.\n+Any options not recognized by Jet are passed, in case of remote files,\n+directly to Hadoop client. For local files they are simply ignored.\n+\n+You can find detailed information for the options below.\n+\n+## Location options\n+\n+`path` is an absolute path to the directory containing your data. These\n+are the supported schemes:\n+\n+* `hdfs`: HDFS\n+* `s3a`: Amazon S3\n+* `wasbs`: Azure Cloud Storage\n+* `adl`: Azure Data Lake Generation 1\n+* `abfs`: Azure Data Lake Generation 2\n+* `gs`: Google Cloud Storage\n+\n+So for instance, path to data residing in a Hadoop cluster could look\n+like `hdfs://path/to/directory/`. Any path not starting with any of the\n+above is considered local (i.e. files residing on Jet members), e.g.\n+`/path/to/directory/`.\n+\n+`glob` is a pattern to filter the files in the specified directory.\n+The default value is '*', matching all files.\n+\n+## Serialization options\n+\n+`format` defines the serialization used to read the files. We assume all\n+records in files have the same format. These are the supported `format`\n+values:\n+\n+* `csv`\n+* `jsonl`\n+* `avro`\n+* `parquet`: remote files only\n+\n+If you omit a file list from the `CREATE MAPPING` command, Jet will read\n+a sample file and try to determine column names and types from it. In\n+some cases you can use a different type if you specify the columns\n+explicitly. For example, the CSV format uses `VARCHAR` for all fields -\n+if you specify `DATE` manually, the behavior would be as if `CAST(column\n+AS DATE)` was used, using the same rules for conversion from `VARCHAR`\n+to `DATE` as `CAST` uses.\n+\n+Also if you don't specify the columns, the directory needs to be\n+available at the time you execute the `CREATE MAPPING` and it must not\n+be empty. In case of local files, every cluster member must have some\n+file. If you specify the columns, an empty directory is OK.\n+\n+See the examples for individual serialization options below.\n+\n+### CSV Serialization\n+\n+The `csv` files are expected to be comma-separated and `UTF-8` encoded.\n+Each file must have a header on the first line. If you omit the column\n+list from the mapping declaration, Jet will try to infer the column\n+names from the file header. All columns will have `VARCHAR` type.\n+\n+```sql\n+CREATE MAPPING my_files\n+TYPE File\n+OPTIONS (\n+    'path' = '/path/to/directory',\n+    'format' = 'csv'\n+)\n+```\n+\n+### JSONL serialization\n+\n+The `jsonl` files are expected to contain one valid json document per\n+line and be `UTF-8` encoded. If you skip mapping columns from the\n+declaration, we infer names and types based on a sample.\n+\n+```sql\n+CREATE MAPPING my_files\n+TYPE File\n+OPTIONS (\n+    'path' = '/path/to/directory',\n+    'format' = 'jsonl'\n+)\n+```\n+\n+#### Mapping Between JSON and SQL Types\n+\n+| JSON type | SQL Type  |\n+| - | - |\n+| `BOOLEAN` | `BOOLEAN` |\n+| `NUMBER` | `DOUBLE` |\n+| `STRING` | `VARCHAR` |\n+| all other types | `OBJECT` |\n+\n+### Avro & Parquet Serialization\n+\n+The `avro` & `parquet` files are expected to contain Avro records.\n+\n+```sql\n+CREATE MAPPING my_files\n+TYPE File\n+OPTIONS (\n+    'path' = '/path/to/directory',\n+    'format' = 'avro'\n+)\n+```\n+\n+```sql\n+CREATE MAPPING my_files\n+TYPE File\n+OPTIONS (\n+    'path' = 'hdfs://path/to/directory',\n+    'format' = 'parquet'\n+    /* more Hadoop options ... */\n+)\n+```\n+\n+#### Mapping Between Avro and SQL Types\n+\n+| Avro Type | SQL Type |\n+| - | - |\n+| `BOOLEAN` | `BOOLEAN` |\n+| `INT` | `INT` |\n+| `LONG` | `BIGINT` |\n+| `FLOAT` | `REAL` |\n+| `DOUBLE` | `DOUBLE` |\n+| `STRING` | `VARCHAR` |\n+| all other types | `OBJECT` |\n+\n+## External Name\n+\n+You rarely need to specify the columns in DDL. If you do, you might want\n+to specify the external name. External names are supported for all\n+formats except for `csv`.\n+\n+We don't support nested fields, hence the external name should refer to\n+the top-level field - not containing any `.`.\n+\n+## File Table Functions\n+\n+To execute an ad hoc query against data in files you can use one of the\n+predefined table functions:\n+\n+* `csv_file`\n+* `jsonl_file`\n+* `avro_file`\n+* `parquet_file`\n+\n+Table functions will create a temporary mapping, valid for the duration\n+of the statement. They accept the same options as `CREATE MAPPING`\n+statements.\n+\n+You can use positional arguments:\n+\n+```sql\n+SELECT * FROM TABLE(\n+  CSV_FILE('/path/to/directory', '*.csv', MAP['key', 'value'])\n+)\n+```\n+\n+Or named arguments:\n+\n+```sql\n+SELECT * FROM TABLE(\n+  CSV_FILE(path => '/path/to/directory', options => MAP['key', 'value'])\n+)\n+```\n+\n+## Installation\n+\n+Depending on what formats you want to work with you need different\n+modules on the classpath.\n+\n+`csv` format requires the `hazelcast-jet-csv` module:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-csv:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-csv</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+`avro` format requires the `hazelcast-jet-avro` module:\n+\n+<!--DOCUSAURUS_CODE_TABS-->\n+\n+<!--Gradle-->\n+\n+```groovy\n+compile 'com.hazelcast.jet:hazelcast-jet-avro:{jet-version}'\n+```\n+\n+<!--Maven-->\n+\n+```xml\n+<dependency>\n+    <groupId>com.hazelcast.jet</groupId>\n+    <artifactId>hazelcast-jet-avro</artifactId>\n+    <version>{jet-version}</version>\n+</dependency>\n+```\n+\n+<!--END_DOCUSAURUS_CODE_TABS-->\n+\n+`parquet` format and all remote files require the `hazelcast-jet-hadoop`", "originalCommit": "c65fb479402bba445e4d8e5cd3419d51865b5a4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1NjQwOQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2729#discussion_r540756409", "bodyText": "I have already linked file documentation from here to get rid of duplication.", "author": "gierlachg", "createdAt": "2020-12-11T07:58:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDA0MQ=="}], "type": "inlineReview"}, {"oid": "a6963463d2e95c0bbabda7d970b0cdbda6d514a4", "url": "https://github.com/hazelcast/hazelcast-jet/commit/a6963463d2e95c0bbabda7d970b0cdbda6d514a4", "message": "Update docs", "committedDate": "2020-12-11T07:20:24Z", "type": "commit"}, {"oid": "4eaed37b7f6a59d5738e8748681ddac8ab0d1a37", "url": "https://github.com/hazelcast/hazelcast-jet/commit/4eaed37b7f6a59d5738e8748681ddac8ab0d1a37", "message": "Cleanups", "committedDate": "2020-12-11T07:42:31Z", "type": "commit"}, {"oid": "01fb2af1359f21f65981e8050e614cae9c12f708", "url": "https://github.com/hazelcast/hazelcast-jet/commit/01fb2af1359f21f65981e8050e614cae9c12f708", "message": "jsonl -> json", "committedDate": "2020-12-11T08:15:02Z", "type": "commit"}, {"oid": "bc1f3f345e179a9db142d3ebea11f0c94ef22391", "url": "https://github.com/hazelcast/hazelcast-jet/commit/bc1f3f345e179a9db142d3ebea11f0c94ef22391", "message": "Generate avro & parquet data on the fly", "committedDate": "2020-12-11T10:07:54Z", "type": "commit"}, {"oid": "e6337620e0769c67a2a0a9518546f578576619cc", "url": "https://github.com/hazelcast/hazelcast-jet/commit/e6337620e0769c67a2a0a9518546f578576619cc", "message": "Fix tests", "committedDate": "2020-12-11T10:27:01Z", "type": "commit"}, {"oid": "74eccd6b98d8b0fba59d7197e3a8abd9985440f4", "url": "https://github.com/hazelcast/hazelcast-jet/commit/74eccd6b98d8b0fba59d7197e3a8abd9985440f4", "message": "Fix checkstyle", "committedDate": "2020-12-11T10:44:43Z", "type": "commit"}, {"oid": "d5ea1617253d174a0b3822ae5a1faaafd9698035", "url": "https://github.com/hazelcast/hazelcast-jet/commit/d5ea1617253d174a0b3822ae5a1faaafd9698035", "message": "Cleanup dependencies", "committedDate": "2020-12-11T11:11:43Z", "type": "commit"}]}