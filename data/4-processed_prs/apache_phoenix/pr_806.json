{"pr_number": 806, "pr_title": "PHOENIX-5946: Implement SchemaExtractionTool utility to get effective\u2026", "pr_createdAt": "2020-06-19T05:57:31Z", "pr_url": "https://github.com/apache/phoenix/pull/806", "timeline": [{"oid": "517fed053ba7b27ed6a00ff631cc2afad9ddb772", "url": "https://github.com/apache/phoenix/commit/517fed053ba7b27ed6a00ff631cc2afad9ddb772", "message": "PHOENIX-5946: Implement SchemaExtractionTool utility to get effective DDL from cluster (Co-authored by Tanuj Khurana)", "committedDate": "2020-07-02T00:56:16Z", "type": "commit"}, {"oid": "517fed053ba7b27ed6a00ff631cc2afad9ddb772", "url": "https://github.com/apache/phoenix/commit/517fed053ba7b27ed6a00ff631cc2afad9ddb772", "message": "PHOENIX-5946: Implement SchemaExtractionTool utility to get effective DDL from cluster (Co-authored by Tanuj Khurana)", "committedDate": "2020-07-02T00:56:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTg5NQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r454025895", "bodyText": "The test is only comparing the properties but not the entire create table statement.", "author": "tkhurana", "createdAt": "2020-07-14T00:30:41Z", "path": "phoenix-core/src/it/java/org/apache/phoenix/end2end/SchemaExtractionToolIT.java", "diffHunk": "@@ -0,0 +1,164 @@\n+package org.apache.phoenix.end2end;\n+\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.query.BaseTest;\n+import org.apache.phoenix.schema.SchemaExtractionTool;\n+import org.apache.phoenix.util.PropertiesUtil;\n+import org.apache.phoenix.util.ReadOnlyProps;\n+import org.apache.phoenix.util.SchemaUtil;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.util.Collections;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.phoenix.util.TestUtil.TEST_PROPERTIES;\n+\n+public class SchemaExtractionToolIT extends BaseTest {\n+\n+    @BeforeClass\n+    public static void setup() throws Exception {\n+        Map<String, String> props = Collections.emptyMap();\n+        setUpTestDriver(new ReadOnlyProps(props.entrySet().iterator()));\n+    }\n+\n+    @Test\n+    public void testCreateTableStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            conn.commit();\n+            String [] args = {\"-tb\", tableName, \"-s\", schemaName};\n+\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+            set.run(args);\n+            String actualProperties = set.output.substring(set.output.lastIndexOf(\")\")+1).replace(\" \",\"\");\n+            Assert.assertEquals(5, actualProperties.split(\",\").length);", "originalCommit": "517fed053ba7b27ed6a00ff631cc2afad9ddb772", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MDExNg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456070116", "bodyText": "I am going to heavily rely on Rachel's PR for test coverage if you don't mind.", "author": "swaroopak", "createdAt": "2020-07-16T20:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTg5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxNzgyNg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r454517826", "bodyText": "might be better if we have 2 different APIs getColumnInfoStringForTable and getColumnInfoStringForView rather than using null", "author": "tkhurana", "createdAt": "2020-07-14T17:21:21Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,473 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+    public static Configuration conf;\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+    public String output;\n+\n+    @Override\n+    public int run(String[] args) throws Exception {\n+        populateToolAttributes(args);\n+        conf = HBaseConfiguration.addHbaseResources(getConf());\n+        PTable table = getPTable(pSchemaName, pTableName);\n+        output = getDDL(table);\n+        return 0;\n+    }\n+\n+    private String getDDL(PTable table) throws Exception {\n+        String ddl = null;\n+        if(table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(table);\n+        } else if(table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(table);\n+        } else if(table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pSchemaName = indexPTable.getSchemaName().getString();\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pSchemaName, pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    // test with different default CF, key is a included column\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString, String coveredColumnString, boolean local, String pSchemaName, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX, local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {\n+        try (Connection conn = getConnection()) {\n+            return PhoenixRuntime.getTable(conn, pTableFullName);\n+        }\n+    }\n+\n+    protected String extractCreateViewDDL(PTable table) throws SQLException {\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+        String baseTableName = table.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, baseTableName);\n+        PTable baseTable = getPTable(baseTableFullName);\n+        String columnInfoString = getColumnInfoString(table, baseTable);\n+\n+        String whereClause = table.getViewStatement();\n+        if(whereClause != null) {\n+            whereClause = whereClause.substring(whereClause.indexOf(\"WHERE\"));\n+        }\n+        return generateCreateViewDDL(columnInfoString, baseTableFullName, whereClause == null ? \"\" : \" \"+whereClause, pSchemaName, pTableName);\n+    }\n+\n+    private String generateCreateViewDDL(String columnInfoString, String baseTableFullName, String whereClause, String pSchemaName, String pTableName) {\n+        String viewFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_VIEW, viewFullName, columnInfoString, baseTableFullName, whereClause));\n+        return outputBuilder.toString();\n+    }\n+\n+    public String extractCreateTableDDL(PTable table) throws IOException, SQLException {\n+\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+\n+        ConnectionQueryServices cqsi = getCQSIObject();\n+        HTableDescriptor htd = getHTableDescriptor(cqsi, table);\n+        HColumnDescriptor hcd = htd.getFamily(SchemaUtil.getEmptyColumnFamily(table));\n+\n+        populateDefaultProperties(table);\n+        setPTableProperties(table);\n+        setHTableProperties(htd);\n+        setHColumnFamilyProperties(hcd);\n+\n+        String columnInfoString = getColumnInfoString(table, null);", "originalCommit": "517fed053ba7b27ed6a00ff631cc2afad9ddb772", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxOTU1Mw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r454519553", "bodyText": "As mentioned above also, it might be cleaner to refactor this API into two different APIs for table and view and move common functionality to a shared function.", "author": "tkhurana", "createdAt": "2020-07-14T17:24:07Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,473 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+    public static Configuration conf;\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+    public String output;\n+\n+    @Override\n+    public int run(String[] args) throws Exception {\n+        populateToolAttributes(args);\n+        conf = HBaseConfiguration.addHbaseResources(getConf());\n+        PTable table = getPTable(pSchemaName, pTableName);\n+        output = getDDL(table);\n+        return 0;\n+    }\n+\n+    private String getDDL(PTable table) throws Exception {\n+        String ddl = null;\n+        if(table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(table);\n+        } else if(table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(table);\n+        } else if(table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pSchemaName = indexPTable.getSchemaName().getString();\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pSchemaName, pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    // test with different default CF, key is a included column\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString, String coveredColumnString, boolean local, String pSchemaName, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX, local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {\n+        try (Connection conn = getConnection()) {\n+            return PhoenixRuntime.getTable(conn, pTableFullName);\n+        }\n+    }\n+\n+    protected String extractCreateViewDDL(PTable table) throws SQLException {\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+        String baseTableName = table.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, baseTableName);\n+        PTable baseTable = getPTable(baseTableFullName);\n+        String columnInfoString = getColumnInfoString(table, baseTable);\n+\n+        String whereClause = table.getViewStatement();\n+        if(whereClause != null) {\n+            whereClause = whereClause.substring(whereClause.indexOf(\"WHERE\"));\n+        }\n+        return generateCreateViewDDL(columnInfoString, baseTableFullName, whereClause == null ? \"\" : \" \"+whereClause, pSchemaName, pTableName);\n+    }\n+\n+    private String generateCreateViewDDL(String columnInfoString, String baseTableFullName, String whereClause, String pSchemaName, String pTableName) {\n+        String viewFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_VIEW, viewFullName, columnInfoString, baseTableFullName, whereClause));\n+        return outputBuilder.toString();\n+    }\n+\n+    public String extractCreateTableDDL(PTable table) throws IOException, SQLException {\n+\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+\n+        ConnectionQueryServices cqsi = getCQSIObject();\n+        HTableDescriptor htd = getHTableDescriptor(cqsi, table);\n+        HColumnDescriptor hcd = htd.getFamily(SchemaUtil.getEmptyColumnFamily(table));\n+\n+        populateDefaultProperties(table);\n+        setPTableProperties(table);\n+        setHTableProperties(htd);\n+        setHColumnFamilyProperties(hcd);\n+\n+        String columnInfoString = getColumnInfoString(table, null);\n+        String propertiesString = convertPropertiesToString();\n+\n+        return generateTableDDLString(columnInfoString, propertiesString, pSchemaName, pTableName);\n+    }\n+    private String generateTableDDLString(String columnInfoString,String propertiesString,String pSchemaName,String pTableName) {\n+        String pTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_TABLE, pTableFullName));\n+        outputBuilder.append(columnInfoString).append(\" \").append(propertiesString);\n+        return outputBuilder.toString();\n+    }\n+\n+    private void populateDefaultProperties(PTable table) {\n+        Map<String, String> propsMap = HColumnDescriptor.getDefaultValues();\n+        for (Map.Entry<String, String> entry : propsMap.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            defaultProps.put(key, value);\n+            if(key.equalsIgnoreCase(BLOOMFILTER) || key.equalsIgnoreCase(COMPRESSION)) {\n+                defaultProps.put(key, \"NONE\");\n+            }\n+            if(key.equalsIgnoreCase(DATA_BLOCK_ENCODING)) {\n+                defaultProps.put(key, String.valueOf(DEFAULT_DATA_BLOCK_ENCODING));\n+            }\n+        }\n+        defaultProps.putAll(table.getDefaultValues());\n+    }\n+\n+    private void setHTableProperties(HTableDescriptor htd) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = htd.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            if(Bytes.toString(key.get()).contains(\"coprocessor\") || Bytes.toString(key.get()).contains(IS_META)) {\n+                continue;\n+            }\n+            defaultProps.put(Bytes.toString(key.get()), \"false\");\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));\n+        }\n+    }\n+\n+    private void setHColumnFamilyProperties(HColumnDescriptor columnDescriptor) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = columnDescriptor.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));\n+        }\n+    }\n+\n+    private void setPTableProperties(PTable table) {\n+        Map <String, String> map = table.getValues();\n+        for(Map.Entry<String, String> entry : map.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            if(value != null) {\n+                definedProps.put(key, value);\n+            }\n+        }\n+    }\n+\n+    private HTableDescriptor getHTableDescriptor(ConnectionQueryServices cqsi, PTable table)\n+            throws SQLException, IOException {\n+        return cqsi.getAdmin().getTableDescriptor(\n+                TableName.valueOf(table.getPhysicalName().getString()));\n+    }\n+\n+    private String convertPropertiesToString() {\n+        StringBuilder optionBuilder = new StringBuilder();\n+\n+        for(Map.Entry<String, String> entry : definedProps.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            if(value!=null && defaultProps.get(key) != null && !value.equals(defaultProps.get(key))) {\n+                if (optionBuilder.length() != 0) {\n+                    optionBuilder.append(\", \");\n+                }\n+                optionBuilder.append(key+\"=\"+value);\n+            }\n+        }\n+        return optionBuilder.toString();\n+    }\n+\n+    private PTable getPTable(String pSchemaName, String pTableName) throws SQLException {\n+        String pTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        return getPTable(pTableFullName);\n+    }\n+\n+    private ConnectionQueryServices getCQSIObject() throws SQLException {\n+        try(Connection conn = getConnection()) {\n+            return conn.unwrap(PhoenixConnection.class).getQueryServices();\n+        }\n+    }\n+\n+    public static Connection getConnection() throws SQLException {\n+        return ConnectionUtil.getInputConnection(conf);\n+    }\n+\n+    private String getColumnInfoString(PTable table, PTable baseTable) {", "originalCommit": "517fed053ba7b27ed6a00ff631cc2afad9ddb772", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "744025c30ab6f31038b17e28f3c2bab2695f3374", "url": "https://github.com/apache/phoenix/commit/744025c30ab6f31038b17e28f3c2bab2695f3374", "message": "Fixing review comments", "committedDate": "2020-07-16T21:20:46Z", "type": "commit"}, {"oid": "e2382e0aae613357b68db6e6aea710e637b340b4", "url": "https://github.com/apache/phoenix/commit/e2382e0aae613357b68db6e6aea710e637b340b4", "message": "Fixing review comments 2", "committedDate": "2020-07-16T23:35:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzMzk5NA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456133994", "bodyText": "nit: long line", "author": "gjacoby126", "createdAt": "2020-07-16T23:30:11Z", "path": "phoenix-core/src/it/java/org/apache/phoenix/end2end/SchemaExtractionToolIT.java", "diffHunk": "@@ -0,0 +1,164 @@\n+package org.apache.phoenix.end2end;\n+\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.query.BaseTest;\n+import org.apache.phoenix.schema.SchemaExtractionTool;\n+import org.apache.phoenix.util.PropertiesUtil;\n+import org.apache.phoenix.util.ReadOnlyProps;\n+import org.apache.phoenix.util.SchemaUtil;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.util.Collections;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.phoenix.util.TestUtil.TEST_PROPERTIES;\n+\n+public class SchemaExtractionToolIT extends BaseTest {\n+\n+    @BeforeClass\n+    public static void setup() throws Exception {\n+        Map<String, String> props = Collections.emptyMap();\n+        setUpTestDriver(new ReadOnlyProps(props.entrySet().iterator()));\n+    }\n+\n+    @Test\n+    public void testCreateTableStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            conn.commit();\n+            String [] args = {\"-tb\", tableName, \"-s\", schemaName};\n+\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+            set.run(args);\n+            String actualProperties = set.output.substring(set.output.lastIndexOf(\")\")+1).replace(\" \",\"\");\n+            Assert.assertEquals(5, actualProperties.split(\",\").length);\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateIndexStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String indexName = generateUniqueName();\n+        String indexName1 = generateUniqueName();\n+        String indexName2 = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+\n+            String createIndexStatement = \"CREATE INDEX \"+indexName + \" ON \"+pTableFullName+\"(v1 DESC) INCLUDE (v2)\";\n+\n+            String createIndexStatement1 = \"CREATE INDEX \"+indexName1 + \" ON \"+pTableFullName+\"(v2 DESC) INCLUDE (v1)\";\n+\n+            String createIndexStatement2 = \"CREATE INDEX \"+indexName2 + \" ON \"+pTableFullName+\"(k)\";\n+\n+            conn.createStatement().execute(createIndexStatement);\n+            conn.createStatement().execute(createIndexStatement1);\n+            conn.createStatement().execute(createIndexStatement2);\n+            conn.commit();\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+\n+            String [] args = {\"-tb\", indexName, \"-s\", schemaName};\n+            set.run(args);\n+            Assert.assertEquals(createIndexStatement.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args1 = {\"-tb\", indexName1, \"-s\", schemaName};\n+            set.run(args1);\n+            Assert.assertEquals(createIndexStatement1.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args2 = {\"-tb\", indexName2, \"-s\", schemaName};\n+            set.run(args2);\n+            Assert.assertEquals(createIndexStatement2.toUpperCase(), set.output.toUpperCase());\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateViewStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String viewName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k BIGINT NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            String viewFullName = SchemaUtil.getQualifiedTableName(schemaName, viewName);\n+            String viewFullName1 = SchemaUtil.getQualifiedTableName(schemaName, viewName+\"1\");\n+\n+\n+            String createView = \"CREATE VIEW \"+viewFullName + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;", "originalCommit": "744025c30ab6f31038b17e28f3c2bab2695f3374", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNDA0Mg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456134042", "bodyText": "ditto", "author": "gjacoby126", "createdAt": "2020-07-16T23:30:18Z", "path": "phoenix-core/src/it/java/org/apache/phoenix/end2end/SchemaExtractionToolIT.java", "diffHunk": "@@ -0,0 +1,164 @@\n+package org.apache.phoenix.end2end;\n+\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.query.BaseTest;\n+import org.apache.phoenix.schema.SchemaExtractionTool;\n+import org.apache.phoenix.util.PropertiesUtil;\n+import org.apache.phoenix.util.ReadOnlyProps;\n+import org.apache.phoenix.util.SchemaUtil;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.util.Collections;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.phoenix.util.TestUtil.TEST_PROPERTIES;\n+\n+public class SchemaExtractionToolIT extends BaseTest {\n+\n+    @BeforeClass\n+    public static void setup() throws Exception {\n+        Map<String, String> props = Collections.emptyMap();\n+        setUpTestDriver(new ReadOnlyProps(props.entrySet().iterator()));\n+    }\n+\n+    @Test\n+    public void testCreateTableStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            conn.commit();\n+            String [] args = {\"-tb\", tableName, \"-s\", schemaName};\n+\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+            set.run(args);\n+            String actualProperties = set.output.substring(set.output.lastIndexOf(\")\")+1).replace(\" \",\"\");\n+            Assert.assertEquals(5, actualProperties.split(\",\").length);\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateIndexStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String indexName = generateUniqueName();\n+        String indexName1 = generateUniqueName();\n+        String indexName2 = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+\n+            String createIndexStatement = \"CREATE INDEX \"+indexName + \" ON \"+pTableFullName+\"(v1 DESC) INCLUDE (v2)\";\n+\n+            String createIndexStatement1 = \"CREATE INDEX \"+indexName1 + \" ON \"+pTableFullName+\"(v2 DESC) INCLUDE (v1)\";\n+\n+            String createIndexStatement2 = \"CREATE INDEX \"+indexName2 + \" ON \"+pTableFullName+\"(k)\";\n+\n+            conn.createStatement().execute(createIndexStatement);\n+            conn.createStatement().execute(createIndexStatement1);\n+            conn.createStatement().execute(createIndexStatement2);\n+            conn.commit();\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+\n+            String [] args = {\"-tb\", indexName, \"-s\", schemaName};\n+            set.run(args);\n+            Assert.assertEquals(createIndexStatement.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args1 = {\"-tb\", indexName1, \"-s\", schemaName};\n+            set.run(args1);\n+            Assert.assertEquals(createIndexStatement1.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args2 = {\"-tb\", indexName2, \"-s\", schemaName};\n+            set.run(args2);\n+            Assert.assertEquals(createIndexStatement2.toUpperCase(), set.output.toUpperCase());\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateViewStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String viewName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k BIGINT NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            String viewFullName = SchemaUtil.getQualifiedTableName(schemaName, viewName);\n+            String viewFullName1 = SchemaUtil.getQualifiedTableName(schemaName, viewName+\"1\");\n+\n+\n+            String createView = \"CREATE VIEW \"+viewFullName + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;\n+            String createView1 = \"CREATE VIEW \"+viewFullName1 + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;", "originalCommit": "744025c30ab6f31038b17e28f3c2bab2695f3374", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNDE1NA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456134154", "bodyText": "nit: long line", "author": "gjacoby126", "createdAt": "2020-07-16T23:30:39Z", "path": "phoenix-core/src/it/java/org/apache/phoenix/end2end/SchemaExtractionToolIT.java", "diffHunk": "@@ -0,0 +1,164 @@\n+package org.apache.phoenix.end2end;\n+\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.query.BaseTest;\n+import org.apache.phoenix.schema.SchemaExtractionTool;\n+import org.apache.phoenix.util.PropertiesUtil;\n+import org.apache.phoenix.util.ReadOnlyProps;\n+import org.apache.phoenix.util.SchemaUtil;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.util.Collections;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.phoenix.util.TestUtil.TEST_PROPERTIES;\n+\n+public class SchemaExtractionToolIT extends BaseTest {\n+\n+    @BeforeClass\n+    public static void setup() throws Exception {\n+        Map<String, String> props = Collections.emptyMap();\n+        setUpTestDriver(new ReadOnlyProps(props.entrySet().iterator()));\n+    }\n+\n+    @Test\n+    public void testCreateTableStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            conn.commit();\n+            String [] args = {\"-tb\", tableName, \"-s\", schemaName};\n+\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+            set.run(args);\n+            String actualProperties = set.output.substring(set.output.lastIndexOf(\")\")+1).replace(\" \",\"\");\n+            Assert.assertEquals(5, actualProperties.split(\",\").length);\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateIndexStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String indexName = generateUniqueName();\n+        String indexName1 = generateUniqueName();\n+        String indexName2 = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k VARCHAR NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+\n+            String createIndexStatement = \"CREATE INDEX \"+indexName + \" ON \"+pTableFullName+\"(v1 DESC) INCLUDE (v2)\";\n+\n+            String createIndexStatement1 = \"CREATE INDEX \"+indexName1 + \" ON \"+pTableFullName+\"(v2 DESC) INCLUDE (v1)\";\n+\n+            String createIndexStatement2 = \"CREATE INDEX \"+indexName2 + \" ON \"+pTableFullName+\"(k)\";\n+\n+            conn.createStatement().execute(createIndexStatement);\n+            conn.createStatement().execute(createIndexStatement1);\n+            conn.createStatement().execute(createIndexStatement2);\n+            conn.commit();\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+\n+            String [] args = {\"-tb\", indexName, \"-s\", schemaName};\n+            set.run(args);\n+            Assert.assertEquals(createIndexStatement.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args1 = {\"-tb\", indexName1, \"-s\", schemaName};\n+            set.run(args1);\n+            Assert.assertEquals(createIndexStatement1.toUpperCase(), set.output.toUpperCase());\n+\n+            String [] args2 = {\"-tb\", indexName2, \"-s\", schemaName};\n+            set.run(args2);\n+            Assert.assertEquals(createIndexStatement2.toUpperCase(), set.output.toUpperCase());\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateViewStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String viewName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k BIGINT NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            String viewFullName = SchemaUtil.getQualifiedTableName(schemaName, viewName);\n+            String viewFullName1 = SchemaUtil.getQualifiedTableName(schemaName, viewName+\"1\");\n+\n+\n+            String createView = \"CREATE VIEW \"+viewFullName + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;\n+            String createView1 = \"CREATE VIEW \"+viewFullName1 + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;\n+\n+            conn.createStatement().execute(createView);\n+            conn.createStatement().execute(createView1);\n+            conn.commit();\n+            String [] args = {\"-tb\", viewName, \"-s\", schemaName};\n+\n+            SchemaExtractionTool set = new SchemaExtractionTool();\n+            set.setConf(conn.unwrap(PhoenixConnection.class).getQueryServices().getConfiguration());\n+            set.run(args);\n+            Assert.assertEquals(createView.toUpperCase(), set.output.toUpperCase());\n+        }\n+    }\n+\n+    @Test\n+    public void testCreateViewIndexStatement() throws Exception {\n+        String tableName = generateUniqueName();\n+        String schemaName = generateUniqueName();\n+        String viewName = generateUniqueName();\n+        String childView = generateUniqueName();\n+        String indexName = generateUniqueName();\n+        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n+        String properties = \"TTL=2592000,IMMUTABLE_ROWS=true,DISABLE_MIGRATION=true,DISABLE_SOR=true,DISABLE_WAL=true\";\n+\n+        try (Connection conn = DriverManager.getConnection(getUrl(), props)) {\n+\n+            String pTableFullName = SchemaUtil.getQualifiedTableName(schemaName, tableName);\n+            conn.createStatement().execute(\"CREATE TABLE \"+pTableFullName + \"(k BIGINT NOT NULL PRIMARY KEY, v1 VARCHAR, v2 VARCHAR)\"\n+                    + properties);\n+            String viewFullName = SchemaUtil.getQualifiedTableName(schemaName, viewName);\n+            String childviewName = SchemaUtil.getQualifiedTableName(schemaName, childView);\n+\n+            String createView = \"CREATE VIEW \"+viewFullName + \"(id1 BIGINT, id2 BIGINT NOT NULL, id3 VARCHAR NOT NULL CONSTRAINT PKVIEW PRIMARY KEY (id2, id3 DESC)) AS SELECT * FROM \"+pTableFullName;", "originalCommit": "744025c30ab6f31038b17e28f3c2bab2695f3374", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNTI0Nw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456135247", "bodyText": "Maybe a more precise name here? If these are table properties, then getPropertyValues() and getDefaultPropertyValues()?", "author": "gjacoby126", "createdAt": "2020-07-16T23:33:45Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java", "diffHunk": "@@ -800,6 +801,8 @@ private static int getReservedQualifier(byte[] bytes, int offset, int length) {\n     Boolean useStatsForParallelization();\n     boolean hasViewModifiedUpdateCacheFrequency();\n     boolean hasViewModifiedUseStatsForParallelization();\n+    Map<String, String> getValues();", "originalCommit": "744025c30ab6f31038b17e28f3c2bab2695f3374", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNzM1MQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458337351", "bodyText": "This was used after HTableDescriptor#getValues() but I renamed it for PTable per your suggestion", "author": "swaroopak", "createdAt": "2020-07-21T19:29:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNTI0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNjA2Mw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456136063", "bodyText": "What if a table has more than one column family?", "author": "gjacoby126", "createdAt": "2020-07-16T23:36:27Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,488 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+    public static Configuration conf;\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+    public String output;\n+\n+    @Override\n+    public int run(String[] args) throws Exception {\n+        populateToolAttributes(args);\n+        conf = HBaseConfiguration.addHbaseResources(getConf());\n+        PTable table = getPTable(pSchemaName, pTableName);\n+        output = getDDL(table);\n+        return 0;\n+    }\n+\n+    private String getDDL(PTable table) throws Exception {\n+        String ddl = null;\n+        if(table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(table);\n+        } else if(table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(table);\n+        } else if(table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pSchemaName = indexPTable.getSchemaName().getString();\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);", "originalCommit": "744025c30ab6f31038b17e28f3c2bab2695f3374", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNzEwMQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456137101", "bodyText": "(If you want to make supporting multiple column families a separate subtask JIRA and not block this PR that's fine, but I think it would be part of the requirement to merge the feature branch into a release branch.)", "author": "gjacoby126", "createdAt": "2020-07-16T23:40:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNjA2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MTAwMg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458341002", "bodyText": "It would come as a Test and handle this particular case.", "author": "swaroopak", "createdAt": "2020-07-21T19:37:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzNjA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzODQyMQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456138421", "bodyText": "As this class evolves, and before merging it to a release branch, let's split out the business logic from the \"tool\" parts that handle command-line parsing. Having an object API will make unit testing (and future reuse) easier.\nThe example to avoid here is IndexTool, which mixes in command-line parsing, MapReduce configuration, and some business logic all together.", "author": "gjacoby126", "createdAt": "2020-07-16T23:44:17Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,488 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {", "originalCommit": "e2382e0aae613357b68db6e6aea710e637b340b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0MjQ2MQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456142461", "bodyText": "It's also just useful to have a class that takes in a PTable and spits out SQL.", "author": "gjacoby126", "createdAt": "2020-07-16T23:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEzODQyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0Mjc1Mg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456142752", "bodyText": "How do we make sure these are kept up to date?", "author": "gjacoby126", "createdAt": "2020-07-16T23:58:18Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,488 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+    public static Configuration conf;\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+    public String output;\n+\n+    @Override\n+    public int run(String[] args) throws Exception {\n+        populateToolAttributes(args);\n+        conf = HBaseConfiguration.addHbaseResources(getConf());\n+        PTable table = getPTable(pSchemaName, pTableName);\n+        output = getDDL(table);\n+        return 0;\n+    }\n+\n+    private String getDDL(PTable table) throws Exception {\n+        String ddl = null;\n+        if(table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(table);\n+        } else if(table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(table);\n+        } else if(table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pSchemaName = indexPTable.getSchemaName().getString();\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    // test with different default CF, key is a included column\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString, String coveredColumnString, boolean local, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX, local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {\n+        try (Connection conn = getConnection()) {\n+            return PhoenixRuntime.getTable(conn, pTableFullName);\n+        }\n+    }\n+\n+    protected String extractCreateViewDDL(PTable table) throws SQLException {\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+        String baseTableName = table.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, baseTableName);\n+        PTable baseTable = getPTable(baseTableFullName);\n+        String columnInfoString = getColumnInfoStringForView(table, baseTable);\n+\n+        String whereClause = table.getViewStatement();\n+        if(whereClause != null) {\n+            whereClause = whereClause.substring(whereClause.indexOf(\"WHERE\"));\n+        }\n+        return generateCreateViewDDL(columnInfoString, baseTableFullName, whereClause == null ? \"\" : \" \"+whereClause, pSchemaName, pTableName);\n+    }\n+\n+    private String generateCreateViewDDL(String columnInfoString, String baseTableFullName, String whereClause, String pSchemaName, String pTableName) {\n+        String viewFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_VIEW, viewFullName, columnInfoString, baseTableFullName, whereClause));\n+        return outputBuilder.toString();\n+    }\n+\n+    public String extractCreateTableDDL(PTable table) throws IOException, SQLException {\n+\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+\n+        ConnectionQueryServices cqsi = getCQSIObject();\n+        HTableDescriptor htd = getHTableDescriptor(cqsi, table);\n+        HColumnDescriptor hcd = htd.getFamily(SchemaUtil.getEmptyColumnFamily(table));\n+\n+        populateDefaultProperties(table);\n+        setPTableProperties(table);\n+        setHTableProperties(htd);\n+        setHColumnFamilyProperties(hcd);\n+\n+        String columnInfoString = getColumnInfoStringForTable(table);\n+        String propertiesString = convertPropertiesToString();\n+\n+        return generateTableDDLString(columnInfoString, propertiesString, pSchemaName, pTableName);\n+    }\n+    private String generateTableDDLString(String columnInfoString,String propertiesString,String pSchemaName,String pTableName) {\n+        String pTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_TABLE, pTableFullName));\n+        outputBuilder.append(columnInfoString).append(\" \").append(propertiesString);\n+        return outputBuilder.toString();\n+    }\n+\n+    private void populateDefaultProperties(PTable table) {\n+        Map<String, String> propsMap = HColumnDescriptor.getDefaultValues();\n+        for (Map.Entry<String, String> entry : propsMap.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            defaultProps.put(key, value);\n+            if(key.equalsIgnoreCase(BLOOMFILTER) || key.equalsIgnoreCase(COMPRESSION)) {", "originalCommit": "e2382e0aae613357b68db6e6aea710e637b340b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0MjkxOA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456142918", "bodyText": "And what about if they're different for different major or minor HBase versions?", "author": "gjacoby126", "createdAt": "2020-07-16T23:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0Mjc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MDUzNw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458340537", "bodyText": "This is just to override the values that are specifically defaulted in Phoenix. Otherwise, we use HColumnDescriptor.getDefaultValues() to get all version-specific default values for hbase.", "author": "swaroopak", "createdAt": "2020-07-21T19:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0Mjc1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0MzIzMA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r456143230", "bodyText": "Easier for forward porting to call this getTableDescriptor and have it return a TableDescriptor, since HTableDescriptor is deprecated in 2.x", "author": "gjacoby126", "createdAt": "2020-07-16T23:59:55Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,488 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.logging.Logger;\n+\n+import static org.apache.hadoop.hbase.HColumnDescriptor.BLOOMFILTER;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.COMPRESSION;\n+import static org.apache.hadoop.hbase.HColumnDescriptor.DATA_BLOCK_ENCODING;\n+import static org.apache.hadoop.hbase.HTableDescriptor.IS_META;\n+import static org.apache.phoenix.util.MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME;\n+import static org.apache.phoenix.util.SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+    public static Configuration conf;\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+    public String output;\n+\n+    @Override\n+    public int run(String[] args) throws Exception {\n+        populateToolAttributes(args);\n+        conf = HBaseConfiguration.addHbaseResources(getConf());\n+        PTable table = getPTable(pSchemaName, pTableName);\n+        output = getDDL(table);\n+        return 0;\n+    }\n+\n+    private String getDDL(PTable table) throws Exception {\n+        String ddl = null;\n+        if(table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(table);\n+        } else if(table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(table);\n+        } else if(table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pSchemaName = indexPTable.getSchemaName().getString();\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    // test with different default CF, key is a included column\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString, String coveredColumnString, boolean local, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX, local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {\n+        try (Connection conn = getConnection()) {\n+            return PhoenixRuntime.getTable(conn, pTableFullName);\n+        }\n+    }\n+\n+    protected String extractCreateViewDDL(PTable table) throws SQLException {\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+        String baseTableName = table.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, baseTableName);\n+        PTable baseTable = getPTable(baseTableFullName);\n+        String columnInfoString = getColumnInfoStringForView(table, baseTable);\n+\n+        String whereClause = table.getViewStatement();\n+        if(whereClause != null) {\n+            whereClause = whereClause.substring(whereClause.indexOf(\"WHERE\"));\n+        }\n+        return generateCreateViewDDL(columnInfoString, baseTableFullName, whereClause == null ? \"\" : \" \"+whereClause, pSchemaName, pTableName);\n+    }\n+\n+    private String generateCreateViewDDL(String columnInfoString, String baseTableFullName, String whereClause, String pSchemaName, String pTableName) {\n+        String viewFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_VIEW, viewFullName, columnInfoString, baseTableFullName, whereClause));\n+        return outputBuilder.toString();\n+    }\n+\n+    public String extractCreateTableDDL(PTable table) throws IOException, SQLException {\n+\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+\n+        ConnectionQueryServices cqsi = getCQSIObject();\n+        HTableDescriptor htd = getHTableDescriptor(cqsi, table);\n+        HColumnDescriptor hcd = htd.getFamily(SchemaUtil.getEmptyColumnFamily(table));\n+\n+        populateDefaultProperties(table);\n+        setPTableProperties(table);\n+        setHTableProperties(htd);\n+        setHColumnFamilyProperties(hcd);\n+\n+        String columnInfoString = getColumnInfoStringForTable(table);\n+        String propertiesString = convertPropertiesToString();\n+\n+        return generateTableDDLString(columnInfoString, propertiesString, pSchemaName, pTableName);\n+    }\n+    private String generateTableDDLString(String columnInfoString,String propertiesString,String pSchemaName,String pTableName) {\n+        String pTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_TABLE, pTableFullName));\n+        outputBuilder.append(columnInfoString).append(\" \").append(propertiesString);\n+        return outputBuilder.toString();\n+    }\n+\n+    private void populateDefaultProperties(PTable table) {\n+        Map<String, String> propsMap = HColumnDescriptor.getDefaultValues();\n+        for (Map.Entry<String, String> entry : propsMap.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            defaultProps.put(key, value);\n+            if(key.equalsIgnoreCase(BLOOMFILTER) || key.equalsIgnoreCase(COMPRESSION)) {\n+                defaultProps.put(key, \"NONE\");\n+            }\n+            if(key.equalsIgnoreCase(DATA_BLOCK_ENCODING)) {\n+                defaultProps.put(key, String.valueOf(DEFAULT_DATA_BLOCK_ENCODING));\n+            }\n+        }\n+        defaultProps.putAll(table.getDefaultValues());\n+    }\n+\n+    private void setHTableProperties(HTableDescriptor htd) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = htd.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            if(Bytes.toString(key.get()).contains(\"coprocessor\") || Bytes.toString(key.get()).contains(IS_META)) {\n+                continue;\n+            }\n+            defaultProps.put(Bytes.toString(key.get()), \"false\");\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));\n+        }\n+    }\n+\n+    private void setHColumnFamilyProperties(HColumnDescriptor columnDescriptor) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = columnDescriptor.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));\n+        }\n+    }\n+\n+    private void setPTableProperties(PTable table) {\n+        Map <String, String> map = table.getValues();\n+        for(Map.Entry<String, String> entry : map.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            if(value != null) {\n+                definedProps.put(key, value);\n+            }\n+        }\n+    }\n+\n+    private HTableDescriptor getHTableDescriptor(ConnectionQueryServices cqsi, PTable table)", "originalCommit": "e2382e0aae613357b68db6e6aea710e637b340b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzODgyNw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458338827", "bodyText": "That would be a merge conflict change I believe. Otherwise code won't compile on 1.x", "author": "swaroopak", "createdAt": "2020-07-21T19:32:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0MzIzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NDYwNQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458364605", "bodyText": "The least I could do is change the function name as you suggested.", "author": "swaroopak", "createdAt": "2020-07-21T20:22:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0MzIzMA=="}], "type": "inlineReview"}, {"oid": "f4c717ea07ca7271829c6bcab20678499fdd1d53", "url": "https://github.com/apache/phoenix/commit/f4c717ea07ca7271829c6bcab20678499fdd1d53", "message": "Fixing review comments 2", "committedDate": "2020-07-21T23:32:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk0Nzk2Ng==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r458947966", "bodyText": "\"values\" seem like a bad name for maps that has both keys and values - how about \"properties\"? Looks like that's what are being put there.", "author": "priyankporwal", "createdAt": "2020-07-22T17:05:54Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java", "diffHunk": "@@ -230,6 +254,7 @@\n         private Boolean useStatsForParallelization;\n         private long viewTTL;\n         private long viewTTLHighWaterMark;\n+        private Map<String, String> values = new HashMap<>();", "originalCommit": "f4c717ea07ca7271829c6bcab20678499fdd1d53", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "455795967624c46fc9f174691b717ba7254a1a4e", "url": "https://github.com/apache/phoenix/commit/455795967624c46fc9f174691b717ba7254a1a4e", "message": "Fixing review comments 4", "committedDate": "2020-07-22T18:41:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTAzNjY5Mw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461036693", "bodyText": "output should have a getter since you're accessing it lots of times from tests and I assume it's read-only externally.", "author": "gjacoby126", "createdAt": "2020-07-27T17:02:29Z", "path": "phoenix-tools/src/main/java/org/apache/phoenix/schema/SchemaExtractionTool.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package org.apache.phoenix.schema;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import java.util.logging.Logger;\n+\n+public class SchemaExtractionTool extends Configured implements Tool {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SchemaExtractionTool.class.getName());\n+    private static final Option HELP_OPTION = new Option(\"h\", \"help\",\n+            false, \"Help\");\n+    private static final Option TABLE_OPTION = new Option(\"tb\", \"table\", true,\n+            \"[Required] Table name ex. table1\");\n+    private static final Option SCHEMA_OPTION = new Option(\"s\", \"schema\", true,\n+            \"[Optional] Schema name ex. schema\");\n+\n+    private String pTableName;\n+    private String pSchemaName;\n+\n+    public static Configuration conf;\n+    public String output;", "originalCommit": "455795967624c46fc9f174691b717ba7254a1a4e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA0MDAxNg==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461040016", "bodyText": "This will be incorrect for multiple column families. (e.g two CFs with different values for the same property will overwrite each other)", "author": "gjacoby126", "createdAt": "2020-07-27T17:07:50Z", "path": "phoenix-tools/src/main/java/org/apache/phoenix/schema/SchemaExtractionProcessor.java", "diffHunk": "@@ -0,0 +1,410 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.MetaDataUtil;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class SchemaExtractionProcessor {\n+\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+\n+    private PTable table;\n+    private Configuration conf;\n+    public SchemaExtractionProcessor(Configuration conf, String pSchemaName, String pTableName)\n+            throws SQLException {\n+        this.conf = conf;\n+        this.table = getPTable(pSchemaName, pTableName);\n+    }\n+\n+    public String process() throws Exception {\n+        String ddl = null;\n+        if(this.table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(this.table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil\n+                .getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    //TODO: test with different CF\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString,\n+            String coveredColumnString, boolean local, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX,\n+                local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {\n+        try (Connection conn = getConnection()) {\n+            return PhoenixRuntime.getTable(conn, pTableFullName);\n+        }\n+    }\n+\n+    protected String extractCreateViewDDL(PTable table) throws SQLException {\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+        String baseTableName = table.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, baseTableName);\n+        PTable baseTable = getPTable(baseTableFullName);\n+        String columnInfoString = getColumnInfoStringForView(table, baseTable);\n+\n+        String whereClause = table.getViewStatement();\n+        if(whereClause != null) {\n+            whereClause = whereClause.substring(whereClause.indexOf(\"WHERE\"));\n+        }\n+        return generateCreateViewDDL(columnInfoString, baseTableFullName,\n+                whereClause == null ? \"\" : \" \"+whereClause, pSchemaName, pTableName);\n+    }\n+\n+    private String generateCreateViewDDL(String columnInfoString, String baseTableFullName,\n+            String whereClause, String pSchemaName, String pTableName) {\n+        String viewFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_VIEW, viewFullName,\n+                columnInfoString, baseTableFullName, whereClause));\n+        return outputBuilder.toString();\n+    }\n+\n+    public String extractCreateTableDDL(PTable table) throws IOException, SQLException {\n+\n+        String pSchemaName = table.getSchemaName().getString();\n+        String pTableName = table.getTableName().getString();\n+\n+        ConnectionQueryServices cqsi = getCQSIObject();\n+        HTableDescriptor htd = getTableDescriptor(cqsi, table);\n+        HColumnDescriptor hcd = htd.getFamily(SchemaUtil.getEmptyColumnFamily(table));\n+\n+        populateDefaultProperties(table);\n+        setPTableProperties(table);\n+        setHTableProperties(htd);\n+        setHColumnFamilyProperties(hcd);\n+\n+        String columnInfoString = getColumnInfoStringForTable(table);\n+        String propertiesString = convertPropertiesToString();\n+\n+        return generateTableDDLString(columnInfoString, propertiesString, pSchemaName, pTableName);\n+    }\n+    private String generateTableDDLString(String columnInfoString, String propertiesString,\n+            String pSchemaName, String pTableName) {\n+        String pTableFullName = SchemaUtil.getQualifiedTableName(pSchemaName, pTableName);\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_TABLE, pTableFullName));\n+        outputBuilder.append(columnInfoString).append(\" \").append(propertiesString);\n+        return outputBuilder.toString();\n+    }\n+\n+    private void populateDefaultProperties(PTable table) {\n+        Map<String, String> propsMap = HColumnDescriptor.getDefaultValues();\n+        for (Map.Entry<String, String> entry : propsMap.entrySet()) {\n+            String key = entry.getKey();\n+            String value = entry.getValue();\n+            defaultProps.put(key, value);\n+            if(key.equalsIgnoreCase(HColumnDescriptor.BLOOMFILTER) || key.equalsIgnoreCase(\n+                    HColumnDescriptor.COMPRESSION)) {\n+                defaultProps.put(key, \"NONE\");\n+            }\n+            if(key.equalsIgnoreCase(HColumnDescriptor.DATA_BLOCK_ENCODING)) {\n+                defaultProps.put(key, String.valueOf(SchemaUtil.DEFAULT_DATA_BLOCK_ENCODING));\n+            }\n+        }\n+        defaultProps.putAll(table.getDefaultPropertyValues());\n+    }\n+\n+    private void setHTableProperties(HTableDescriptor htd) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = htd.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            if(Bytes.toString(key.get()).contains(\"coprocessor\") || Bytes.toString(key.get()).contains(\n+                    HTableDescriptor.IS_META)) {\n+                continue;\n+            }\n+            defaultProps.put(Bytes.toString(key.get()), \"false\");\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));\n+        }\n+    }\n+\n+    private void setHColumnFamilyProperties(HColumnDescriptor columnDescriptor) {\n+        Map<ImmutableBytesWritable, ImmutableBytesWritable> propsMap = columnDescriptor.getValues();\n+        for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry : propsMap.entrySet()) {\n+            ImmutableBytesWritable key = entry.getKey();\n+            ImmutableBytesWritable value = entry.getValue();\n+            definedProps.put(Bytes.toString(key.get()), Bytes.toString(value.get()));", "originalCommit": "455795967624c46fc9f174691b717ba7254a1a4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA1NTM4NA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461055384", "bodyText": "Would like to get it fixed in subsequent commits.", "author": "swaroopak", "createdAt": "2020-07-27T17:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA0MDAxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE0MDc1Ng==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461140756", "bodyText": "Added a message to throw Exception with \"Not Supported\" message", "author": "swaroopak", "createdAt": "2020-07-27T20:10:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA0MDAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA0MDQ0NQ==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461040445", "bodyText": "Since the object's immutable, any reason not to cache the generated DDL and return the cached copy if process() is called more than once?", "author": "gjacoby126", "createdAt": "2020-07-27T17:08:41Z", "path": "phoenix-tools/src/main/java/org/apache/phoenix/schema/SchemaExtractionProcessor.java", "diffHunk": "@@ -0,0 +1,410 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.MetaDataUtil;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class SchemaExtractionProcessor {\n+\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+\n+    private PTable table;\n+    private Configuration conf;\n+    public SchemaExtractionProcessor(Configuration conf, String pSchemaName, String pTableName)\n+            throws SQLException {\n+        this.conf = conf;\n+        this.table = getPTable(pSchemaName, pTableName);\n+    }\n+\n+    public String process() throws Exception {\n+        String ddl = null;\n+        if(this.table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(this.table);\n+        }\n+        return ddl;", "originalCommit": "455795967624c46fc9f174691b717ba7254a1a4e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA1MzA2NA==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461053064", "bodyText": "This won't work for tenant-owned schema objects -- if you're not going to tackle this now please file a subtask JIRA to do so before we merge into a real branch", "author": "gjacoby126", "createdAt": "2020-07-27T17:30:00Z", "path": "phoenix-tools/src/main/java/org/apache/phoenix/schema/SchemaExtractionProcessor.java", "diffHunk": "@@ -0,0 +1,410 @@\n+package org.apache.phoenix.schema;\n+\n+import com.google.common.collect.Sets;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.phoenix.jdbc.PhoenixConnection;\n+import org.apache.phoenix.mapreduce.util.ConnectionUtil;\n+import org.apache.phoenix.query.ConnectionQueryServices;\n+import org.apache.phoenix.util.MetaDataUtil;\n+import org.apache.phoenix.util.PhoenixRuntime;\n+import org.apache.phoenix.util.SchemaUtil;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class SchemaExtractionProcessor {\n+\n+    Map<String, String> defaultProps = new HashMap<>();\n+    Map<String, String> definedProps = new HashMap<>();\n+\n+    private static final String CREATE_TABLE = \"CREATE TABLE %s\";\n+    private static final String CREATE_INDEX = \"CREATE %sINDEX %s ON %s\";\n+    private static final String CREATE_VIEW = \"CREATE VIEW %s%s AS SELECT * FROM %s%s\";\n+\n+    private PTable table;\n+    private Configuration conf;\n+    public SchemaExtractionProcessor(Configuration conf, String pSchemaName, String pTableName)\n+            throws SQLException {\n+        this.conf = conf;\n+        this.table = getPTable(pSchemaName, pTableName);\n+    }\n+\n+    public String process() throws Exception {\n+        String ddl = null;\n+        if(this.table.getType().equals(PTableType.TABLE)) {\n+            ddl = extractCreateTableDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.INDEX)) {\n+            ddl = extractCreateIndexDDL(this.table);\n+        } else if(this.table.getType().equals(PTableType.VIEW)) {\n+            ddl = extractCreateViewDDL(this.table);\n+        }\n+        return ddl;\n+    }\n+\n+    protected String extractCreateIndexDDL(PTable indexPTable)\n+            throws SQLException {\n+        String pTableName = indexPTable.getTableName().getString();\n+\n+        String baseTableName = indexPTable.getParentTableName().getString();\n+        String baseTableFullName = SchemaUtil\n+                .getQualifiedTableName(indexPTable.getSchemaName().getString(), baseTableName);\n+        PTable dataPTable = getPTable(baseTableFullName);\n+\n+        String defaultCF = SchemaUtil.getEmptyColumnFamilyAsString(indexPTable);\n+        String indexedColumnsString = getIndexedColumnsString(indexPTable, dataPTable, defaultCF);\n+        String coveredColumnsString = getCoveredColumnsString(indexPTable, defaultCF);\n+\n+        return generateIndexDDLString(baseTableFullName, indexedColumnsString, coveredColumnsString,\n+                indexPTable.getIndexType().equals(PTable.IndexType.LOCAL), pTableName);\n+    }\n+\n+    //TODO: Indexed on an expression\n+    //TODO: test with different CF\n+    private String getIndexedColumnsString(PTable indexPTable, PTable dataPTable, String defaultCF) {\n+\n+        List<PColumn> indexPK = indexPTable.getPKColumns();\n+        List<PColumn> dataPK = dataPTable.getPKColumns();\n+        Set<String> indexPkSet = new HashSet<>();\n+        Set<String> dataPkSet = new HashSet<>();\n+        Map<String, SortOrder> sortOrderMap = new HashMap<>();\n+        StringBuilder indexedColumnsBuilder = new StringBuilder();\n+        for (PColumn indexedColumn : indexPK) {\n+            String indexColumn = extractIndexColumn(indexedColumn.getName().getString(), defaultCF);\n+            if(indexColumn.equalsIgnoreCase(MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME)) {\n+                continue;\n+            }\n+            indexPkSet.add(indexColumn);\n+            sortOrderMap.put(indexColumn, indexedColumn.getSortOrder());\n+        }\n+\n+        for(PColumn pColumn : dataPK) {\n+            dataPkSet.add(pColumn.getName().getString());\n+        }\n+\n+        Set<String> effectivePK = Sets.symmetricDifference(indexPkSet, dataPkSet);\n+        if (effectivePK.isEmpty()) {\n+            effectivePK = indexPkSet;\n+        }\n+        for (String column : effectivePK) {\n+            if(indexedColumnsBuilder.length()!=0) {\n+                indexedColumnsBuilder.append(\", \");\n+            }\n+            indexedColumnsBuilder.append(column);\n+            if(sortOrderMap.get(column)!= SortOrder.getDefault()) {\n+                indexedColumnsBuilder.append(\" \");\n+                indexedColumnsBuilder.append(sortOrderMap.get(column));\n+            }\n+        }\n+        return indexedColumnsBuilder.toString();\n+    }\n+\n+    private String extractIndexColumn(String columnName, String defaultCF) {\n+        String [] columnNameSplit = columnName.split(\":\");\n+        if(columnNameSplit[0].equals(\"\") || columnNameSplit[0].equalsIgnoreCase(defaultCF)) {\n+            return columnNameSplit[1];\n+        } else {\n+            return columnName.replace(\":\", \".\");\n+        }\n+    }\n+\n+    private String getCoveredColumnsString(PTable indexPTable, String defaultCF) {\n+        StringBuilder coveredColumnsBuilder = new StringBuilder();\n+        List<PColumn> pkColumns = indexPTable.getColumns();\n+        for (PColumn cc : pkColumns) {\n+            if(coveredColumnsBuilder.length()!=0) {\n+                coveredColumnsBuilder.append(\", \");\n+            }\n+            if(cc.getFamilyName()!=null) {\n+                String indexColumn = extractIndexColumn(cc.getName().getString(), defaultCF);\n+                coveredColumnsBuilder.append(indexColumn);\n+            }\n+        }\n+        return coveredColumnsBuilder.toString();\n+    }\n+\n+    protected String generateIndexDDLString(String baseTableFullName, String indexedColumnString,\n+            String coveredColumnString, boolean local, String pTableName) {\n+        StringBuilder outputBuilder = new StringBuilder(String.format(CREATE_INDEX,\n+                local ? \"LOCAL \" : \"\", pTableName, baseTableFullName));\n+        outputBuilder.append(\"(\");\n+        outputBuilder.append(indexedColumnString);\n+        outputBuilder.append(\")\");\n+        if(!coveredColumnString.equals(\"\")) {\n+            outputBuilder.append(\" INCLUDE (\");\n+            outputBuilder.append(coveredColumnString);\n+            outputBuilder.append(\")\");\n+        }\n+        return outputBuilder.toString();\n+    }\n+\n+    PTable getPTable(String pTableFullName) throws SQLException {", "originalCommit": "455795967624c46fc9f174691b717ba7254a1a4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA1NTYyNw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461055627", "bodyText": "Would like to get it fixed in subsequent commits. Yes, Let me file one.", "author": "swaroopak", "createdAt": "2020-07-27T17:34:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA1MzA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE0MDM2Nw==", "url": "https://github.com/apache/phoenix/pull/806#discussion_r461140367", "bodyText": "I fixed it in the same commit.", "author": "swaroopak", "createdAt": "2020-07-27T20:10:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA1MzA2NA=="}], "type": "inlineReview"}, {"oid": "a889a7415f49520b4f584f8c8c52e5854c68b06f", "url": "https://github.com/apache/phoenix/commit/a889a7415f49520b4f584f8c8c52e5854c68b06f", "message": "Fixing review comments", "committedDate": "2020-07-27T17:48:11Z", "type": "commit"}, {"oid": "3b548b0512a72f77b329c4e04920f205df55d14b", "url": "https://github.com/apache/phoenix/commit/3b548b0512a72f77b329c4e04920f205df55d14b", "message": "Added appropriate message and support for tenant schema object", "committedDate": "2020-07-27T20:09:16Z", "type": "commit"}]}