{"pr_number": 758, "pr_title": "PHOENIX-5804: Implement strong verification with -v ONLY option for o\u2026", "pr_createdAt": "2020-04-10T00:27:23Z", "pr_url": "https://github.com/apache/phoenix/pull/758", "timeline": [{"oid": "b86e81ad219b2e7ab6695772f1b5d35e834f3896", "url": "https://github.com/apache/phoenix/commit/b86e81ad219b2e7ab6695772f1b5d35e834f3896", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-10T00:33:56Z", "type": "forcePushed"}, {"oid": "ad4c7b69a236943ccce858d37aedc33d978be9dc", "url": "https://github.com/apache/phoenix/commit/ad4c7b69a236943ccce858d37aedc33d978be9dc", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-10T16:54:28Z", "type": "forcePushed"}, {"oid": "8d57fdfb5738548012cee7120ac1c5f0192ffcbf", "url": "https://github.com/apache/phoenix/commit/8d57fdfb5738548012cee7120ac1c5f0192ffcbf", "message": "PHOENIX-5825 Remove PhoenixCanaryTool and CanaryTestResult from phoenix repo\n\nalso\n* remove related tests\n* remove argparse4j dependency", "committedDate": "2020-04-09T06:37:38Z", "type": "forcePushed"}, {"oid": "cb9e3d24fb72a183227e1c80bfb05e1187a8e239", "url": "https://github.com/apache/phoenix/commit/cb9e3d24fb72a183227e1c80bfb05e1187a8e239", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-13T22:30:41Z", "type": "forcePushed"}, {"oid": "d350ce7d972ae02512f79bb02e43649f46a20279", "url": "https://github.com/apache/phoenix/commit/d350ce7d972ae02512f79bb02e43649f46a20279", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-14T00:41:45Z", "type": "forcePushed"}, {"oid": "9ecb60dcdde6b3c64b9535c90961cca210a4f6c3", "url": "https://github.com/apache/phoenix/commit/9ecb60dcdde6b3c64b9535c90961cca210a4f6c3", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-14T00:55:16Z", "type": "forcePushed"}, {"oid": "e3493ebbb183ec9aba069783b77f14b6957f2d18", "url": "https://github.com/apache/phoenix/commit/e3493ebbb183ec9aba069783b77f14b6957f2d18", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-14T00:58:30Z", "type": "forcePushed"}, {"oid": "09199059a57e2b13f886e25b309ff252379e6952", "url": "https://github.com/apache/phoenix/commit/09199059a57e2b13f886e25b309ff252379e6952", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-14T19:43:57Z", "type": "forcePushed"}, {"oid": "4951c12e335664421ba458d04bbda526faffcf10", "url": "https://github.com/apache/phoenix/commit/4951c12e335664421ba458d04bbda526faffcf10", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-14T22:42:25Z", "type": "forcePushed"}, {"oid": "553a812965c93f85ead8bd638efc568aa19e8c6e", "url": "https://github.com/apache/phoenix/commit/553a812965c93f85ead8bd638efc568aa19e8c6e", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-16T00:45:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczMTA3Ng==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409731076", "bodyText": "Since the scan we use for this scanner is not a raw scan, we should not get any delete cell. We should trow an exception if the type of the cell is not put.", "author": "kadirozde", "createdAt": "2020-04-16T17:33:52Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {\n+        long ts = 0;\n+        for (List<Cell> cells : put.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell cell : cells) {\n+                if (ts < cell.getTimestamp()) {\n+                    ts = cell.getTimestamp();\n+                }\n+            }\n+        }\n+        return ts;\n+    }\n+\n+    private boolean verifySingleIndexRow(Result indexRow, final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        long ts = getMaxTimestamp(dataRow);\n+        Put indexPut = indexMaintainer.buildUpdateMutation(GenericKeyValueBuilder.INSTANCE,\n+                valueGetter, new ImmutableBytesWritable(dataRow.getRow()), ts, null, null);\n+\n+        if (indexPut == null) {\n+            // This means the data row does not have any covered column values\n+            indexPut = new Put(indexRow.getRow());\n+        }\n+        // Add the empty column\n+        indexPut.addColumn(indexMaintainer.getEmptyKeyValueFamily().copyBytesIfNecessary(),\n+                indexMaintainer.getEmptyKeyValueQualifier(), ts, EMPTY_COLUMN_VALUE_BYTES);\n+\n+        int cellCount = 0;\n+        long currentTime = EnvironmentEdgeManager.currentTime();\n+        for (List<Cell> cells : indexPut.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell expectedCell : cells) {\n+                byte[] family = CellUtil.cloneFamily(expectedCell);\n+                byte[] qualifier = CellUtil.cloneQualifier(expectedCell);\n+                Cell actualCell = indexRow.getColumnLatestCell(family, qualifier);\n+                if (actualCell == null) {\n+                    // Check if cell expired as per the current server's time and data table ttl\n+                    // Index table should have the same ttl as the data table, hence we might not\n+                    // get a value back from index if it has already expired between our rebuild and\n+                    // verify\n+                    if (isTimestampBeforeTTL(currentTime, expectedCell.getTimestamp())) {\n+                        continue;\n+                    }\n+                    return false;\n+                }\n+                if (actualCell.getTimestamp() < ts) {\n+                    // Skip older cells since a Phoenix index row is composed of cells with the same timestamp\n+                    continue;\n+                }\n+                // Check all columns\n+                if (!CellUtil.matchingValue(actualCell, expectedCell)) {\n+                    return false;\n+                } else if (actualCell.getTimestamp() != ts) {\n+                    return false;\n+                }\n+                cellCount++;\n+            }\n+        }\n+        if (cellCount != indexRow.rawCells().length) {\n+            return false;\n+        }\n+        return true;\n+    }\n+\n+    private void verifyIndexRows(List<KeyRange> keys, Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        int expectedRowCount = keys.size();\n+        ScanRanges scanRanges = ScanRanges.createPointLookup(keys);\n+        Scan indexScan = new Scan();\n+        indexScan.setTimeRange(scan.getTimeRange().getMin(), scan.getTimeRange().getMax());\n+        scanRanges.initializeScan(indexScan);\n+        SkipScanFilter skipScanFilter = scanRanges.getSkipScanFilter();\n+        indexScan.setFilter(skipScanFilter);\n+        int rowCount = 0;\n+        try (ResultScanner resultScanner = indexHTable.getScanner(indexScan)) {\n+            for (Result result = resultScanner.next(); (result != null); result = resultScanner.next()) {\n+                Put dataPut = indexKeyToDataPutMap.get(result.getRow());\n+                if (dataPut == null) {\n+                    // This should never happen\n+                    exceptionMessage = \"Index verify failed - Missing data row - \" + indexHTable.getName();\n+                    throw new IOException(exceptionMessage);\n+                }\n+                if (verifySingleIndexRow(result, dataPut)) {\n+                    verificationPhaseResult.setValidIndexRowCount(verificationPhaseResult.getValidIndexRowCount()+1);                    perTaskDataKeyToDataPutMap.remove(dataPut.getRow());\n+                } else {\n+                    verificationPhaseResult.setInvalidIndexRowCount(verificationPhaseResult.getInvalidIndexRowCount()+1);\n+                }\n+                rowCount++;\n+            }\n+        } catch (Throwable t) {\n+            ServerUtil.throwIOException(indexHTable.getName().toString(), t);\n+        }\n+        // Check if any expected rows from index(which we didn't get) are already expired due to TTL\n+        if (!perTaskDataKeyToDataPutMap.isEmpty()) {\n+            Iterator<Entry<byte[], Put>> itr = perTaskDataKeyToDataPutMap.entrySet().iterator();\n+            long currentTime = EnvironmentEdgeManager.currentTime();\n+            while(itr.hasNext()) {\n+                Entry<byte[], Put> entry = itr.next();\n+                long ts = getMaxTimestamp(entry.getValue());\n+                if (isTimestampBeforeTTL(currentTime, ts)) {\n+                    itr.remove();\n+                    rowCount++;\n+                    verificationPhaseResult.setExpiredIndexRowCount(verificationPhaseResult.getExpiredIndexRowCount()+1);                }\n+            }\n+        }\n+        if (rowCount != expectedRowCount) {\n+            verificationPhaseResult.setMissingIndexRowCount(verificationPhaseResult.getMissingIndexRowCount()+expectedRowCount - rowCount);\n+        }\n+    }\n+\n+    private boolean isTimestampBeforeTTL(long currentTime, long tsToCheck) {\n+        if (indexTableTTL == HConstants.FOREVER) {\n+            return false;\n+        }\n+        return tsToCheck < (currentTime - (long) indexTableTTL * 1000);\n+    }\n+\n+    private void addVerifyTask(final List<KeyRange> keys, final Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            final IndexToolVerificationResult.PhaseResult verificationPhaseResult) {\n+        tasks.add(new Task<Boolean>() {\n+            @Override\n+            public Boolean call() throws Exception {\n+                try {\n+                    if (Thread.currentThread().isInterrupted()) {\n+                        exceptionMessage = \"Pool closed, not attempting to verify index rows! \" + indexHTable.getName();\n+                        throw new IOException(exceptionMessage);\n+                    }\n+                    verifyIndexRows(keys, perTaskDataKeyToDataPutMap, verificationPhaseResult);\n+                } catch (Exception e) {\n+                    throw e;\n+                }\n+                return Boolean.TRUE;\n+            }\n+        });\n+    }\n+\n+    private void parallelizeIndexVerify(IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        for (Mutation mutation : mutations) {\n+            indexKeyToDataPutMap.put(getIndexRowKey((Put)mutation), (Put)mutation);\n+        }\n+        int taskCount = (indexKeyToDataPutMap.size() + rowCountPerTask - 1) / rowCountPerTask;\n+        tasks = new TaskBatch<>(taskCount);\n+        List<Map<byte[], Put>> dataPutMapList = new ArrayList<>(taskCount);\n+        List<IndexToolVerificationResult.PhaseResult> verificationPhaseResultList = new ArrayList<>(taskCount);\n+        List<KeyRange> keys = new ArrayList<>(rowCountPerTask);\n+        Map<byte[], Put> perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+        dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+        IndexToolVerificationResult.PhaseResult perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+        verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+        for (Map.Entry<byte[], Put> entry: indexKeyToDataPutMap.entrySet()) {\n+            keys.add(PVarbinary.INSTANCE.getKeyRange(entry.getKey()));\n+            perTaskDataKeyToDataPutMap.put(entry.getValue().getRow(), entry.getValue());\n+            if (keys.size() == rowCountPerTask) {\n+                addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+                keys = new ArrayList<>(rowCountPerTask);\n+                perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+                perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+                verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+            }\n+        }\n+        if (keys.size() > 0) {\n+            addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+        }\n+        List<Boolean> taskResultList = null;\n+        try {\n+            LOGGER.debug(\"Waiting on index verify tasks to complete...\");\n+            taskResultList = this.pool.submitUninterruptible(tasks);\n+        } catch (ExecutionException e) {\n+            throw new RuntimeException(\"Should not fail on the results while using a WaitForCompletionTaskRunner\", e);\n+        } catch (EarlyExitFailure e) {\n+            throw new RuntimeException(\"Stopped while waiting for batch, quitting!\", e);\n+        }\n+        for (Boolean result : taskResultList) {\n+            if (result == null) {\n+                // there was a failure\n+                throw new IOException(exceptionMessage);\n+            }\n+        }\n+        if (verifyType == IndexTool.IndexVerifyType.BEFORE || verifyType == IndexTool.IndexVerifyType.BOTH) {\n+            for (Map<byte[], Put> dataPutMap : dataPutMapList) {\n+                dataKeyToDataPutMap.putAll(dataPutMap);\n+            }\n+        }\n+        for (IndexToolVerificationResult.PhaseResult result : verificationPhaseResultList) {\n+            verificationPhaseResult.add(result);\n+        }\n+    }\n+\n+    private void verifyIndex() throws IOException {\n+        IndexToolVerificationResult nextVerificationResult = new IndexToolVerificationResult(scan);\n+        nextVerificationResult.setScannedDataRowCount(mutations.size());\n+        IndexToolVerificationResult.PhaseResult verificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+        // For these options we start with verifying index rows\n+        parallelizeIndexVerify(verificationPhaseResult);\n+        nextVerificationResult.getBefore().add(verificationPhaseResult);\n+        indexKeyToDataPutMap.clear();\n+        verificationResult.add(nextVerificationResult);\n+    }\n+\n+    @Override\n+    public boolean next(List<Cell> results) throws IOException {\n+        Cell lastCell = null;\n+        int rowCount = 0;\n+        region.startRegionOperation();\n+        try {\n+            synchronized (innerScanner) {\n+                do {\n+                    List<Cell> row = new ArrayList<>();\n+                    hasMore = innerScanner.nextRaw(row);\n+                    if (!row.isEmpty()) {\n+                        lastCell = row.get(0);\n+                        Put put = null;\n+                        Delete del = null;\n+                        for (Cell cell : row) {\n+                            if (KeyValue.Type.codeToType(cell.getTypeByte()) == KeyValue.Type.Put) {\n+                                if (put == null) {\n+                                    put = new Put(CellUtil.cloneRow(cell));\n+                                    mutations.add(put);\n+                                }\n+                                put.add(cell);\n+                            } else {\n+                                if (del == null) {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczNjA5OA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409736098", "bodyText": "There is no need to have a mutation list. It should be removed. Instead of mutations.add(put), move the line\nindexKeyToDataPutMap.put(getIndexRowKey((Put)mutation), (Put)mutation) from parallelizeIndexVerify here.", "author": "kadirozde", "createdAt": "2020-04-16T17:42:18Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {\n+        long ts = 0;\n+        for (List<Cell> cells : put.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell cell : cells) {\n+                if (ts < cell.getTimestamp()) {\n+                    ts = cell.getTimestamp();\n+                }\n+            }\n+        }\n+        return ts;\n+    }\n+\n+    private boolean verifySingleIndexRow(Result indexRow, final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        long ts = getMaxTimestamp(dataRow);\n+        Put indexPut = indexMaintainer.buildUpdateMutation(GenericKeyValueBuilder.INSTANCE,\n+                valueGetter, new ImmutableBytesWritable(dataRow.getRow()), ts, null, null);\n+\n+        if (indexPut == null) {\n+            // This means the data row does not have any covered column values\n+            indexPut = new Put(indexRow.getRow());\n+        }\n+        // Add the empty column\n+        indexPut.addColumn(indexMaintainer.getEmptyKeyValueFamily().copyBytesIfNecessary(),\n+                indexMaintainer.getEmptyKeyValueQualifier(), ts, EMPTY_COLUMN_VALUE_BYTES);\n+\n+        int cellCount = 0;\n+        long currentTime = EnvironmentEdgeManager.currentTime();\n+        for (List<Cell> cells : indexPut.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell expectedCell : cells) {\n+                byte[] family = CellUtil.cloneFamily(expectedCell);\n+                byte[] qualifier = CellUtil.cloneQualifier(expectedCell);\n+                Cell actualCell = indexRow.getColumnLatestCell(family, qualifier);\n+                if (actualCell == null) {\n+                    // Check if cell expired as per the current server's time and data table ttl\n+                    // Index table should have the same ttl as the data table, hence we might not\n+                    // get a value back from index if it has already expired between our rebuild and\n+                    // verify\n+                    if (isTimestampBeforeTTL(currentTime, expectedCell.getTimestamp())) {\n+                        continue;\n+                    }\n+                    return false;\n+                }\n+                if (actualCell.getTimestamp() < ts) {\n+                    // Skip older cells since a Phoenix index row is composed of cells with the same timestamp\n+                    continue;\n+                }\n+                // Check all columns\n+                if (!CellUtil.matchingValue(actualCell, expectedCell)) {\n+                    return false;\n+                } else if (actualCell.getTimestamp() != ts) {\n+                    return false;\n+                }\n+                cellCount++;\n+            }\n+        }\n+        if (cellCount != indexRow.rawCells().length) {\n+            return false;\n+        }\n+        return true;\n+    }\n+\n+    private void verifyIndexRows(List<KeyRange> keys, Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        int expectedRowCount = keys.size();\n+        ScanRanges scanRanges = ScanRanges.createPointLookup(keys);\n+        Scan indexScan = new Scan();\n+        indexScan.setTimeRange(scan.getTimeRange().getMin(), scan.getTimeRange().getMax());\n+        scanRanges.initializeScan(indexScan);\n+        SkipScanFilter skipScanFilter = scanRanges.getSkipScanFilter();\n+        indexScan.setFilter(skipScanFilter);\n+        int rowCount = 0;\n+        try (ResultScanner resultScanner = indexHTable.getScanner(indexScan)) {\n+            for (Result result = resultScanner.next(); (result != null); result = resultScanner.next()) {\n+                Put dataPut = indexKeyToDataPutMap.get(result.getRow());\n+                if (dataPut == null) {\n+                    // This should never happen\n+                    exceptionMessage = \"Index verify failed - Missing data row - \" + indexHTable.getName();\n+                    throw new IOException(exceptionMessage);\n+                }\n+                if (verifySingleIndexRow(result, dataPut)) {\n+                    verificationPhaseResult.setValidIndexRowCount(verificationPhaseResult.getValidIndexRowCount()+1);                    perTaskDataKeyToDataPutMap.remove(dataPut.getRow());\n+                } else {\n+                    verificationPhaseResult.setInvalidIndexRowCount(verificationPhaseResult.getInvalidIndexRowCount()+1);\n+                }\n+                rowCount++;\n+            }\n+        } catch (Throwable t) {\n+            ServerUtil.throwIOException(indexHTable.getName().toString(), t);\n+        }\n+        // Check if any expected rows from index(which we didn't get) are already expired due to TTL\n+        if (!perTaskDataKeyToDataPutMap.isEmpty()) {\n+            Iterator<Entry<byte[], Put>> itr = perTaskDataKeyToDataPutMap.entrySet().iterator();\n+            long currentTime = EnvironmentEdgeManager.currentTime();\n+            while(itr.hasNext()) {\n+                Entry<byte[], Put> entry = itr.next();\n+                long ts = getMaxTimestamp(entry.getValue());\n+                if (isTimestampBeforeTTL(currentTime, ts)) {\n+                    itr.remove();\n+                    rowCount++;\n+                    verificationPhaseResult.setExpiredIndexRowCount(verificationPhaseResult.getExpiredIndexRowCount()+1);                }\n+            }\n+        }\n+        if (rowCount != expectedRowCount) {\n+            verificationPhaseResult.setMissingIndexRowCount(verificationPhaseResult.getMissingIndexRowCount()+expectedRowCount - rowCount);\n+        }\n+    }\n+\n+    private boolean isTimestampBeforeTTL(long currentTime, long tsToCheck) {\n+        if (indexTableTTL == HConstants.FOREVER) {\n+            return false;\n+        }\n+        return tsToCheck < (currentTime - (long) indexTableTTL * 1000);\n+    }\n+\n+    private void addVerifyTask(final List<KeyRange> keys, final Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            final IndexToolVerificationResult.PhaseResult verificationPhaseResult) {\n+        tasks.add(new Task<Boolean>() {\n+            @Override\n+            public Boolean call() throws Exception {\n+                try {\n+                    if (Thread.currentThread().isInterrupted()) {\n+                        exceptionMessage = \"Pool closed, not attempting to verify index rows! \" + indexHTable.getName();\n+                        throw new IOException(exceptionMessage);\n+                    }\n+                    verifyIndexRows(keys, perTaskDataKeyToDataPutMap, verificationPhaseResult);\n+                } catch (Exception e) {\n+                    throw e;\n+                }\n+                return Boolean.TRUE;\n+            }\n+        });\n+    }\n+\n+    private void parallelizeIndexVerify(IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        for (Mutation mutation : mutations) {\n+            indexKeyToDataPutMap.put(getIndexRowKey((Put)mutation), (Put)mutation);\n+        }\n+        int taskCount = (indexKeyToDataPutMap.size() + rowCountPerTask - 1) / rowCountPerTask;\n+        tasks = new TaskBatch<>(taskCount);\n+        List<Map<byte[], Put>> dataPutMapList = new ArrayList<>(taskCount);\n+        List<IndexToolVerificationResult.PhaseResult> verificationPhaseResultList = new ArrayList<>(taskCount);\n+        List<KeyRange> keys = new ArrayList<>(rowCountPerTask);\n+        Map<byte[], Put> perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+        dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+        IndexToolVerificationResult.PhaseResult perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+        verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+        for (Map.Entry<byte[], Put> entry: indexKeyToDataPutMap.entrySet()) {\n+            keys.add(PVarbinary.INSTANCE.getKeyRange(entry.getKey()));\n+            perTaskDataKeyToDataPutMap.put(entry.getValue().getRow(), entry.getValue());\n+            if (keys.size() == rowCountPerTask) {\n+                addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+                keys = new ArrayList<>(rowCountPerTask);\n+                perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+                perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+                verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+            }\n+        }\n+        if (keys.size() > 0) {\n+            addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+        }\n+        List<Boolean> taskResultList = null;\n+        try {\n+            LOGGER.debug(\"Waiting on index verify tasks to complete...\");\n+            taskResultList = this.pool.submitUninterruptible(tasks);\n+        } catch (ExecutionException e) {\n+            throw new RuntimeException(\"Should not fail on the results while using a WaitForCompletionTaskRunner\", e);\n+        } catch (EarlyExitFailure e) {\n+            throw new RuntimeException(\"Stopped while waiting for batch, quitting!\", e);\n+        }\n+        for (Boolean result : taskResultList) {\n+            if (result == null) {\n+                // there was a failure\n+                throw new IOException(exceptionMessage);\n+            }\n+        }\n+        if (verifyType == IndexTool.IndexVerifyType.BEFORE || verifyType == IndexTool.IndexVerifyType.BOTH) {\n+            for (Map<byte[], Put> dataPutMap : dataPutMapList) {\n+                dataKeyToDataPutMap.putAll(dataPutMap);\n+            }\n+        }\n+        for (IndexToolVerificationResult.PhaseResult result : verificationPhaseResultList) {\n+            verificationPhaseResult.add(result);\n+        }\n+    }\n+\n+    private void verifyIndex() throws IOException {\n+        IndexToolVerificationResult nextVerificationResult = new IndexToolVerificationResult(scan);\n+        nextVerificationResult.setScannedDataRowCount(mutations.size());\n+        IndexToolVerificationResult.PhaseResult verificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+        // For these options we start with verifying index rows\n+        parallelizeIndexVerify(verificationPhaseResult);\n+        nextVerificationResult.getBefore().add(verificationPhaseResult);\n+        indexKeyToDataPutMap.clear();\n+        verificationResult.add(nextVerificationResult);\n+    }\n+\n+    @Override\n+    public boolean next(List<Cell> results) throws IOException {\n+        Cell lastCell = null;\n+        int rowCount = 0;\n+        region.startRegionOperation();\n+        try {\n+            synchronized (innerScanner) {\n+                do {\n+                    List<Cell> row = new ArrayList<>();\n+                    hasMore = innerScanner.nextRaw(row);\n+                    if (!row.isEmpty()) {\n+                        lastCell = row.get(0);\n+                        Put put = null;\n+                        Delete del = null;\n+                        for (Cell cell : row) {\n+                            if (KeyValue.Type.codeToType(cell.getTypeByte()) == KeyValue.Type.Put) {\n+                                if (put == null) {\n+                                    put = new Put(CellUtil.cloneRow(cell));\n+                                    mutations.add(put);", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxNjMwMQ==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r411716301", "bodyText": "Are you planning to remove the mutation list (i.e.. mutations)?", "author": "kadirozde", "createdAt": "2020-04-20T21:50:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczNjA5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc1MzE2MA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r411753160", "bodyText": "Changing so is failing the tests that I added. We can pass this refactoring for followup if you would like?", "author": "swaroopak", "createdAt": "2020-04-20T23:13:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczNjA5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjU1OTE0OA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r412559148", "bodyText": "Thanks for the help @kadirozde :)", "author": "swaroopak", "createdAt": "2020-04-21T23:24:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczNjA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0MDQzMA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409740430", "bodyText": "Remove this block. Also remove dataKeyToDataPutMap as it is not used anymore.", "author": "kadirozde", "createdAt": "2020-04-16T17:49:19Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {\n+        long ts = 0;\n+        for (List<Cell> cells : put.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell cell : cells) {\n+                if (ts < cell.getTimestamp()) {\n+                    ts = cell.getTimestamp();\n+                }\n+            }\n+        }\n+        return ts;\n+    }\n+\n+    private boolean verifySingleIndexRow(Result indexRow, final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        long ts = getMaxTimestamp(dataRow);\n+        Put indexPut = indexMaintainer.buildUpdateMutation(GenericKeyValueBuilder.INSTANCE,\n+                valueGetter, new ImmutableBytesWritable(dataRow.getRow()), ts, null, null);\n+\n+        if (indexPut == null) {\n+            // This means the data row does not have any covered column values\n+            indexPut = new Put(indexRow.getRow());\n+        }\n+        // Add the empty column\n+        indexPut.addColumn(indexMaintainer.getEmptyKeyValueFamily().copyBytesIfNecessary(),\n+                indexMaintainer.getEmptyKeyValueQualifier(), ts, EMPTY_COLUMN_VALUE_BYTES);\n+\n+        int cellCount = 0;\n+        long currentTime = EnvironmentEdgeManager.currentTime();\n+        for (List<Cell> cells : indexPut.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell expectedCell : cells) {\n+                byte[] family = CellUtil.cloneFamily(expectedCell);\n+                byte[] qualifier = CellUtil.cloneQualifier(expectedCell);\n+                Cell actualCell = indexRow.getColumnLatestCell(family, qualifier);\n+                if (actualCell == null) {\n+                    // Check if cell expired as per the current server's time and data table ttl\n+                    // Index table should have the same ttl as the data table, hence we might not\n+                    // get a value back from index if it has already expired between our rebuild and\n+                    // verify\n+                    if (isTimestampBeforeTTL(currentTime, expectedCell.getTimestamp())) {\n+                        continue;\n+                    }\n+                    return false;\n+                }\n+                if (actualCell.getTimestamp() < ts) {\n+                    // Skip older cells since a Phoenix index row is composed of cells with the same timestamp\n+                    continue;\n+                }\n+                // Check all columns\n+                if (!CellUtil.matchingValue(actualCell, expectedCell)) {\n+                    return false;\n+                } else if (actualCell.getTimestamp() != ts) {\n+                    return false;\n+                }\n+                cellCount++;\n+            }\n+        }\n+        if (cellCount != indexRow.rawCells().length) {\n+            return false;\n+        }\n+        return true;\n+    }\n+\n+    private void verifyIndexRows(List<KeyRange> keys, Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        int expectedRowCount = keys.size();\n+        ScanRanges scanRanges = ScanRanges.createPointLookup(keys);\n+        Scan indexScan = new Scan();\n+        indexScan.setTimeRange(scan.getTimeRange().getMin(), scan.getTimeRange().getMax());\n+        scanRanges.initializeScan(indexScan);\n+        SkipScanFilter skipScanFilter = scanRanges.getSkipScanFilter();\n+        indexScan.setFilter(skipScanFilter);\n+        int rowCount = 0;\n+        try (ResultScanner resultScanner = indexHTable.getScanner(indexScan)) {\n+            for (Result result = resultScanner.next(); (result != null); result = resultScanner.next()) {\n+                Put dataPut = indexKeyToDataPutMap.get(result.getRow());\n+                if (dataPut == null) {\n+                    // This should never happen\n+                    exceptionMessage = \"Index verify failed - Missing data row - \" + indexHTable.getName();\n+                    throw new IOException(exceptionMessage);\n+                }\n+                if (verifySingleIndexRow(result, dataPut)) {\n+                    verificationPhaseResult.setValidIndexRowCount(verificationPhaseResult.getValidIndexRowCount()+1);                    perTaskDataKeyToDataPutMap.remove(dataPut.getRow());\n+                } else {\n+                    verificationPhaseResult.setInvalidIndexRowCount(verificationPhaseResult.getInvalidIndexRowCount()+1);\n+                }\n+                rowCount++;\n+            }\n+        } catch (Throwable t) {\n+            ServerUtil.throwIOException(indexHTable.getName().toString(), t);\n+        }\n+        // Check if any expected rows from index(which we didn't get) are already expired due to TTL\n+        if (!perTaskDataKeyToDataPutMap.isEmpty()) {\n+            Iterator<Entry<byte[], Put>> itr = perTaskDataKeyToDataPutMap.entrySet().iterator();\n+            long currentTime = EnvironmentEdgeManager.currentTime();\n+            while(itr.hasNext()) {\n+                Entry<byte[], Put> entry = itr.next();\n+                long ts = getMaxTimestamp(entry.getValue());\n+                if (isTimestampBeforeTTL(currentTime, ts)) {\n+                    itr.remove();\n+                    rowCount++;\n+                    verificationPhaseResult.setExpiredIndexRowCount(verificationPhaseResult.getExpiredIndexRowCount()+1);                }\n+            }\n+        }\n+        if (rowCount != expectedRowCount) {\n+            verificationPhaseResult.setMissingIndexRowCount(verificationPhaseResult.getMissingIndexRowCount()+expectedRowCount - rowCount);\n+        }\n+    }\n+\n+    private boolean isTimestampBeforeTTL(long currentTime, long tsToCheck) {\n+        if (indexTableTTL == HConstants.FOREVER) {\n+            return false;\n+        }\n+        return tsToCheck < (currentTime - (long) indexTableTTL * 1000);\n+    }\n+\n+    private void addVerifyTask(final List<KeyRange> keys, final Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            final IndexToolVerificationResult.PhaseResult verificationPhaseResult) {\n+        tasks.add(new Task<Boolean>() {\n+            @Override\n+            public Boolean call() throws Exception {\n+                try {\n+                    if (Thread.currentThread().isInterrupted()) {\n+                        exceptionMessage = \"Pool closed, not attempting to verify index rows! \" + indexHTable.getName();\n+                        throw new IOException(exceptionMessage);\n+                    }\n+                    verifyIndexRows(keys, perTaskDataKeyToDataPutMap, verificationPhaseResult);\n+                } catch (Exception e) {\n+                    throw e;\n+                }\n+                return Boolean.TRUE;\n+            }\n+        });\n+    }\n+\n+    private void parallelizeIndexVerify(IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        for (Mutation mutation : mutations) {\n+            indexKeyToDataPutMap.put(getIndexRowKey((Put)mutation), (Put)mutation);\n+        }\n+        int taskCount = (indexKeyToDataPutMap.size() + rowCountPerTask - 1) / rowCountPerTask;\n+        tasks = new TaskBatch<>(taskCount);\n+        List<Map<byte[], Put>> dataPutMapList = new ArrayList<>(taskCount);\n+        List<IndexToolVerificationResult.PhaseResult> verificationPhaseResultList = new ArrayList<>(taskCount);\n+        List<KeyRange> keys = new ArrayList<>(rowCountPerTask);\n+        Map<byte[], Put> perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+        dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+        IndexToolVerificationResult.PhaseResult perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+        verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+        for (Map.Entry<byte[], Put> entry: indexKeyToDataPutMap.entrySet()) {\n+            keys.add(PVarbinary.INSTANCE.getKeyRange(entry.getKey()));\n+            perTaskDataKeyToDataPutMap.put(entry.getValue().getRow(), entry.getValue());\n+            if (keys.size() == rowCountPerTask) {\n+                addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+                keys = new ArrayList<>(rowCountPerTask);\n+                perTaskDataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataPutMapList.add(perTaskDataKeyToDataPutMap);\n+                perTaskVerificationPhaseResult = new IndexToolVerificationResult.PhaseResult();\n+                verificationPhaseResultList.add(perTaskVerificationPhaseResult);\n+            }\n+        }\n+        if (keys.size() > 0) {\n+            addVerifyTask(keys, perTaskDataKeyToDataPutMap, perTaskVerificationPhaseResult);\n+        }\n+        List<Boolean> taskResultList = null;\n+        try {\n+            LOGGER.debug(\"Waiting on index verify tasks to complete...\");\n+            taskResultList = this.pool.submitUninterruptible(tasks);\n+        } catch (ExecutionException e) {\n+            throw new RuntimeException(\"Should not fail on the results while using a WaitForCompletionTaskRunner\", e);\n+        } catch (EarlyExitFailure e) {\n+            throw new RuntimeException(\"Stopped while waiting for batch, quitting!\", e);\n+        }\n+        for (Boolean result : taskResultList) {\n+            if (result == null) {\n+                // there was a failure\n+                throw new IOException(exceptionMessage);\n+            }\n+        }\n+        if (verifyType == IndexTool.IndexVerifyType.BEFORE || verifyType == IndexTool.IndexVerifyType.BOTH) {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0MjQ0MA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409742440", "bodyText": "You can remove this class and use the one from IndexRebuildRegionScanner", "author": "kadirozde", "createdAt": "2020-04-16T17:52:35Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0MzI1MA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409743250", "bodyText": "You can remove this method and use the one from IndexRebuildRegionScanner", "author": "kadirozde", "createdAt": "2020-04-16T17:53:45Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0Mzg3MQ==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409743871", "bodyText": "You can remove this method and use the one from IndexRebuildRegionScanner", "author": "kadirozde", "createdAt": "2020-04-16T17:54:41Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0NjMyNA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409746324", "bodyText": "VerifyTpe for this class is always IndexTool.IndexVerifyType.ONLY. No need to have verifyType. You can remove the code checking the verify type.", "author": "kadirozde", "createdAt": "2020-04-16T17:58:38Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0NzIzNQ==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409747235", "bodyText": "You can remove this method and use the one from IndexRebuildRegionScanner", "author": "kadirozde", "createdAt": "2020-04-16T18:00:06Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {\n+        long ts = 0;\n+        for (List<Cell> cells : put.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell cell : cells) {\n+                if (ts < cell.getTimestamp()) {\n+                    ts = cell.getTimestamp();\n+                }\n+            }\n+        }\n+        return ts;\n+    }\n+\n+    private boolean verifySingleIndexRow(Result indexRow, final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        long ts = getMaxTimestamp(dataRow);\n+        Put indexPut = indexMaintainer.buildUpdateMutation(GenericKeyValueBuilder.INSTANCE,\n+                valueGetter, new ImmutableBytesWritable(dataRow.getRow()), ts, null, null);\n+\n+        if (indexPut == null) {\n+            // This means the data row does not have any covered column values\n+            indexPut = new Put(indexRow.getRow());\n+        }\n+        // Add the empty column\n+        indexPut.addColumn(indexMaintainer.getEmptyKeyValueFamily().copyBytesIfNecessary(),\n+                indexMaintainer.getEmptyKeyValueQualifier(), ts, EMPTY_COLUMN_VALUE_BYTES);\n+\n+        int cellCount = 0;\n+        long currentTime = EnvironmentEdgeManager.currentTime();\n+        for (List<Cell> cells : indexPut.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell expectedCell : cells) {\n+                byte[] family = CellUtil.cloneFamily(expectedCell);\n+                byte[] qualifier = CellUtil.cloneQualifier(expectedCell);\n+                Cell actualCell = indexRow.getColumnLatestCell(family, qualifier);\n+                if (actualCell == null) {\n+                    // Check if cell expired as per the current server's time and data table ttl\n+                    // Index table should have the same ttl as the data table, hence we might not\n+                    // get a value back from index if it has already expired between our rebuild and\n+                    // verify\n+                    if (isTimestampBeforeTTL(currentTime, expectedCell.getTimestamp())) {\n+                        continue;\n+                    }\n+                    return false;\n+                }\n+                if (actualCell.getTimestamp() < ts) {\n+                    // Skip older cells since a Phoenix index row is composed of cells with the same timestamp\n+                    continue;\n+                }\n+                // Check all columns\n+                if (!CellUtil.matchingValue(actualCell, expectedCell)) {\n+                    return false;\n+                } else if (actualCell.getTimestamp() != ts) {\n+                    return false;\n+                }\n+                cellCount++;\n+            }\n+        }\n+        if (cellCount != indexRow.rawCells().length) {\n+            return false;\n+        }\n+        return true;\n+    }\n+\n+    private void verifyIndexRows(List<KeyRange> keys, Map<byte[], Put> perTaskDataKeyToDataPutMap,\n+            IndexToolVerificationResult.PhaseResult verificationPhaseResult) throws IOException {\n+        int expectedRowCount = keys.size();\n+        ScanRanges scanRanges = ScanRanges.createPointLookup(keys);\n+        Scan indexScan = new Scan();\n+        indexScan.setTimeRange(scan.getTimeRange().getMin(), scan.getTimeRange().getMax());\n+        scanRanges.initializeScan(indexScan);\n+        SkipScanFilter skipScanFilter = scanRanges.getSkipScanFilter();\n+        indexScan.setFilter(skipScanFilter);\n+        int rowCount = 0;\n+        try (ResultScanner resultScanner = indexHTable.getScanner(indexScan)) {\n+            for (Result result = resultScanner.next(); (result != null); result = resultScanner.next()) {\n+                Put dataPut = indexKeyToDataPutMap.get(result.getRow());\n+                if (dataPut == null) {\n+                    // This should never happen\n+                    exceptionMessage = \"Index verify failed - Missing data row - \" + indexHTable.getName();\n+                    throw new IOException(exceptionMessage);\n+                }\n+                if (verifySingleIndexRow(result, dataPut)) {\n+                    verificationPhaseResult.setValidIndexRowCount(verificationPhaseResult.getValidIndexRowCount()+1);                    perTaskDataKeyToDataPutMap.remove(dataPut.getRow());\n+                } else {\n+                    verificationPhaseResult.setInvalidIndexRowCount(verificationPhaseResult.getInvalidIndexRowCount()+1);\n+                }\n+                rowCount++;\n+            }\n+        } catch (Throwable t) {\n+            ServerUtil.throwIOException(indexHTable.getName().toString(), t);\n+        }\n+        // Check if any expected rows from index(which we didn't get) are already expired due to TTL\n+        if (!perTaskDataKeyToDataPutMap.isEmpty()) {\n+            Iterator<Entry<byte[], Put>> itr = perTaskDataKeyToDataPutMap.entrySet().iterator();\n+            long currentTime = EnvironmentEdgeManager.currentTime();\n+            while(itr.hasNext()) {\n+                Entry<byte[], Put> entry = itr.next();\n+                long ts = getMaxTimestamp(entry.getValue());\n+                if (isTimestampBeforeTTL(currentTime, ts)) {\n+                    itr.remove();\n+                    rowCount++;\n+                    verificationPhaseResult.setExpiredIndexRowCount(verificationPhaseResult.getExpiredIndexRowCount()+1);                }\n+            }\n+        }\n+        if (rowCount != expectedRowCount) {\n+            verificationPhaseResult.setMissingIndexRowCount(verificationPhaseResult.getMissingIndexRowCount()+expectedRowCount - rowCount);\n+        }\n+    }\n+\n+    private boolean isTimestampBeforeTTL(long currentTime, long tsToCheck) {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODk2Ng==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409748966", "bodyText": "No need to have separate config params for this class. Please use the ones from IndexRebuilRegionScanner.", "author": "kadirozde", "createdAt": "2020-04-16T18:03:00Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MjcxOA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r409752718", "bodyText": "This method should also check the max lookback window and update the max lookback counters", "author": "kadirozde", "createdAt": "2020-04-16T18:09:27Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/IndexerRegionScanner.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import static org.apache.phoenix.hbase.index.write.AbstractParallelWriterIndexCommitter.INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY;\n+import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;\n+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_VALUE_BYTES;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;\n+import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;\n+import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;\n+import static org.apache.phoenix.query.QueryServices.INDEX_REBUILD_PAGE_SIZE_IN_ROWS;\n+import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.concurrent.ExecutionException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellUtil;\n+\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.client.Delete;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+\n+import org.apache.phoenix.compile.ScanRanges;\n+import org.apache.phoenix.filter.SkipScanFilter;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.EarlyExitFailure;\n+import org.apache.phoenix.hbase.index.parallel.Task;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolBuilder;\n+import org.apache.phoenix.hbase.index.parallel.ThreadPoolManager;\n+import org.apache.phoenix.hbase.index.parallel.WaitForCompletionTaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;\n+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;\n+\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.index.PhoenixIndexCodec;\n+import org.apache.phoenix.mapreduce.index.IndexTool;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+import org.apache.phoenix.query.KeyRange;\n+import org.apache.phoenix.query.QueryServicesOptions;\n+import org.apache.phoenix.schema.types.PLong;\n+import org.apache.phoenix.schema.types.PVarbinary;\n+import org.apache.phoenix.util.KeyValueUtil;\n+import org.apache.phoenix.util.ServerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Maps;\n+\n+public class IndexerRegionScanner extends BaseRegionScanner {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(IndexerRegionScanner.class);\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    private static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    private long pageSizeInRows = Long.MAX_VALUE;\n+    private int rowCountPerTask;\n+    private boolean hasMore;\n+    private final int maxBatchSize;\n+    private UngroupedAggregateRegionObserver.MutationList mutations;\n+    private byte[] indexMetaData;\n+    private Scan scan;\n+    private RegionScanner innerScanner;\n+    private Region region;\n+    private IndexMaintainer indexMaintainer;\n+    private Table indexHTable = null;\n+    private IndexTool.IndexVerifyType verifyType = IndexTool.IndexVerifyType.NONE;\n+    private boolean verify = false;\n+    private Map<byte[], Put> indexKeyToDataPutMap;\n+    private Map<byte[], Put> dataKeyToDataPutMap;\n+    private TaskRunner pool;\n+    private TaskBatch<Boolean> tasks;\n+    private String exceptionMessage;\n+    private HTableFactory hTableFactory;\n+    private int indexTableTTL;\n+    private IndexToolVerificationResult verificationResult;\n+\n+    private IndexVerificationResultRepository verificationResultRepository;\n+\n+    IndexerRegionScanner (final RegionScanner innerScanner, final Region region, final Scan scan,\n+            final RegionCoprocessorEnvironment env) throws IOException {\n+        super(innerScanner);\n+        final Configuration config = env.getConfiguration();\n+        if (scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_PAGING) != null) {\n+            pageSizeInRows = config.getLong(INDEX_REBUILD_PAGE_SIZE_IN_ROWS,\n+                    QueryServicesOptions.DEFAULT_INDEX_REBUILD_PAGE_SIZE_IN_ROWS);\n+        }\n+        maxBatchSize = config.getInt(MUTATE_BATCH_SIZE_ATTRIB, QueryServicesOptions.DEFAULT_MUTATE_BATCH_SIZE);\n+        mutations = new UngroupedAggregateRegionObserver.MutationList(maxBatchSize);\n+        indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_PROTO_MD);\n+        if (indexMetaData == null) {\n+            indexMetaData = scan.getAttribute(PhoenixIndexCodec.INDEX_MD);\n+        }\n+        List<IndexMaintainer> maintainers = IndexMaintainer.deserialize(indexMetaData, true);\n+        indexMaintainer = maintainers.get(0);\n+        this.scan = scan;\n+        this.innerScanner = innerScanner;\n+        this.region = region;\n+        byte[] valueBytes = scan.getAttribute(BaseScannerRegionObserver.INDEX_REBUILD_VERIFY_TYPE);\n+        if (valueBytes != null) {\n+            verificationResult = new IndexToolVerificationResult(scan);\n+            verifyType = IndexTool.IndexVerifyType.fromValue(valueBytes);\n+            if (verifyType != IndexTool.IndexVerifyType.NONE) {\n+                verify = true;\n+                // Create the following objects only for rebuilds by IndexTool\n+                hTableFactory = ServerUtil.getDelegateHTableFactory(env, ServerUtil.ConnectionType.INDEX_WRITER_CONNECTION);\n+                indexHTable = hTableFactory.getTable(new ImmutableBytesPtr(indexMaintainer.getIndexTableName()));\n+                indexTableTTL = indexHTable.getTableDescriptor().getColumnFamilies()[0].getTimeToLive();\n+                verificationResultRepository =\n+                        new IndexVerificationResultRepository(indexMaintainer.getIndexTableName(), hTableFactory);\n+                indexKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                dataKeyToDataPutMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);\n+                pool = new WaitForCompletionTaskRunner(ThreadPoolManager.getExecutor(\n+                        new ThreadPoolBuilder(\"IndexVerify\",\n+                                env.getConfiguration()).setMaxThread(NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY,\n+                                DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS).setCoreTimeout(\n+                                INDEX_WRITER_KEEP_ALIVE_TIME_CONF_KEY), env));\n+                rowCountPerTask = config.getInt(INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY,\n+                        DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public HRegionInfo getRegionInfo() {\n+        return region.getRegionInfo();\n+    }\n+\n+    @Override\n+    public boolean isFilterDone() { return false; }\n+\n+    @Override\n+    public void close() throws IOException {\n+        innerScanner.close();\n+        if (verify) {\n+            try {\n+                verificationResultRepository.logToIndexToolResultTable(verificationResult,\n+                        verifyType, region.getRegionInfo().getRegionName());\n+            } finally {\n+                this.pool.stop(\"IndexerRegionScanner is closing\");\n+                hTableFactory.shutdown();\n+                indexHTable.close();\n+                verificationResultRepository.close();\n+            }\n+        }\n+    }\n+\n+    private class SimpleValueGetter implements ValueGetter {\n+        final ImmutableBytesWritable valuePtr = new ImmutableBytesWritable();\n+        final Put put;\n+        SimpleValueGetter (final Put put) {\n+            this.put = put;\n+        }\n+        @Override\n+        public ImmutableBytesWritable getLatestValue(ColumnReference ref, long ts) throws IOException {\n+            List<Cell> cellList = put.get(ref.getFamily(), ref.getQualifier());\n+            if (cellList == null || cellList.isEmpty()) {\n+                return null;\n+            }\n+            Cell cell = cellList.get(0);\n+            valuePtr.set(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n+            return valuePtr;\n+        }\n+\n+        @Override\n+        public byte[] getRowKey() {\n+            return put.getRow();\n+        }\n+\n+    }\n+\n+    private byte[] getIndexRowKey(final Put dataRow) throws IOException {\n+        ValueGetter valueGetter = new SimpleValueGetter(dataRow);\n+        byte[] builtIndexRowKey = indexMaintainer.buildRowKey(valueGetter, new ImmutableBytesWritable(dataRow.getRow()),\n+                null, null, HConstants.LATEST_TIMESTAMP);\n+        return builtIndexRowKey;\n+    }\n+\n+    private long getMaxTimestamp(Put put) {\n+        long ts = 0;\n+        for (List<Cell> cells : put.getFamilyCellMap().values()) {\n+            if (cells == null) {\n+                break;\n+            }\n+            for (Cell cell : cells) {\n+                if (ts < cell.getTimestamp()) {\n+                    ts = cell.getTimestamp();\n+                }\n+            }\n+        }\n+        return ts;\n+    }\n+\n+    private boolean verifySingleIndexRow(Result indexRow, final Put dataRow) throws IOException {", "originalCommit": "553a812965c93f85ead8bd638efc568aa19e8c6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6d1532ce1074c28e4cdab322c35f4585a818daf7", "url": "https://github.com/apache/phoenix/commit/6d1532ce1074c28e4cdab322c35f4585a818daf7", "message": "Fixing review comments", "committedDate": "2020-04-17T00:26:15Z", "type": "forcePushed"}, {"oid": "63e4d7829630eaef2f7a950ee9e0ff1810ec21aa", "url": "https://github.com/apache/phoenix/commit/63e4d7829630eaef2f7a950ee9e0ff1810ec21aa", "message": "Fixing review comments", "committedDate": "2020-04-17T01:25:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxNTYyOA==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r411715628", "bodyText": "This constructor can initialize the attributes that are defined in this class. This will allow you to remove the common code from the subclass constructors.", "author": "kadirozde", "createdAt": "2020-04-20T21:49:21Z", "path": "phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GlobalIndexRegionScanner.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.phoenix.coprocessor;\n+\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.regionserver.Region;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.regionserver.ScanInfoUtil;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.phoenix.hbase.index.ValueGetter;\n+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;\n+import org.apache.phoenix.hbase.index.parallel.TaskBatch;\n+import org.apache.phoenix.hbase.index.parallel.TaskRunner;\n+import org.apache.phoenix.hbase.index.table.HTableFactory;\n+import org.apache.phoenix.index.IndexMaintainer;\n+import org.apache.phoenix.mapreduce.index.IndexVerificationResultRepository;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public abstract class GlobalIndexRegionScanner extends BaseRegionScanner {\n+\n+    public static final String NUM_CONCURRENT_INDEX_VERIFY_THREADS_CONF_KEY = \"index.verify.threads.max\";\n+    public static final int DEFAULT_CONCURRENT_INDEX_VERIFY_THREADS = 17;\n+    public static final String INDEX_VERIFY_ROW_COUNTS_PER_TASK_CONF_KEY = \"index.verify.threads.max\";\n+    public static final int DEFAULT_INDEX_VERIFY_ROW_COUNTS_PER_TASK = 2048;\n+    public static final String NO_EXPECTED_MUTATION = \"No expected mutation\";\n+    public static final String ACTUAL_MUTATION_IS_NULL_OR_EMPTY = \"actualMutationList is null or empty\";\n+    public static final String ERROR_MESSAGE_MISSING_INDEX_ROW_BEYOND_MAX_LOOKBACK = \"Missing index row beyond maxLookBack\";\n+    public static final String ERROR_MESSAGE_MISSING_INDEX_ROW = \"Missing index row\";\n+\n+    protected long pageSizeInRows = Long.MAX_VALUE;\n+    protected int rowCountPerTask;\n+    protected boolean hasMore;\n+    protected int maxBatchSize;\n+    protected UngroupedAggregateRegionObserver.MutationList mutations;\n+    protected byte[] indexMetaData;\n+    protected Scan scan;\n+    protected RegionScanner innerScanner;\n+    protected Region region;\n+    protected IndexMaintainer indexMaintainer;\n+    protected Table indexHTable = null;\n+    protected Map<byte[], Put> indexKeyToDataPutMap;\n+    protected TaskRunner pool;\n+    protected TaskBatch<Boolean> tasks;\n+    protected String exceptionMessage;\n+    protected HTableFactory hTableFactory;\n+    protected int indexTableTTL;\n+    protected long maxLookBackInMills;\n+    protected IndexToolVerificationResult verificationResult;\n+    protected IndexVerificationResultRepository verificationResultRepository;\n+\n+    public GlobalIndexRegionScanner(RegionScanner delegate) {\n+        super(delegate);", "originalCommit": "9c1a1011384365d5390a1b343c6beae30f1b9ded", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxNjQ3Nw==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r411716477", "bodyText": "Oh, yeah that's right!", "author": "swaroopak", "createdAt": "2020-04-20T21:50:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxNTYyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcyNzE2Ng==", "url": "https://github.com/apache/phoenix/pull/758#discussion_r411727166", "bodyText": "Thanks for moving the test cases that are specific to non transactional global indexes here. Good idea!", "author": "kadirozde", "createdAt": "2020-04-20T22:13:11Z", "path": "phoenix-core/src/it/java/org/apache/phoenix/end2end/IndexToolForNonTxGlobalIndexIT.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/*", "originalCommit": "9c1a1011384365d5390a1b343c6beae30f1b9ded", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c4a96bab7e34bdc7003176a888004becad613568", "url": "https://github.com/apache/phoenix/commit/c4a96bab7e34bdc7003176a888004becad613568", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-21T22:32:36Z", "type": "forcePushed"}, {"oid": "86911b2b3509585e2c59c5651d9967464f90805d", "url": "https://github.com/apache/phoenix/commit/86911b2b3509585e2c59c5651d9967464f90805d", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondary indexes", "committedDate": "2020-04-22T21:20:17Z", "type": "forcePushed"}, {"oid": "333a3db5c83f21ccb022e7193698baec1740ec74", "url": "https://github.com/apache/phoenix/commit/333a3db5c83f21ccb022e7193698baec1740ec74", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondry indexes", "committedDate": "2020-04-22T21:44:57Z", "type": "commit"}, {"oid": "333a3db5c83f21ccb022e7193698baec1740ec74", "url": "https://github.com/apache/phoenix/commit/333a3db5c83f21ccb022e7193698baec1740ec74", "message": "PHOENIX-5804: Implement strong verification with -v ONLY option for old design of secondry indexes", "committedDate": "2020-04-22T21:44:57Z", "type": "forcePushed"}]}