{"pr_number": 1493, "pr_title": "DBZ-2021 Updated doc for 1.1 SMTs for topic routing and event flattening", "pr_createdAt": "2020-05-12T21:51:51Z", "pr_url": "https://github.com/debezium/debezium/pull/1493", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE3NzIzMQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425177231", "bodyText": "I'm fairly confident we can remove this include since we've migrated attributes to antora.yml.", "author": "Naros", "createdAt": "2020-05-14T14:23:44Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -1,3 +1,8 @@\n+// Category: cdc-using\n+// Type: assembly\n+\n+ifdef::community[]\n+[id=\"new-record-state-extraction\"]\n = New Record State Extraction\n include::../_attributes.adoc[]", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4NDMzNg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425184336", "bodyText": "I believe this sentence is a tiny bit misleading because a Debezium connector is only ever a source connector.  Perhaps a better wording here might be:\nYou can configure the `ExtractNewRecordState` SMT for a {prodname} connector or for a sink connector that will consume messages emitted by a {prodname} connector.", "author": "Naros", "createdAt": "2020-05-14T14:32:39Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. ", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE5MDE1Mg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425190152", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To add metadat to a simplified Kafka record that is for a `DELETE` operation, you must also configure `delete.handling.mode=rewrite`.\n          \n          \n            \n            To add metadata to a simplified Kafka record that is for a `DELETE` operation, you must also configure `delete.handling.mode=rewrite`.", "author": "Naros", "createdAt": "2020-05-14T14:40:36Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. \n+\n+You can configure the transformation to do any of the following: \n+\n+* Add metadata from the change event to the simplified Kafka record or record header. The default behavior is that the SMT does not add metadata to the simplified Kafka record.\n \n-The SMT provides three main functions.\n-It\n+* Keep Kafka records that contain change events for `DELETE` operations in the stream. The default behavior is that the SMT drops Kafka records for `DELETE` operation change events because most consumers cannot yet handle them. \n \n-* extracts the `after` field from change events and replaces the original event just with this part\n-* optionally filters delete and tombstone records, as per the capabilities and requirements of downstream consumers\n-* optionally adds metadata fields from the change event to the outgoing flattened record\n-* optionally add metadata fields to the header\n+A database `DELETE` operation causes Debezium to generate two Kafka records: \n \n-The SMT can be applied either to a source connector (Debezium) or a sink connector.\n-We generally recommend to apply the transformation on the sink side as it means that the messages stored in Apache Kafka will contain the whole context.\n-The final decision depends on use case for each user.\n+* A record that contains `\"op\": \"d\",` the `before` row data, and some other fields.\n+* A tombstone record that has the same key as the deleted row and a value of `null`. This record is a marker for Apache Kafka. It indicates that \n+link:https://kafka.apache.org/documentation/#compaction[log compaction] can remove all records that have this key. \n \n+Instead of dropping the record that contains the `before` row data, you can configure the `ExtractNewRecordData` SMT to do one of the following: \n+\n+* Keep the record in the stream and edit it to have only the `\"value\": \"null\"` field.\n+ \n+* Keep the record in the stream and edit it to have a `value` field that contains the key/value pairs that were in the `before` field with an added `\"__deleted\": \"true\"` entry.\n+\n+Similary, instead of dropping the tombstone record, you can configure the `ExtractNewRecordData` SMT to keep the tombstone record in the stream. \n+\n+ifdef::community[]\n == Configuration\n-The configuration is a part of source/sink task connector and is expressed in a set of properties:\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"configuration-of-extractnewrecordstate-transformation\"]\n+== Configuration of `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+Configure the Debezium `ExtractNewRecordState` SMT in a Kafka Connect source or sink connector `.properties` file. To obtain the default behavior, specify something like the following: \n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+----\n+\n+As in any Kafka Connect connector `.properties` file, you can set `transforms=` to multiple, comma-separated, SMT aliases in the order in which you want Kafka Connect  to apply the SMTs. \n+\n+The following example sets several `ExtractNewRecordState` options: \n \n [source]\n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.drop.tombstones=false\n transforms.unwrap.delete.handling.mode=rewrite\n-transforms.unwrap.add.source.fields=table,lsn\n+transforms.unwrap.add.fields=table,lsn\n ----\n \n-=== Record filtering for delete records\n-\n-The SMT provides a special handling for events that signal a `delete` operation.\n-When a `DELETE` is executed on a datasource then Debezium generates two events:\n+`drop-tombstones=false`:: Keeps tombstone records for `DELETE` operations in the event stream. \n \n-* a record with `d` operation that contains only old row data\n-* (optionally) a record with `null` value and the same key (a \"tombstone\" message). This record serves as a marker for Apache Kafka that all messages with this key can be removed from the topic during {link-kafka-docs}/#compaction[log compaction].\n+`delete-handling-mode=rewrite`:: For `DELETE` operations, edits the Kafka record by flattening the `value` field that was in the change event. The `value` field directly contains the key/value pairs that were in the `before` field. The SMT adds `__deleted` and sets it to `true`, for example:   \n++\n+----\n+\"value\": {\n+  \"pk\": 2,\n+  \"cola\": null,\n+  \"__deleted\": \"true\"\n+}\n+----\n \n-Upon processing these two records, the SMT can pass on the `d` record as is,\n-convert it into another tombstone record or drop it.\n-The original tombstone message can be passed on as is or also be dropped.\n+`add.fields=table,lsn`:: Adds change event metadata for the `table` and `lsn` fields to the simplified Kafka record. \n \n-[NOTE]\n-====\n-The SMT by default filters out *both* delete records as widely used sink connectors do not support handling of tombstone messages at this point.\n-====\n+ifdef::community[]\n+== Adding metadata\n+endif::community[]\n+ \n+ifdef::product[]\n+// Type: concept\n+[id=\"example-of-adding-metadata-to-the-kafka-record-or-its-header\"]\n+== Example of adding metadata to the Kafka record or its header\n+endif::product[]\n \n-=== Adding metadata fields to the message\n+The `ExtractNewRecordState` SMT can add original, change event metadata to the simplified Kafka record or its header. For example, you might want the simplified record or its header to contain any of the following: \n \n-The SMT can optionally add metadata fields from the original change event to the final flattened record. This functionality can be used to add things like the operation or the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available see xref:connectors/index.adoc[the documentation for each connector].\n+* The type of operation that made the change\n+* The name of the database or table that was changed\n+* Connector-specific fields such as the Postgres LSN field\n \n-In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>_\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+ifdef::community[]\n+For more information on what is available see xref:connectors/index.adoc[the documentation for each connector].\n+endif::community[]\n \n-For example, the configuration\n+To add metadata to the simplified Kafka record, specify the `add.fields` option. \n+To add metadata to the header of the simplified Kafka record, specify the `add.header` option. Each of these options takes a comma separated list of change event field names. Do not specify spaces. When there are duplicate field names, to add metadata for one of those fields, specify the struct as well as the field. For example:\n \n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.add.fields=op,table,lsn,source.ts_ms\n+transforms.unwrap.add.headers=db\n+transforms.unwrap.delete.handling.mode=rewrite\n ----\n \n-will add\n+With that configuration, a simplified Kafka record would contain something like the following: \n \n ----\n { \"__op\" : \"c\", __table\": \"MY_TABLE\", \"__lsn\": \"123456789\", \"__source_ts_ms\" : \"123456789\", ...}\n ----\n \n-to the final flattened record.\n+Also, simplified Kafka records would have a `__db` header. \n+\n+In the simplified Kafka record, the SMT prefixes the metadata field names with a double underscore. When you specify a struct, the SMT also inserts an underscore between the struct name and the field name. \n+\n+To add metadat to a simplified Kafka record that is for a `DELETE` operation, you must also configure `delete.handling.mode=rewrite`.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE5MjI4Mw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425192283", "bodyText": "We've gradually been removing the use of options on tables based on work in other recent docs changes.  This might imply a small change to the first row to render properly, but overall removing it seems like a nice change as specifying it is really unnecessary.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [cols=\"35%a,10%a,55%a\",options=\"header\"]\n          \n          \n            \n            [cols=\"35%a,10%a,55%a\"]", "author": "Naros", "createdAt": "2020-05-14T14:43:23Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. \n+\n+You can configure the transformation to do any of the following: \n+\n+* Add metadata from the change event to the simplified Kafka record or record header. The default behavior is that the SMT does not add metadata to the simplified Kafka record.\n \n-The SMT provides three main functions.\n-It\n+* Keep Kafka records that contain change events for `DELETE` operations in the stream. The default behavior is that the SMT drops Kafka records for `DELETE` operation change events because most consumers cannot yet handle them. \n \n-* extracts the `after` field from change events and replaces the original event just with this part\n-* optionally filters delete and tombstone records, as per the capabilities and requirements of downstream consumers\n-* optionally adds metadata fields from the change event to the outgoing flattened record\n-* optionally add metadata fields to the header\n+A database `DELETE` operation causes Debezium to generate two Kafka records: \n \n-The SMT can be applied either to a source connector (Debezium) or a sink connector.\n-We generally recommend to apply the transformation on the sink side as it means that the messages stored in Apache Kafka will contain the whole context.\n-The final decision depends on use case for each user.\n+* A record that contains `\"op\": \"d\",` the `before` row data, and some other fields.\n+* A tombstone record that has the same key as the deleted row and a value of `null`. This record is a marker for Apache Kafka. It indicates that \n+link:https://kafka.apache.org/documentation/#compaction[log compaction] can remove all records that have this key. \n \n+Instead of dropping the record that contains the `before` row data, you can configure the `ExtractNewRecordData` SMT to do one of the following: \n+\n+* Keep the record in the stream and edit it to have only the `\"value\": \"null\"` field.\n+ \n+* Keep the record in the stream and edit it to have a `value` field that contains the key/value pairs that were in the `before` field with an added `\"__deleted\": \"true\"` entry.\n+\n+Similary, instead of dropping the tombstone record, you can configure the `ExtractNewRecordData` SMT to keep the tombstone record in the stream. \n+\n+ifdef::community[]\n == Configuration\n-The configuration is a part of source/sink task connector and is expressed in a set of properties:\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"configuration-of-extractnewrecordstate-transformation\"]\n+== Configuration of `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+Configure the Debezium `ExtractNewRecordState` SMT in a Kafka Connect source or sink connector `.properties` file. To obtain the default behavior, specify something like the following: \n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+----\n+\n+As in any Kafka Connect connector `.properties` file, you can set `transforms=` to multiple, comma-separated, SMT aliases in the order in which you want Kafka Connect  to apply the SMTs. \n+\n+The following example sets several `ExtractNewRecordState` options: \n \n [source]\n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.drop.tombstones=false\n transforms.unwrap.delete.handling.mode=rewrite\n-transforms.unwrap.add.source.fields=table,lsn\n+transforms.unwrap.add.fields=table,lsn\n ----\n \n-=== Record filtering for delete records\n-\n-The SMT provides a special handling for events that signal a `delete` operation.\n-When a `DELETE` is executed on a datasource then Debezium generates two events:\n+`drop-tombstones=false`:: Keeps tombstone records for `DELETE` operations in the event stream. \n \n-* a record with `d` operation that contains only old row data\n-* (optionally) a record with `null` value and the same key (a \"tombstone\" message). This record serves as a marker for Apache Kafka that all messages with this key can be removed from the topic during {link-kafka-docs}/#compaction[log compaction].\n+`delete-handling-mode=rewrite`:: For `DELETE` operations, edits the Kafka record by flattening the `value` field that was in the change event. The `value` field directly contains the key/value pairs that were in the `before` field. The SMT adds `__deleted` and sets it to `true`, for example:   \n++\n+----\n+\"value\": {\n+  \"pk\": 2,\n+  \"cola\": null,\n+  \"__deleted\": \"true\"\n+}\n+----\n \n-Upon processing these two records, the SMT can pass on the `d` record as is,\n-convert it into another tombstone record or drop it.\n-The original tombstone message can be passed on as is or also be dropped.\n+`add.fields=table,lsn`:: Adds change event metadata for the `table` and `lsn` fields to the simplified Kafka record. \n \n-[NOTE]\n-====\n-The SMT by default filters out *both* delete records as widely used sink connectors do not support handling of tombstone messages at this point.\n-====\n+ifdef::community[]\n+== Adding metadata\n+endif::community[]\n+ \n+ifdef::product[]\n+// Type: concept\n+[id=\"example-of-adding-metadata-to-the-kafka-record-or-its-header\"]\n+== Example of adding metadata to the Kafka record or its header\n+endif::product[]\n \n-=== Adding metadata fields to the message\n+The `ExtractNewRecordState` SMT can add original, change event metadata to the simplified Kafka record or its header. For example, you might want the simplified record or its header to contain any of the following: \n \n-The SMT can optionally add metadata fields from the original change event to the final flattened record. This functionality can be used to add things like the operation or the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available see xref:connectors/index.adoc[the documentation for each connector].\n+* The type of operation that made the change\n+* The name of the database or table that was changed\n+* Connector-specific fields such as the Postgres LSN field\n \n-In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>_\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+ifdef::community[]\n+For more information on what is available see xref:connectors/index.adoc[the documentation for each connector].\n+endif::community[]\n \n-For example, the configuration\n+To add metadata to the simplified Kafka record, specify the `add.fields` option. \n+To add metadata to the header of the simplified Kafka record, specify the `add.header` option. Each of these options takes a comma separated list of change event field names. Do not specify spaces. When there are duplicate field names, to add metadata for one of those fields, specify the struct as well as the field. For example:\n \n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.add.fields=op,table,lsn,source.ts_ms\n+transforms.unwrap.add.headers=db\n+transforms.unwrap.delete.handling.mode=rewrite\n ----\n \n-will add\n+With that configuration, a simplified Kafka record would contain something like the following: \n \n ----\n { \"__op\" : \"c\", __table\": \"MY_TABLE\", \"__lsn\": \"123456789\", \"__source_ts_ms\" : \"123456789\", ...}\n ----\n \n-to the final flattened record.\n+Also, simplified Kafka records would have a `__db` header. \n+\n+In the simplified Kafka record, the SMT prefixes the metadata field names with a double underscore. When you specify a struct, the SMT also inserts an underscore between the struct name and the field name. \n+\n+To add metadat to a simplified Kafka record that is for a `DELETE` operation, you must also configure `delete.handling.mode=rewrite`.\n+\n+ifdef::community[]\n+// Do not include deprecated content in downstream doc\n+== Determine original operation  [DEPRECATED]\n+\n+_The `operation.header` option is deprecated and scheduled for removal. Please use add.headers instead. If both add.headers and operation.header are specified, the latter will be ignored._\n+\n+When a Kafka record is flattened the final result won't show whether it was an insert, update or first read\n+(deletions can be detected via tombstones or rewrites, see link:#options-for-configuring-extractnewrecordstate-transformation[Configuration options]).\n+\n+To solve this problem Debezium offers an option to propagate the original operation via a header added to the Kafka record.\n+To enable this feature the option `operation.header` must be set to `true`.\n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+transforms.unwrap.operation.header=true\n+----\n \n-For `DELETE` events, this option is only supported when the <<configuration-option-delete-handling-mode, `delete.handling.mode`>> option is set to \"rewrite\".\n+The possible values are the ones from the `op` field of the original change event.\n+endif::community[]\n \n-=== Adding metadata fields to the header\n+ifdef::community[]\n+// Do not include deprecated content in downstream doc\n+== Adding source metadata fields [DEPRECATED]\n \n-The SMT can optionally add metadata fields from the original change event to the header of the final flattened record. This functionality can be used to add things like the operation or the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available see xref:connectors/index.adoc[the documentation for each connector].\n+_The `add.source.fields` option is deprecated and scheduled for removal. Please use add.fields instead. If both add.fields and add.source.fields are specified, the latter will be ignored._\n \n-In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>_\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+The SMT can optionally add metadata fields from the original change event's `source` structure to the final flattened record (prefixed with \"__\"). This functionality can be used to add things like the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available in the source structure see xref:connectors/index.adoc[the documentation for each connector].\n \n For example, the configuration\n \n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n-transforms.unwrap.add.headers=op,table,lsn,source.ts_ms\n+transforms.unwrap.add.source.fields=table,lsn\n ----\n \n-will add headers `__op`, `__table`, `__lsn` and `__source_ts_ms` to the outgoing record.\n+will add\n+\n+----\n+{ \"__table\": \"MY_TABLE\", \"__lsn\": \"123456789\", ...}\n+----\n+\n+to the final flattened record.\n+\n+For `DELETE` events, this option is only supported when the `delete.handling.mode` option is set to \"rewrite\".\n \n-[[configuration_options]]\n == Configuration options\n-[cols=\"35%a,10%a,55%a\"]\n+\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: reference\n+[id=\"options-for-configuring-extractnewrecordstate-transformation\"]\n+== Options for configuring `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+The following table describes the options that you can specify for the `ExtractNewRecordState` SMT. \n+\n+[cols=\"35%a,10%a,55%a\",options=\"header\"]", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTIxNTkwMQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425215901", "bodyText": "We can likely drop the include to attributes.adoc here as well.", "author": "Naros", "createdAt": "2020-05-14T15:14:21Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -1,3 +1,8 @@\n+// Category: cdc-using\n+// Type: assembly\n+\n+ifdef::community[]\n+[id=\"topic-routing\"]\n = Topic Routing\n include::../_attributes.adoc[]", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTIzNjI1Mw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425236253", "bodyText": "I don't think we necessarily want to consider loosing this section because being able to quickly browse all configurable options for a given feature is useful and concise for users who may know what they're looking for.  I think its fine to reference these options in the example and various texts, but having their definition and description in a common area with other options feels better to me.  What do you think @gunnarmorling?", "author": "Naros", "createdAt": "2020-05-14T15:40:59Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -56,79 +79,61 @@ transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n transforms.Reroute.topic.replacement=$1customers_all_shards\n ----\n \n-The configuration above will match topics such as `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc. and replace it with `myserver.mydb.customers_all_shards.`\n+`topic.regex`:: Specifies a regular expression that the transformation applies to each change event record to determine if it should be routed to a particular topic.  \n++\n+In the example, the regular expression, `(.*)customers_shard(.*)` matches records for changes to tables whose names include the `customers_shard` string. This would re-route records for tables with the following names:\n++\n+`myserver.mydb.customers_shard1` +\n+`myserver.mydb.customers_shard2` +\n+`myserver.mydb.customers_shard3`\n \n-== Key Fields\n+`topic.replacement`:: Specifies a regular expression that represents the destination topic name. The transformation routes each matching record to the topic identified by this expression. In this example, records for the three sharded tables listed above would be routed to the `myserver.mydb.customers_all_shards` topic. \n \n-To address the concern of uniqueness across all the original tables discussed above,\n-one more field for identifying the original (physical) table may be inserted into the key structure of the change events.\n-By default, this field is named `\\__dbz__physicalTableIdentifier` and has the original topic name as its value.\n+ifdef::community[]\n+== Ensure unique key\n+endif::community[]\n \n-If your tables already contain globally unique keys and you do not need to change the key structure, you can set the `key.enforce.uniqueness` property to `false`:\n+ifdef::product[]\n+// Type: procedure\n+[id=\"ensuring-unique-keys-across-records-routed-to-the-same-topic\"]\n+== Ensuring unique keys across records routed to the same topic\n+endif::product[]\n \n-[source]\n-----\n-...\n-transforms.Reroute.key.enforce.uniqueness=false\n-...\n-----\n+A Debezium change event key uses the table columns that make up the table's primary key. To route records for multiple physical tables to one topic, the event key must be unique across all of those tables. However, it is possible for each physical table to have a primary key that is unique within only that table. For example, a row in the `myserver.mydb.customers_shard1` table might have the same key value as a row in the `myserver.mydb.customers_shard2` table. \n+\n+To ensure that each event key is unique across the tables whose change event records go to the same topic, the `ByLogicalTableRouter` transformation inserts a field into change event keys. By default, the name of the inserted field is `+__dbz__physicalTableIdentifier+`. The value of the inserted field is the default destination topic name.\n \n-If needed, another _field name_ can be chosen by means of the `key.field.name` property\n-(obviously you'll want to choose a field name that doesn't clash with existing primary key fields).\n-For example the following configuration will use the name `shard_id` for the key field:\n+If you want to, you can configure the `ByLogicalTableRouter` transformation to insert a different field into the key. To do this, specify the `key.field.name` option and set it to a field name that does not clash with existing primary key field names. For example: \n \n [source]\n ----\n-...\n+transforms=Reroute\n+transforms.Reroute.type=io.debezium.transforms.ByLogicalTableRouter\n+transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n+transforms.Reroute.topic.replacement=$1customers_all_shards\n transforms.Reroute.key.field.name=shard_id\n-...\n ----\n \n-The _value_ of the field can be adjusted via the `key.field.regex` and `key.field.replacement` properties.\n-The former allows you to define a regular expression that will be applied to the original topic name to capture one or more groups of characters.\n-The latter lets you specify an expression that defines the value for the field in terms of those captured groups.\n-For example:\n+This example adds the `shard_id` field to the key structure in routed records.\n+\n+If you want to adjust the value of the key's new field, configure both of these options:\n+\n+`key.field.regex`:: Specifies a regular expression that the transformation applies to the default destination topic name to capture one or more groups of characters. \n+\n+`key.field.replacement`:: Specifies a regular expression for determining the value of the inserted key field in terms of those captured groups. \n+\n+For example: \n \n [source]\n ----\n-...\n transforms.Reroute.key.field.regex=(.*)customers_shard(.*)\n transforms.Reroute.key.field.replacement=$2\n ----\n \n-This will apply the given regular expression to original topic names and use the second capturing group as value for the key field.\n-Assuming the source topics are named `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc., the key field's values would be `1`, `2` etc.\n-\n-[[configuration-options]]", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTIzNzIzMA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425237230", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                community: \n          \n          \n            \n                community: true\n          \n      \n    \n    \n  \n\nIn order for the Antora build to actually recognize this attribute, the yaml file must specify an accompanying value in order for it to be injected into the build definitions; otherwise its omitted entirely and omits any section that is scoped by ifdef::community[].", "author": "Naros", "createdAt": "2020-05-14T15:42:21Z", "path": "documentation/antora.yml", "diffHunk": "@@ -15,6 +15,7 @@ asciidoc:\n     modules: '../../modules'\n     mysql-version: '8.0'\n     strimzi-version: '0.13.0'\n+    community: ", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI0MDQ5MQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425240491", "bodyText": "Would you mind adding links to operation.header and add.source.fields in this table like we've done for the other options for consistency?", "author": "Naros", "createdAt": "2020-05-14T15:46:40Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. \n+\n+You can configure the transformation to do any of the following: \n+\n+* Add metadata from the change event to the simplified Kafka record or record header. The default behavior is that the SMT does not add metadata to the simplified Kafka record.\n \n-The SMT provides three main functions.\n-It\n+* Keep Kafka records that contain change events for `DELETE` operations in the stream. The default behavior is that the SMT drops Kafka records for `DELETE` operation change events because most consumers cannot yet handle them. \n \n-* extracts the `after` field from change events and replaces the original event just with this part\n-* optionally filters delete and tombstone records, as per the capabilities and requirements of downstream consumers\n-* optionally adds metadata fields from the change event to the outgoing flattened record\n-* optionally add metadata fields to the header\n+A database `DELETE` operation causes Debezium to generate two Kafka records: \n \n-The SMT can be applied either to a source connector (Debezium) or a sink connector.\n-We generally recommend to apply the transformation on the sink side as it means that the messages stored in Apache Kafka will contain the whole context.\n-The final decision depends on use case for each user.\n+* A record that contains `\"op\": \"d\",` the `before` row data, and some other fields.\n+* A tombstone record that has the same key as the deleted row and a value of `null`. This record is a marker for Apache Kafka. It indicates that \n+link:https://kafka.apache.org/documentation/#compaction[log compaction] can remove all records that have this key. \n \n+Instead of dropping the record that contains the `before` row data, you can configure the `ExtractNewRecordData` SMT to do one of the following: \n+\n+* Keep the record in the stream and edit it to have only the `\"value\": \"null\"` field.\n+ \n+* Keep the record in the stream and edit it to have a `value` field that contains the key/value pairs that were in the `before` field with an added `\"__deleted\": \"true\"` entry.\n+\n+Similary, instead of dropping the tombstone record, you can configure the `ExtractNewRecordData` SMT to keep the tombstone record in the stream. \n+\n+ifdef::community[]\n == Configuration\n-The configuration is a part of source/sink task connector and is expressed in a set of properties:\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"configuration-of-extractnewrecordstate-transformation\"]\n+== Configuration of `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+Configure the Debezium `ExtractNewRecordState` SMT in a Kafka Connect source or sink connector `.properties` file. To obtain the default behavior, specify something like the following: \n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+----\n+\n+As in any Kafka Connect connector `.properties` file, you can set `transforms=` to multiple, comma-separated, SMT aliases in the order in which you want Kafka Connect  to apply the SMTs. \n+\n+The following example sets several `ExtractNewRecordState` options: \n \n [source]\n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.drop.tombstones=false\n transforms.unwrap.delete.handling.mode=rewrite\n-transforms.unwrap.add.source.fields=table,lsn\n+transforms.unwrap.add.fields=table,lsn\n ----\n \n-=== Record filtering for delete records\n-\n-The SMT provides a special handling for events that signal a `delete` operation.\n-When a `DELETE` is executed on a datasource then Debezium generates two events:\n+`drop-tombstones=false`:: Keeps tombstone records for `DELETE` operations in the event stream. \n \n-* a record with `d` operation that contains only old row data\n-* (optionally) a record with `null` value and the same key (a \"tombstone\" message). This record serves as a marker for Apache Kafka that all messages with this key can be removed from the topic during {link-kafka-docs}/#compaction[log compaction].\n+`delete-handling-mode=rewrite`:: For `DELETE` operations, edits the Kafka record by flattening the `value` field that was in the change event. The `value` field directly contains the key/value pairs that were in the `before` field. The SMT adds `__deleted` and sets it to `true`, for example:   \n++\n+----\n+\"value\": {\n+  \"pk\": 2,\n+  \"cola\": null,\n+  \"__deleted\": \"true\"\n+}\n+----\n \n-Upon processing these two records, the SMT can pass on the `d` record as is,\n-convert it into another tombstone record or drop it.\n-The original tombstone message can be passed on as is or also be dropped.\n+`add.fields=table,lsn`:: Adds change event metadata for the `table` and `lsn` fields to the simplified Kafka record. \n \n-[NOTE]\n-====\n-The SMT by default filters out *both* delete records as widely used sink connectors do not support handling of tombstone messages at this point.\n-====\n+ifdef::community[]\n+== Adding metadata\n+endif::community[]\n+ \n+ifdef::product[]\n+// Type: concept\n+[id=\"example-of-adding-metadata-to-the-kafka-record-or-its-header\"]\n+== Example of adding metadata to the Kafka record or its header\n+endif::product[]\n \n-=== Adding metadata fields to the message\n+The `ExtractNewRecordState` SMT can add original, change event metadata to the simplified Kafka record or its header. For example, you might want the simplified record or its header to contain any of the following: \n \n-The SMT can optionally add metadata fields from the original change event to the final flattened record. This functionality can be used to add things like the operation or the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available see xref:connectors/index.adoc[the documentation for each connector].\n+* The type of operation that made the change\n+* The name of the database or table that was changed\n+* Connector-specific fields such as the Postgres LSN field\n \n-In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>_\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+ifdef::community[]\n+For more information on what is available see xref:connectors/index.adoc[the documentation for each connector].\n+endif::community[]\n \n-For example, the configuration\n+To add metadata to the simplified Kafka record, specify the `add.fields` option. \n+To add metadata to the header of the simplified Kafka record, specify the `add.header` option. Each of these options takes a comma separated list of change event field names. Do not specify spaces. When there are duplicate field names, to add metadata for one of those fields, specify the struct as well as the field. For example:\n \n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.add.fields=op,table,lsn,source.ts_ms\n+transforms.unwrap.add.headers=db\n+transforms.unwrap.delete.handling.mode=rewrite\n ----\n \n-will add\n+With that configuration, a simplified Kafka record would contain something like the following: \n \n ----\n { \"__op\" : \"c\", __table\": \"MY_TABLE\", \"__lsn\": \"123456789\", \"__source_ts_ms\" : \"123456789\", ...}\n ----\n \n-to the final flattened record.\n+Also, simplified Kafka records would have a `__db` header. \n+\n+In the simplified Kafka record, the SMT prefixes the metadata field names with a double underscore. When you specify a struct, the SMT also inserts an underscore between the struct name and the field name. \n+\n+To add metadat to a simplified Kafka record that is for a `DELETE` operation, you must also configure `delete.handling.mode=rewrite`.\n+\n+ifdef::community[]\n+// Do not include deprecated content in downstream doc\n+== Determine original operation  [DEPRECATED]\n+\n+_The `operation.header` option is deprecated and scheduled for removal. Please use add.headers instead. If both add.headers and operation.header are specified, the latter will be ignored._\n+\n+When a Kafka record is flattened the final result won't show whether it was an insert, update or first read\n+(deletions can be detected via tombstones or rewrites, see link:#options-for-configuring-extractnewrecordstate-transformation[Configuration options]).\n+\n+To solve this problem Debezium offers an option to propagate the original operation via a header added to the Kafka record.\n+To enable this feature the option `operation.header` must be set to `true`.\n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+transforms.unwrap.operation.header=true\n+----\n \n-For `DELETE` events, this option is only supported when the <<configuration-option-delete-handling-mode, `delete.handling.mode`>> option is set to \"rewrite\".\n+The possible values are the ones from the `op` field of the original change event.\n+endif::community[]\n \n-=== Adding metadata fields to the header\n+ifdef::community[]\n+// Do not include deprecated content in downstream doc\n+== Adding source metadata fields [DEPRECATED]\n \n-The SMT can optionally add metadata fields from the original change event to the header of the final flattened record. This functionality can be used to add things like the operation or the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available see xref:connectors/index.adoc[the documentation for each connector].\n+_The `add.source.fields` option is deprecated and scheduled for removal. Please use add.fields instead. If both add.fields and add.source.fields are specified, the latter will be ignored._\n \n-In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>_\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+The SMT can optionally add metadata fields from the original change event's `source` structure to the final flattened record (prefixed with \"__\"). This functionality can be used to add things like the table from the change event, or connector-specific fields like the Postgres LSN field. For more information on what's available in the source structure see xref:connectors/index.adoc[the documentation for each connector].\n \n For example, the configuration\n \n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n-transforms.unwrap.add.headers=op,table,lsn,source.ts_ms\n+transforms.unwrap.add.source.fields=table,lsn\n ----\n \n-will add headers `__op`, `__table`, `__lsn` and `__source_ts_ms` to the outgoing record.\n+will add\n+\n+----\n+{ \"__table\": \"MY_TABLE\", \"__lsn\": \"123456789\", ...}\n+----\n+\n+to the final flattened record.\n+\n+For `DELETE` events, this option is only supported when the `delete.handling.mode` option is set to \"rewrite\".\n \n-[[configuration_options]]\n == Configuration options\n-[cols=\"35%a,10%a,55%a\"]\n+\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: reference\n+[id=\"options-for-configuring-extractnewrecordstate-transformation\"]\n+== Options for configuring `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+The following table describes the options that you can specify for the `ExtractNewRecordState` SMT. \n+\n+[cols=\"35%a,10%a,55%a\",options=\"header\"]\n |===\n-|Property |Default |Description\n+|Property\n+|Default\n+|Description\n \n |[[configuration-option-drop-tombstones]]<<configuration-option-drop-tombstones, `drop.tombstones`>>\n |`true`\n-|The SMT removes the tombstone generated by Debezium from the stream.\n+|Debezium generates a tombstone record for each `DELETE` operation. The default behavior is that `ExtractNewRecordState` removes tombstone records from the stream. To keep tombstone records in the stream, specify `drop.tombstones=false`.  \n \n |[[configuration-option-delete-handling-mode]]<<configuration-option-delete-handling-mode, `delete.handling.mode`>>\n |`drop`\n-|The SMT can `drop` (the default), `rewrite` or pass delete events (`none`). The rewrite mode will add a `__deleted` column with true/false values based on record operation.\n-\n+|Debezium generates a change event record for each `DELETE` operation. The default behavior is that `ExtractNewRecordState` removes these records from the stream. To keep Kafka records for `DELETE` operations in the stream, set `delete.handling.mode` to `none` or `rewrite`. +\n+ +\n+Specify `none` to keep the change event record in the stream. The record contains only `\"value\": \"null\"`.  + \n+ +\n+Specify `rewrite` to keep the change event record in the stream and edit the record to have a `value` field that contains the key/value pairs that were in the `before` field and also add `+__deleted: true+` to the `value`. This is another way to indicate that the record has been deleted. +\n+ +\n+When you  specify `rewrite`, the updated simplified records for `DELETE` operations might be all you need to track deleted records. You can consider accepting the default behavior of dropping the tombstone records that the Debezium connector creates.\n \n |[[configuration-option-route-by-field]]<<configuration-option-route-by-field, `route.by.field`>>\n |\n-|The column which determines how the events will be routed, the value will the topic name; obtained from the old record state for delete events, and from the new record state otherwise\n+|To use row data to determine the topic to route the record to, set this option to an `after` field attribute. The SMT routes the record to the topic whose name matches the value of the specified `after` field attribute. For a `DELETE` operation, set this option to a `before` field attribute. +\n+ +\n+For example, configuration of `route.by.field=destination` routes records to the topic whose name is the value of `after.destination`. The default behavior is that a Debezium connector sends each change event record to a topic whose name is formed from the name of the database and the name of the table in which the change was made. + \n+ +\n+If you are configuring the `ExtractNewRecordState` SMT on a sink connector, setting this option might be useful when the destination topic name dictates the name of the database table that will be updated with the simplified change event record. If the topic name is not correct for your use case, you can configure `route.by.field` to re-route the event.\n \n |[[configuration-option-add-fields]]<<configuration-option-add-fields, `add.fields`>>\n |\n-|Specify a list of metadata fields to add to the flattened message. In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>__\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+|Set this option to a comma-separated list, with no spaces, of metadata fields to add to the simplified Kafka record. When there are duplicate field names, to add metadata for one of those fields, specify the struct as well as the field, for example `source.ts_ms`. +\n+ +\n+When the SMT adds metadata fields to the simplified record, it prefixes each metadata field name with a double underscore. For a struct specification, the SMT also inserts an underscore between the struct name and the field name. +\n+ +\n+If you specify a field that is not in the change event record, the SMT adds the field.  \n \n |[[configuration-option-add-headers]]<<configuration-option-add-headers, `add.headers`>>\n |\n-|Specify a list of metadata fields to add to the header of the flattened message. In case of duplicate field names (e.g. \"ts_ms\" exists twice), the struct should be specified to get the correct field (e.g. \"source.ts_ms\"). The fields will be prefixed with \"\\\\__\" or \"__<struct>__\", depending on the specification of the struct. Please use a comma separated list without spaces.\n+|Set this option to a comma-separated list, with no spaces, of metadata fields to add to the header of the simplified Kafka record. When there are duplicate field names, to add metadata for one of those fields, specify the struct as well as the field, for example `source.ts_ms`. +\n+ +\n+When the SMT adds metadata fields to the simplified record's header, it prefixes each metadata field name with a double underscore. For a struct specification, the SMT also inserts an underscore between the struct name and the field name. +\n+ +\n+If you specify a field that is not in the change event record, the SMT does not add the field to the header.\n+\n+ifdef::community[]\n+// Do not include deprecated content in downstream doc\n+|`operation.header` DEPRECATED", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI0NDg3OA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425244878", "bodyText": "I have seen this in multiple spots i the changed files in this PR and I'm not sure this differentiation is needed.  In the MySQL documentation as an example, a number of headings that were much shorter in the original writings are now much more verbose and resemble what you have scoped as product here.  I'm inclined to think we can omit the separation between community/product and merely use the more verbose section headings.\nAny preference @gunnarmorling?", "author": "Naros", "createdAt": "2020-05-14T15:52:42Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI2NTE2Ng==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425265166", "bodyText": "Take a look at this page: https://debezium.io/documentation/reference/1.0/connectors/mysql.html\nThe entries on the right are impossible to scan quickly. A user has to read them. I'm trying to provide a quicker way for the user to find the information they want. When the Event Flattening page appears,  in the right toc panel, do you want the user to see this:\nChange event structure\nBehavior\nConfiguration\nExample\nOptions\n\nOr this:\nDescription of Debezium change event structure\nBehavior of Debezium ExtractNewRecordState transformation\nConfiguration of ExtractNewRecordState transformation\nExample of adding metadata to the Kafka record or its header\nOptions for configuring ExtractNewRecordState transformation\n\nI think the first option provides a better user experience. Upstream, the user does not need the context that is required to be provided in the downstream headings. But I will do whichever you prefer.", "author": "TovaCohen", "createdAt": "2020-05-14T16:21:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI0NDg3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY3NTMwOA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425675308", "bodyText": "So I'm not 100% up-to-date on the latest state of this discussion, but my preference would definitely be on the shorter, more concise captions in the upstream ToC on the right-hand side of docs. If that gets enabled by this PR, I'm +1 on that.", "author": "gunnarmorling", "createdAt": "2020-05-15T09:20:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI0NDg3OA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwNTQzNw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r424905437", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A Debezium database change event has a complex structure that provides a wealth of information. Kafka records that convey Debezium change events contain all of this information. \n          \n          \n            \n            A Debezium data change event has a complex structure that provides a wealth of information. Kafka records that convey Debezium change events contain all of this information. \n          \n      \n    \n    \n  \n\nWe usually use \"data change event\", because it's more data that changed, not so much the database itself. Also \"Debezium\" should be {productname} probably?", "author": "gunnarmorling", "createdAt": "2020-05-14T06:45:57Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -10,18 +15,56 @@ toc::[]\n \n [NOTE]\n ====\n-This SMT is supported only for the SQL database connectors, it does not work with the MongoDB connector.\n-See xref:configuration/mongodb-event-flattening.adoc[here] for the MongoDB equivalent to this SMT.\n+This single message transformation (SMT) is supported for only the SQL database connectors. For the MongoDB connector, see the xref:configuration/mongodb-event-flattening.adoc[documentation for the MongoDB equivalent to this SMT].\n ====\n+endif::community[]\n+\n+ifdef::product[]\n+[id=\"extracting-source-record-after-state-from-debezium-change-events\"]\n+= Extracting source record `after` state from Debezium change events\n+endif::product[]\n+\n+A Debezium database change event has a complex structure that provides a wealth of information. Kafka records that convey Debezium change events contain all of this information. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg1ODc4NQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425858785", "bodyText": "Sure. I'll make sure the doc uses \"data change event\".\nAs for replacing instances of Debezium with {productname} -- In the upstream doc, I have searched for but am not finding an AsciiDoc attribute specification for the product name. Many doc source files use {prodname} and the published doc does indeed render \"Debezium\" but I cannot see where this is coming from. Can someone point me to the declaration?\nAlso, downstream, we set :prodname: to Integration. I can override that for the downstream Debezium books and set :prodname: to Debezium for just those two books, or we can use an attribute with a different name upstream and then have no collision downstream. Downstream, there is already a definition of:\n:ProductName: Debezium\nDo you want to use that upstream?", "author": "TovaCohen", "createdAt": "2020-05-15T14:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwNTQzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4Njk1Mw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425886953", "bodyText": "I believe what Gunnar was referring to was using {prodname}.\nWe have a few attributes which we define as global and relatively static in that they'd be used across all modules of a given playbook.  Our antora playbook files are maintained in a different repository, see debezium.github.io.\nIn that repository we define the following attributes:\n# Global asciidoc attributes here, used across all versions of documentation\nasciidoc:\n  attributes:\n    prodname: 'Debezium'\n    context: 'debezium'\n    jira-url: 'https://issues.redhat.com'\n    # because of how handlebars templates work with page.attributes, this must be prefixed with \"page-\"\n    page-copyright-year: '2020'", "author": "Naros", "createdAt": "2020-05-15T15:41:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwNTQzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkyMDExMQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425920111", "bodyText": "Good to know! Thanks. We'll continue to use prodname downstream then", "author": "TovaCohen", "createdAt": "2020-05-15T16:38:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwNTQzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwODgwNg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r424908806", "bodyText": "Ah, so that touches on an interesting point. We show these examples in a properties file syntax which only can be taken as-is in Kafka Connect's \"standalone mode\". Mostly though, the \"distributed mode\" is used, which is configured via as REST API, thus using JSON syntax. To further complexify things, in the downstream product, connectors are configured via the AMQ Streams operator, thusing YAML syntax.\nThat's why I think we should avoid using the notion of \"properties files\", as that's only one of the three (and I'd argue the least commonly used one) ways of making use of these properties. Using the properties syntax just as the common way for showing these things is IMO fine; if you wanted to go the extra mile, you could think about using YAML for downstream.", "author": "gunnarmorling", "createdAt": "2020-05-14T06:53:37Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. \n+\n+You can configure the transformation to do any of the following: \n+\n+* Add metadata from the change event to the simplified Kafka record or record header. The default behavior is that the SMT does not add metadata to the simplified Kafka record.\n \n-The SMT provides three main functions.\n-It\n+* Keep Kafka records that contain change events for `DELETE` operations in the stream. The default behavior is that the SMT drops Kafka records for `DELETE` operation change events because most consumers cannot yet handle them. \n \n-* extracts the `after` field from change events and replaces the original event just with this part\n-* optionally filters delete and tombstone records, as per the capabilities and requirements of downstream consumers\n-* optionally adds metadata fields from the change event to the outgoing flattened record\n-* optionally add metadata fields to the header\n+A database `DELETE` operation causes Debezium to generate two Kafka records: \n \n-The SMT can be applied either to a source connector (Debezium) or a sink connector.\n-We generally recommend to apply the transformation on the sink side as it means that the messages stored in Apache Kafka will contain the whole context.\n-The final decision depends on use case for each user.\n+* A record that contains `\"op\": \"d\",` the `before` row data, and some other fields.\n+* A tombstone record that has the same key as the deleted row and a value of `null`. This record is a marker for Apache Kafka. It indicates that \n+link:https://kafka.apache.org/documentation/#compaction[log compaction] can remove all records that have this key. \n \n+Instead of dropping the record that contains the `before` row data, you can configure the `ExtractNewRecordData` SMT to do one of the following: \n+\n+* Keep the record in the stream and edit it to have only the `\"value\": \"null\"` field.\n+ \n+* Keep the record in the stream and edit it to have a `value` field that contains the key/value pairs that were in the `before` field with an added `\"__deleted\": \"true\"` entry.\n+\n+Similary, instead of dropping the tombstone record, you can configure the `ExtractNewRecordData` SMT to keep the tombstone record in the stream. \n+\n+ifdef::community[]\n == Configuration\n-The configuration is a part of source/sink task connector and is expressed in a set of properties:\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"configuration-of-extractnewrecordstate-transformation\"]\n+== Configuration of `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+Configure the Debezium `ExtractNewRecordState` SMT in a Kafka Connect source or sink connector `.properties` file. To obtain the default behavior, specify something like the following: ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgyMTgzNg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425821836", "bodyText": "Ahh, thanks! How's this:\nConfigure the {prodname} ExtractNewRecordStateSMT in a Kafka Connect source or sink connector. Add the SMT configuration details to your connector's configuration. To obtain the default behavior, in a.propertiesfile, you would specify something like the following: ...\nAnd of course I will update the doc for both SMTs with this kind of wording. And I'll look at all instances of .properties and adjust as needed.\nAs for .yml examples, I looked in debezium-examples but I am not finding an example .yml configuration of an SMT. If you can point me to an example, I can use that to create .yml examples in the doc.", "author": "TovaCohen", "createdAt": "2020-05-15T14:00:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzODg3MA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425838870", "bodyText": "Somewhat related, I found an extension that allows us to group back-to-back source blocks into a tabbed panel view which I believe fits nicely into the concept where we have a code block for how the properties file would look, one for json, and another for yaml.\n@TovaCohen, can you confirm whether using a role attribute on source blocks would have any negative impacts on downstream, e.g.:\n[source,properties,role=\"primary\"]\n----\nkey=value\n----\n\n[source,json,role=\"secondary\"]\n----\n{ \"key\": \"value\" }\n----", "author": "Naros", "createdAt": "2020-05-15T14:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkyMjEwMQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425922101", "bodyText": "I'm looking into this. Sounds helpful.", "author": "TovaCohen", "createdAt": "2020-05-15T16:41:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTQ3OQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r426701479", "bodyText": "Awesome.  I don't think its something to hold-up the PR now, we can always introduce whatever extension to support this later both up- and down- stream to make this flow better as a follow-up.", "author": "Naros", "createdAt": "2020-05-18T15:15:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwODgwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwOTMxNA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r424909314", "bodyText": "Technically, the header is part of the Kafka record. Perhaps \"Kafka record value or its header\"?", "author": "gunnarmorling", "createdAt": "2020-05-14T06:54:46Z", "path": "documentation/modules/ROOT/pages/configuration/event-flattening.adoc", "diffHunk": "@@ -55,114 +101,247 @@ The downside of using the complex format is that other connectors or other parts\n }\n ----\n \n-Debezium provides {link-kafka-docs}/#connect_transforms[a single message transformation] that crosses the bridge between the complex and simple formats, the https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[ExtractNewRecordState] SMT.\n+To provide the needed Kafka record format for consumers, configure the `ExtractNewRecordState` SMT.\n+\n+ifdef::community[]\n+== Behavior\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"behavior-of-debezium-extractnewrecordstate-transformation\"]\n+== Behavior of Debezium `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+link:https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/ExtractNewRecordState.java[The `ExtractNewRecordState` SMT] extracts the `after` field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its `after` field to create a simple Kafka record. \n+\n+You can configure the `ExtractNewRecordState` SMT for a Debezium connector, that is, for a source connector, or for a sink connector. The advantage of configuring `ExtractNewRecordState` for a sink connector is that records stored in Apache Kafka contain whole Debezium change events. The decision to apply the SMT to a source or sink connector depends on your particular use case. \n+\n+You can configure the transformation to do any of the following: \n+\n+* Add metadata from the change event to the simplified Kafka record or record header. The default behavior is that the SMT does not add metadata to the simplified Kafka record.\n \n-The SMT provides three main functions.\n-It\n+* Keep Kafka records that contain change events for `DELETE` operations in the stream. The default behavior is that the SMT drops Kafka records for `DELETE` operation change events because most consumers cannot yet handle them. \n \n-* extracts the `after` field from change events and replaces the original event just with this part\n-* optionally filters delete and tombstone records, as per the capabilities and requirements of downstream consumers\n-* optionally adds metadata fields from the change event to the outgoing flattened record\n-* optionally add metadata fields to the header\n+A database `DELETE` operation causes Debezium to generate two Kafka records: \n \n-The SMT can be applied either to a source connector (Debezium) or a sink connector.\n-We generally recommend to apply the transformation on the sink side as it means that the messages stored in Apache Kafka will contain the whole context.\n-The final decision depends on use case for each user.\n+* A record that contains `\"op\": \"d\",` the `before` row data, and some other fields.\n+* A tombstone record that has the same key as the deleted row and a value of `null`. This record is a marker for Apache Kafka. It indicates that \n+link:https://kafka.apache.org/documentation/#compaction[log compaction] can remove all records that have this key. \n \n+Instead of dropping the record that contains the `before` row data, you can configure the `ExtractNewRecordData` SMT to do one of the following: \n+\n+* Keep the record in the stream and edit it to have only the `\"value\": \"null\"` field.\n+ \n+* Keep the record in the stream and edit it to have a `value` field that contains the key/value pairs that were in the `before` field with an added `\"__deleted\": \"true\"` entry.\n+\n+Similary, instead of dropping the tombstone record, you can configure the `ExtractNewRecordData` SMT to keep the tombstone record in the stream. \n+\n+ifdef::community[]\n == Configuration\n-The configuration is a part of source/sink task connector and is expressed in a set of properties:\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: concept\n+[id=\"configuration-of-extractnewrecordstate-transformation\"]\n+== Configuration of `ExtractNewRecordState` transformation\n+endif::product[]\n+\n+Configure the Debezium `ExtractNewRecordState` SMT in a Kafka Connect source or sink connector `.properties` file. To obtain the default behavior, specify something like the following: \n+\n+[source]\n+----\n+transforms=unwrap,...\n+transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n+----\n+\n+As in any Kafka Connect connector `.properties` file, you can set `transforms=` to multiple, comma-separated, SMT aliases in the order in which you want Kafka Connect  to apply the SMTs. \n+\n+The following example sets several `ExtractNewRecordState` options: \n \n [source]\n ----\n transforms=unwrap,...\n transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState\n transforms.unwrap.drop.tombstones=false\n transforms.unwrap.delete.handling.mode=rewrite\n-transforms.unwrap.add.source.fields=table,lsn\n+transforms.unwrap.add.fields=table,lsn\n ----\n \n-=== Record filtering for delete records\n-\n-The SMT provides a special handling for events that signal a `delete` operation.\n-When a `DELETE` is executed on a datasource then Debezium generates two events:\n+`drop-tombstones=false`:: Keeps tombstone records for `DELETE` operations in the event stream. \n \n-* a record with `d` operation that contains only old row data\n-* (optionally) a record with `null` value and the same key (a \"tombstone\" message). This record serves as a marker for Apache Kafka that all messages with this key can be removed from the topic during {link-kafka-docs}/#compaction[log compaction].\n+`delete-handling-mode=rewrite`:: For `DELETE` operations, edits the Kafka record by flattening the `value` field that was in the change event. The `value` field directly contains the key/value pairs that were in the `before` field. The SMT adds `__deleted` and sets it to `true`, for example:   \n++\n+----\n+\"value\": {\n+  \"pk\": 2,\n+  \"cola\": null,\n+  \"__deleted\": \"true\"\n+}\n+----\n \n-Upon processing these two records, the SMT can pass on the `d` record as is,\n-convert it into another tombstone record or drop it.\n-The original tombstone message can be passed on as is or also be dropped.\n+`add.fields=table,lsn`:: Adds change event metadata for the `table` and `lsn` fields to the simplified Kafka record. \n \n-[NOTE]\n-====\n-The SMT by default filters out *both* delete records as widely used sink connectors do not support handling of tombstone messages at this point.\n-====\n+ifdef::community[]\n+== Adding metadata\n+endif::community[]\n+ \n+ifdef::product[]\n+// Type: concept\n+[id=\"example-of-adding-metadata-to-the-kafka-record-or-its-header\"]\n+== Example of adding metadata to the Kafka record or its header\n+endif::product[]\n \n-=== Adding metadata fields to the message\n+The `ExtractNewRecordState` SMT can add original, change event metadata to the simplified Kafka record or its header. For example, you might want the simplified record or its header to contain any of the following: ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTE2Ng==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425885166", "bodyText": "I updated the content to reflect this. This update will be in my next commit, shortly.", "author": "TovaCohen", "createdAt": "2020-05-15T15:38:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkwOTMxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r424910646", "bodyText": "Did that go somewhere else? I just added that one recently as it's an important usage for this SMT.", "author": "gunnarmorling", "createdAt": "2020-05-14T06:57:41Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -7,46 +12,64 @@ include::../_attributes.adoc[]\n :source-highlighter: highlight.js\n \n toc::[]\n+endif::community[]\n+\n+ifdef::product[]\n+[id=\"routing-change-event-records-to-topics-that-you-specify\"]\n+= Routing change event records to topics that you specify\n+endif::product[]\n+\n+Each Kafka record that contains a database change event has a default destination topic. If you need to, you can re-route records to topics that you specify before the records reach the Kafka Connect converter. \n+To do this, Debezium provides the `ByLogicalTableRouter` single message transformation (SMT). Configure this transformation in the Debezium connector's Kafka Connect `.properties` file. Configuration options enable you to specify the following: \n \n-Debezium enables you to re-route the emitted change before the message reaches the converter using a single message transformation,\n-or {link-kafka-docs}/#connect_transforms[SMT].\n-The SMT provided by Debezium enables you to rewrite the topic and the key according to a regular expression and a replacement pattern,\n-configurable per instance of Debezium.\n+* An expression for identifying the records to re-route\n+* An expression that resolves to the destination topic\n+* How to ensure a unique key among the records being re-routed to the destination topic\n \n-The implementation does not care about the sanity of the change, this is in the responsibility of the user.\n+It is up to you to ensure that the transformation configuration provides the behavior that you want. Debezium does not validate the behavior that results from your configuration of the transformation. \n \n-== Use-Cases\n+The `ByLogicalTableRouter` transformation is a \n+link:https://kafka.apache.org/documentation/#connect_transforms[Kafka Connect SMT].\n \n-=== Logical Tables\n+ifdef::product[]\n+The following topics provide details: \n \n-A logical table consists of one or more physical tables with the same table structure.\n-A common use case is sharding, where for example two physical tables `db_shard1.my_table` and `db_shard2.my_table` together form one logical table.\n+* xref:use-case-for-routing-records-to-topics-that-you-specify[]\n+* xref:example-of-routing-records-for-multiple-tables-to-one-topic[]\n+* xref:ensuring-unique-keys-across-records-routed-to-the-same-topic[]\n+endif::product[]\n \n-Typically the physical tables share the same schema.\n+ifdef::community[]\n+== Use case\n+endif::community[]\n \n-Normally, Debezium connectors send each change event to a topic that is named by the database and table.\n-But since the sharded tables have the same schema, we'd instead like to re-route each change event to a topic named by the _logical_ table name.\n-This way, all changes events for any of the shards all go to the same topic.\n+ifdef::product[]\n+// Type: concept\n+[id=\"use-case-for-routing-records-to-topics-that-you-specify\"]\n+== Use case for routing records to topics that you specify\n+endif::product[]\n \n-What happens if each physical table has a primary key that is only unique within that table?\n-In this case, a row in shard 1 can have the same primary key as a row in shard 2.\n-Since Debezium events are keyed by the columns that make up the primary key, the events for that row in shard 1 would have the same key as the row in shard 2,\n-even though globally they are different rows.\n-So, in addition to changing the topic name, we may also want to _modify the event key_ to add a field that makes the key globally unique.\n+The default behavior is that a Debezium connector sends each change event record to a topic whose name is formed from the name of the database and the name of the table in which the change was made. In other words, a topic receives records for one physical table. When you want a topic to receive records for more than one physical table, you must configure the Debezium connector to re-route the records to that topic. \n \n-This SMT lets you specify how you want to choose the new topic name and then specify how to modify the change event key to ensure it is globally unique.\n+A logical table is a common use case for routing records for multiple physical tables to one topic. In a logical table, there are multiple physical tables that all have the same schema. For example, sharded tables have the same schema. A logical table might consist of two or more sharded tables: `db_shard1.my_table` and `db_shard2.my_table`. The tables are in different shards and are physically distinct but together they form a logical table. \n+You can re-route change event records for tables in any of the shards to the same topic.\n \n-=== Partitioned Postgres Tables", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4OTk2MQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425789961", "bodyText": "This content was not in the source file I worked on. I was pretty good about running git pull upstream master, but I missed it. Then when there were merge conflicts, Chris asked me to resolve them, and I had to make a choice between this content and my updated content. Looking back, I should have asked someone about this content since I had never seen it before. Instead, I chose my updated content thinking that perhaps this content was old. So certainly my error. In the future, I will ask before I resolve a conflict with content I have not seen before.\nHow can I know when someone adds new content to a file that I am working in?\nAnd since I am working on files in the master branch, how would I know if someone adds content that is only for, for example 1.2 when I am working toward a merge to 1.1?  I'm concerned about a situation where you add content for only 1.2 and I update that 1.2 content to fix a typo or clarify a sentence. Does my update to the 1.2 content get merged to 1.1 along with my other updates in that file?", "author": "TovaCohen", "createdAt": "2020-05-15T13:09:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgyNDYyOQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425824629", "bodyText": "How can I know when someone adds new content to a file that I am working in?\n\nThe most obvious is when you get merge conflicts, but if git is able to fast-forward, you could always check the commit history after your rebase to see what changes may have been recently applied that git was able to fast-forward.\n\nAnd since I am working on files in the master branch, how would I know if someone adds content that is only for, for example 1.2 when I am working toward a merge to 1.1?\n\nI would say one of the easiest ways would be to review the file's git history and lookup the associated issue number.  If that's not enough, feel free to ask about any discrepency.\n\nI'm concerned about a situation where you add content for only 1.2 and I update that 1.2 content to fix a typo or clarify a sentence. Does my update to the 1.2 content get merged to 1.1 along with my other updates in that file?\n\nNo.\nThis process heavily depends on what changes need to be made and how those changes are to be applied to a given branch.  If those changes are 1.1 specific but can be applied in a version agnostic way, then a cherry-pick would suffice for 1.1 from master.  But if the changes are intertwined with version-specific features, you may end up creating multiple pull requests each designed to target a specific branch.", "author": "Naros", "createdAt": "2020-05-15T14:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkyMzMzMw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425923333", "bodyText": "Thanks very much for the info. Now I have some alternatives for what to do for a merge conflict. Let me know if there is anything I can do to get the missing postgresql partition content back.", "author": "TovaCohen", "createdAt": "2020-05-15T16:43:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMzQ1MA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r426703450", "bodyText": "I think the best way might be to check-out master, make a copy topic-routing.adoc and then re-compare it with what you've done in your topic branch; adding back in what Gunnar mentioned was missing.  Once that's done I can give this a final review and merge.", "author": "Naros", "createdAt": "2020-05-18T15:18:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMTMyNg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r426711326", "bodyText": "@Naros  - is this the content that you just referred to in chat? If yes, I don't know what I need to do.", "author": "TovaCohen", "createdAt": "2020-05-18T15:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc1MDg3Nw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r426750877", "bodyText": "I just pushed a commit that returns the PostgreSQL partitioned table content to topic-routing.adoc.", "author": "TovaCohen", "createdAt": "2020-05-18T16:27:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMDY0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMTkwMw==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r424911903", "bodyText": "Hum, hum. So we used to have tables for all options, across all documentation pages (connectors and SMTs). Using this different representation here definitely add some kind of inconsistency. Why is it you think that style is better (open to discuss, but ideally we'd apply this change consistently then).", "author": "gunnarmorling", "createdAt": "2020-05-14T07:00:27Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -56,79 +79,61 @@ transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n transforms.Reroute.topic.replacement=$1customers_all_shards\n ----\n \n-The configuration above will match topics such as `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc. and replace it with `myserver.mydb.customers_all_shards.`\n+`topic.regex`:: Specifies a regular expression that the transformation applies to each change event record to determine if it should be routed to a particular topic.  \n++\n+In the example, the regular expression, `(.*)customers_shard(.*)` matches records for changes to tables whose names include the `customers_shard` string. This would re-route records for tables with the following names:\n++\n+`myserver.mydb.customers_shard1` +\n+`myserver.mydb.customers_shard2` +\n+`myserver.mydb.customers_shard3`\n \n-== Key Fields\n+`topic.replacement`:: Specifies a regular expression that represents the destination topic name. The transformation routes each matching record to the topic identified by this expression. In this example, records for the three sharded tables listed above would be routed to the `myserver.mydb.customers_all_shards` topic. \n \n-To address the concern of uniqueness across all the original tables discussed above,\n-one more field for identifying the original (physical) table may be inserted into the key structure of the change events.\n-By default, this field is named `\\__dbz__physicalTableIdentifier` and has the original topic name as its value.\n+ifdef::community[]\n+== Ensure unique key\n+endif::community[]\n \n-If your tables already contain globally unique keys and you do not need to change the key structure, you can set the `key.enforce.uniqueness` property to `false`:\n+ifdef::product[]\n+// Type: procedure\n+[id=\"ensuring-unique-keys-across-records-routed-to-the-same-topic\"]\n+== Ensuring unique keys across records routed to the same topic\n+endif::product[]\n \n-[source]\n-----\n-...\n-transforms.Reroute.key.enforce.uniqueness=false\n-...\n-----\n+A Debezium change event key uses the table columns that make up the table's primary key. To route records for multiple physical tables to one topic, the event key must be unique across all of those tables. However, it is possible for each physical table to have a primary key that is unique within only that table. For example, a row in the `myserver.mydb.customers_shard1` table might have the same key value as a row in the `myserver.mydb.customers_shard2` table. \n+\n+To ensure that each event key is unique across the tables whose change event records go to the same topic, the `ByLogicalTableRouter` transformation inserts a field into change event keys. By default, the name of the inserted field is `+__dbz__physicalTableIdentifier+`. The value of the inserted field is the default destination topic name.\n \n-If needed, another _field name_ can be chosen by means of the `key.field.name` property\n-(obviously you'll want to choose a field name that doesn't clash with existing primary key fields).\n-For example the following configuration will use the name `shard_id` for the key field:\n+If you want to, you can configure the `ByLogicalTableRouter` transformation to insert a different field into the key. To do this, specify the `key.field.name` option and set it to a field name that does not clash with existing primary key field names. For example: \n \n [source]\n ----\n-...\n+transforms=Reroute\n+transforms.Reroute.type=io.debezium.transforms.ByLogicalTableRouter\n+transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n+transforms.Reroute.topic.replacement=$1customers_all_shards\n transforms.Reroute.key.field.name=shard_id\n-...\n ----\n \n-The _value_ of the field can be adjusted via the `key.field.regex` and `key.field.replacement` properties.\n-The former allows you to define a regular expression that will be applied to the original topic name to capture one or more groups of characters.\n-The latter lets you specify an expression that defines the value for the field in terms of those captured groups.\n-For example:\n+This example adds the `shard_id` field to the key structure in routed records.\n+\n+If you want to adjust the value of the key's new field, configure both of these options:\n+\n+`key.field.regex`:: Specifies a regular expression that the transformation applies to the default destination topic name to capture one or more groups of characters. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4MDQ1OA==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r425780458", "bodyText": "The configuration options table for ByLogicalTableRouter was inadvertently missing from the source file I worked on. It is now back - I added it with commit bb1bd31. I absolutely agree that the reference tables are useful.", "author": "TovaCohen", "createdAt": "2020-05-15T12:53:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxMTkwMw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0NTUxNg==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r427445516", "bodyText": "Incorrect Syntax:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |{link-prefix}:{link-topic-routing}by-logical-table-router-topic-replacement#[`topic.replacement`]\n          \n          \n            \n            |{link-prefix}:{link-topic-routing}#by-logical-table-router-topic-replacement[`topic.replacement`]", "author": "Naros", "createdAt": "2020-05-19T16:42:03Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -56,79 +84,112 @@ transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n transforms.Reroute.topic.replacement=$1customers_all_shards\n ----\n \n-The configuration above will match topics such as `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc. and replace it with `myserver.mydb.customers_all_shards.`\n+`topic.regex`:: Specifies a regular expression that the transformation applies to each change event record to determine if it should be routed to a particular topic.  \n++\n+In the example, the regular expression, `(.*)customers_shard(.*)` matches records for changes to tables whose names include the `customers_shard` string. This would re-route records for tables with the following names:\n++\n+`myserver.mydb.customers_shard1` +\n+`myserver.mydb.customers_shard2` +\n+`myserver.mydb.customers_shard3`\n \n-== Key Fields\n+`topic.replacement`:: Specifies a regular expression that represents the destination topic name. The transformation routes each matching record to the topic identified by this expression. In this example, records for the three sharded tables listed above would be routed to the `myserver.mydb.customers_all_shards` topic. \n \n-To address the concern of uniqueness across all the original tables discussed above,\n-one more field for identifying the original (physical) table may be inserted into the key structure of the change events.\n-By default, this field is named `\\__dbz__physicalTableIdentifier` and has the original topic name as its value.\n+ifdef::community[]\n+== Ensure unique key\n+endif::community[]\n \n-If your tables already contain globally unique keys and you do not need to change the key structure, you can set the `key.enforce.uniqueness` property to `false`:\n+ifdef::product[]\n+// Type: procedure\n+[id=\"ensuring-unique-keys-across-records-routed-to-the-same-topic\"]\n+== Ensuring unique keys across records routed to the same topic\n+endif::product[]\n \n-[source]\n-----\n-...\n-transforms.Reroute.key.enforce.uniqueness=false\n-...\n-----\n+A {prodname} change event key uses the table columns that make up the table's primary key. To route records for multiple physical tables to one topic, the event key must be unique across all of those tables. However, it is possible for each physical table to have a primary key that is unique within only that table. For example, a row in the `myserver.mydb.customers_shard1` table might have the same key value as a row in the `myserver.mydb.customers_shard2` table. \n+\n+To ensure that each event key is unique across the tables whose change event records go to the same topic, the `ByLogicalTableRouter` transformation inserts a field into change event keys. By default, the name of the inserted field is `+__dbz__physicalTableIdentifier+`. The value of the inserted field is the default destination topic name.\n \n-If needed, another _field name_ can be chosen by means of the `key.field.name` property\n-(obviously you'll want to choose a field name that doesn't clash with existing primary key fields).\n-For example the following configuration will use the name `shard_id` for the key field:\n+If you want to, you can configure the `ByLogicalTableRouter` transformation to insert a different field into the key. To do this, specify the `key.field.name` option and set it to a field name that does not clash with existing primary key field names. For example: \n \n [source]\n ----\n-...\n+transforms=Reroute\n+transforms.Reroute.type=io.debezium.transforms.ByLogicalTableRouter\n+transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n+transforms.Reroute.topic.replacement=$1customers_all_shards\n transforms.Reroute.key.field.name=shard_id\n-...\n ----\n \n-The _value_ of the field can be adjusted via the `key.field.regex` and `key.field.replacement` properties.\n-The former allows you to define a regular expression that will be applied to the original topic name to capture one or more groups of characters.\n-The latter lets you specify an expression that defines the value for the field in terms of those captured groups.\n-For example:\n+This example adds the `shard_id` field to the key structure in routed records.\n+\n+If you want to adjust the value of the key's new field, configure both of these options:\n+\n+`key.field.regex`:: Specifies a regular expression that the transformation applies to the default destination topic name to capture one or more groups of characters. \n+\n+`key.field.replacement`:: Specifies a regular expression for determining the value of the inserted key field in terms of those captured groups. \n+\n+For example: \n \n [source]\n ----\n-...\n transforms.Reroute.key.field.regex=(.*)customers_shard(.*)\n transforms.Reroute.key.field.replacement=$2\n ----\n \n-This will apply the given regular expression to original topic names and use the second capturing group as value for the key field.\n-Assuming the source topics are named `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc., the key field's values would be `1`, `2` etc.\n+With this configuration, suppose that the default destination topic names are: \n+\n+`myserver.mydb.customers_shard1` +\n+`myserver.mydb.customers_shard2` +\n+`myserver.mydb.customers_shard3`\n+\n+The transformation uses the values in the second captured group, the shard numbers, as the value of the key's new field. In this example, the inserted key field's values would be `1`, `2`, or `3`.\n \n+ifdef::community[]\n [[configuration-options]]\n-== Configuration Options\n-[cols=\"35%a,10%a,55%a\",options=\"header\"]\n-|=======================\n+== Configuration options\n+endif::community[]\n+\n+ifdef::product[]\n+// Type: reference\n+[id=\"options-for-configuring-bylogicaltablerouter-transformation\"]\n+== Options for configuring `ByLogicalTableRouter` transformation\n+endif::product[]\n+\n+[cols=\"35%a,10%a,55%a\"]\n+|===\n |Property\n |Default\n |Description\n \n-|`topic.regex`\n+[id=\"by-logical-table-router-topic-regex\"]\n+|{link-prefix}:{link-topic-routing}#by-logical-table-router-topic-regex[`topic.regex`]\n |\n-|A regular expression for matching the topic(s) which should be re-routed.\n+|Specifies a regular expression that the transformation applies to each change event record to determine if it should be routed to a particular topic.\n \n-|`topic.replacement`\n+[id=\"by-logical-table-router-topic-replacement\"]\n+|{link-prefix}:{link-topic-routing}by-logical-table-router-topic-replacement#[`topic.replacement`]", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1MDE0NQ==", "url": "https://github.com/debezium/debezium/pull/1493#discussion_r427450145", "bodyText": "Was the example configuration for transforms.Reroute.key.enforce.uniqueness and its descriptive paragraph intended to be omitted?  This is the specific section from master:\nIf your tables already contain globally unique keys and you do not need to change the key structure, you can set the `key.enforce.uniqueness` property to `false`:\n\n[source]\n----\n...\ntransforms.Reroute.key.enforce.uniqueness=false\n...\n----", "author": "Naros", "createdAt": "2020-05-19T16:49:16Z", "path": "documentation/modules/ROOT/pages/configuration/topic-routing.adoc", "diffHunk": "@@ -56,79 +84,112 @@ transforms.Reroute.topic.regex=(.*)customers_shard(.*)\n transforms.Reroute.topic.replacement=$1customers_all_shards\n ----\n \n-The configuration above will match topics such as `myserver.mydb.customers_shard1`, `myserver.mydb.customers_shard2` etc. and replace it with `myserver.mydb.customers_all_shards.`\n+`topic.regex`:: Specifies a regular expression that the transformation applies to each change event record to determine if it should be routed to a particular topic.  \n++\n+In the example, the regular expression, `(.*)customers_shard(.*)` matches records for changes to tables whose names include the `customers_shard` string. This would re-route records for tables with the following names:\n++\n+`myserver.mydb.customers_shard1` +\n+`myserver.mydb.customers_shard2` +\n+`myserver.mydb.customers_shard3`\n \n-== Key Fields\n+`topic.replacement`:: Specifies a regular expression that represents the destination topic name. The transformation routes each matching record to the topic identified by this expression. In this example, records for the three sharded tables listed above would be routed to the `myserver.mydb.customers_all_shards` topic. \n \n-To address the concern of uniqueness across all the original tables discussed above,\n-one more field for identifying the original (physical) table may be inserted into the key structure of the change events.\n-By default, this field is named `\\__dbz__physicalTableIdentifier` and has the original topic name as its value.\n+ifdef::community[]\n+== Ensure unique key\n+endif::community[]\n \n-If your tables already contain globally unique keys and you do not need to change the key structure, you can set the `key.enforce.uniqueness` property to `false`:\n+ifdef::product[]\n+// Type: procedure\n+[id=\"ensuring-unique-keys-across-records-routed-to-the-same-topic\"]\n+== Ensuring unique keys across records routed to the same topic\n+endif::product[]\n \n-[source]\n-----\n-...\n-transforms.Reroute.key.enforce.uniqueness=false\n-...\n-----\n+A {prodname} change event key uses the table columns that make up the table's primary key. To route records for multiple physical tables to one topic, the event key must be unique across all of those tables. However, it is possible for each physical table to have a primary key that is unique within only that table. For example, a row in the `myserver.mydb.customers_shard1` table might have the same key value as a row in the `myserver.mydb.customers_shard2` table. \n+\n+To ensure that each event key is unique across the tables whose change event records go to the same topic, the `ByLogicalTableRouter` transformation inserts a field into change event keys. By default, the name of the inserted field is `+__dbz__physicalTableIdentifier+`. The value of the inserted field is the default destination topic name.\n \n-If needed, another _field name_ can be chosen by means of the `key.field.name` property\n-(obviously you'll want to choose a field name that doesn't clash with existing primary key fields).\n-For example the following configuration will use the name `shard_id` for the key field:\n+If you want to, you can configure the `ByLogicalTableRouter` transformation to insert a different field into the key. To do this, specify the `key.field.name` option and set it to a field name that does not clash with existing primary key field names. For example: ", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ba020436a1eea49659544114c75478acdc4679ea", "url": "https://github.com/debezium/debezium/commit/ba020436a1eea49659544114c75478acdc4679ea", "message": "DBZ-2021 Updated SMT doc for topic routing and event flattening", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "dfb9f1bb62174e187aaa0aee7a2d51ae6753ec3a", "url": "https://github.com/debezium/debezium/commit/dfb9f1bb62174e187aaa0aee7a2d51ae6753ec3a", "message": "DBZ-2021 Completed revision of doc for topic routing and event flattening", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "fe9319092347e7498aa594dc5fb50f22582b8e32", "url": "https://github.com/debezium/debezium/commit/fe9319092347e7498aa594dc5fb50f22582b8e32", "message": "DBZ-2021 Updates some anchor IDs to avoid duplication problem downstream", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "1878804fefecedd93ee7f6fa62ec05a510541086", "url": "https://github.com/debezium/debezium/commit/1878804fefecedd93ee7f6fa62ec05a510541086", "message": "DBZ-2021 Corrected the description of the default name of the inserted key field", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "184a8f449d2abd456356c5d211ea6a84bd9748b1", "url": "https://github.com/debezium/debezium/commit/184a8f449d2abd456356c5d211ea6a84bd9748b1", "message": "DBZ-2021 Clarifies default name of inserted key field", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "d4b522a63c6587c279653015b2b624fa8fca34c7", "url": "https://github.com/debezium/debezium/commit/d4b522a63c6587c279653015b2b624fa8fca34c7", "message": "DBZ-2021 Updates based on PR review comments", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "d26045f20e33a2c644ea6f5f0d0aaa4d65749f7b", "url": "https://github.com/debezium/debezium/commit/d26045f20e33a2c644ea6f5f0d0aaa4d65749f7b", "message": "DBZ-2021 Clarified config examples and record headers. Changed SMT config option anchor IDs.", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "d2bbf84050bfba09b38b8d9f88c8f1e1d7e3d193", "url": "https://github.com/debezium/debezium/commit/d2bbf84050bfba09b38b8d9f88c8f1e1d7e3d193", "message": "DBZ-2021 Re-added, the missing PostgreSQL partitioned table content", "committedDate": "2020-05-19T16:57:36Z", "type": "commit"}, {"oid": "443576861313899f123de412bff08df50d97abbd", "url": "https://github.com/debezium/debezium/commit/443576861313899f123de412bff08df50d97abbd", "message": "DBZ-2021 Moved # to correct place. Returned example of no key field added.", "committedDate": "2020-05-19T17:05:01Z", "type": "commit"}, {"oid": "443576861313899f123de412bff08df50d97abbd", "url": "https://github.com/debezium/debezium/commit/443576861313899f123de412bff08df50d97abbd", "message": "DBZ-2021 Moved # to correct place. Returned example of no key field added.", "committedDate": "2020-05-19T17:05:01Z", "type": "forcePushed"}]}