{"pr_number": 10383, "pr_title": "[Storage][Jumbo blobs] Bump single upload max threshold. Preserve defaults.", "pr_createdAt": "2020-04-20T22:40:11Z", "pr_url": "https://github.com/Azure/azure-sdk-for-java/pull/10383", "timeline": [{"oid": "59c78b217a5f3bcd188b051e0d8a3b54f5e6ba9b", "url": "https://github.com/Azure/azure-sdk-for-java/commit/59c78b217a5f3bcd188b051e0d8a3b54f5e6ba9b", "message": "change versioned account for tests variable names", "committedDate": "2020-04-13T16:37:13Z", "type": "commit"}, {"oid": "cbfa86af23cf366fc9904fa86040a473c042ef75", "url": "https://github.com/Azure/azure-sdk-for-java/commit/cbfa86af23cf366fc9904fa86040a473c042ef75", "message": "Merge remote-tracking branch 'upstream/feature/storage/stg73' into feature/storage/stg73", "committedDate": "2020-04-20T15:00:42Z", "type": "commit"}, {"oid": "ded73b52c0c044bbe0500fe587b84b68c4d06a25", "url": "https://github.com/Azure/azure-sdk-for-java/commit/ded73b52c0c044bbe0500fe587b84b68c4d06a25", "message": "[Storage][Jumbo blobs] Bump single upload threshold. Preserve current default single upload.", "committedDate": "2020-04-20T22:36:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411749479", "bodyText": "Is this TODO meant to be part of this PR?", "author": "gapra-msft", "createdAt": "2020-04-20T23:04:13Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -70,6 +77,84 @@ class LargeBlobTest extends APISpec {\n         blockList.committedBlocks.get(0).getSizeLong() == maxBlockSize\n     }\n \n+    @Requires({ liveMode() })\n+    @Ignore(\"Takes really long time\")\n+    // This test sends payload over the wire\n+    def \"Upload Real Large Blob in Single Upload\"() {\n+        given:\n+        // TODO (kasobol-msft) Bump this to 5000MB.", "originalCommit": "ded73b52c0c044bbe0500fe587b84b68c4d06a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTk1NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411749955", "bodyText": "And do we need to modify the tests for datalake too? Could we add a test that somehow makes sure the default max size is adhered to?", "author": "gapra-msft", "createdAt": "2020-04-20T23:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc1MDQ2OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411750469", "bodyText": "It seems to be test account/server side issue. I'm following up on that, but I'd rather not block on this now.", "author": "kasobol-msft", "createdAt": "2020-04-20T23:06:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc1Mjg0OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411752848", "bodyText": "I'll take a look on datalake tests.", "author": "kasobol-msft", "createdAt": "2020-04-20T23:12:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkyODkxOQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411928919", "bodyText": "@gapra-msft  I added test to LargeFileTest.groovy where default single upload for data lake is triggerd.\nBTW. thank you for this suggestion. This prompted me to run full blown live tests on datalake file client and caught one nasty bug. I added fix for that in DataLakeFileAsyncClient. The confusing remaining method from BufferAggregator is also hidden and renamed to avoid this in the future.", "author": "kasobol-msft", "createdAt": "2020-04-21T07:11:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ=="}], "type": "inlineReview"}, {"oid": "cf3b2ee230f6876929ab843905b148b3934b777b", "url": "https://github.com/Azure/azure-sdk-for-java/commit/cf3b2ee230f6876929ab843905b148b3934b777b", "message": "cover default data lake single upload threshod. and fix bug in datalake parallel upload.", "committedDate": "2020-04-21T07:06:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkyOTgyMw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411929823", "bodyText": "Should have used .lenght() everywhere in this  method after transition from ByteBuffer to BufferAggregator.\nConfusing method has been hidden and renamed...", "author": "kasobol-msft", "createdAt": "2020-04-21T07:12:58Z", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -307,7 +307,7 @@ public String getFileName() {\n         return chunkedSource.concatMap(pool::write)\n             .concatWith(Flux.defer(pool::flush))\n             /* Map the data to a tuple 3, of buffer, buffer length, buffer offset */\n-            .map(buffer -> Tuples.of(buffer, buffer.remaining(), 0L))\n+            .map(bufferAggregator -> Tuples.of(bufferAggregator, bufferAggregator.length(), 0L))", "originalCommit": "cf3b2ee230f6876929ab843905b148b3934b777b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM3ODQ2OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412378468", "bodyText": "Sweet great to hear we found something!", "author": "gapra-msft", "createdAt": "2020-04-21T18:07:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkyOTgyMw=="}], "type": "inlineReview"}, {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1", "url": "https://github.com/Azure/azure-sdk-for-java/commit/165658247ecc4023ffca708a676deb8ac181d1b1", "message": "fix find bugs.", "committedDate": "2020-04-21T16:13:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4MTQ5Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412381497", "bodyText": "Should these be test class level properties or be migrated into the pipeline policy being used to test this functionality? I don't know when in the test loop, if ever, these would be reset to the default value again.", "author": "alzimmermsft", "createdAt": "2020-04-21T18:11:49Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -28,8 +32,11 @@ class LargeBlobTest extends APISpec {\n     BlobClient blobClient\n     BlobAsyncClient blobAsyncClient\n     String blobName\n-    List<Mono<Long>> putBlockPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n-    AtomicLong count = new AtomicLong()\n+    List<Long> putBlockPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n+    AtomicLong blocksCount = new AtomicLong()\n+    List<Long> putBlobPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n+    AtomicLong singleUploadCount = new AtomicLong()\n+    ConcurrentHashMap<String, Boolean> retryTracker = new ConcurrentHashMap<>()", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQwMDA5OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412400099", "bodyText": "Every test framework I worked with JUnit, TestNG (Java) or NUnit, XUnit, MSTest (.Net) instantiates new instance of test class for each test case. Therefore these are reset for each test.\nI could potentially put them into policy but then I'd need to keep reference to policy in test class.\nI'd rather keep them in test class (they represent test case state) rather than in policy (that is used to update test case state).\nReference: https://stackoverflow.com/questions/19381352/does-junit-reinitialize-the-class-with-each-test-method-invocation", "author": "kasobol-msft", "createdAt": "2020-04-21T18:39:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4MTQ5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQ1OTI5NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412459295", "bodyText": "Good with me, was verifying that no bad state would be reached with these in the class scope.", "author": "alzimmermsft", "createdAt": "2020-04-21T20:12:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4MTQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4NzYyMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412387622", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    int reminder = (int) (size - numberOfSubBuffers * bufferSize)\n          \n          \n            \n                    int remainder = (int) (size - numberOfSubBuffers * bufferSize)", "author": "alzimmermsft", "createdAt": "2020-04-21T18:20:44Z", "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/LargeFileTest.groovy", "diffHunk": "@@ -148,16 +152,44 @@ class LargeFileTest extends APISpec{\n         notThrown(DataLakeStorageException)\n     }\n \n+    @Unroll\n+    @Requires({ liveMode() })\n+    // This test does not send large payload over the wire\n+    def \"Should honor default single upload threshold\"() {\n+        given:\n+        def data = createLargeBuffer(dataSize)\n+        def transferOptions = new ParallelTransferOptions()\n+            .setBlockSizeLong(10L * Constants.MB) // set this much lower than default single upload size to make it tempting.\n+\n+        when:\n+        fcAsyncPayloadDropping.upload(data, transferOptions, true).block()\n+\n+        then:\n+        notThrown(DataLakeStorageException)\n+        count.get() == expectedAppendRequests\n+\n+        where:\n+        dataSize                         | expectedAppendRequests\n+        defaultSingleUploadThreshold     | 1\n+        defaultSingleUploadThreshold + 1 | 11\n+    }\n+\n     private Flux<ByteBuffer> createLargeBuffer(long size) {\n         return createLargeBuffer(size, Constants.MB)\n     }\n \n     private Flux<ByteBuffer> createLargeBuffer(long size, int bufferSize) {\n         def bytes = getRandomByteArray(bufferSize)\n         long numberOfSubBuffers = (long) (size / bufferSize)\n-        return Flux.just(ByteBuffer.wrap(bytes))\n+        int reminder = (int) (size - numberOfSubBuffers * bufferSize)", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxNDE4Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412414187", "bodyText": "Ups.", "author": "kasobol-msft", "createdAt": "2020-04-21T19:00:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4NzYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MTk5OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412391999", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    int reminder = (int) (size - numberOfSubBuffers * bufferSize)\n          \n          \n            \n                    int remainder = (int) (size - numberOfSubBuffers * bufferSize)", "author": "alzimmermsft", "createdAt": "2020-04-21T18:27:10Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -204,9 +286,15 @@ class LargeBlobTest extends APISpec {\n     private Flux<ByteBuffer> createLargeBuffer(long size, int bufferSize) {\n         def bytes = getRandomByteArray(bufferSize)\n         long numberOfSubBuffers = (long) (size / bufferSize)\n-        return Flux.just(ByteBuffer.wrap(bytes))\n+        int reminder = (int) (size - numberOfSubBuffers * bufferSize)", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxNDI1Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412414256", "bodyText": "Ups.", "author": "kasobol-msft", "createdAt": "2020-04-21T19:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MTk5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzE5NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393194", "bodyText": "Would it be better to check for certain parts of the query string being null in case this gets used by a client authorizing with a SAS token.", "author": "alzimmermsft", "createdAt": "2020-04-21T18:28:56Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");\n+        }\n+\n+        private boolean isSinglePutBlobRequest(HttpRequest request) {\n+            return request.getHttpMethod().equals(HttpMethod.PUT) &&\n+                request.getUrl().toString().endsWith(blobName) &&\n+                request.getUrl().getQuery() == null", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxNzI3NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412417274", "bodyText": "I agree. However, I don't expect adding SAS tokens scenarios here (orthogonal feature). This Policy is not intended to be used outside of large payload testing. Therefore, I'd rather keep it simple until we need more functionality here.", "author": "kasobol-msft", "createdAt": "2020-04-21T19:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQ1OTQ4Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412459487", "bodyText": "Sounds good", "author": "alzimmermsft", "createdAt": "2020-04-21T20:12:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzUwMA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393500", "bodyText": "I think this should check that the path ends with the blob name.", "author": "alzimmermsft", "createdAt": "2020-04-21T18:29:23Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");\n+        }\n+\n+        private boolean isSinglePutBlobRequest(HttpRequest request) {\n+            return request.getHttpMethod().equals(HttpMethod.PUT) &&\n+                request.getUrl().toString().endsWith(blobName) &&", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxNzQ5Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412417497", "bodyText": "Make sense. Thank you!", "author": "kasobol-msft", "createdAt": "2020-04-21T19:05:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5Mzg1OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393858", "bodyText": "Should this check that the query contains comp=block.", "author": "alzimmermsft", "createdAt": "2020-04-21T18:29:54Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");", "originalCommit": "165658247ecc4023ffca708a676deb8ac181d1b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxNzU5Mg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412417592", "bodyText": "Make sense. Thank you!", "author": "kasobol-msft", "createdAt": "2020-04-21T19:06:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5Mzg1OA=="}], "type": "inlineReview"}, {"oid": "e8591eeeac05e14f0c7debbb63d3214b6a72f984", "url": "https://github.com/Azure/azure-sdk-for-java/commit/e8591eeeac05e14f0c7debbb63d3214b6a72f984", "message": "pr feedback.", "committedDate": "2020-04-21T19:09:28Z", "type": "commit"}]}