{"pr_number": 17774, "pr_title": "OLTP spark query pipeline draft on DataSourceV2 spark3", "pr_createdAt": "2020-11-24T02:35:44Z", "pr_url": "https://github.com/Azure/azure-sdk-for-java/pull/17774", "timeline": [{"oid": "5d4ecfac5fc349c98201d82d4379a961401363b4", "url": "https://github.com/Azure/azure-sdk-for-java/commit/5d4ecfac5fc349c98201d82d4379a961401363b4", "message": "spark query draft", "committedDate": "2020-11-24T02:29:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1NzYxMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529257612", "bodyText": "I think we should discuss the format name - \"cosmos.items\" feels a little off to me.. can we go with the style we have in the unified Spark connector in Synapse", "author": "tknandu", "createdAt": "2020-11-24T07:35:15Z", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/test/scala/com/azure/cosmos/spark/TestReadE2EMain.scala", "diffHunk": "@@ -0,0 +1,43 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.SparkSession\n+\n+/** sample test for query */\n+object TestReadE2EMain {\n+  def main(args: Array[String]) {\n+    val cosmosEndpoint = TestConfigurations.HOST\n+    val cosmosMasterKey = TestConfigurations.MASTER_KEY\n+    val cosmosDatabase = \"testDB\"\n+    val cosmosContainer = \"testContainer\"\n+\n+//    val client = new CosmosClientBuilder()\n+//      .endpoint(cosmosEndpoint)\n+//      .key(cosmosMasterKey)\n+//      .consistencyLevel(ConsistencyLevel.EVENTUAL)\n+//      .buildAsyncClient()\n+//\n+//    client.createDatabaseIfNotExists(cosmosDatabase).block()\n+//    client.getDatabase(cosmosDatabase).createContainerIfNotExists(cosmosContainer, \"/id\").block()\n+//    client.close()\n+\n+    val cfg = Map(\"spark.cosmos.accountEndpoint\" -> cosmosEndpoint,\n+      \"spark.cosmos.accountKey\" -> cosmosMasterKey,\n+      \"spark.cosmos.database\" -> cosmosDatabase,\n+      \"spark.cosmos.container\" -> cosmosContainer\n+    )\n+\n+    val spark = SparkSession.builder()\n+      .appName(\"spark connector sample\")\n+      .master(\"local\")\n+      .getOrCreate()\n+\n+    val df = spark.read.format(\"cosmos.items\").options(cfg).load()", "originalCommit": "5d4ecfac5fc349c98201d82d4379a961401363b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNzc5MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529827791", "bodyText": "let's discuss in the scrum.", "author": "moderakh", "createdAt": "2020-11-24T19:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1NzYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI2MDA4OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529260088", "bodyText": "Critical for us to land support for FeedRange (single FeedRange covering multiple physical partition by eventual GA) mapping to 1 Spark task since that is core large customer requirement", "author": "tknandu", "createdAt": "2020-11-24T07:40:09Z", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosScan.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.apache.spark.sql.connector.read.{Batch, InputPartition, PartitionReaderFactory, Scan}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+case class CosmosScan(config: Map[String, String], sqlQuerySpec: String)\n+  extends Scan\n+    with Batch\n+    with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def readSchema(): StructType = {\n+    // TODO: moderakh add support for schema inference\n+    // for now schema is hard coded to make TestE2EMain to work\n+    StructType(Seq(StructField(\"number\", IntegerType), StructField(\"word\", StringType)))\n+  }\n+\n+  override def planInputPartitions(): Array[InputPartition] = {\n+    // TODO: moderakh use get feed range?\n+    // for now we are returning one partition hence only one spark task will be created.", "originalCommit": "5d4ecfac5fc349c98201d82d4379a961401363b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNzYwMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529827601", "bodyText": "of course. without this we are not prod ready.", "author": "moderakh", "createdAt": "2020-11-24T19:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI2MDA4OA=="}], "type": "inlineReview"}, {"oid": "404703e463e039e178fd0355affbb065a8efb2f6", "url": "https://github.com/Azure/azure-sdk-for-java/commit/404703e463e039e178fd0355affbb065a8efb2f6", "message": "fixed checkstyle warns", "committedDate": "2020-11-24T18:07:49Z", "type": "commit"}]}