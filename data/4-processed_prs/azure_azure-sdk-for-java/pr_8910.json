{"pr_number": 8910, "pr_title": "added some datalake buffered upload samples", "pr_createdAt": "2020-03-10T21:57:51Z", "pr_url": "https://github.com/Azure/azure-sdk-for-java/pull/8910", "timeline": [{"oid": "df3d683d0eebc15b0e44318f9203294ade6dcc98", "url": "https://github.com/Azure/azure-sdk-for-java/commit/df3d683d0eebc15b0e44318f9203294ade6dcc98", "message": "added some samples", "committedDate": "2020-03-10T21:57:11Z", "type": "commit"}, {"oid": "01eadfa7eb87a501aed31165682c07ca6636de17", "url": "https://github.com/Azure/azure-sdk-for-java/commit/01eadfa7eb87a501aed31165682c07ca6636de17", "message": "removed upload download example", "committedDate": "2020-03-10T21:59:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNzUyMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8910#discussion_r390637522", "bodyText": "This isn't actually necessary, especially since we specify overwrite on downloadToFile.", "author": "rickle-msft", "createdAt": "2020-03-10T22:03:36Z", "path": "sdk/storage/azure-storage-file-datalake/src/samples/java/com/azure/storage/file/datalake/FileTransferExample.java", "diffHunk": "@@ -0,0 +1,174 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake;\n+\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.Locale;\n+\n+/**\n+ * This class shows how to upload the file as fast as possible in parallel using the optimized upload API.\n+ */\n+public class FileTransferExample {\n+    private static final String LARGE_TEST_FOLDER = \"test-large-files/\";\n+\n+    /**\n+     * Entry point into the file transfer examples for Storage datalake.\n+     * @param args Unused. Arguments to the program.\n+     * @throws IOException If an I/O error occurs\n+     * @throws NoSuchAlgorithmException If {@code MD5} isn't supported\n+     * @throws RuntimeException If the uploaded or downloaded file wasn't found\n+     */\n+    public static void main(String[] args) throws IOException, NoSuchAlgorithmException {\n+\n+        /*\n+         * From the Azure portal, get your Storage account's name and account key.\n+         */\n+        String accountName = SampleHelper.getAccountName();\n+        String accountKey = SampleHelper.getAccountKey();\n+\n+        /*\n+         * Use your Storage account's name and key to create a credential object; this is used to access your account.\n+         */\n+        StorageSharedKeyCredential credential = new StorageSharedKeyCredential(accountName, accountKey);\n+\n+        /*\n+         * From the Azure portal, get your Storage account datalake service URL endpoint.\n+         * The URL typically looks like this:\n+         */\n+        String endPoint = String.format(Locale.ROOT, \"https://%s.dfs.core.windows.net\", accountName);\n+\n+        /*\n+         * Create a DataLakeServiceClient object that wraps the service endpoint, credential and a request pipeline.\n+         * Now you can use the storageClient to perform various file system and path operations.\n+         */\n+        DataLakeServiceClient storageClient = new DataLakeServiceClientBuilder().endpoint(endPoint).credential(credential).buildClient();\n+\n+\n+        /*\n+         * This example shows several common operations just to get you started.\n+         */\n+\n+\n+        /*\n+         * Create a client that references a to-be-created file system in your Azure Storage account. This returns a\n+         * FileSystemClient uses the same endpoint, credential and pipeline from storageClient.\n+         * Note that file system names require lowercase.\n+         */\n+        DataLakeFileSystemClient fileSystemClient = storageClient.getFileSystemClient(\"myjavafilesystemparallelupload\" + System.currentTimeMillis());\n+\n+        /*\n+         * Create a file system in Storage datalake account.\n+         */\n+        fileSystemClient.create();\n+\n+        /*\n+         * Create a FileClient object that wraps a file's endpoint and a default pipeline, the client give us access to upload the file.\n+         */\n+        String filename = \"BigFile.bin\";\n+        DataLakeFileClient fileClient = fileSystemClient.getFileClient(filename);\n+\n+        /*\n+         * Create the empty uploadFile and downloadFile.\n+         */\n+        File largeFile = createTempEmptyFile(filename);", "originalCommit": "01eadfa7eb87a501aed31165682c07ca6636de17", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY3Nzk5OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8910#discussion_r390677999", "bodyText": "I think its fine in the sample just cause that helper will go through and make sure the upload/download file is in the same directory.", "author": "gapra-msft", "createdAt": "2020-03-11T00:00:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNzUyMg=="}], "type": "inlineReview"}]}