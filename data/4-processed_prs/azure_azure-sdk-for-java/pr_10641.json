{"pr_number": 10641, "pr_title": "Implemented query for blob and datalake", "pr_createdAt": "2020-04-30T17:27:02Z", "pr_url": "https://github.com/Azure/azure-sdk-for-java/pull/10641", "timeline": [{"oid": "9369a62d220c4b1fa033c6cc75e84907a2e595c1", "url": "https://github.com/Azure/azure-sdk-for-java/commit/9369a62d220c4b1fa033c6cc75e84907a2e595c1", "message": "Implemented an Avro Parser", "committedDate": "2020-04-28T21:17:32Z", "type": "commit"}, {"oid": "1eb955cc40a9fd16efff0fae26c022239ead0dbf", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1eb955cc40a9fd16efff0fae26c022239ead0dbf", "message": "Added changes to pass CI", "committedDate": "2020-04-28T21:37:49Z", "type": "commit"}, {"oid": "5c5b444647eb0136581ecaea15a63bb3f03c8bad", "url": "https://github.com/Azure/azure-sdk-for-java/commit/5c5b444647eb0136581ecaea15a63bb3f03c8bad", "message": "Added more excludes due to false positives", "committedDate": "2020-04-28T22:31:21Z", "type": "commit"}, {"oid": "4a62fa7f567e97af090af03d1cb551f43a53c06b", "url": "https://github.com/Azure/azure-sdk-for-java/commit/4a62fa7f567e97af090af03d1cb551f43a53c06b", "message": "Fixed excludes", "committedDate": "2020-04-28T22:49:43Z", "type": "commit"}, {"oid": "faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "url": "https://github.com/Azure/azure-sdk-for-java/commit/faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "message": "Regenerated for quick query", "committedDate": "2020-04-29T17:17:14Z", "type": "commit"}, {"oid": "cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "url": "https://github.com/Azure/azure-sdk-for-java/commit/cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "message": "Added tests and code for blob quick query", "committedDate": "2020-04-29T20:38:21Z", "type": "commit"}, {"oid": "93ed5b61459373a6fbb418c09086d73e92aa433a", "url": "https://github.com/Azure/azure-sdk-for-java/commit/93ed5b61459373a6fbb418c09086d73e92aa433a", "message": "Moved qq to blob base and included snapshot test", "committedDate": "2020-04-29T22:48:54Z", "type": "commit"}, {"oid": "ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "url": "https://github.com/Azure/azure-sdk-for-java/commit/ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-04-29T22:49:32Z", "type": "commit"}, {"oid": "f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "url": "https://github.com/Azure/azure-sdk-for-java/commit/f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "message": "Added test for OS", "committedDate": "2020-04-30T00:00:56Z", "type": "commit"}, {"oid": "594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "url": "https://github.com/Azure/azure-sdk-for-java/commit/594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "message": "Added samples", "committedDate": "2020-04-30T17:26:14Z", "type": "commit"}, {"oid": "3f799610e72930a1d022c484dad280bd1534fed6", "url": "https://github.com/Azure/azure-sdk-for-java/commit/3f799610e72930a1d022c484dad280bd1534fed6", "message": "Added datalake and reocrdings", "committedDate": "2020-04-30T21:43:17Z", "type": "commit"}, {"oid": "2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "url": "https://github.com/Azure/azure-sdk-for-java/commit/2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "message": "removed avro test file", "committedDate": "2020-04-30T21:45:26Z", "type": "commit"}, {"oid": "dc0b18494949405051686bd79dca2136f8582507", "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc0b18494949405051686bd79dca2136f8582507", "message": "Added files for query", "committedDate": "2020-04-30T22:42:56Z", "type": "commit"}, {"oid": "1cae0e10415c60c9fe72eb29873d2575de285a3c", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1cae0e10415c60c9fe72eb29873d2575de285a3c", "message": "Fixed code snippets", "committedDate": "2020-04-30T22:50:57Z", "type": "commit"}, {"oid": "b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "url": "https://github.com/Azure/azure-sdk-for-java/commit/b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "message": "Added more samples", "committedDate": "2020-04-30T23:11:25Z", "type": "commit"}, {"oid": "725c00758f840e6052cbb743f36a8ae7e3d5e321", "url": "https://github.com/Azure/azure-sdk-for-java/commit/725c00758f840e6052cbb743f36a8ae7e3d5e321", "message": "More doc fixes", "committedDate": "2020-04-30T23:45:42Z", "type": "commit"}, {"oid": "26bf158cd98fcc472d0da3e105f0c3a79d103faa", "url": "https://github.com/Azure/azure-sdk-for-java/commit/26bf158cd98fcc472d0da3e105f0c3a79d103faa", "message": "throw through logger", "committedDate": "2020-05-01T00:02:20Z", "type": "commit"}, {"oid": "d9271b5dbd56061592aa2afcf727d7e191cb670d", "url": "https://github.com/Azure/azure-sdk-for-java/commit/d9271b5dbd56061592aa2afcf727d7e191cb670d", "message": "Ovveride encrypted client to not support query", "committedDate": "2020-05-01T00:16:54Z", "type": "commit"}, {"oid": "0a767c0ee138aeec0121563ee915388962b122a0", "url": "https://github.com/Azure/azure-sdk-for-java/commit/0a767c0ee138aeec0121563ee915388962b122a0", "message": "more flux is exception logger", "committedDate": "2020-05-01T00:24:31Z", "type": "commit"}, {"oid": "30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "url": "https://github.com/Azure/azure-sdk-for-java/commit/30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "message": "Added safe locking", "committedDate": "2020-05-01T00:54:30Z", "type": "commit"}, {"oid": "1edeef9367b189a7e692be81dfed9add514f683c", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1edeef9367b189a7e692be81dfed9add514f683c", "message": "Added extra lock", "committedDate": "2020-05-01T01:07:22Z", "type": "commit"}, {"oid": "c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "url": "https://github.com/Azure/azure-sdk-for-java/commit/c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "message": "Added stuff for CI", "committedDate": "2020-05-01T14:27:33Z", "type": "commit"}, {"oid": "856c969baa43ae5910655dfd7280bcdcb94ea708", "url": "https://github.com/Azure/azure-sdk-for-java/commit/856c969baa43ae5910655dfd7280bcdcb94ea708", "message": "CI stuff", "committedDate": "2020-05-01T14:57:15Z", "type": "commit"}, {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "url": "https://github.com/Azure/azure-sdk-for-java/commit/0fc0a75a37861868e014ed5ecc52f71e8627238b", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-01T19:04:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwMjgzNw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418702837", "bodyText": "fix doc", "author": "gapra-msft", "createdAt": "2020-05-01T19:36:33Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryAsyncResponse.java", "diffHunk": "@@ -0,0 +1,30 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.core.http.HttpHeaders;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.rest.ResponseBase;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * This class contains the response information returned from the server when running a quick query a blob.", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwMzk3MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418703970", "bodyText": "todo: i think i generated off a slightly older version of swagger, so I need to undo this", "author": "gapra-msft", "createdAt": "2020-05-01T19:39:25Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1227,7 +1247,8 @@ private void downloadToFileCleanup(AsynchronousFileChannel channel, String fileP\n                     hd.isIncrementalCopy(), hd.getDestinationSnapshot(), AccessTier.fromString(hd.getAccessTier()),\n                     hd.isAccessTierInferred(), ArchiveStatus.fromString(hd.getArchiveStatus()),\n                     hd.getEncryptionKeySha256(), hd.getAccessTierChangeTime(), hd.getMetadata(),\n-                    hd.getBlobCommittedBlockCount(), hd.getVersionId(), hd.isCurrentVersion());\n+                    hd.getBlobCommittedBlockCount(), null, null);", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNDU5MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418704591", "bodyText": "todo: comment this part", "author": "gapra-msft", "createdAt": "2020-05-01T19:40:59Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNTY1NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418705654", "bodyText": "remove : this is redundant. hmm or should i? i need to return a mono error to convey an error to the FluxIS - I could remove the below check", "author": "gapra-msft", "createdAt": "2020-05-01T19:43:37Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNzAzMA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418707030", "bodyText": "This can be just a IllegalStateException or something?", "author": "gapra-msft", "createdAt": "2020-05-01T19:47:01Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        AvroSchema.checkType(\"record\", quickQueryRecord, Map.class);\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new UncheckedIOException(new IOException(String.format(\"Unknown record type %s \"", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzI5Mg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717292", "bodyText": "uh oh - need to revert this", "author": "gapra-msft", "createdAt": "2020-05-01T20:12:26Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/BlobAPITest.groovy", "diffHunk": "@@ -3,30 +3,11 @@\n \n package com.azure.storage.blob\n \n-\n import com.azure.core.http.RequestConditions\n import com.azure.core.util.CoreUtils\n import com.azure.core.util.polling.LongRunningOperationStatus\n import com.azure.identity.DefaultAzureCredentialBuilder\n-import com.azure.storage.blob.models.AccessTier\n-import com.azure.storage.blob.models.ArchiveStatus\n-import com.azure.storage.blob.models.BlobErrorCode\n-import com.azure.storage.blob.models.BlobHttpHeaders\n-import com.azure.storage.blob.models.BlobRange\n-import com.azure.storage.blob.models.BlobRequestConditions\n-import com.azure.storage.blob.models.BlobStorageException\n-import com.azure.storage.blob.models.BlobType\n-import com.azure.storage.blob.models.BlockListType\n-import com.azure.storage.blob.models.CopyStatusType\n-import com.azure.storage.blob.models.CustomerProvidedKey\n-import com.azure.storage.blob.models.DeleteSnapshotsOptionType\n-import com.azure.storage.blob.models.DownloadRetryOptions\n-import com.azure.storage.blob.models.LeaseStateType\n-import com.azure.storage.blob.models.LeaseStatusType\n-import com.azure.storage.blob.models.ParallelTransferOptions\n-import com.azure.storage.blob.models.PublicAccessType\n-import com.azure.storage.blob.models.RehydratePriority\n-import com.azure.storage.blob.models.SyncCopyStatusType\n+import com.azure.storage.blob.models.*", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzM4Mw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717383", "bodyText": "revert", "author": "gapra-msft", "createdAt": "2020-05-01T20:12:39Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlockBlobAPITest.groovy", "diffHunk": "@@ -11,22 +11,8 @@ import com.azure.core.http.HttpRequest\n import com.azure.core.util.Context\n import com.azure.core.util.FluxUtil\n import com.azure.identity.DefaultAzureCredentialBuilder\n-import com.azure.storage.blob.APISpec\n-import com.azure.storage.blob.BlobAsyncClient\n-import com.azure.storage.blob.BlobClient\n-import com.azure.storage.blob.BlobServiceClientBuilder\n-import com.azure.storage.blob.BlobUrlParts\n-import com.azure.storage.blob.ProgressReceiver\n-import com.azure.storage.blob.models.AccessTier\n-import com.azure.storage.blob.models.BlobErrorCode\n-import com.azure.storage.blob.models.BlobHttpHeaders\n-import com.azure.storage.blob.models.BlobRange\n-import com.azure.storage.blob.models.BlobRequestConditions\n-import com.azure.storage.blob.models.BlobStorageException\n-import com.azure.storage.blob.models.BlockListType\n-import com.azure.storage.blob.models.CustomerProvidedKey\n-import com.azure.storage.blob.models.ParallelTransferOptions\n-import com.azure.storage.blob.models.PublicAccessType\n+import com.azure.storage.blob.*\n+import com.azure.storage.blob.models.*", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzg0Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717846", "bodyText": "pointed at specific version to make generating easy", "author": "gapra-msft", "createdAt": "2020-05-01T20:13:54Z", "path": "sdk/storage/azure-storage-blob/swagger/README.md", "diffHunk": "@@ -15,7 +15,8 @@ autorest --use=@microsoft.azure/autorest.java@3.0.4 --use=jianghaolu/autorest.mo\n \n ### Code generation settings\n ``` yaml\n-input-file: https://raw.githubusercontent.com/Azure/azure-rest-api-specs/storage-dataplane-preview/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json\n+# input-file: https://raw.githubusercontent.com/Azure/azure-rest-api-specs/storage-dataplane-preview/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json\n+input-file: https://github.com/Azure/azure-rest-api-specs/blob/6cd6f02d824569dccb2427cf1611fe1cd8cc4350/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json", "originalCommit": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d7bf395fdfc02b2c002a9101cb092238a1272c4d", "url": "https://github.com/Azure/azure-sdk-for-java/commit/d7bf395fdfc02b2c002a9101cb092238a1272c4d", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-13T21:58:10Z", "type": "commit"}, {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1d3819f3cb3deea49d63d89591c3e411c4769319", "message": "regenerated", "committedDate": "2020-05-13T22:07:08Z", "type": "commit"}, {"oid": "cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "url": "https://github.com/Azure/azure-sdk-for-java/commit/cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "message": "Bumped avro reactor version", "committedDate": "2020-05-13T22:23:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTMwNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r424775304", "bodyText": "Can you add a description of what's happening here and what the data looks like? Like we get a stream of bytes that's avro. It's expected to have this schema. Once we get the data out of it, we need to apply these transformations, etc.", "author": "rickle-msft", "createdAt": "2020-05-13T22:51:12Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1623,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {", "originalCommit": "1d3819f3cb3deea49d63d89591c3e411c4769319", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NjYxNg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425946616", "bodyText": "Also, if this is the main BlobClient, I think it might be best to put the Query private helper methods somewhere else, so we don't clutter it.", "author": "seanmcc-msft", "createdAt": "2020-05-15T17:29:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTMwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjAxODI1Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426018256", "bodyText": "Yeah I can move these into a BlobQueryUtil in implementation", "author": "gapra-msft", "createdAt": "2020-05-15T19:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTMwNA=="}], "type": "inlineReview"}, {"oid": "213a4173d7bf59ef03d053310c3ebec1b5401f31", "url": "https://github.com/Azure/azure-sdk-for-java/commit/213a4173d7bf59ef03d053310c3ebec1b5401f31", "message": "Added binary recordings for qq", "committedDate": "2020-05-13T23:26:43Z", "type": "commit"}, {"oid": "10d84f04e1b84d741b110a137cec51ef4aa3cde0", "url": "https://github.com/Azure/azure-sdk-for-java/commit/10d84f04e1b84d741b110a137cec51ef4aa3cde0", "message": "Added binary recordings for datalake", "committedDate": "2020-05-14T00:01:22Z", "type": "commit"}, {"oid": "3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "url": "https://github.com/Azure/azure-sdk-for-java/commit/3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-14T17:14:22Z", "type": "commit"}, {"oid": "31a43d592085371137dcd7c6e715918b648f4fad", "url": "https://github.com/Azure/azure-sdk-for-java/commit/31a43d592085371137dcd7c6e715918b648f4fad", "message": "Addressed my comments", "committedDate": "2020-05-14T17:20:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNzgxMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425317811", "bodyText": "Seems like the recordSeparator variable can just be a constant.", "author": "rickle-msft", "createdAt": "2020-05-14T17:41:20Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,609 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.BlobQueryDelimitedSerialization\n+import com.azure.storage.blob.models.BlobQueryError\n+import com.azure.storage.blob.models.BlobQueryJsonSerialization\n+import com.azure.storage.blob.models.BlobQueryOptions\n+import com.azure.storage.blob.models.BlobQuerySerialization\n+import com.azure.storage.blob.models.BlobRequestConditions\n+import com.azure.storage.blob.models.BlobStorageException\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _", "originalCommit": "1d3819f3cb3deea49d63d89591c3e411c4769319", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTY1MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425465651", "bodyText": "I'll add some tests testing other ones", "author": "gapra-msft", "createdAt": "2020-05-14T22:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNzgxMQ=="}], "type": "inlineReview"}, {"oid": "0d36d397a83b0e08a894bff01d4531ceb5013fcd", "url": "https://github.com/Azure/azure-sdk-for-java/commit/0d36d397a83b0e08a894bff01d4531ceb5013fcd", "message": "Added some imports", "committedDate": "2020-05-14T17:43:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyMjIyNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425322224", "bodyText": "I think in groovy you can just do a == between arrays and it'll compare them for you", "author": "rickle-msft", "createdAt": "2020-05-14T17:46:56Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,604 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {", "originalCommit": "31a43d592085371137dcd7c6e715918b648f4fad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NjUzMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425466531", "bodyText": "ooh yeah", "author": "gapra-msft", "createdAt": "2020-05-14T22:29:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyMjIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MjQ3MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426082470", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-15T23:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyMjIyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0NjQ3Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425346477", "bodyText": "Do you test all the options like escape character and line delimiter? I don't remember seeing tests for that. Maybe you can make your upload data methods more generic to accept these options.", "author": "rickle-msft", "createdAt": "2020-05-14T18:28:26Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,604 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _\n+        10        | '\\n'            || _\n+        100       | '\\n'            || _\n+        1000      | '\\n'            || _\n+    }\n+\n+    def \"Query Input csv Output json\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization inSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(inSer, 1)\n+        BlobQueryJsonSerialization outSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"{\\\"_1\\\":\\\"100\\\",\\\"_2\\\":\\\"200\\\",\\\"_3\\\":\\\"300\\\",\\\"_4\\\":\\\"400\\\"}\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query Input json Output csv\"() {\n+        setup:\n+        BlobQueryJsonSerialization inSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        uploadSmallJson(2)\n+        BlobQueryDelimitedSerialization outSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"owner0,owner1\\n\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query non fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        MockErrorReceiver receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        def expression = \"SELECT _1 from BlobStorage WHERE _2 > 250\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        receiver.numErrors > 0\n+        notThrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        notThrown(IOException)\n+        receiver.numErrors > 0\n+    }\n+\n+    def \"Query fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(true)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(new BlobQueryJsonSerialization())\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        thrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(Exceptions.ReactiveException)\n+    }\n+\n+    def \"Query progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def sizeofBlobToRead = bc.getProperties().getBlobSize()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The QQ Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, Constants.MB)\n+\n+        then:\n+        // At least the size of blob to read will be in the progress list\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+    }\n+\n+    @Requires( { liveMode() } ) // Large amount of data.\n+    def \"Query multiple records with progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 512000)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, 16 * Constants.MB)\n+\n+        then:\n+        long temp = 0\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        temp = 0\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+    }\n+\n+    class MockProgressReceiver implements ProgressReceiver {\n+\n+        List<Long> progressList\n+\n+        MockProgressReceiver() {\n+            this.progressList = new ArrayList<>()\n+        }\n+\n+        @Override\n+        void reportProgress(long bytesRead) {\n+            progressList.add(bytesRead)\n+        }\n+    }\n+\n+    class MockErrorReceiver implements ErrorReceiver<BlobQueryError> {\n+\n+        String expectedType\n+        int numErrors\n+\n+        MockErrorReceiver(String expectedType) {\n+            this.expectedType = expectedType\n+            this.numErrors = 0\n+        }\n+\n+        @Override\n+        void reportError(BlobQueryError nonFatalError) {\n+            assert !nonFatalError.isFatal()\n+            assert nonFatalError.getName() == expectedType\n+            numErrors++\n+        }\n+    }\n+\n+    def \"Query snapshot\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        /* Create snapshot of blob. */\n+        def snapshotClient = bc.createSnapshot()\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0, true) /* Make the blob empty. */\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        snapshotClient.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = snapshotClient.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        snapshotClient.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+    }\n+\n+    @Unroll\n+    def \"Query input output IA\"() {\n+        setup:\n+        /* Mock random impl of QQ Serialization*/\n+        BlobQuerySerialization ser = Spy() {\n+            return '\\n'\n+        }\n+        def inSer = input ? ser : null\n+        def outSer = output ? ser : null\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(inSer)\n+            .setOutputSerialization(outSer)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        def e = thrown(IOException)\n+        assert e.getCause() instanceof IllegalArgumentException\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(IllegalArgumentException)\n+\n+        where:\n+        input   | output   || _\n+        true    | false    || _\n+        false   | true     || _\n+    }\n+\n+    @Unroll\n+    def \"Query AC\"() {\n+        setup:\n+        match = setupBlobMatchCondition(bc, match)\n+        leaseID = setupBlobLeaseCondition(bc, leaseID)\n+        def bac = new BlobRequestConditions()\n+            .setLeaseId(leaseID)\n+            .setIfMatch(match)\n+            .setIfNoneMatch(noneMatch)\n+            .setIfModifiedSince(modified)\n+            .setIfUnmodifiedSince(unmodified)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setRequestConditions(bac)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options,null, null)\n+\n+        then:\n+        notThrown(BlobStorageException)\n+\n+        where:\n+        modified | unmodified | match        | noneMatch   | leaseID\n+        null     | null       | null         | null        | null\n+        oldDate  | null       | null         | null        | null\n+        null     | newDate    | null         | null        | null\n+        null     | null       | receivedEtag | null        | null\n+        null     | null       | null         | garbageEtag | null\n+        null     | null       | null         | null        | receivedLeaseID\n+    }\n+\n+    @Unroll\n+    def \"Query AC fail\"() {\n+        setup:\n+        setupBlobLeaseCondition(bc, leaseID)\n+        def bac = new BlobRequestConditions()\n+            .setLeaseId(leaseID)\n+            .setIfMatch(match)\n+            .setIfNoneMatch(setupBlobMatchCondition(bc, noneMatch))\n+            .setIfModifiedSince(modified)\n+            .setIfUnmodifiedSince(unmodified)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setRequestConditions(bac)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        def e = thrown(IOException)\n+        assert e.getCause() instanceof BlobStorageException\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(BlobStorageException)\n+\n+        where:\n+        modified | unmodified | match       | noneMatch    | leaseID\n+        newDate  | null       | null        | null         | null\n+        null     | oldDate    | null        | null         | null\n+        null     | null       | garbageEtag | null         | null\n+        null     | null       | null        | receivedEtag | null\n+        null     | null       | null        | null         | garbageLeaseID\n+    }\n+}", "originalCommit": "31a43d592085371137dcd7c6e715918b648f4fad", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-14T18:28:32Z", "type": "commit"}, {"oid": "5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "url": "https://github.com/Azure/azure-sdk-for-java/commit/5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "message": "Added semicolon", "committedDate": "2020-05-14T18:39:04Z", "type": "commit"}, {"oid": "8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "url": "https://github.com/Azure/azure-sdk-for-java/commit/8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "message": "Added change to try and fix tests", "committedDate": "2020-05-14T19:22:32Z", "type": "commit"}, {"oid": "894d77ab4d30475e1d238bb6aba2dccbf1c18536", "url": "https://github.com/Azure/azure-sdk-for-java/commit/894d77ab4d30475e1d238bb6aba2dccbf1c18536", "message": "imports", "committedDate": "2020-05-14T19:40:33Z", "type": "commit"}, {"oid": "1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "message": "potential fix for no error found", "committedDate": "2020-05-14T19:43:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQxNTU3NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425415574", "bodyText": "This line feels spurious. Since the comment says it should only be null if there was an error or completion, and both tof those cases exit the method, we shouldn't need this line, right?", "author": "rickle-msft", "createdAt": "2020-05-14T20:37:32Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);", "originalCommit": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0Mzk5NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425443994", "bodyText": "I think I added this since I wanted buffer to be null only before the subscription. But now that I think about it, that doesnt need to be a semantic I follow since I have the subscribed boolean.\nI think if I remove it I need to throw if there was no error or completion", "author": "gapra-msft", "createdAt": "2020-05-14T21:33:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQxNTU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4MzYwMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425983601", "bodyText": "Yea I'd rather throw IllegalState in that case so that it's consistent with the comments and with the intended behavior.", "author": "rickle-msft", "createdAt": "2020-05-15T18:43:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQxNTU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MjU2NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426082565", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-15T23:05:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQxNTU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425432427", "bodyText": "I think you may want to lock around checking if buffer is null and assigning to it here. You lock around that in your onNext below, so modifying it here without locking will give you a race condition, i think", "author": "rickle-msft", "createdAt": "2020-05-14T21:10:09Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.", "originalCommit": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjU4MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425432580", "bodyText": "Same thing wherever you access this.buffer\n\nIn reply to: 425432427 [](ancestors = 425432427)", "author": "rickle-msft", "createdAt": "2020-05-14T21:10:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0NTU0Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425445547", "bodyText": "Do you think it'd be ok if I just move the buffer population above the locking logic? The intent was for the waitingForData variable to be protected by the lock moreso than the buffer", "author": "gapra-msft", "createdAt": "2020-05-14T21:37:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4NzQ5Mw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425987493", "bodyText": "Uhh. It feels safest to put the buffer population in the lock because what if you get two items emitted really quickly and they have different amounts of data or one of them is the end of the stream and is actually empty. Then we may have entered an if block based on one buffer but are operating on a different buffer for which the conditions are no longer true. But we can also walk through this offline and think about possibilities if you think it's safe to put the buffer stuff outside the lock", "author": "rickle-msft", "createdAt": "2020-05-15T18:51:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MjcwOQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426082709", "bodyText": "We only request when we blockForData though so I'm not sure that could happen. We can talk about this next week", "author": "gapra-msft", "createdAt": "2020-05-15T23:06:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzE0OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425433148", "bodyText": "Is there any value in checking if the flux has completed before cancelling the subscription? Or is this a no-op in that case?", "author": "rickle-msft", "createdAt": "2020-05-14T21:11:26Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();", "originalCommit": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0NjQxMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425446411", "bodyText": "I don't think it would matter if the flux completed or not because the user is done with the stream regardless", "author": "gapra-msft", "createdAt": "2020-05-14T21:39:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzE0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4NDAyNg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425984026", "bodyText": "Yea true. I more was just worried about the flux throwing an exception if you cancel a completed subscription.", "author": "rickle-msft", "createdAt": "2020-05-15T18:44:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzE0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MjgyMA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426082820", "bodyText": "I don;t see any documentation about it throwing if the subscription completed. https://www.reactive-streams.org/reactive-streams-1.0.0-javadoc/org/reactivestreams/Subscription.html#cancel--", "author": "gapra-msft", "createdAt": "2020-05-15T23:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzE0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425435529", "bodyText": "I don't think we need to wrap IllegalArgumentException in an IOException. And I don't think we need to wrap ioExceptions in another IOException.", "author": "rickle-msft", "createdAt": "2020-05-14T21:16:29Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();\n+        if (this.buffer != null) {\n+            this.buffer.close();\n+        }\n+        super.close();\n+        if (this.lastError != null) {\n+            throw logger.logThrowableAsError(this.lastError);\n+        }\n+    }\n+\n+    /**\n+     * Request more data and wait on data to become available.\n+     */\n+    private void blockForData() {\n+        lock.lock();\n+        try {\n+            waitingForData = true;\n+            if (!subscribed) {\n+                subscribeToData();\n+            } else {\n+                subscription.request(1);\n+            }\n+            // Block current thread until data is available.\n+            while (waitingForData) {\n+                if (fluxComplete) {\n+                    break;\n+                } else {\n+                    try {\n+                        dataAvailable.await();\n+                    } catch (InterruptedException e) {\n+                        throw logger.logExceptionAsError(new RuntimeException(e));\n+                    }\n+                }\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Subscribes to the data with a special subscriber.\n+     */\n+    private void subscribeToData() {\n+        this.data\n+            .onBackpressureBuffer(Constants.GB)\n+            .subscribe(\n+                // ByteBuffer consumer\n+                byteBuffer -> {\n+                    lock.lock();\n+                    try {\n+                        this.buffer = new ByteArrayInputStream(FluxUtil.byteBufferToArray(byteBuffer));\n+                        this.waitingForData = false;\n+                        // Signal the consumer when data is available.\n+                        dataAvailable.signal();\n+                    } finally {\n+                        lock.unlock();\n+                    }\n+                },\n+                // Error consumer\n+                throwable -> {\n+                    // Signal the consumer in case an error occurs (indicates we completed without data).\n+                    signalOnCompleteOrError();\n+                    if (throwable instanceof HttpResponseException) {\n+                        this.lastError = new IOException(throwable);\n+                    } else if (throwable instanceof IllegalArgumentException) {\n+                        this.lastError = new IOException(throwable);", "originalCommit": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzODA4Mg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425438082", "bodyText": "at least for IllegalArgument - is the read API allowed to throw anything other than IOexception? I'm not certain", "author": "gapra-msft", "createdAt": "2020-05-14T21:21:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2Njc2Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425466767", "bodyText": "IllegalArgumentException sounds like validation failed somewhere. That usually should bubble up. Isn't it runtimeexception?", "author": "kasobol-msft", "createdAt": "2020-05-14T22:30:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzEzMQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083131", "bodyText": "Here actually should I just wrap any type of exception except IOException in an IOEXception? I'm forgetting why I even checked for specific types of throwables.", "author": "gapra-msft", "createdAt": "2020-05-15T23:08:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzI1NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083255", "bodyText": "Like this\nif (throwable instanceof IOException) {\nthis.lastError = (IOException) throwable;\n} else {\nthis.lastError = new IOException(throwable);\n}", "author": "gapra-msft", "createdAt": "2020-05-15T23:09:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzM3NzI5Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r427377296", "bodyText": "I don't think we want to wrap IllegalArgumentException, etc. Probably shouldn't wrap any RuntimeExceptions, so rethrow IOExceptions and RuntimeExceptions, wrap HttpResponseExceptions. How does that sound?", "author": "rickle-msft", "createdAt": "2020-05-19T15:06:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NTUxNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r430685514", "bodyText": "If we do that exactly though we would be silently ignoring any other Exception/Throwable which would be really bad too", "author": "gapra-msft", "createdAt": "2020-05-26T20:22:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY5NjY1NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r430696654", "bodyText": "https://stackoverflow.com/questions/52195112/what-is-a-ioexception-and-how-do-i-fix-it/52195182\nhttps://docs.oracle.com/javase/7/docs/api/java/io/IOException.html\nhttps://stackoverflow.com/questions/13216148/java-what-throws-an-ioexception\nThe IOException is quite ambiguous. Do we know what are scenarios when RuntimeException is thrown here?", "author": "kasobol-msft", "createdAt": "2020-05-26T20:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxMzQ1NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r430713455", "bodyText": "I think it's dependent on the behavior of the Flux. In our primary use case, the Flux we care about is the one we return in blob.query - the code that handles that would take in the inputs, and it could return some type of error if the query failed (that would be IOException), or the input validation failed (IllegalArgument), etc", "author": "gapra-msft", "createdAt": "2020-05-26T21:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczMTkwNQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r430731905", "bodyText": "Summarizing our offline discussion. Anything thrown during pulling data is legit IOException. For IllegalArgument and it's family we should find a way to invoke validation eagerly (i.e. these exceptions should come from query/queryWithResponse and shouldn't be deferred until someone tries to read from stream).", "author": "kasobol-msft", "createdAt": "2020-05-26T22:00:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczNTU3OA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r430735578", "bodyText": "Resolveed by moving input validation further up by blocking on the network request", "author": "gapra-msft", "createdAt": "2020-05-26T22:10:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}], "type": "inlineReview"}, {"oid": "d3d0dc63f1505d080b0875919195f5206a7f9f75", "url": "https://github.com/Azure/azure-sdk-for-java/commit/d3d0dc63f1505d080b0875919195f5206a7f9f75", "message": "Fixed datalake test", "committedDate": "2020-05-14T21:19:21Z", "type": "commit"}, {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "url": "https://github.com/Azure/azure-sdk-for-java/commit/6c831acb690de37a1da0b7e70f354f94efb3b87d", "message": "addressed some comments and mock IA tests", "committedDate": "2020-05-14T21:43:46Z", "type": "commit"}, {"oid": "809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "url": "https://github.com/Azure/azure-sdk-for-java/commit/809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "message": "Mock not compatible with java 9+", "committedDate": "2020-05-14T22:05:27Z", "type": "commit"}, {"oid": "db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "url": "https://github.com/Azure/azure-sdk-for-java/commit/db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "message": "Added docs", "committedDate": "2020-05-14T22:24:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MjcyNQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425452725", "bodyText": "I'd rephrase this into something like \"Cannot query data encrypted on client side\"", "author": "kasobol-msft", "createdAt": "2020-05-14T21:54:02Z", "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobAsyncClient.java", "diffHunk": "@@ -447,4 +449,22 @@ This should be the only place we allocate memory in encryptBlob(). Although ther\n             throw logger.logExceptionAsError(Exceptions.propagate(e));\n         }\n     }\n+\n+    /**\n+     * Unsupported.\n+     */\n+    @Override\n+    public Flux<ByteBuffer> query(String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query a client side encrypted client\"));", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzI3Mw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083273", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-15T23:09:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MjcyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MzAwNw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425453007", "bodyText": "This may require more content explaining why.", "author": "kasobol-msft", "createdAt": "2020-05-14T21:54:41Z", "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobAsyncClient.java", "diffHunk": "@@ -447,4 +449,22 @@ This should be the only place we allocate memory in encryptBlob(). Although ther\n             throw logger.logExceptionAsError(Exceptions.propagate(e));\n         }\n     }\n+\n+    /**\n+     * Unsupported.", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzMxMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083312", "bodyText": "added more info", "author": "gapra-msft", "createdAt": "2020-05-15T23:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MzAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NTI4MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425455280", "bodyText": "This should be annotated with @Fluent", "author": "kasobol-msft", "createdAt": "2020-05-14T22:00:10Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryOptions.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.storage.common.ErrorReceiver;\n+import com.azure.storage.common.ProgressReceiver;\n+\n+/**\n+ * Optional parameters for Blob Query.\n+ */\n+public class BlobQueryOptions {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzMyOQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083329", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-15T23:09:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NTI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NzM0MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425457341", "bodyText": "@seanmcc-msft discovered that .NET XMLSerializer transform line endings. We should check if Java's XML engine does the same thing and if it does then we'd probably need to ask to extend generator.\nFor example \\n might be converted into \\r\\n. We should make sure whitespaces are sent to server side verbatim.\nOther thing to check is column separator in delimited serialization. For example adding test that would query a TSV file.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:05:27Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input and output serialization for a blob quick query request.\n+ * either {@link BlobQueryJsonSerialization} or {@link BlobQueryDelimitedSerialization}\n+ */\n+public abstract class BlobQuerySerialization {\n+\n+    protected char recordSeparator;", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwNDkzNg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425904936", "bodyText": "Yep! Just added a few tests for this. It seems like \\t works. Some that don't work however, include \\r, \\n. I'll bring this up in standup - NVM I think the ASCII values are being sent", "author": "gapra-msft", "createdAt": "2020-05-15T16:12:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NzM0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MDMzMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425460332", "bodyText": "This doesn't seem to throw anything.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:12:53Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                /* Parse the avro reactive stream. */\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new IllegalStateException(String.format(\"Unknown record type %s \"\n+                    + \"while parsing query response. \", recordSchema.toString())));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query result data record.\n+     * @param dataRecord The quick query result data record.\n+     * @return The data in the record.\n+     */\n+    private Mono<ByteBuffer> parseResultData(Map<?, ?> dataRecord) {\n+        Object data = dataRecord.get(\"data\");\n+\n+        if (checkParametersNotNull(data)) {\n+            AvroSchema.checkType(\"data\", data, List.class);\n+            return Mono.just(ByteBuffer.wrap(AvroSchema.getBytes((List<?>) data)));\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse result data record from \"\n+                + \"query response stream.\"));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query end record.\n+     * @param endRecord The quick query end record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseEnd(Map<?, ?> endRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object total = endRecord.get(\"totalBytes\");\n+\n+            if (checkParametersNotNull(total)) {\n+                AvroSchema.checkType(\"total\", total, Long.class);\n+                progressReceiver.reportProgress((Long) total);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse end record from query \"\n+                    + \"response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query progress record.\n+     * @param progressRecord The quick query progress record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseProgress(Map<?, ?> progressRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object bytesScanned = progressRecord.get(\"bytesScanned\");\n+\n+            if (checkParametersNotNull(bytesScanned)) {\n+                AvroSchema.checkType(\"bytesScanned\", bytesScanned, Long.class);\n+                progressReceiver.reportProgress((Long) bytesScanned);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse progress record from \"\n+                    + \"query response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query error record.\n+     * @param errorRecord The quick query error record.\n+     * @param errorReceiver The error receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseError(Map<?, ?> errorRecord, ErrorReceiver<BlobQueryError> errorReceiver) {\n+        Object fatal = errorRecord.get(\"fatal\");\n+        Object name = errorRecord.get(\"name\");\n+        Object description = errorRecord.get(\"description\");\n+        Object position = errorRecord.get(\"position\");\n+\n+        if (checkParametersNotNull(fatal, name, description, position)) {\n+            AvroSchema.checkType(\"fatal\", fatal, Boolean.class);\n+            AvroSchema.checkType(\"name\", name, String.class);\n+            AvroSchema.checkType(\"description\", description, String.class);\n+            AvroSchema.checkType(\"position\", position, Long.class);\n+\n+            BlobQueryError error = new BlobQueryError((Boolean) fatal, (String) name,\n+                (String) description, (Long) position);\n+\n+            if (errorReceiver != null) {\n+                errorReceiver.reportError(error);\n+            } else {\n+                return Mono.error(new IOException(\"An error was reported during query response processing, \"\n+                        + System.lineSeparator() + error.toString()));\n+            }\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse error record from \"\n+                + \"query response stream.\"));\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Validates that all parameters are non-null. Throws IOException if any of them are.", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwNjA1Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425906057", "bodyText": "That's poor porting on my part. It used to throw. Will fix the doc", "author": "gapra-msft", "createdAt": "2020-05-15T16:14:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MDMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MjI1NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425462254", "bodyText": "Why do we treat \\0 specially? Can customer choose \\0 as record separator (for whatever weird reason)?", "author": "kasobol-msft", "createdAt": "2020-05-14T22:18:03Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                /* Parse the avro reactive stream. */\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new IllegalStateException(String.format(\"Unknown record type %s \"\n+                    + \"while parsing query response. \", recordSchema.toString())));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query result data record.\n+     * @param dataRecord The quick query result data record.\n+     * @return The data in the record.\n+     */\n+    private Mono<ByteBuffer> parseResultData(Map<?, ?> dataRecord) {\n+        Object data = dataRecord.get(\"data\");\n+\n+        if (checkParametersNotNull(data)) {\n+            AvroSchema.checkType(\"data\", data, List.class);\n+            return Mono.just(ByteBuffer.wrap(AvroSchema.getBytes((List<?>) data)));\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse result data record from \"\n+                + \"query response stream.\"));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query end record.\n+     * @param endRecord The quick query end record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseEnd(Map<?, ?> endRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object total = endRecord.get(\"totalBytes\");\n+\n+            if (checkParametersNotNull(total)) {\n+                AvroSchema.checkType(\"total\", total, Long.class);\n+                progressReceiver.reportProgress((Long) total);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse end record from query \"\n+                    + \"response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query progress record.\n+     * @param progressRecord The quick query progress record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseProgress(Map<?, ?> progressRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object bytesScanned = progressRecord.get(\"bytesScanned\");\n+\n+            if (checkParametersNotNull(bytesScanned)) {\n+                AvroSchema.checkType(\"bytesScanned\", bytesScanned, Long.class);\n+                progressReceiver.reportProgress((Long) bytesScanned);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse progress record from \"\n+                    + \"query response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query error record.\n+     * @param errorRecord The quick query error record.\n+     * @param errorReceiver The error receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseError(Map<?, ?> errorRecord, ErrorReceiver<BlobQueryError> errorReceiver) {\n+        Object fatal = errorRecord.get(\"fatal\");\n+        Object name = errorRecord.get(\"name\");\n+        Object description = errorRecord.get(\"description\");\n+        Object position = errorRecord.get(\"position\");\n+\n+        if (checkParametersNotNull(fatal, name, description, position)) {\n+            AvroSchema.checkType(\"fatal\", fatal, Boolean.class);\n+            AvroSchema.checkType(\"name\", name, String.class);\n+            AvroSchema.checkType(\"description\", description, String.class);\n+            AvroSchema.checkType(\"position\", position, Long.class);\n+\n+            BlobQueryError error = new BlobQueryError((Boolean) fatal, (String) name,\n+                (String) description, (Long) position);\n+\n+            if (errorReceiver != null) {\n+                errorReceiver.reportError(error);\n+            } else {\n+                return Mono.error(new IOException(\"An error was reported during query response processing, \"\n+                        + System.lineSeparator() + error.toString()));\n+            }\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse error record from \"\n+                + \"query response stream.\"));\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Validates that all parameters are non-null. Throws IOException if any of them are.\n+     */\n+    private boolean checkParametersNotNull(Object... data) {\n+        for (Object o : data) {\n+            if (o == null || o instanceof AvroNullSchema.Null) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Transforms a generic BlobQuickQuerySerialization into a QuickQuerySerialization.\n+     * @param userSerialization {@link BlobQuerySerialization}\n+     * @param logger {@link ClientLogger}\n+     * @return {@link QuickQuerySerialization}\n+     */\n+    private static QuickQuerySerialization transformSerialization(BlobQuerySerialization userSerialization,\n+        ClientLogger logger) {\n+        if (userSerialization == null) {\n+            return null;\n+        }\n+\n+        QuickQueryFormat generatedFormat = new QuickQueryFormat();\n+        if (userSerialization instanceof BlobQueryDelimitedSerialization) {\n+\n+            generatedFormat.setType(QuickQueryFormatType.DELIMITED);\n+            generatedFormat.setDelimitedTextConfiguration(transformDelimited(\n+                (BlobQueryDelimitedSerialization) userSerialization));\n+\n+        } else if (userSerialization instanceof BlobQueryJsonSerialization) {\n+\n+            generatedFormat.setType(QuickQueryFormatType.JSON);\n+            generatedFormat.setJsonTextConfiguration(transformJson(\n+                (BlobQueryJsonSerialization) userSerialization));\n+\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalArgumentException(\n+                String.format(\"'input' must be one of %s or %s\", BlobQueryJsonSerialization.class.getSimpleName(),\n+                    BlobQueryDelimitedSerialization.class.getSimpleName())));\n+        }\n+        return new QuickQuerySerialization().setFormat(generatedFormat);\n+    }\n+\n+    /**\n+     * Transforms a BlobQuickQueryDelimitedSerialization into a DelimitedTextConfiguration.\n+     *\n+     * @param delimitedSerialization {@link BlobQueryDelimitedSerialization}\n+     * @return {@link DelimitedTextConfiguration}\n+     */\n+    private static DelimitedTextConfiguration transformDelimited(\n+        BlobQueryDelimitedSerialization delimitedSerialization) {\n+        if (delimitedSerialization == null) {\n+            return null;\n+        }\n+        return new DelimitedTextConfiguration()\n+            .setColumnSeparator(charToString(delimitedSerialization.getColumnSeparator()))\n+            .setEscapeChar(charToString(delimitedSerialization.getEscapeChar()))\n+            .setFieldQuote(charToString(delimitedSerialization.getFieldQuote()))\n+            .setHeadersPresent(delimitedSerialization.isHeadersPresent())\n+            .setRecordSeparator(charToString(delimitedSerialization.getRecordSeparator()));\n+    }\n+\n+    /**\n+     * Transforms a BlobQuickQueryJsonSerialization into a JsonTextConfiguration.\n+     *\n+     * @param jsonSerialization {@link BlobQueryJsonSerialization}\n+     * @return {@link JsonTextConfiguration}\n+     */\n+    private static JsonTextConfiguration transformJson(BlobQueryJsonSerialization jsonSerialization) {\n+        if (jsonSerialization == null) {\n+            return null;\n+        }\n+        return new JsonTextConfiguration()\n+            .setRecordSeparator(charToString(jsonSerialization.getRecordSeparator()));\n+    }\n+\n+    private static String charToString(char c) {\n+        return c == '\\0' ? \"\" : Character.toString(c);", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0MDMyNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426040324", "bodyText": "I thought that Karim told me that null should be \"\" but I have sent an email reconfirming.", "author": "gapra-msft", "createdAt": "2020-05-15T20:50:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MjI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0MDY2Mw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426040663", "bodyText": "null is the default param for field quote and escape char according to the QQ folks.", "author": "gapra-msft", "createdAt": "2020-05-15T20:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MjI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MzU5Mg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425463592", "bodyText": "This type doesn't seem to bring any value. Should we just use FluxInputStream or even just InputStream as return type from query method family ? I think if we should declare most abstract type as return type as we can.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:21:41Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobQueryInputStream.java", "diffHunk": "@@ -0,0 +1,23 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.specialized;\n+\n+import com.azure.storage.common.implementation.FluxInputStream;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * An <code>InputStream</code> interface that represents the stream to use for reading the query response.\n+ */\n+public class BlobQueryInputStream extends FluxInputStream {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA0MDc2NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426040764", "bodyText": "I think that's a good idea. Changed it to that", "author": "gapra-msft", "createdAt": "2020-05-15T20:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MzU5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NDkzOQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425464939", "bodyText": "I'd move these classes to the bottom of the files so that we don't mix tests with test aids.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:25:17Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,602 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _\n+        10        | '\\n'            || _\n+        100       | '\\n'            || _\n+        1000      | '\\n'            || _\n+    }\n+\n+    def \"Query Input csv Output json\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization inSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(inSer, 1)\n+        BlobQueryJsonSerialization outSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"{\\\"_1\\\":\\\"100\\\",\\\"_2\\\":\\\"200\\\",\\\"_3\\\":\\\"300\\\",\\\"_4\\\":\\\"400\\\"}\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query Input json Output csv\"() {\n+        setup:\n+        BlobQueryJsonSerialization inSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        uploadSmallJson(2)\n+        BlobQueryDelimitedSerialization outSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"owner0,owner1\\n\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query non fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        MockErrorReceiver receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        def expression = \"SELECT _1 from BlobStorage WHERE _2 > 250\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        receiver.numErrors > 0\n+        notThrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        notThrown(IOException)\n+        receiver.numErrors > 0\n+    }\n+\n+    def \"Query fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(true)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(new BlobQueryJsonSerialization())\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        thrown(Throwable)\n+\n+        /* Output Stream. */\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(Exceptions.ReactiveException)\n+    }\n+\n+    def \"Query progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def sizeofBlobToRead = bc.getProperties().getBlobSize()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The QQ Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, Constants.MB)\n+\n+        then:\n+        // At least the size of blob to read will be in the progress list\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+    }\n+\n+    @Requires( { liveMode() } ) // Large amount of data.\n+    def \"Query multiple records with progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 512000)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, 16 * Constants.MB)\n+\n+        then:\n+        long temp = 0\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        temp = 0\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+    }\n+\n+    class MockProgressReceiver implements ProgressReceiver {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzUzMg==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426083532", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-15T23:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NDkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425465503", "bodyText": "I think we could replace this with https://docs.oracle.com/javase/8/docs/api/java/util/function/Consumer.html in respective API and just call parameters errorConsumer.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:26:52Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ErrorReceiver.java", "diffHunk": "@@ -0,0 +1,20 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common;\n+\n+/**\n+ * An {@code ErrorReceiver} is a class that can be used to report errors on transfers. When specified on transfer\n+ * operations, the {@code reportError} method will be called whenever errors are encountered. The user may configure\n+ * this method to report errors in whatever format desired.\n+ */\n+public interface ErrorReceiver<T> {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0Nzc1MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425947751", "bodyText": "Ya in .NET we replaced this interface with a lamda.", "author": "seanmcc-msft", "createdAt": "2020-05-15T17:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4MDYwNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425980604", "bodyText": "Do you think it's not important to follow the pattern of Progress Receiver?", "author": "gapra-msft", "createdAt": "2020-05-15T18:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4NTA3MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425985070", "bodyText": "I think so. In next major rev we could think about using standard Java functional interfaces for Progress receiver as well.\nLet's not create types where standard JDK types can be used.", "author": "kasobol-msft", "createdAt": "2020-05-15T18:46:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjAyMzkxNw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426023917", "bodyText": "@KrzysztofCwalina wasn't a fan of the interface in .NET:\nAzure/azure-sdk-for-net#11586 (comment)", "author": "seanmcc-msft", "createdAt": "2020-05-15T20:10:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzI2Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426057267", "bodyText": "Ok I'm cool with using a consumer interface.", "author": "gapra-msft", "createdAt": "2020-05-15T21:33:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA2Nzc3NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426067774", "bodyText": "I also changed the ProgressReceiver and made a custom BlobQueryProgress class", "author": "gapra-msft", "createdAt": "2020-05-15T22:07:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NjIwNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425466204", "bodyText": "What does it do? 1GB is quite high threshold .", "author": "kasobol-msft", "createdAt": "2020-05-14T22:28:52Z", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,243 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();\n+        if (this.buffer != null) {\n+            this.buffer.close();\n+        }\n+        super.close();\n+        if (this.lastError != null) {\n+            throw logger.logThrowableAsError(this.lastError);\n+        }\n+    }\n+\n+    /**\n+     * Request more data and wait on data to become available.\n+     */\n+    private void blockForData() {\n+        lock.lock();\n+        try {\n+            waitingForData = true;\n+            if (!subscribed) {\n+                subscribeToData();\n+            } else {\n+                subscription.request(1);\n+            }\n+            // Block current thread until data is available.\n+            while (waitingForData) {\n+                if (fluxComplete) {\n+                    break;\n+                } else {\n+                    try {\n+                        dataAvailable.await();\n+                    } catch (InterruptedException e) {\n+                        throw logger.logExceptionAsError(new RuntimeException(e));\n+                    }\n+                }\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Subscribes to the data with a special subscriber.\n+     */\n+    private void subscribeToData() {\n+        this.data\n+            .onBackpressureBuffer(Constants.GB)", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc3OTc1OQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426779759", "bodyText": "This is in case downstream is slower for some reason than the actual upstream flow of data. We dont want to drop any bytebuffers so I think this is the way we tell Reactor don't drop anything. Perhaps the default onBackpressureBuffer() may be more appropriate here than setting it to a GB", "author": "gapra-msft", "createdAt": "2020-05-18T17:18:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NjIwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NzY4NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425467685", "bodyText": "Shouldn't we just use ResponseBase<> ? this type doesn't seem to do anything new. If we need we can always introduce this in the future without breaking change.\nOr is this generated code?", "author": "kasobol-msft", "createdAt": "2020-05-14T22:33:11Z", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQueryAsyncResponse.java", "diffHunk": "@@ -0,0 +1,30 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+import com.azure.core.http.HttpHeaders;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.rest.ResponseBase;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * This class contains the response information returned from the server when running a query on a file.\n+ */\n+public final class FileQueryAsyncResponse extends ResponseBase<FileQueryHeaders, Flux<ByteBuffer>> {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwNzIyOA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425907228", "bodyText": "This is more just following the pattern of what we return on downloads. FileQueryHeaders is a generated type", "author": "gapra-msft", "createdAt": "2020-05-15T16:16:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NzY4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2Nzk0Mw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425467943", "bodyText": "Same feedback here. I think APIs should just return InputStream. We don't need this type.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:34:01Z", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQueryInputStream.java", "diffHunk": "@@ -0,0 +1,23 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+import com.azure.storage.common.implementation.FluxInputStream;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * An <code>InputStream</code> interface that represents the stream to use for reading the query response.\n+ */\n+public class FileQueryInputStream extends FluxInputStream {", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425468986", "bodyText": "I think we should just have normal setter here. Derivatives shouldn't be concerned about this field - I assume we're using inheritance to share this property.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:37:17Z", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+/**\n+ * Defines the input and output serialization for a file query request.\n+ * either {@link FileQueryJsonSerialization} or {@link FileQueryDelimitedSerialization}\n+ */\n+public abstract class FileQuerySerialization {\n+\n+    protected char recordSeparator;\n+\n+    /**\n+     * Gets the record separator.\n+     *\n+     * @return the record separator.\n+     */\n+    public char getRecordSeparator() {\n+        return recordSeparator;\n+    }\n+\n+    abstract FileQuerySerialization setRecordSeparator(char recordSeparator);", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwODkxNQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425908915", "bodyText": "I originally wanted to do that but the issue I had with that was we want to preserve the specific type to keep things  Fluent. Otherwise I wouldnt be able to do something like this to make a BlobQuerySerialization.\nblobQuerySerialization = new BlobQueryDelimitedSerialization().setRecordSeparator('\\n').setColumnSep.....", "author": "gapra-msft", "createdAt": "2020-05-15T16:19:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxMzc4NA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425913784", "bodyText": "Take a look on this creative solution https://stackoverflow.com/questions/14105275/fluent-setters-with-inheritance-in-java .\nOther option would be to move implementation here and just call super.setRecordSeparator + cast in derivatives to avoid repeating implementation.", "author": "kasobol-msft", "createdAt": "2020-05-15T16:27:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NDIwNA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426074204", "bodyText": "Yeah I've seen the one where you suppress warnings before, but I didnt like the fact that I would need to suppress warnings.", "author": "gapra-msft", "createdAt": "2020-05-15T22:31:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkyNDEwNQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426924105", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-18T22:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2OTUzNw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425469537", "bodyText": "Same here. I think this should be just usual setter.", "author": "kasobol-msft", "createdAt": "2020-05-14T22:38:45Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input and output serialization for a blob quick query request.\n+ * either {@link BlobQueryJsonSerialization} or {@link BlobQueryDelimitedSerialization}\n+ */\n+public abstract class BlobQuerySerialization {\n+\n+    protected char recordSeparator;\n+\n+    /**\n+     * Gets the record separator.\n+     *\n+     * @return the record separator.\n+     */\n+    public char getRecordSeparator() {\n+        return recordSeparator;\n+    }\n+\n+    abstract BlobQuerySerialization setRecordSeparator(char recordSeparator);", "originalCommit": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkyNDEzOA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426924138", "bodyText": "done", "author": "gapra-msft", "createdAt": "2020-05-18T22:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2OTUzNw=="}], "type": "inlineReview"}, {"oid": "1400710d00c999789865097c4a751f7230429d35", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1400710d00c999789865097c4a751f7230429d35", "message": "changes to tests", "committedDate": "2020-05-14T23:32:51Z", "type": "commit"}, {"oid": "1c27dada6785e357d40fc729e77a12c1cefe1ef4", "url": "https://github.com/Azure/azure-sdk-for-java/commit/1c27dada6785e357d40fc729e77a12c1cefe1ef4", "message": "Added weird char tests for rec sep", "committedDate": "2020-05-15T17:04:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NDk4Ng==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425944986", "bodyText": "I put the Query API on BlockBlobClient, since it currently only works with block blobs.  If other blobs, we can move it to BaseBlobClient (assuming you have a BaseBlobClient in Java)", "author": "seanmcc-msft", "createdAt": "2020-05-15T17:26:15Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {", "originalCommit": "1c27dada6785e357d40fc729e77a12c1cefe1ef4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4MDA5Nw==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425980097", "bodyText": "I had to do this since you can't get a snapshot block blob client. We figured this is fine since there are some other APIs on Blob Base that don't work for specific types of blobs", "author": "gapra-msft", "createdAt": "2020-05-15T18:36:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NDk4Ng=="}], "type": "inlineReview"}, {"oid": "7c9da64f63e9e71e33e385549f717dd4634eaeea", "url": "https://github.com/Azure/azure-sdk-for-java/commit/7c9da64f63e9e71e33e385549f717dd4634eaeea", "message": "Modified to accoutn for new line", "committedDate": "2020-05-15T18:13:44Z", "type": "commit"}, {"oid": "309ce91eb4191f5d7877275bc336695c4aa1c2d0", "url": "https://github.com/Azure/azure-sdk-for-java/commit/309ce91eb4191f5d7877275bc336695c4aa1c2d0", "message": "Moved blob query logic to implementation class", "committedDate": "2020-05-15T20:09:52Z", "type": "commit"}, {"oid": "dc3dc842f19f768362ddd3b244a67338e5b51074", "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc3dc842f19f768362ddd3b244a67338e5b51074", "message": "Addressed some comments", "committedDate": "2020-05-15T21:31:09Z", "type": "commit"}, {"oid": "f415fecefeab32f9bad2381f7607c81657ddd4e9", "url": "https://github.com/Azure/azure-sdk-for-java/commit/f415fecefeab32f9bad2381f7607c81657ddd4e9", "message": "Converted receiver to Consumer interface", "committedDate": "2020-05-15T22:06:32Z", "type": "commit"}, {"oid": "319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "url": "https://github.com/Azure/azure-sdk-for-java/commit/319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "message": "Addressed more comments", "committedDate": "2020-05-18T17:24:49Z", "type": "commit"}, {"oid": "f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "url": "https://github.com/Azure/azure-sdk-for-java/commit/f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-18T17:28:54Z", "type": "commit"}, {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "url": "https://github.com/Azure/azure-sdk-for-java/commit/13fcf2ee1285db2ac87223d185b180cc9fee03dd", "message": "Added heades", "committedDate": "2020-05-18T19:10:54Z", "type": "commit"}, {"oid": "bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "url": "https://github.com/Azure/azure-sdk-for-java/commit/bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "message": "Added headers and changelog", "committedDate": "2020-05-18T19:16:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0NTk2MQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426845961", "bodyText": "just noticed this was missed.  will fix", "author": "gapra-msft", "createdAt": "2020-05-18T19:25:59Z", "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobClient.java", "diffHunk": "@@ -202,4 +207,41 @@ public PageBlobClient getPageBlobClient() {\n             + \" blob client\"));\n     }\n \n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public InputStream openQueryInputStream(String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query data encrypted on client side.\"));\n+    }\n+\n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public InputStream openQueryInputStream(String expression, BlobQueryOptions queryOptions) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query data encrypted on client side.\"));\n+    }\n+\n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public void query(OutputStream stream, String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query a client side encrypted client\"));", "originalCommit": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0ODY4NQ==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426848685", "bodyText": "I think this will continue to be char from what I can tell from the discussions we've had with the QQ team", "author": "gapra-msft", "createdAt": "2020-05-18T19:31:37Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryDelimitedSerialization.java", "diffHunk": "@@ -0,0 +1,95 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input or output delimited (CSV) serialization for a blob quick query request.\n+ */\n+public class BlobQueryDelimitedSerialization extends\n+    BlobQuerySerialization {\n+\n+    /* TODO(gapra) : Consider diff datatype in case future break. */", "originalCommit": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1Mjg0MA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426852840", "bodyText": "TODO : rename to BlobQueryHeaders", "author": "gapra-msft", "createdAt": "2020-05-18T19:40:23Z", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuickQueryHeaders.java", "diffHunk": "@@ -0,0 +1,1151 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+// Code generated by Microsoft (R) AutoRest Code Generator.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.annotation.HeaderCollection;\n+import com.azure.core.util.CoreUtils;\n+import com.azure.core.util.DateTimeRfc1123;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.dataformat.xml.annotation.JacksonXmlRootElement;\n+import java.time.OffsetDateTime;\n+import java.util.Map;\n+\n+/**\n+ * Defines headers for QuickQuery operation.\n+ */\n+@JacksonXmlRootElement(localName = \"Blob-QuickQuery-Headers\")", "originalCommit": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTkyOA==", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426855928", "bodyText": "i'm not sure why or how some of my other cases got deleted. I'll add them back. Also forgot to add to file", "author": "gapra-msft", "createdAt": "2020-05-18T19:46:47Z", "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,667 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+import java.util.function.Consumer\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        queryData == downloadedData\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        osData == downloadedData\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    @Unroll\n+    def \"Query csv serialization\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+            .setColumnSeparator(columnSeparator as char)\n+            .setEscapeChar(escapeChar as char)\n+            .setFieldQuote(fieldQuote as char)\n+            .setHeadersPresent(headersPresent)\n+        uploadCsv(ser, 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser))\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        if (headersPresent) {\n+            /* Account for 16 bytes of header. */\n+            for (int j = 16; j < downloadedData.length; j++) {\n+                assert queryData[j - 16] == downloadedData[j]\n+            }\n+            for (int k = downloadedData.length - 16; k < downloadedData.length; k++) {\n+                assert queryData[k] == 0\n+            }\n+        } else {\n+            queryData == downloadedData\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser), null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        if (headersPresent) {\n+            assert osData.length == downloadedData.length - 16\n+            /* Account for 16 bytes of header. */\n+            for (int j = 16; j < downloadedData.length; j++) {\n+                assert osData[j - 16] == downloadedData[j]\n+            }\n+        } else {\n+            osData == downloadedData\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        recordSeparator | columnSeparator | escapeChar | fieldQuote | headersPresent || _\n+        '\\n'            | ','             | '\\0'       | '\\0'       | false          || _ /* Default. */\n+        '\\n'            | ','             | '\\0'       | '\\0'       | true           || _ /* Headers. */\n+        '\\t'            | ','             | '\\0'       | '\\0'       | false          || _ /* Record Separators. */\n+        '\\r'            | ','             | '\\0'       | '\\0'       | false          || _\n+        '<'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '>'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '&'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '\\\\'            | ','             | '\\0'       | '\\0'       | false          || _\n+        ','             | '.'             | '\\0'       | '\\0'       | false          || _ /* Column separator. */\n+//        | ','             | '\\0'       | '\\\\'       | false          || _ /* Field quote. */", "originalCommit": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "url": "https://github.com/Azure/azure-sdk-for-java/commit/c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "message": "Modified swagger to use the term Quick Query to Query", "committedDate": "2020-05-18T21:02:34Z", "type": "commit"}, {"oid": "f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "url": "https://github.com/Azure/azure-sdk-for-java/commit/f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "message": "updated abstract class", "committedDate": "2020-05-18T22:21:41Z", "type": "commit"}, {"oid": "aabeb305831ffa3f2c95bc1326e2364b8a54d8e8", "url": "https://github.com/Azure/azure-sdk-for-java/commit/aabeb305831ffa3f2c95bc1326e2364b8a54d8e8", "message": "flux input stream lock changes", "committedDate": "2020-05-20T00:07:41Z", "type": "commit"}, {"oid": "2e7a6a49990e488159a87768259640f60164ca46", "url": "https://github.com/Azure/azure-sdk-for-java/commit/2e7a6a49990e488159a87768259640f60164ca46", "message": "Added separator tests", "committedDate": "2020-05-26T17:43:55Z", "type": "commit"}, {"oid": "84b49796666eef3534f7219dda90e003b214e3c0", "url": "https://github.com/Azure/azure-sdk-for-java/commit/84b49796666eef3534f7219dda90e003b214e3c0", "message": "Added escape and field tests", "committedDate": "2020-05-26T18:05:41Z", "type": "commit"}, {"oid": "22041d34a2a3901b94a7de887700606dc01196b6", "url": "https://github.com/Azure/azure-sdk-for-java/commit/22041d34a2a3901b94a7de887700606dc01196b6", "message": "rerecorded all tests", "committedDate": "2020-05-26T18:39:45Z", "type": "commit"}, {"oid": "75d7c2f58172df2b957140c3b3026aee87e61084", "url": "https://github.com/Azure/azure-sdk-for-java/commit/75d7c2f58172df2b957140c3b3026aee87e61084", "message": "Modified inputstream logic to error on network and input validation early", "committedDate": "2020-05-26T22:09:09Z", "type": "commit"}, {"oid": "94d46e6ce11aa4e1c60e5c263dab7bc8fa794da3", "url": "https://github.com/Azure/azure-sdk-for-java/commit/94d46e6ce11aa4e1c60e5c263dab7bc8fa794da3", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery", "committedDate": "2020-05-26T22:40:04Z", "type": "commit"}, {"oid": "a6968bca9722837f0b3369719dfa11a5f1fe4605", "url": "https://github.com/Azure/azure-sdk-for-java/commit/a6968bca9722837f0b3369719dfa11a5f1fe4605", "message": "Throw via logger", "committedDate": "2020-05-26T23:29:11Z", "type": "commit"}]}