{"pr_number": 734, "pr_title": "Add support for exporting timely events as CSV file.", "pr_createdAt": "2020-08-12T16:16:53Z", "pr_url": "https://github.com/vmware/differential-datalog/pull/734", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4MTg1MA==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r469381850", "bodyText": "I don't know how you feel about adding more dependencies to the project? We can also consider other file formats if CSV seems to verbose or slow.", "author": "gatoWololo", "createdAt": "2020-08-12T16:18:44Z", "path": "rust/template/differential_datalog/Cargo.toml", "diffHunk": "@@ -16,6 +16,7 @@ ordered-float = {version = \"2.0.0\", features = [\"serde\"]}\n fnv = \"1.0.2\"\n timely = \"0.11\"\n libc = \"0.2\"\n+csv = \"1.1\"", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0NTc3NA==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471145774", "bodyText": "I've no problem with adding this dependency.", "author": "ryzhyk", "createdAt": "2020-08-16T18:52:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4MTg1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NDY1Mw==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r469384653", "bodyText": "Before, the context String was attached to every message. From what I can tell, this string doesn't change at the \"batch\" granularity. Is this true? So I only include it once per batch (this allows the profile_statistics code to use the same vector as the existing profiling code Vec<(Duration, usize, TimelyEvent)>. Mainly:\n- TimelyMessage(Vec<((Duration, usize, TimelyEvent), String)>)\n+ TimelyMessage((Vec<(Duration, usize, TimelyEvent)>, String))", "author": "gatoWololo", "createdAt": "2020-08-12T16:23:25Z", "path": "rust/template/differential_datalog/program.rs", "diffHunk": "@@ -1179,14 +1181,21 @@ impl Program {\n                         let profcpu: &AtomicBool = &*profile_cpu3;\n                         /* Filter out events we don't care about to avoid the overhead of sending\n                          * the event around just to drop it eventually. */\n-                        let mut filtered:Vec<((Duration, usize, TimelyEvent), String)> = data.drain(..).filter(|event| match event.2 {\n+                        let mut filtered: Vec<(Duration, usize, TimelyEvent)> = data.drain(..).filter(|event| match event.2 {\n                             TimelyEvent::Operates(_) => true,\n-                            TimelyEvent::Schedule(_) => profcpu.load(Ordering::Acquire),\n+                            TimelyEvent::Schedule(_) |\n+                            TimelyEvent::GuardedMessage(_) |\n+                            TimelyEvent::Park(_) |\n+                            TimelyEvent::Progress(_) |\n+                            TimelyEvent::PushProgress(_) => profcpu.load(Ordering::Acquire),\n                             _ => false\n-                        }).map(|x|(x, get_prof_context())).collect();\n+                        }).collect();\n+\n                         if !filtered.is_empty() {\n                             //eprintln!(\"timely event {:?}\", filtered);\n-                            prof_send1.send(ProfMsg::TimelyMessage(filtered.drain(..).collect())).unwrap();\n+                            // Context doesn't change within a batch, so attach once.", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0ODk0OQ==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471148949", "bodyText": "Umm, looks like the old logic was actually broken. The context string can probably change across messages; however it is only meaningful for TimelyEvents::Operates, so it should only be attached to those events, i.e., the type should be\nVec<((Duration, usize, TimelyEvent), Option<String>)>", "author": "ryzhyk", "createdAt": "2020-08-16T19:27:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NDY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NTY2Nw==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r469385667", "bodyText": "How should I handle failure to create this file? Currently I panic on failure. Should I instead continue executing without writing statistics and print a warning?\nShould the file be named this or have a different name?", "author": "gatoWololo", "createdAt": "2020-08-12T16:24:59Z", "path": "rust/template/differential_datalog/program.rs", "diffHunk": "@@ -1618,9 +1627,10 @@ impl Program {\n \n     /* Profiler thread function */\n     fn prof_thread_func(chan: mpsc::Receiver<ProfMsg>, profile: Arc<Mutex<Profile>>) {\n+        let mut stats = Statistics::new(\"stats.csv\").expect(\"TODO\");", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0ODk3NA==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471148974", "bodyText": "Ideally this should return an error in the appropriate API.", "author": "ryzhyk", "createdAt": "2020-08-16T19:27:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NTY2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4Njk4OQ==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r469386989", "bodyText": "This thread seems to be spawned whether or not profile_cpu is on? And it merely blocks on recv() forever if profiling isn't on. This means that the stat.csv file is always created but without cpu profiling it is empty. Is this okay? I can change the logic so the empty file isn't created when logging is off.", "author": "gatoWololo", "createdAt": "2020-08-12T16:27:13Z", "path": "rust/template/differential_datalog/program.rs", "diffHunk": "@@ -1618,9 +1627,10 @@ impl Program {\n \n     /* Profiler thread function */\n     fn prof_thread_func(chan: mpsc::Receiver<ProfMsg>, profile: Arc<Mutex<Profile>>) {\n+        let mut stats = Statistics::new(\"stats.csv\").expect(\"TODO\");", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0ODExMw==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471148113", "bodyText": "Yes, the thread is spawned because we forward differential events to it unconditionally. And yes, ideally the csv file should not get created if CPU profiling is disabled.", "author": "ryzhyk", "createdAt": "2020-08-16T19:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4Njk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5MDQzNw==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r469390437", "bodyText": "This file needs to have the comments and documentation revised before merging by the way.", "author": "gatoWololo", "createdAt": "2020-08-12T16:32:59Z", "path": "rust/template/differential_datalog/profile_statistics.rs", "diffHunk": "@@ -0,0 +1,301 @@\n+use std::sync::mpsc::{Receiver, Sender, SyncSender};", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0NzU4NQ==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471147585", "bodyText": "\ud83d\udc4d", "author": "ryzhyk", "createdAt": "2020-08-16T19:12:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5MDQzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0NzY4MA==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471147680", "bodyText": "typos in comment", "author": "ryzhyk", "createdAt": "2020-08-16T19:13:20Z", "path": "rust/template/differential_datalog/profile_statistics.rs", "diffHunk": "@@ -0,0 +1,301 @@\n+use std::sync::mpsc::{Receiver, Sender, SyncSender};\n+use std::time::Duration;\n+use std::collections::HashMap;\n+use std::sync::{Arc, Mutex};\n+use std::hash::Hash;\n+use std::thread::JoinHandle;\n+\n+use serde::{Serialize, Deserialize};\n+use std::io::Write;\n+use csv::Writer;\n+\n+use timely::logging::{TimelyEvent, StartStop, BatchLogger, ParkEvent};\n+use std::fs::File;\n+\n+#[derive(Serialize, Deserialize, Debug, Clone)]\n+enum CSVEventType {\n+    Invalid,\n+    OperatorCreation,\n+    OperatorCall,\n+    Schedule,\n+    Progress,\n+    PushProgress,\n+    Messages,\n+    Shutdown,\n+    ChannelCreation,\n+    Application,\n+    GuardedMessage,\n+    GuardedProgress,\n+    CommChannels,\n+    Input,\n+    Park,\n+    Text,\n+    Activation,\n+    EventCounts\n+}\n+\n+impl Default for CSVEventType {\n+    fn default() -> Self { CSVEventType::Invalid }\n+}\n+\n+/// Map from (worker_id, op_id) to start time for timely events.\n+struct StartTimeKeeper<K> {\n+    start_times: HashMap<K, Duration>,\n+}\n+\n+impl <K> StartTimeKeeper<K>\n+    where K: Hash + Eq,\n+{\n+    fn new() -> StartTimeKeeper<K> {\n+        StartTimeKeeper {\n+            start_times: HashMap::new(),\n+        }\n+    }\n+\n+    fn new_start_time(&mut self, key: K, time: Duration) {\n+        assert!(!self.start_times.contains_key(& key));\n+        self.start_times.insert(key, time);\n+    }\n+\n+    fn pop_start_time(&mut self, key: &K) -> Duration {\n+        self.start_times.remove(key).expect(\"Key not present!\")\n+    }\n+}\n+\n+\n+/// This struct will be searilze to crate a row for our CSV format where the columns are this", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE1MjY3OQ==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r471152679", "bodyText": "where is this function used?", "author": "ryzhyk", "createdAt": "2020-08-16T20:08:37Z", "path": "rust/template/differential_datalog/profile_statistics.rs", "diffHunk": "@@ -0,0 +1,301 @@\n+use std::sync::mpsc::{Receiver, Sender, SyncSender};\n+use std::time::Duration;\n+use std::collections::HashMap;\n+use std::sync::{Arc, Mutex};\n+use std::hash::Hash;\n+use std::thread::JoinHandle;\n+\n+use serde::{Serialize, Deserialize};\n+use std::io::Write;\n+use csv::Writer;\n+\n+use timely::logging::{TimelyEvent, StartStop, BatchLogger, ParkEvent};\n+use std::fs::File;\n+\n+#[derive(Serialize, Deserialize, Debug, Clone)]\n+enum CSVEventType {\n+    Invalid,\n+    OperatorCreation,\n+    OperatorCall,\n+    Schedule,\n+    Progress,\n+    PushProgress,\n+    Messages,\n+    Shutdown,\n+    ChannelCreation,\n+    Application,\n+    GuardedMessage,\n+    GuardedProgress,\n+    CommChannels,\n+    Input,\n+    Park,\n+    Text,\n+    Activation,\n+    EventCounts\n+}\n+\n+impl Default for CSVEventType {\n+    fn default() -> Self { CSVEventType::Invalid }\n+}\n+\n+/// Map from (worker_id, op_id) to start time for timely events.\n+struct StartTimeKeeper<K> {\n+    start_times: HashMap<K, Duration>,\n+}\n+\n+impl <K> StartTimeKeeper<K>\n+    where K: Hash + Eq,\n+{\n+    fn new() -> StartTimeKeeper<K> {\n+        StartTimeKeeper {\n+            start_times: HashMap::new(),\n+        }\n+    }\n+\n+    fn new_start_time(&mut self, key: K, time: Duration) {\n+        assert!(!self.start_times.contains_key(& key));\n+        self.start_times.insert(key, time);\n+    }\n+\n+    fn pop_start_time(&mut self, key: &K) -> Duration {\n+        self.start_times.remove(key).expect(\"Key not present!\")\n+    }\n+}\n+\n+\n+/// This struct will be searilze to crate a row for our CSV format where the columns are this\n+/// fields. Use CSVLogEvent::default() to pre-fill all fields and only assign fields relevant for\n+/// your logging event.\n+#[derive(Default, Debug, Serialize, Deserialize, Clone)]\n+struct CSVLogEvent {\n+    worker_id: usize,\n+    event_counts: Option<usize>,\n+    // Physical time the event took place.\n+    start_time: Option<u128>,\n+    end_time: Option<u128>,\n+    // Logical time of event according to timely.\n+    // timestamp: usize,\n+    // Worker-unique identifier for this operator.\n+    operator_id: Option<usize>,\n+    event_type: CSVEventType,\n+    // Address of the operator referred to by the event.\n+    // operator_address: Option<Vec<usize>>,\n+    // Total execution time of event. Only applicable to operator activation right now.\n+    elapsed_time: Option<u128>,\n+    // String name of operator.\n+    operator_name: Option<String>,\n+    // Address of message.\n+    // operator_addr: Option<()>,\n+    // We use string for the addr instead of Vec<usize> since CSV writer attempts to turn the vec\n+    // into multiple columns for an entry. Not what we want.\n+    operator_addr: Option<String>,\n+    // Keep track of whether send or receive event.\n+    is_send: Option<bool>,\n+    source_worker: Option<usize>,\n+    channel: Option<usize>,\n+    sequence_number: Option<usize>\n+}\n+\n+impl CSVLogEvent {\n+    /// CSV serializer will try to turn vectors into comma delimted values. Turn vector into a string\n+    /// with elements separated by '-'. E.g: \"1-2-3\" == vec_to_csv_string(vec![1, 2, 3])\n+    fn vec_to_csv_string(v: &Vec<usize>) -> String {\n+        let mut s = String::new();\n+        for n in v {\n+            s.push_str(&n.to_string());\n+            s.push_str(\"-\");\n+        }\n+        s\n+    }\n+\n+    fn guarded_message_entry(worker_id: usize, start_time: &Duration, end_time: &Duration) -> CSVLogEvent {\n+        let elapsed_time = (*end_time - *start_time).as_nanos();\n+\n+        CSVLogEvent {\n+            worker_id,\n+            start_time: Some(start_time.as_nanos()),\n+            end_time: Some(end_time.as_nanos()),\n+            event_type: CSVEventType::GuardedMessage,\n+            elapsed_time: Some(elapsed_time),\n+            ..CSVLogEvent::default()\n+        }\n+    }\n+\n+    fn park_entry(worker_id: usize, start_time: &Duration, end_time: &Duration) -> CSVLogEvent {\n+        let elapsed_time = (*end_time - *start_time).as_nanos();\n+\n+        CSVLogEvent {\n+            worker_id,\n+            start_time: Some(start_time.as_nanos()),\n+            end_time: Some(end_time.as_nanos()),\n+            event_type: CSVEventType::Park,\n+            elapsed_time: Some(elapsed_time),\n+            ..CSVLogEvent::default()\n+        }\n+    }\n+\n+    fn activation_entry(worker_id: usize, elapsed_time: &Duration) -> CSVLogEvent {\n+        CSVLogEvent {\n+            worker_id: worker_id,\n+            event_type: CSVEventType::Activation,\n+            elapsed_time: Some(elapsed_time.as_nanos()),\n+            ..CSVLogEvent::default()\n+        }\n+    }\n+\n+    fn schedule_entry(worker_id: usize, start_time: &Duration, end_time: &Duration,\n+                      operator_id: usize, operator_name: String, operator_addr: &Vec<usize>) -> CSVLogEvent {\n+        let execution_time = *end_time - *start_time;\n+\n+        CSVLogEvent {\n+            worker_id: worker_id,\n+            start_time: Some(start_time.as_nanos()),\n+            end_time: Some(end_time.as_nanos()),\n+            operator_id: Some(operator_id),\n+            event_type: CSVEventType::Schedule,\n+            elapsed_time: Some(execution_time.as_nanos()),\n+            operator_name: Some(operator_name),\n+            operator_addr: Some(CSVLogEvent::vec_to_csv_string(operator_addr)),\n+            ..CSVLogEvent::default()\n+        }\n+    }\n+\n+    fn progress_entry(worker_id: usize, source_worker: usize, operator_addr: &Vec<usize>,\n+                      sequence_number: usize, is_send: bool, channel_id: usize) -> CSVLogEvent {\n+        CSVLogEvent {\n+            worker_id,\n+            source_worker: Some(source_worker),\n+            event_type: CSVEventType::Progress,\n+            operator_addr: Some(CSVLogEvent::vec_to_csv_string(operator_addr)),\n+            sequence_number: Some(sequence_number),\n+            is_send: Some(is_send),\n+            channel: Some(channel_id),\n+            .. CSVLogEvent::default()\n+        }\n+    }\n+\n+    fn push_progress(worker_id: usize, operator_id: usize) -> CSVLogEvent {\n+        CSVLogEvent {\n+            worker_id,\n+            operator_id: Some(operator_id),\n+            event_type: CSVEventType::PushProgress,\n+            .. CSVLogEvent::default()\n+        }\n+    }\n+}\n+\n+pub struct Statistics {\n+    // Tuple of (worker_id, op_id) make for unique key.\n+    schedule_start_time: StartTimeKeeper<(usize, usize)>,\n+    park_start_time: StartTimeKeeper<usize>,\n+    guarded_message_start_time: StartTimeKeeper<usize>,\n+    // According to documentation ids are worker-unique so we use the worker as part of our key.\n+    // Maps key to operator name and address.\n+\n+    operator_info: HashMap<(usize, usize), (String, Vec<usize>)>,\n+    csv_writer: Writer<File>,\n+}\n+\n+impl Statistics {\n+    pub fn new(path: &str) -> csv::Result<Statistics> {\n+        let csv_writer = Writer::from_path(path)?;\n+        Ok(Statistics {\n+            schedule_start_time: StartTimeKeeper::new(),\n+            park_start_time:  StartTimeKeeper::new(),\n+            guarded_message_start_time: StartTimeKeeper::new(),\n+            operator_info: HashMap::new(),\n+            csv_writer,\n+        })\n+    }\n+\n+    pub fn handle_batch(&mut self, events: &[(Duration, usize, TimelyEvent)]) {\n+        for (timestamp, worker_index, data) in events {\n+            match data {\n+                TimelyEvent::GuardedMessage(g) => {\n+                    if g.is_start {\n+                        self.guarded_message_start_time.new_start_time(*worker_index, *timestamp);\n+                    }\n+                    else {\n+                        let start_time = self.guarded_message_start_time.pop_start_time(&worker_index);\n+                        let execution_time = *timestamp - start_time;\n+                        let e = CSVLogEvent::guarded_message_entry(*worker_index, &start_time, &timestamp);\n+                        self.csv_writer.serialize(e).expect(\"unable to serialize record\");\n+                    }\n+                }\n+                TimelyEvent::Park(ParkEvent::Park(Some(duration))) => {\n+                    panic!(\"Park event with duration not handled!\")\n+                }\n+                TimelyEvent::Park(ParkEvent::Park(None)) => {\n+                    self.park_start_time.new_start_time(*worker_index, *timestamp);\n+                }\n+                TimelyEvent::Park(ParkEvent::Unpark) => {\n+                    let start_time = self.park_start_time.pop_start_time(&worker_index);\n+\n+                    self.csv_writer.serialize(CSVLogEvent::park_entry(*worker_index, &start_time, &timestamp)).\n+                        expect(\"unable to serialize record\");\n+                }\n+                // TimelyEvent::ActivationAdvance(e) => {\n+                //     self.csv_writer.serialize(CSVLogEvent::activation_entry(*worker_index, &e.elapsed)).\n+                //         expect(\"unable to serialize record\");\n+                // }\n+                TimelyEvent::Operates(e) => {\n+                    let key = (*worker_index, e.id);\n+                    debug_assert!(!self.operator_info.contains_key(&key));\n+\n+                    self.operator_info.insert(key, (e.name.clone(), e.addr.clone()));\n+                },\n+                TimelyEvent::Schedule(s) => {\n+                    match s.start_stop {\n+                        StartStop::Start => {\n+                            self.schedule_start_time.new_start_time((*worker_index, s.id), *timestamp);\n+                        }\n+                        StartStop::Stop => {\n+                            let key = & (*worker_index, s.id);\n+                            let start_time = self.schedule_start_time.pop_start_time(key);\n+\n+                            let (op_name, op_addr) = self.operator_info.get(key).expect(\"Name should have been there\");\n+                            let e = CSVLogEvent::schedule_entry(*worker_index, &start_time, &timestamp,\n+                                                                s.id, op_name.clone(), op_addr);\n+                            self.csv_writer.serialize(e).expect(\"unable to serialize record\");\n+                        }\n+                    }\n+                },\n+                TimelyEvent::Progress(p) => {\n+                    let e = CSVLogEvent::progress_entry(*worker_index, p.source, &p.addr,\n+                                                        p.seq_no, p.is_send, p.channel);\n+                    self.csv_writer.serialize(e).expect(\"unable to serialize record\");\n+                }\n+                TimelyEvent::PushProgress(p) => {\n+                    self.csv_writer.serialize(CSVLogEvent::push_progress(*worker_index, p.op_id)).\n+                        expect(\"unable to serialize record\");\n+                }\n+                _ => {\n+                    // Skip.\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+type LogEvents = Vec<(Duration, usize, TimelyEvent)>;\n+type MessageBatch = (Duration, LogEvents);\n+\n+pub fn record_events_csv(path: &str) -> csv::Result<(Arc<Mutex<Sender<MessageBatch>>>, JoinHandle<()>)> {", "originalCommit": "5afd1158eaf16ce49a06e7cee973544285b5b410", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "65fb8fdf32dccec5dc81170ea082dbb1d5e16475", "url": "https://github.com/vmware/differential-datalog/commit/65fb8fdf32dccec5dc81170ea082dbb1d5e16475", "message": "Add support for exporting timely events as CSV file.\n\nAdd new profiling command `profile timely on`.", "committedDate": "2020-08-20T19:53:08Z", "type": "forcePushed"}, {"oid": "30c107db0d35c05770b11a1ca064bb7c46364537", "url": "https://github.com/vmware/differential-datalog/commit/30c107db0d35c05770b11a1ca064bb7c46364537", "message": "Add support for exporting timely events as CSV file.\n\nAdd new profiling command `profile timely on`.", "committedDate": "2020-08-21T00:45:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM2NDkyMg==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r474364922", "bodyText": "Should this be called ddlog_enable_timely_profiling?", "author": "ryzhyk", "createdAt": "2020-08-21T01:56:33Z", "path": "rust/template/ddlog.h", "diffHunk": "@@ -750,6 +750,19 @@ extern int ddlog_enable_cpu_profiling(ddlog_prog prog, bool enable);\n  */\n extern char* ddlog_profile(ddlog_prog prog);\n \n+/*\n+ * Controls recording of timely operator runtimes. When enabled,\n+ * DDlog receives timely dataflow events and writes them out to a CSV file\n+ * where they can be queried later for useful information about program\n+ * execution. This is particularly useful for DDlog programs running with\n+ * multiple workers. When disabled, the recording stops, but the previously\n+ * accumulated profile is preserved.\n+ *\n+ * Recording timely events can be expensive in large dataflows and is\n+ * therefore disabled by default.\n+ */\n+extern int ddlog_enable_cpu_profiling(ddlog_prog prog, bool enable);", "originalCommit": "30c107db0d35c05770b11a1ca064bb7c46364537", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM2NzA5MA==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r474367090", "bodyText": "cpu -> timely", "author": "ryzhyk", "createdAt": "2020-08-21T02:04:49Z", "path": "rust/template/src/api.rs", "diffHunk": "@@ -494,6 +500,17 @@ impl HDDlog {\n         }\n     }\n \n+    fn record_enable_timely_profiling(&self, enable: bool) {\n+        if let Some(ref f) = self.replay_file {\n+            let _ = f.lock().unwrap().record_timely_profiling(enable).map_err(|_| {\n+                self.eprintln(\n+                    \"ddlog_cpu_profiling_enable(): failed to record invocation in replay file\",", "originalCommit": "30c107db0d35c05770b11a1ca064bb7c46364537", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM2NzUwNQ==", "url": "https://github.com/vmware/differential-datalog/pull/734#discussion_r474367505", "bodyText": "you also need to declare ddlog_enable_timely_profiling(), by analogy with ddlog_enable_cpu_profiling() in this function.", "author": "ryzhyk", "createdAt": "2020-08-21T02:06:35Z", "path": "rust/template/src/api.rs", "diffHunk": "@@ -168,6 +168,12 @@ impl HDDlog {\n         self.prog.lock().unwrap().enable_cpu_profiling(enable);\n     }\n \n+    pub fn enable_timely_profiling(&self, enable: bool) {", "originalCommit": "30c107db0d35c05770b11a1ca064bb7c46364537", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d318def56ffb7ddd18dc750e61db72ccd270d6ff", "url": "https://github.com/vmware/differential-datalog/commit/d318def56ffb7ddd18dc750e61db72ccd270d6ff", "message": "Add support for exporting timely events as CSV file.\n\nAdd new profiling command `profile timely on`.", "committedDate": "2020-08-21T14:39:50Z", "type": "forcePushed"}, {"oid": "0050a1681773784592e4d3fb5f259454c930f886", "url": "https://github.com/vmware/differential-datalog/commit/0050a1681773784592e4d3fb5f259454c930f886", "message": "Add support for exporting timely events as CSV file.\n\nAdd new profiling command `profile timely on`.", "committedDate": "2020-08-21T15:30:58Z", "type": "commit"}, {"oid": "0050a1681773784592e4d3fb5f259454c930f886", "url": "https://github.com/vmware/differential-datalog/commit/0050a1681773784592e4d3fb5f259454c930f886", "message": "Add support for exporting timely events as CSV file.\n\nAdd new profiling command `profile timely on`.", "committedDate": "2020-08-21T15:30:58Z", "type": "forcePushed"}]}