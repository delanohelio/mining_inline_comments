{"pr_number": 3949, "pr_title": "[RW-5268, RW-5267] Schema updates and data  pipeline for MVP delivery", "pr_createdAt": "2020-09-01T17:02:53Z", "pr_url": "https://github.com/all-of-us/workbench/pull/3949", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTcwOQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r486535709", "bodyText": "TODO: generate the mocks and assertions for the projections and DTOs.", "author": "jaycarlton", "createdAt": "2020-09-10T18:06:54Z", "path": "api/src/test/java/org/pmiops/workbench/reporting/ReportingSnapshotServiceTest.java", "diffHunk": "@@ -127,29 +128,28 @@ public void testGetSnapshot_someEntries() {\n     final ReportingWorkspace workspace1 = snapshot.getWorkspaces().get(0);\n     assertThat(workspace1.getWorkspaceId()).isEqualTo(101L);\n     assertThat(workspace1.getName()).isEqualTo(\"A Tale of Two Cities\");\n-    assertThat(workspace1.getFakeSize()).isEqualTo(100L);\n     assertThat(workspace1.getCreatorId()).isNull(); // not stubbed\n   }\n \n   private void mockUsers() {\n-    final List<DbUser> users =\n+    final List<PrjReportingUser> users =\n         ImmutableList.of(\n-            createFakeUser(USER_GIVEN_NAME_1, USER_DISABLED_1, USER_ID_1),\n-            createFakeUser(\"Homer\", false, 102L));\n-    doReturn(users).when(mockUserService).getAllUsers();\n+            mockUserProjection(USER_GIVEN_NAME_1, USER_DISABLED_1, USER_ID_1),\n+            mockUserProjection(\"Homer\", false, 102L));\n+    doReturn(users).when(mockUserService).getRepotingUsers();\n   }\n \n-  private DbUser createFakeUser(String givenName, boolean disabled, long userId) {\n-    DbUser user = new DbUser();\n-    user.setUserId(userId);\n-    user.setGivenName(givenName);\n-    user.setFamilyName(FAMILY_NAME);\n-    user.setUsername(PRIMARY_EMAIL);\n-    user.setContactEmail(CONTACT_EMAIL);\n-    user.setOrganization(ORGANIZATION);\n-    user.setCurrentPosition(CURRENT_POSITION);\n-    user.setAreaOfResearch(RESEARCH_PURPOSE);\n-    user.setDisabled(disabled);\n+  private PrjReportingUser mockUserProjection(String givenName, boolean disabled, long userId) {\n+    final PrjReportingUser user = mock(PrjReportingUser.class);", "originalCommit": "235a771e1b1f3ced9a228b7b0e552d131f77fd55", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8", "url": "https://github.com/all-of-us/workbench/commit/a55490889d0741363394bf2e1ffea68f922c04b8", "message": "rebased", "committedDate": "2020-09-10T18:13:07Z", "type": "commit"}, {"oid": "71e921c6c7390eee4639fc7d7fbe6e66c2a189eb", "url": "https://github.com/all-of-us/workbench/commit/71e921c6c7390eee4639fc7d7fbe6e66c2a189eb", "message": "rebased to get tooling", "committedDate": "2020-09-10T18:15:09Z", "type": "forcePushed"}, {"oid": "6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "url": "https://github.com/all-of-us/workbench/commit/6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "message": "alphabetize columns and print output from subprorcess", "committedDate": "2020-09-10T18:33:29Z", "type": "commit"}, {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "url": "https://github.com/all-of-us/workbench/commit/bf7327da516c8bed19c634bd5e9846866ff82c2d", "message": "fixup exclude processing", "committedDate": "2020-09-10T18:39:47Z", "type": "commit"}, {"oid": "1ff4b81feac7cf58005dfc391bad623e425fdd14", "url": "https://github.com/all-of-us/workbench/commit/1ff4b81feac7cf58005dfc391bad623e425fdd14", "message": "cleanup output, indent yaml, and make interface public", "committedDate": "2020-09-10T21:37:42Z", "type": "commit"}, {"oid": "23b9f7394a33cf2c888ca22666f9307b43e31bf0", "url": "https://github.com/all-of-us/workbench/commit/23b9f7394a33cf2c888ca22666f9307b43e31bf0", "message": "email -> username at source & use BqDto prefix", "committedDate": "2020-09-10T21:41:26Z", "type": "commit"}, {"oid": "e29dc6686493e0332b83903e26ea47ce5675e1b1", "url": "https://github.com/all-of-us/workbench/commit/e29dc6686493e0332b83903e26ea47ce5675e1b1", "message": "remove comment", "committedDate": "2020-09-10T21:45:13Z", "type": "forcePushed"}, {"oid": "67330d68aeb791423abaeb912b1dd441c6de880b", "url": "https://github.com/all-of-us/workbench/commit/67330d68aeb791423abaeb912b1dd441c6de880b", "message": "fix newlines in queries and add projection fucntion decl", "committedDate": "2020-09-10T22:30:07Z", "type": "commit"}, {"oid": "27b20c516b4d37722344cf788806fffe72c9c0eb", "url": "https://github.com/all-of-us/workbench/commit/27b20c516b4d37722344cf788806fffe72c9c0eb", "message": "integrrate more of the new types", "committedDate": "2020-09-10T22:30:57Z", "type": "forcePushed"}, {"oid": "ffef953e334f4967bd7040820d39f80f81548415", "url": "https://github.com/all-of-us/workbench/commit/ffef953e334f4967bd7040820d39f80f81548415", "message": "patch for research purpose entity naming convention", "committedDate": "2020-09-11T15:50:22Z", "type": "commit"}, {"oid": "6ba36379bf720f3e7132c3098c63eb0351e11f60", "url": "https://github.com/all-of-us/workbench/commit/6ba36379bf720f3e7132c3098c63eb0351e11f60", "message": "fixes", "committedDate": "2020-09-11T18:47:27Z", "type": "commit"}, {"oid": "6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "url": "https://github.com/all-of-us/workbench/commit/6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "message": "PR comments", "committedDate": "2020-09-11T19:22:12Z", "type": "commit"}, {"oid": "14ca852747d563fdc3ed733643ad0507ab490de7", "url": "https://github.com/all-of-us/workbench/commit/14ca852747d563fdc3ed733643ad0507ab490de7", "message": "rebased to get tooling", "committedDate": "2020-09-11T19:24:35Z", "type": "commit"}, {"oid": "364956bb5c0652cb0dcfdc70d7c77ae9f78d514b", "url": "https://github.com/all-of-us/workbench/commit/364956bb5c0652cb0dcfdc70d7c77ae9f78d514b", "message": "remove comment", "committedDate": "2020-09-11T19:24:35Z", "type": "commit"}, {"oid": "80d78a7ced0d963e10deef01abe298aa051ad26e", "url": "https://github.com/all-of-us/workbench/commit/80d78a7ced0d963e10deef01abe298aa051ad26e", "message": "starting to merge the generated stuff", "committedDate": "2020-09-11T19:24:35Z", "type": "commit"}, {"oid": "2d71c07d8d13b8a903460d996b07b334f9bab83e", "url": "https://github.com/all-of-us/workbench/commit/2d71c07d8d13b8a903460d996b07b334f9bab83e", "message": "remove comment", "committedDate": "2020-09-11T19:24:35Z", "type": "commit"}, {"oid": "2dcded35a425dbd1bd31585eb217cb69c1b89bcc", "url": "https://github.com/all-of-us/workbench/commit/2dcded35a425dbd1bd31585eb217cb69c1b89bcc", "message": "integrrate more of the new types", "committedDate": "2020-09-11T19:24:35Z", "type": "commit"}, {"oid": "5d9a96b86929208f47a6d97ff7209302b31a086f", "url": "https://github.com/all-of-us/workbench/commit/5d9a96b86929208f47a6d97ff7209302b31a086f", "message": "fixes", "committedDate": "2020-09-11T19:24:36Z", "type": "commit"}, {"oid": "12e5c66a6a8c2aab98b72c66582ba6fa602d2aa4", "url": "https://github.com/all-of-us/workbench/commit/12e5c66a6a8c2aab98b72c66582ba6fa602d2aa4", "message": "more fixes", "committedDate": "2020-09-11T19:24:36Z", "type": "commit"}, {"oid": "91713a59b7c854b4ed99e40b13e950a8f13b4c51", "url": "https://github.com/all-of-us/workbench/commit/91713a59b7c854b4ed99e40b13e950a8f13b4c51", "message": "updated", "committedDate": "2020-09-11T19:24:36Z", "type": "commit"}, {"oid": "8aa7af28360219041ed59516c34e3670ed4c06a5", "url": "https://github.com/all-of-us/workbench/commit/8aa7af28360219041ed59516c34e3670ed4c06a5", "message": "compiles & runs test now", "committedDate": "2020-09-11T19:43:56Z", "type": "commit"}, {"oid": "c9a32063ac848f20cb4188b5cc2edae3022ff8f9", "url": "https://github.com/all-of-us/workbench/commit/c9a32063ac848f20cb4188b5cc2edae3022ff8f9", "message": "integrating unit test", "committedDate": "2020-09-11T21:53:09Z", "type": "commit"}, {"oid": "c9a32063ac848f20cb4188b5cc2edae3022ff8f9", "url": "https://github.com/all-of-us/workbench/commit/c9a32063ac848f20cb4188b5cc2edae3022ff8f9", "message": "integrating unit test", "committedDate": "2020-09-11T21:53:09Z", "type": "forcePushed"}, {"oid": "7f61354bbbb32bfbe1054f643090822d1f2eff3f", "url": "https://github.com/all-of-us/workbench/commit/7f61354bbbb32bfbe1054f643090822d1f2eff3f", "message": "merge master", "committedDate": "2020-09-11T22:10:01Z", "type": "commit"}, {"oid": "846aacd2d102e024bccd06b8c192376ec92b9865", "url": "https://github.com/all-of-us/workbench/commit/846aacd2d102e024bccd06b8c192376ec92b9865", "message": "user test case", "committedDate": "2020-09-12T22:45:52Z", "type": "commit"}, {"oid": "a54c82584fa540571f04ef6b8aa8f13c3fa6d25a", "url": "https://github.com/all-of-us/workbench/commit/a54c82584fa540571f04ef6b8aa8f13c3fa6d25a", "message": "timestamp fixes and unique values for all constants", "committedDate": "2020-09-14T15:13:12Z", "type": "commit"}, {"oid": "1e0a104e4b25bf6b9d182b2e456a4adcca26f1c7", "url": "https://github.com/all-of-us/workbench/commit/1e0a104e4b25bf6b9d182b2e456a4adcca26f1c7", "message": "spotless", "committedDate": "2020-09-14T15:30:35Z", "type": "commit"}, {"oid": "968e2dbabce6b031f36c38d3a28050edc1aed692", "url": "https://github.com/all-of-us/workbench/commit/968e2dbabce6b031f36c38d3a28050edc1aed692", "message": "integrate query parameter column entries adn fixup tests", "committedDate": "2020-09-14T20:26:17Z", "type": "commit"}, {"oid": "054b9ec97f73070be8a6cd478403598ccbc92c17", "url": "https://github.com/all-of-us/workbench/commit/054b9ec97f73070be8a6cd478403598ccbc92c17", "message": "manage things a bit better", "committedDate": "2020-09-14T21:21:57Z", "type": "commit"}, {"oid": "9740201eff4ba053b1dd7cb99d03456241b3aba2", "url": "https://github.com/all-of-us/workbench/commit/9740201eff4ba053b1dd7cb99d03456241b3aba2", "message": "stub the DTO too", "committedDate": "2020-09-14T21:45:27Z", "type": "commit"}, {"oid": "42aa8019c20769f8dfb19cc2255bfd2852033cf5", "url": "https://github.com/all-of-us/workbench/commit/42aa8019c20769f8dfb19cc2255bfd2852033cf5", "message": "rename method", "committedDate": "2020-09-14T21:47:58Z", "type": "commit"}, {"oid": "c28c7b3832f03111656e0f8fe0e133c137fb0679", "url": "https://github.com/all-of-us/workbench/commit/c28c7b3832f03111656e0f8fe0e133c137fb0679", "message": "start fixing up the time strings", "committedDate": "2020-09-15T00:40:09Z", "type": "commit"}, {"oid": "211a5fe466a7f07f4e83d1c1cc24c255817e46c5", "url": "https://github.com/all-of-us/workbench/commit/211a5fe466a7f07f4e83d1c1cc24c255817e46c5", "message": "complete updates to parameter enums", "committedDate": "2020-09-15T14:18:40Z", "type": "commit"}, {"oid": "cf82bcea877f10e7462742dfeb7338b8233c5625", "url": "https://github.com/all-of-us/workbench/commit/cf82bcea877f10e7462742dfeb7338b8233c5625", "message": "use LocalDateTiem to parse, then add zone later", "committedDate": "2020-09-15T14:33:46Z", "type": "commit"}, {"oid": "c24ba2ede63a8db3127c6a0df13b3ef559f0ea5d", "url": "https://github.com/all-of-us/workbench/commit/c24ba2ede63a8db3127c6a0df13b3ef559f0ea5d", "message": "update schemas", "committedDate": "2020-09-15T15:09:34Z", "type": "commit"}, {"oid": "a992889b782a691ba5d03927d9b628fc71e6b6d6", "url": "https://github.com/all-of-us/workbench/commit/a992889b782a691ba5d03927d9b628fc71e6b6d6", "message": "restore cron script", "committedDate": "2020-09-15T15:33:27Z", "type": "commit"}, {"oid": "6212d96a394199d49838f03f24fed87d55258809", "url": "https://github.com/all-of-us/workbench/commit/6212d96a394199d49838f03f24fed87d55258809", "message": "Merge branch 'master' into jaycarlton/projectionTest2", "committedDate": "2020-09-15T15:33:31Z", "type": "commit"}, {"oid": "1987b532e9b3606d1ecd6f6b52b34539674717b2", "url": "https://github.com/all-of-us/workbench/commit/1987b532e9b3606d1ecd6f6b52b34539674717b2", "message": "more fixes for types like Short", "committedDate": "2020-09-16T17:38:38Z", "type": "commit"}, {"oid": "a7eb77cc193af075050653a8e066a2dd042f7457", "url": "https://github.com/all-of-us/workbench/commit/a7eb77cc193af075050653a8e066a2dd042f7457", "message": "test stuff", "committedDate": "2020-09-16T20:49:13Z", "type": "commit"}, {"oid": "3e4f149b79e2c88752ac2493b7fc451d98e2834f", "url": "https://github.com/all-of-us/workbench/commit/3e4f149b79e2c88752ac2493b7fc451d98e2834f", "message": "Merge branch 'master' into jaycarlton/projectionTest2", "committedDate": "2020-09-16T21:17:35Z", "type": "commit"}, {"oid": "06f09fbad6b4a417a646dc456055a8f72cf3500c", "url": "https://github.com/all-of-us/workbench/commit/06f09fbad6b4a417a646dc456055a8f72cf3500c", "message": "start using enum columns", "committedDate": "2020-09-16T23:10:04Z", "type": "commit"}, {"oid": "397ee33665f1a8a3e696e25a09afbcad261ed3d6", "url": "https://github.com/all-of-us/workbench/commit/397ee33665f1a8a3e696e25a09afbcad261ed3d6", "message": "workspace dao test works", "committedDate": "2020-09-16T23:23:40Z", "type": "commit"}, {"oid": "3fbc4d12b4a32e265af4d5c750e3127dbe4c27c9", "url": "https://github.com/all-of-us/workbench/commit/3fbc4d12b4a32e265af4d5c750e3127dbe4c27c9", "message": "fixes for enums", "committedDate": "2020-09-17T14:29:06Z", "type": "commit"}, {"oid": "3da11436ead5ae3dbf820ee1f113e38a794d52e6", "url": "https://github.com/all-of-us/workbench/commit/3da11436ead5ae3dbf820ee1f113e38a794d52e6", "message": "fixed up more idiosyncracies and tests pass now", "committedDate": "2020-09-17T18:42:00Z", "type": "commit"}, {"oid": "21c5e815ef6ccd7e4dc7fdebb2b4e53ac74c042c", "url": "https://github.com/all-of-us/workbench/commit/21c5e815ef6ccd7e4dc7fdebb2b4e53ac74c042c", "message": "fix", "committedDate": "2020-09-17T19:36:35Z", "type": "commit"}, {"oid": "8a6fa04fdd7e9d813e95662d96063e77ad70a4d6", "url": "https://github.com/all-of-us/workbench/commit/8a6fa04fdd7e9d813e95662d96063e77ad70a4d6", "message": "progress", "committedDate": "2020-09-17T22:42:40Z", "type": "commit"}, {"oid": "0dc903d3146f5bb21a3cc31fb0d7134c5b7656c3", "url": "https://github.com/all-of-us/workbench/commit/0dc903d3146f5bb21a3cc31fb0d7134c5b7656c3", "message": "fill in test cases", "committedDate": "2020-09-18T13:21:39Z", "type": "commit"}, {"oid": "a7a264c0b37f3f81d73fa6c21b5ed5a31b319175", "url": "https://github.com/all-of-us/workbench/commit/a7a264c0b37f3f81d73fa6c21b5ed5a31b319175", "message": "spotless & test fixes", "committedDate": "2020-09-18T14:42:42Z", "type": "commit"}, {"oid": "f3c601bc40ca0d31fcd3e64de0b7578a2d7be776", "url": "https://github.com/all-of-us/workbench/commit/f3c601bc40ca0d31fcd3e64de0b7578a2d7be776", "message": "dont include null columns in RowToInsert content map", "committedDate": "2020-09-18T15:43:58Z", "type": "commit"}, {"oid": "ec47fee4d52015d68f2a516d96f00f6a58b201e2", "url": "https://github.com/all-of-us/workbench/commit/ec47fee4d52015d68f2a516d96f00f6a58b201e2", "message": "enum string fix", "committedDate": "2020-09-18T17:29:49Z", "type": "commit"}, {"oid": "c7e740b9f0ef28faa4e9897b3ea79598bb84ab0f", "url": "https://github.com/all-of-us/workbench/commit/c7e740b9f0ef28faa4e9897b3ea79598bb84ab0f", "message": "more fixes", "committedDate": "2020-09-18T20:21:32Z", "type": "commit"}, {"oid": "afc36a3f4cf4961ce3d66041567c791a598ecdaf", "url": "https://github.com/all-of-us/workbench/commit/afc36a3f4cf4961ce3d66041567c791a598ecdaf", "message": "simplified", "committedDate": "2020-09-21T00:42:01Z", "type": "commit"}, {"oid": "7ea1d4335d817765ca2ce9997d55c32ca09ec6d3", "url": "https://github.com/all-of-us/workbench/commit/7ea1d4335d817765ca2ce9997d55c32ca09ec6d3", "message": "fixed stuff I broke Saturday", "committedDate": "2020-09-21T13:07:32Z", "type": "commit"}, {"oid": "628d769bea61356b98a0291ffa5803317b27af39", "url": "https://github.com/all-of-us/workbench/commit/628d769bea61356b98a0291ffa5803317b27af39", "message": "use Short for projection, enum for DTO", "committedDate": "2020-09-21T13:27:48Z", "type": "commit"}, {"oid": "ee8ab076d0da10e1360cda8e52f2f5f051ac581a", "url": "https://github.com/all-of-us/workbench/commit/ee8ab076d0da10e1360cda8e52f2f5f051ac581a", "message": "more fixes", "committedDate": "2020-09-21T15:07:50Z", "type": "commit"}, {"oid": "bb22bd8c66706e03a506db910bba06825757f200", "url": "https://github.com/all-of-us/workbench/commit/bb22bd8c66706e03a506db910bba06825757f200", "message": "test fixes", "committedDate": "2020-09-21T16:37:00Z", "type": "commit"}, {"oid": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "url": "https://github.com/all-of-us/workbench/commit/41864b64e330baa307c8e3dfb4a72886821b5ca1", "message": "spotless", "committedDate": "2020-09-21T17:45:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjIzOTM4Nw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492239387", "bodyText": "To view what the schema looks like for a table, the command is\n bq show --format=prettyjson all-of-us-workbench-test:reporting_local.user", "author": "jaycarlton", "createdAt": "2020-09-21T17:46:48Z", "path": "api/reporting/schemas/latest/user.json", "diffHunk": "@@ -0,0 +1,175 @@\n+[", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjIzOTc5MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492239790", "bodyText": "Enums are mapped to BigQuery strings.", "author": "jaycarlton", "createdAt": "2020-09-21T17:47:26Z", "path": "api/reporting/schemas/latest/user.json", "diffHunk": "@@ -0,0 +1,175 @@\n+[\n+  {\n+    \"description\": \"Time snapshot was taken. Same across all rows in the snapshot, and can be used for partitioning.\",\n+    \"name\": \"snapshot_timestamp\",\n+    \"type\": \"INTEGER\"\n+  },\n+  {\n+    \"name\": \"about_you\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"area_of_research\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"beta_access_bypass_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"beta_access_request_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"compliance_training_bypass_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"compliance_training_completion_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"compliance_training_expiration_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"contact_email\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"creation_time\",\n+    \"type\": \"TIMESTAMP\"\n+  },\n+  {\n+    \"name\": \"current_position\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"data_access_level\",", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MDI3OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492240279", "bodyText": "The snapshot_timestamp ties all the tables together. This PR just has the first two, but adding in the rest should be much simpler.", "author": "jaycarlton", "createdAt": "2020-09-21T17:48:22Z", "path": "api/reporting/schemas/latest/workspace.json", "diffHunk": "@@ -0,0 +1,143 @@\n+[\n+  {\n+    \"description\": \"Time snapshot was taken. Same across all rows in the snapshot, and can be used for partitioning.\",\n+    \"name\": \"snapshot_timestamp\",", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MTAzOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492241038", "bodyText": "This looks bad, but it's almost entirely functional, so there's no state to track really.\nSeveral columns (especially enums) have multiple different representations in different places.", "author": "jaycarlton", "createdAt": "2020-09-21T17:49:42Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MTkzMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492241932", "bodyText": "Several of our entities have columns not named in the standard way. DbWorkspace is the worst offender here.", "author": "jaycarlton", "createdAt": "2020-09-21T17:51:17Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'\n+        },\n+        'billing_account_type' => {\n+            :java => 'BillingAccountType',\n+            :constant => 'BillingAccountType.FREE_TIER'\n+        }\n+     },\n+     'user' => {\n+         'data_access_level' => {\n+             :java => 'DataAccessLevel',\n+             :constant => 'DataAccessLevel.REGISTERED'\n+         },\n+         'email_verification_status' => {\n+             :java => 'EmailVerificationStatus',\n+             :constant => 'EmailVerificationStatus.SUBSCRIBED'\n+         }\n+     }\n+}.freeze\n+\n+# strip size/kind\n+def simple_mysql_type(mysql_type)\n   type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n   match_data = mysql_type.match(type_pattern)\n-  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  result = match_data[:type]\n   raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n   result\n end\n \n-excluded_fields = File.exist?(inputs[:exclude_columns]) \\\n-  ? File.readlines(inputs[:exclude_columns]) \\\n+excluded_fields = File.exist?(INPUTS[:excluded_columns]) \\\n+  ? File.readlines(INPUTS[:excluded_columns]) \\\n       .map{ |c| c.strip } \\\n   : []\n \n def include_field?(excluded_fields, field)\n   !excluded_fields.include?(field)\n end\n \n+ENTITY_MODIFIED_COLUMNS = {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MjQ3Mw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492242473", "bodyText": "This is the most important structure, where types, declarations, or miscellaneous syntax are attached to each column.", "author": "jaycarlton", "createdAt": "2020-09-21T17:52:18Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'\n+        },\n+        'billing_account_type' => {\n+            :java => 'BillingAccountType',\n+            :constant => 'BillingAccountType.FREE_TIER'\n+        }\n+     },\n+     'user' => {\n+         'data_access_level' => {\n+             :java => 'DataAccessLevel',\n+             :constant => 'DataAccessLevel.REGISTERED'\n+         },\n+         'email_verification_status' => {\n+             :java => 'EmailVerificationStatus',\n+             :constant => 'EmailVerificationStatus.SUBSCRIBED'\n+         }\n+     }\n+}.freeze\n+\n+# strip size/kind\n+def simple_mysql_type(mysql_type)\n   type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n   match_data = mysql_type.match(type_pattern)\n-  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  result = match_data[:type]\n   raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n   result\n end\n \n-excluded_fields = File.exist?(inputs[:exclude_columns]) \\\n-  ? File.readlines(inputs[:exclude_columns]) \\\n+excluded_fields = File.exist?(INPUTS[:excluded_columns]) \\\n+  ? File.readlines(INPUTS[:excluded_columns]) \\\n       .map{ |c| c.strip } \\\n   : []\n \n def include_field?(excluded_fields, field)\n   !excluded_fields.include?(field)\n end\n \n+ENTITY_MODIFIED_COLUMNS = {\n+    'workspace' => {\n+        'cdr_version_id' => 'cdrVersion.cdrVersionId AS cdrVersionId',\n+        'creator_id' => 'creator.userId AS creatorId',\n+        'needs_rp_review_prompt' => 'needsResearchPurposeReviewPrompt AS needsRpReviewPrompt'\n+    }\n+}\n+\n ## BigQuery schema\n-describe_rows = CSV.new(File.read(inputs[:describe_csv])).sort_by { |row| row[0]  }\n+describe_rows = CSV.new(File.read(INPUTS[:describe_csv])).sort_by { |row| row[0]  }\n+\n+root_class_name = to_camel_case(table_name, true)\n+TABLE_INFO = {\n+    :name => table_name,\n+    :instance_name => to_camel_case(table_name, false),\n+    :dto_class => \"BqDto#{root_class_name}\",\n+    :entity_class => \"Db#{root_class_name}\",\n+    :mock => \"mock#{root_class_name}\",\n+    :projection_interface => \"Prj#{root_class_name}\",\n+    :sql_alias => table_name[0].downcase,\n+    :enum_column_info => ENUM_TYPES[table_name] || {},\n+    :entity_modified_columns => ENTITY_MODIFIED_COLUMNS[:table_name] || {}\n+}.freeze\n+\n+def to_swagger_name(snake_case, is_class_name)\n+  result =  snake_case.split('_').collect(&:capitalize).join\n+  unless is_class_name\n+    result[0] = result[0].downcase\n+  end\n+  result\n+end\n \n-columns = describe_rows.filter{ |row| include_field?(excluded_fields, row[0])} \\\n-  .map{ |row| {:name => row[0], :mysql_type => row[1], :big_query_type => to_bq_type(row[1])} }\n+def to_property_name(column_name)\n+  to_swagger_name(column_name, false)\n+end\n+\n+COLUMNS = describe_rows.filter{ |row| include_field?(excluded_fields, row[0]) } \\", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MjU4Mw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492242583", "bodyText": "freeze just makes the constant immutable.", "author": "jaycarlton", "createdAt": "2020-09-21T17:52:31Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'\n+        },\n+        'billing_account_type' => {\n+            :java => 'BillingAccountType',\n+            :constant => 'BillingAccountType.FREE_TIER'\n+        }\n+     },\n+     'user' => {\n+         'data_access_level' => {\n+             :java => 'DataAccessLevel',\n+             :constant => 'DataAccessLevel.REGISTERED'\n+         },\n+         'email_verification_status' => {\n+             :java => 'EmailVerificationStatus',\n+             :constant => 'EmailVerificationStatus.SUBSCRIBED'\n+         }\n+     }\n+}.freeze\n+\n+# strip size/kind\n+def simple_mysql_type(mysql_type)\n   type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n   match_data = mysql_type.match(type_pattern)\n-  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  result = match_data[:type]\n   raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n   result\n end\n \n-excluded_fields = File.exist?(inputs[:exclude_columns]) \\\n-  ? File.readlines(inputs[:exclude_columns]) \\\n+excluded_fields = File.exist?(INPUTS[:excluded_columns]) \\\n+  ? File.readlines(INPUTS[:excluded_columns]) \\\n       .map{ |c| c.strip } \\\n   : []\n \n def include_field?(excluded_fields, field)\n   !excluded_fields.include?(field)\n end\n \n+ENTITY_MODIFIED_COLUMNS = {\n+    'workspace' => {\n+        'cdr_version_id' => 'cdrVersion.cdrVersionId AS cdrVersionId',\n+        'creator_id' => 'creator.userId AS creatorId',\n+        'needs_rp_review_prompt' => 'needsResearchPurposeReviewPrompt AS needsRpReviewPrompt'\n+    }\n+}\n+\n ## BigQuery schema\n-describe_rows = CSV.new(File.read(inputs[:describe_csv])).sort_by { |row| row[0]  }\n+describe_rows = CSV.new(File.read(INPUTS[:describe_csv])).sort_by { |row| row[0]  }\n+\n+root_class_name = to_camel_case(table_name, true)\n+TABLE_INFO = {\n+    :name => table_name,\n+    :instance_name => to_camel_case(table_name, false),\n+    :dto_class => \"BqDto#{root_class_name}\",\n+    :entity_class => \"Db#{root_class_name}\",\n+    :mock => \"mock#{root_class_name}\",\n+    :projection_interface => \"Prj#{root_class_name}\",\n+    :sql_alias => table_name[0].downcase,\n+    :enum_column_info => ENUM_TYPES[table_name] || {},\n+    :entity_modified_columns => ENTITY_MODIFIED_COLUMNS[:table_name] || {}\n+}.freeze\n+\n+def to_swagger_name(snake_case, is_class_name)\n+  result =  snake_case.split('_').collect(&:capitalize).join\n+  unless is_class_name\n+    result[0] = result[0].downcase\n+  end\n+  result\n+end\n \n-columns = describe_rows.filter{ |row| include_field?(excluded_fields, row[0])} \\\n-  .map{ |row| {:name => row[0], :mysql_type => row[1], :big_query_type => to_bq_type(row[1])} }\n+def to_property_name(column_name)\n+  to_swagger_name(column_name, false)\n+end\n+\n+COLUMNS = describe_rows.filter{ |row| include_field?(excluded_fields, row[0]) } \\\n+  .map{ |row| \\\n+    col_name = row[0]\n+    mysql_type = simple_mysql_type(row[1])\n+    type_override = TABLE_INFO[:enum_column_info][col_name]\n+    type_info = MYSQL_TO_TYPES[mysql_type]\n+    {\n+        :name => col_name, \\\n+        :lambda_var => table_name[0].downcase, \\\n+        :mysql_type => mysql_type, \\\n+        :big_query_type => type_override ? type_override[:bigquery] : type_info[:bigquery], \\\n+        :is_enum => !type_override.nil?,\n+        :java_type => type_override ? type_override[:java] : type_info[:java], \\\n+        :java_field_name => \"#{to_camel_case(col_name, false)}\", \\\n+        :java_constant_name => \"#{table_name.upcase}__#{col_name.upcase}\", \\\n+        :getter => \"get#{to_camel_case(col_name, true)}\",\n+        :setter => \"set#{to_camel_case(col_name, true)}\",\n+        :property => to_property_name(col_name),\n+        :default_enum => type_override && type_override[:constant]\n+    }\n+}.freeze", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0MzA1NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492243055", "bodyText": "This maps to OffsetDateTime in Java now.", "author": "jaycarlton", "createdAt": "2020-09-21T17:53:19Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -93,40 +217,41 @@ def include_field?(excluded_fields, field)\n         'format' => 'int64'\n     },\n     'TIMESTAMP' =>  {\n-        'type'  => 'integer',\n-        'format' => 'int64'\n+        'type' => 'string',", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0NDc0MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492244741", "bodyText": "I generate all the DTO classes in swagger, and they're just simple objects. Each maps to one row table. I had written this system before realizing how much I needed to write scaffolding code. If I'd known that, then I'd probably try to generate classes. They're very similar to the projection interfaces, but those are 1:1 with MySql queries, not tables.", "author": "jaycarlton", "createdAt": "2020-09-21T17:56:05Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -93,40 +217,41 @@ def include_field?(excluded_fields, field)\n         'format' => 'int64'\n     },\n     'TIMESTAMP' =>  {\n-        'type'  => 'integer',\n-        'format' => 'int64'\n+        'type' => 'string',\n+        'format' => 'date-time'\n     },\n     'BOOLEAN' =>  {\n-        'type'  => 'boolean',\n-        'default' => false\n+        'type'  => 'boolean'\n     },\n     'FLOAT64' => {\n         'type' =>  'number',\n         'format' =>  'double'\n     }\n-}\n+}.freeze\n \n-def to_swagger_name(snake_case, is_class_name)\n-  result =  snake_case.split('_').collect(&:capitalize).join\n-  unless is_class_name\n-    result[0] = result[0].downcase\n-  end\n-  result\n-end\n-\n-def to_property_name(column_name)\n-  to_swagger_name(column_name, false)\n-end\n \n def to_swagger_property(column)\n-  { 'description' => column[:description] || '' } \\\n-    .merge(BIGQUERY_TYPE_TO_SWAGGER[column[:big_query_type]])\n+  if column[:is_enum]\n+    swagger_value = {\n+        '$ref' => \"#/definitions/#{column[:java_type]}\"\n+    }\n+  # elsif column[:name] == 'data_access_level' && TABLE_INFO[:name] == 'workspace'\n+  #   # we're mapping to the enum in MapStruct b/c the entity class is wonky\n+  #   # TODO: make this kind of override more formally supported if there are other places to do it.\n+  #   swagger_value = {\n+  #       '$ref' => \"#/definitions/DataAccessLevel\"\n+  #   }\n+  else\n+    swagger_value = BIGQUERY_TYPE_TO_SWAGGER[column[:big_query_type]]\n+  end\n+  # { 'description' => column[:description] || '' } \\\n+  {}.merge(swagger_value)\n end\n \n-swagger_object =  {  dto_class_name => {\n+swagger_object =  {  TABLE_INFO[:dto_class] => {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0NTk1MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492245950", "bodyText": "Pretty much the entire reason for this script was to keep the projection definitions exctly in line with the DAO queries in type, name, and order. Spring won't tell you you've messed this up until runtime when you try to call one of the methods, and you get the dreaded Projection Type must be an interface!.", "author": "jaycarlton", "createdAt": "2020-09-21T17:57:58Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0NjQ4NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492246485", "bodyText": "research purpose prefix handling. Could be refactored into the enum mapping stuff.", "author": "jaycarlton", "createdAt": "2020-09-21T17:58:56Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0Nzc0Nw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492247747", "bodyText": "for the sake of paranoia, I wanted to make sure almost now columns had the same value for a given row. It's very easy to get things switched around, and if you make an error like that, the preview text in the debugger will show keys and values all mismatched and scare you to death..", "author": "jaycarlton", "createdAt": "2020-09-21T18:01:02Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0ODA4NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492248084", "bodyText": "These sets of constants are used by several unit test classes.", "author": "jaycarlton", "createdAt": "2020-09-21T18:01:38Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0ODc1NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492248755", "bodyText": "If I had to do this again, I'd have a folder structure like out/$table_name/projection.java so all the files for a particular table were together.", "author": "jaycarlton", "createdAt": "2020-09-21T18:02:56Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0OTU3MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492249570", "bodyText": "Timestamp handling is the other bane of all migrations. CommonMappers now has a method to go from Timestamp to OffsetDateTime in the UTC zone, so the assertion here will compare two of the same type.\nSample output from this section:\n    assertThat(user.getAboutYou()).isEqualTo(USER__ABOUT_YOU);\n    assertThat(user.getAreaOfResearch()).isEqualTo(USER__AREA_OF_RESEARCH);\n    assertTimeApprox(user.getBetaAccessBypassTime(), USER__BETA_ACCESS_BYPASS_TIME);\n    assertTimeApprox(user.getBetaAccessRequestTime(), USER__BETA_ACCESS_REQUEST_TIME);", "author": "jaycarlton", "createdAt": "2020-09-21T18:04:29Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')\n+\n+### Assertions\n+dto_assertions = COLUMNS.map{ |col|\n+  getter_call = \"#{to_camel_case(table_name, false)}.#{col[:getter]}()\"\n+  expected = col[:java_constant_name]\n+  if col[:java_type].eql?('Timestamp')\n+    \"    assertTimeApprox(#{getter_call}, #{expected});\"", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1MDg0MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492250840", "bodyText": "The double underscores between the table and column make it easier to read if the table name has a single underscore.", "author": "jaycarlton", "createdAt": "2020-09-21T18:06:48Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'\n+        },\n+        'billing_account_type' => {\n+            :java => 'BillingAccountType',\n+            :constant => 'BillingAccountType.FREE_TIER'\n+        }\n+     },\n+     'user' => {\n+         'data_access_level' => {\n+             :java => 'DataAccessLevel',\n+             :constant => 'DataAccessLevel.REGISTERED'\n+         },\n+         'email_verification_status' => {\n+             :java => 'EmailVerificationStatus',\n+             :constant => 'EmailVerificationStatus.SUBSCRIBED'\n+         }\n+     }\n+}.freeze\n+\n+# strip size/kind\n+def simple_mysql_type(mysql_type)\n   type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n   match_data = mysql_type.match(type_pattern)\n-  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  result = match_data[:type]\n   raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n   result\n end\n \n-excluded_fields = File.exist?(inputs[:exclude_columns]) \\\n-  ? File.readlines(inputs[:exclude_columns]) \\\n+excluded_fields = File.exist?(INPUTS[:excluded_columns]) \\\n+  ? File.readlines(INPUTS[:excluded_columns]) \\\n       .map{ |c| c.strip } \\\n   : []\n \n def include_field?(excluded_fields, field)\n   !excluded_fields.include?(field)\n end\n \n+ENTITY_MODIFIED_COLUMNS = {\n+    'workspace' => {\n+        'cdr_version_id' => 'cdrVersion.cdrVersionId AS cdrVersionId',\n+        'creator_id' => 'creator.userId AS creatorId',\n+        'needs_rp_review_prompt' => 'needsResearchPurposeReviewPrompt AS needsRpReviewPrompt'\n+    }\n+}\n+\n ## BigQuery schema\n-describe_rows = CSV.new(File.read(inputs[:describe_csv])).sort_by { |row| row[0]  }\n+describe_rows = CSV.new(File.read(INPUTS[:describe_csv])).sort_by { |row| row[0]  }\n+\n+root_class_name = to_camel_case(table_name, true)\n+TABLE_INFO = {\n+    :name => table_name,\n+    :instance_name => to_camel_case(table_name, false),\n+    :dto_class => \"BqDto#{root_class_name}\",\n+    :entity_class => \"Db#{root_class_name}\",\n+    :mock => \"mock#{root_class_name}\",\n+    :projection_interface => \"Prj#{root_class_name}\",\n+    :sql_alias => table_name[0].downcase,\n+    :enum_column_info => ENUM_TYPES[table_name] || {},\n+    :entity_modified_columns => ENTITY_MODIFIED_COLUMNS[:table_name] || {}\n+}.freeze\n+\n+def to_swagger_name(snake_case, is_class_name)\n+  result =  snake_case.split('_').collect(&:capitalize).join\n+  unless is_class_name\n+    result[0] = result[0].downcase\n+  end\n+  result\n+end\n \n-columns = describe_rows.filter{ |row| include_field?(excluded_fields, row[0])} \\\n-  .map{ |row| {:name => row[0], :mysql_type => row[1], :big_query_type => to_bq_type(row[1])} }\n+def to_property_name(column_name)\n+  to_swagger_name(column_name, false)\n+end\n+\n+COLUMNS = describe_rows.filter{ |row| include_field?(excluded_fields, row[0]) } \\\n+  .map{ |row| \\\n+    col_name = row[0]\n+    mysql_type = simple_mysql_type(row[1])\n+    type_override = TABLE_INFO[:enum_column_info][col_name]\n+    type_info = MYSQL_TO_TYPES[mysql_type]\n+    {\n+        :name => col_name, \\\n+        :lambda_var => table_name[0].downcase, \\\n+        :mysql_type => mysql_type, \\\n+        :big_query_type => type_override ? type_override[:bigquery] : type_info[:bigquery], \\\n+        :is_enum => !type_override.nil?,\n+        :java_type => type_override ? type_override[:java] : type_info[:java], \\\n+        :java_field_name => \"#{to_camel_case(col_name, false)}\", \\\n+        :java_constant_name => \"#{table_name.upcase}__#{col_name.upcase}\", \\", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxODQyNA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492418424", "bodyText": "This comment and the above comment should be in code rather than on the PR", "author": "calbach", "createdAt": "2020-09-22T00:30:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1MDg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1MjY1Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492252652", "bodyText": "There are two different ultimate output types depending on the uploading mode. For streaming (fast but possibly allows duplicates), we have provide a RowToInsert with a map from column name to all non-null values passed as Objects. There are some surprises here, such as a special way you have to format the timestamps.\nThe QueryParameterValue structure is more familiar, as we use it for select queries from BQ. It's also difficult to work with and error prone, so I've beefed up the QueryParamterValues utility class and added testing to it. As written, we do a static import, which makes sense when calling the same method more than 10 times in quick succcession.", "author": "jaycarlton", "createdAt": "2020-09-21T18:10:03Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')\n+\n+### Assertions\n+dto_assertions = COLUMNS.map{ |col|\n+  getter_call = \"#{to_camel_case(table_name, false)}.#{col[:getter]}()\"\n+  expected = col[:java_constant_name]\n+  if col[:java_type].eql?('Timestamp')\n+    \"    assertTimeApprox(#{getter_call}, #{expected});\"\n+  else\n+    \"    assertThat(#{getter_call}).isEqualTo(#{expected});\"\n+  end\n+\n+}.join(\"\\n\")\n+\n+write_output(OUTPUTS[:dto_assertions], dto_assertions, 'Unit Test DTO Assertions')\n+\n+### Parameter Column Enum\n+def object_value_function(col)", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1Mzg0Ng==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492253846", "bodyText": "Each column knows the function needed to extract either a QueryParameterValue (not null), or an insertable Object (or null so we don't add it to the map). These errors only show up when dealing with BigQuery itself.\nTODO: look into writing a BigQueryTest for this system.", "author": "jaycarlton", "createdAt": "2020-09-21T18:12:09Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')\n+\n+### Assertions\n+dto_assertions = COLUMNS.map{ |col|\n+  getter_call = \"#{to_camel_case(table_name, false)}.#{col[:getter]}()\"\n+  expected = col[:java_constant_name]\n+  if col[:java_type].eql?('Timestamp')\n+    \"    assertTimeApprox(#{getter_call}, #{expected});\"\n+  else\n+    \"    assertThat(#{getter_call}).isEqualTo(#{expected});\"\n+  end\n+\n+}.join(\"\\n\")\n+\n+write_output(OUTPUTS[:dto_assertions], dto_assertions, 'Unit Test DTO Assertions')\n+\n+### Parameter Column Enum\n+def object_value_function(col)\n+  if col[:is_enum]\n+    \"#{col[:lambda_var]} -> enumToString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+  else\n+    case col[:java_type]\n+    when 'Timestamp' # actually OffsetDateTime on the DTO\n+      \"#{col[:lambda_var]} -> toInsertRowString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+    else\n+      \"#{TABLE_INFO[:dto_class]}::#{col[:getter]}\"\n+    end\n+  end\n+end\n+\n+# enum types are special\n+JAVA_TYPE_TO_QPV_FACTORY = {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1NDg5NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492254894", "bodyText": "These are enum entries that implement QueryParameterColumn<DTO_class>, which allows the insert all request builders to work for all table types. This decouples the MySql part from the BigQuery, for what that's worth.\npublic enum UserParameterColumn implements QueryParameterColumn<BqDtoUser> {\n  ABOUT_YOU(\"about_you\", BqDtoUser::getAboutYou, u -> QueryParameterValue.string(u.getAboutYou())),\n  AREA_OF_RESEARCH(\n      \"area_of_research\",\n      BqDtoUser::getAreaOfResearch,\n      u -> QueryParameterValue.string(u.getAreaOfResearch())),\n  BETA_ACCESS_BYPASS_TIME(\n      \"beta_access_bypass_time\",\n      u -> toInsertRowString(u.getBetaAccessBypassTime()),\n      u -> toTimestampQpv(u.getBetaAccessBypassTime())),\n  BETA_ACCESS_REQUEST_TIME(\n      \"beta_access_request_time\",\n      u -> toInsertRowString(u.getBetaAccessRequestTime()),\n      u -> toTimestampQpv(u.getBetaAccessRequestTime())),", "author": "jaycarlton", "createdAt": "2020-09-21T18:14:13Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')\n+\n+### Assertions\n+dto_assertions = COLUMNS.map{ |col|\n+  getter_call = \"#{to_camel_case(table_name, false)}.#{col[:getter]}()\"\n+  expected = col[:java_constant_name]\n+  if col[:java_type].eql?('Timestamp')\n+    \"    assertTimeApprox(#{getter_call}, #{expected});\"\n+  else\n+    \"    assertThat(#{getter_call}).isEqualTo(#{expected});\"\n+  end\n+\n+}.join(\"\\n\")\n+\n+write_output(OUTPUTS[:dto_assertions], dto_assertions, 'Unit Test DTO Assertions')\n+\n+### Parameter Column Enum\n+def object_value_function(col)\n+  if col[:is_enum]\n+    \"#{col[:lambda_var]} -> enumToString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+  else\n+    case col[:java_type]\n+    when 'Timestamp' # actually OffsetDateTime on the DTO\n+      \"#{col[:lambda_var]} -> toInsertRowString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+    else\n+      \"#{TABLE_INFO[:dto_class]}::#{col[:getter]}\"\n+    end\n+  end\n+end\n+\n+# enum types are special\n+JAVA_TYPE_TO_QPV_FACTORY = {\n+    'Integer' => 'QueryParameterValue.int64',\n+    'Long' => 'QueryParameterValue.int64',\n+    'Short' => 'QueryParameterValue.int64',\n+    'String' => 'QueryParameterValue.string',\n+    'Double' => 'QueryParameterValue.float64',\n+    'Boolean' => 'QueryParameterValue.bool',\n+    'Timestamp' => 'toTimestampQpv'\n+}\n+\n+def qpv_function(col)\n+  if col[:is_enum]\n+    convert_fn = 'enumToQpv'\n+  else\n+    convert_fn = \"#{JAVA_TYPE_TO_QPV_FACTORY[col[:java_type]]}\"\n+  end\n+  \"#{col[:lambda_var]} -> #{convert_fn}(#{col[:lambda_var]}.#{col[:getter]}())\"\n+end\n+\n+def query_parameter_column_entry(col)\n+  \"#{col[:name].upcase}(\\\"#{col[:name]}\\\", #{object_value_function(col)}, #{qpv_function(col)})\"\n end\n \n-sql = to_query(table_name, columns)\n-IO.write(outputs[:projection_query], sql)\n-puts \"  Projection Query: #{outputs[:projection_query]}\"\n+QPC_ENUM = (COLUMNS.map do |col|\n+  query_parameter_column_entry(col)\n+end.join(\",\\n\")).freeze\n+\n+write_output(OUTPUTS[:query_parameter_COLUMNS], QPC_ENUM, \"QueryParameterValue Enum Entries\")", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1NTQ0Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492255442", "bodyText": "These are just unit test filler lines to instantiate, set values, or do assertions.", "author": "jaycarlton", "createdAt": "2020-09-21T18:15:08Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +261,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')\n+\n+### Assertions\n+dto_assertions = COLUMNS.map{ |col|\n+  getter_call = \"#{to_camel_case(table_name, false)}.#{col[:getter]}()\"\n+  expected = col[:java_constant_name]\n+  if col[:java_type].eql?('Timestamp')\n+    \"    assertTimeApprox(#{getter_call}, #{expected});\"\n+  else\n+    \"    assertThat(#{getter_call}).isEqualTo(#{expected});\"\n+  end\n+\n+}.join(\"\\n\")\n+\n+write_output(OUTPUTS[:dto_assertions], dto_assertions, 'Unit Test DTO Assertions')\n+\n+### Parameter Column Enum\n+def object_value_function(col)\n+  if col[:is_enum]\n+    \"#{col[:lambda_var]} -> enumToString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+  else\n+    case col[:java_type]\n+    when 'Timestamp' # actually OffsetDateTime on the DTO\n+      \"#{col[:lambda_var]} -> toInsertRowString(#{col[:lambda_var]}.#{col[:getter]}())\"\n+    else\n+      \"#{TABLE_INFO[:dto_class]}::#{col[:getter]}\"\n+    end\n+  end\n+end\n+\n+# enum types are special\n+JAVA_TYPE_TO_QPV_FACTORY = {\n+    'Integer' => 'QueryParameterValue.int64',\n+    'Long' => 'QueryParameterValue.int64',\n+    'Short' => 'QueryParameterValue.int64',\n+    'String' => 'QueryParameterValue.string',\n+    'Double' => 'QueryParameterValue.float64',\n+    'Boolean' => 'QueryParameterValue.bool',\n+    'Timestamp' => 'toTimestampQpv'\n+}\n+\n+def qpv_function(col)\n+  if col[:is_enum]\n+    convert_fn = 'enumToQpv'\n+  else\n+    convert_fn = \"#{JAVA_TYPE_TO_QPV_FACTORY[col[:java_type]]}\"\n+  end\n+  \"#{col[:lambda_var]} -> #{convert_fn}(#{col[:lambda_var]}.#{col[:getter]}())\"\n+end\n+\n+def query_parameter_column_entry(col)\n+  \"#{col[:name].upcase}(\\\"#{col[:name]}\\\", #{object_value_function(col)}, #{qpv_function(col)})\"\n end\n \n-sql = to_query(table_name, columns)\n-IO.write(outputs[:projection_query], sql)\n-puts \"  Projection Query: #{outputs[:projection_query]}\"\n+QPC_ENUM = (COLUMNS.map do |col|\n+  query_parameter_column_entry(col)\n+end.join(\",\\n\")).freeze\n+\n+write_output(OUTPUTS[:query_parameter_COLUMNS], QPC_ENUM, \"QueryParameterValue Enum Entries\")\n+\n+### DTO Fluent Setters", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1NTkzNw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492255937", "bodyText": "Note that apparently the zone argument here merely controls output, but doesn't appear to supply that zone during parsing.", "author": "jaycarlton", "createdAt": "2020-09-21T18:16:01Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -4,58 +4,96 @@\n import com.google.cloud.bigquery.QueryParameterValue;\n import com.google.cloud.bigquery.StandardSQLTypeName;\n import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n import java.time.ZoneOffset;\n import java.time.ZonedDateTime;\n import java.time.format.DateTimeFormatter;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Optional;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n import org.hibernate.engine.jdbc.internal.BasicFormatterImpl;\n+import org.jetbrains.annotations.NotNull;\n import org.pmiops.workbench.utils.Matchers;\n \n public final class QueryParameterValues {\n   private static final int MICROSECONDS_IN_MILLISECOND = 1000;\n \n   // For creating a Timestamp QueryParameterValue, use this formatter.\n   // example error when using the RowToInsert version (below): \"Invalid format:\n-  // \"1989-02-17 00:00:00.000000\" is too short\".\n+  // \"1989-02-17 00:00:00.000000\" is too short\". See https://stackoverflow.com/a/55155067/611672\n+  // for a nice walkthrough of the machinations involved.\n   public static final DateTimeFormatter QPV_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSSZZ\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"uuuu-MM-dd HH:mm:ss.SSSSSSxxx\");\n+\n   // For inserting a Timestamp in a RowToInsert map for an InsertAllRequest, use this format\n   public static final DateTimeFormatter ROW_TO_INSERT_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\")\n+          .withZone(ZoneOffset.UTC)", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1NjM3NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492256375", "bodyText": "Since this class has two or three different nullity contracts, I tried to be explicit with @NotNull and @Nulllable. I'm not necessarily advocating we do that everywhere.", "author": "jaycarlton", "createdAt": "2020-09-21T18:16:50Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -4,58 +4,96 @@\n import com.google.cloud.bigquery.QueryParameterValue;\n import com.google.cloud.bigquery.StandardSQLTypeName;\n import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n import java.time.ZoneOffset;\n import java.time.ZonedDateTime;\n import java.time.format.DateTimeFormatter;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Optional;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n import org.hibernate.engine.jdbc.internal.BasicFormatterImpl;\n+import org.jetbrains.annotations.NotNull;\n import org.pmiops.workbench.utils.Matchers;\n \n public final class QueryParameterValues {\n   private static final int MICROSECONDS_IN_MILLISECOND = 1000;\n \n   // For creating a Timestamp QueryParameterValue, use this formatter.\n   // example error when using the RowToInsert version (below): \"Invalid format:\n-  // \"1989-02-17 00:00:00.000000\" is too short\".\n+  // \"1989-02-17 00:00:00.000000\" is too short\". See https://stackoverflow.com/a/55155067/611672\n+  // for a nice walkthrough of the machinations involved.\n   public static final DateTimeFormatter QPV_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSSZZ\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"uuuu-MM-dd HH:mm:ss.SSSSSSxxx\");\n+\n   // For inserting a Timestamp in a RowToInsert map for an InsertAllRequest, use this format\n   public static final DateTimeFormatter ROW_TO_INSERT_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\")\n+          .withZone(ZoneOffset.UTC)\n+          .withLocale(Locale.US);\n \n   /** Generate a unique parameter name and add it to the parameter map provided. */\n+  @NotNull", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1NzQ5MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492257490", "bodyText": "Remember, QPVs in BQ expect _micro_seconds.", "author": "jaycarlton", "createdAt": "2020-09-21T18:18:48Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -4,58 +4,96 @@\n import com.google.cloud.bigquery.QueryParameterValue;\n import com.google.cloud.bigquery.StandardSQLTypeName;\n import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n import java.time.ZoneOffset;\n import java.time.ZonedDateTime;\n import java.time.format.DateTimeFormatter;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Optional;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n import org.hibernate.engine.jdbc.internal.BasicFormatterImpl;\n+import org.jetbrains.annotations.NotNull;\n import org.pmiops.workbench.utils.Matchers;\n \n public final class QueryParameterValues {\n   private static final int MICROSECONDS_IN_MILLISECOND = 1000;\n \n   // For creating a Timestamp QueryParameterValue, use this formatter.\n   // example error when using the RowToInsert version (below): \"Invalid format:\n-  // \"1989-02-17 00:00:00.000000\" is too short\".\n+  // \"1989-02-17 00:00:00.000000\" is too short\". See https://stackoverflow.com/a/55155067/611672\n+  // for a nice walkthrough of the machinations involved.\n   public static final DateTimeFormatter QPV_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSSZZ\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"uuuu-MM-dd HH:mm:ss.SSSSSSxxx\");\n+\n   // For inserting a Timestamp in a RowToInsert map for an InsertAllRequest, use this format\n   public static final DateTimeFormatter ROW_TO_INSERT_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\")\n+          .withZone(ZoneOffset.UTC)\n+          .withLocale(Locale.US);\n \n   /** Generate a unique parameter name and add it to the parameter map provided. */\n+  @NotNull\n   public static String buildParameter(\n-      Map<String, QueryParameterValue> queryParameterValueMap,\n-      QueryParameterValue queryParameterValue) {\n+      @NotNull Map<String, QueryParameterValue> queryParameterValueMap,\n+      @NotNull QueryParameterValue queryParameterValue) {\n     String parameterName = \"p\" + queryParameterValueMap.size();\n     queryParameterValueMap.put(parameterName, queryParameterValue);\n     return decorateParameterName(parameterName);\n   }\n \n-  public static QueryParameterValue instantToQPValue(Instant instant) {\n-    return QueryParameterValue.timestamp(instant.toEpochMilli() * MICROSECONDS_IN_MILLISECOND);\n+  // QueryParameterValue can have a null value, so no need to return an Optional.\n+  @NotNull\n+  public static QueryParameterValue instantToQPValue(@Nullable Instant instant) {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1Nzg3NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492257875", "bodyText": "Methods whose outputs are not piped directly into BigQuery in some form are Optional\n.", "author": "jaycarlton", "createdAt": "2020-09-21T18:19:29Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -4,58 +4,96 @@\n import com.google.cloud.bigquery.QueryParameterValue;\n import com.google.cloud.bigquery.StandardSQLTypeName;\n import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n import java.time.ZoneOffset;\n import java.time.ZonedDateTime;\n import java.time.format.DateTimeFormatter;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Optional;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n import org.hibernate.engine.jdbc.internal.BasicFormatterImpl;\n+import org.jetbrains.annotations.NotNull;\n import org.pmiops.workbench.utils.Matchers;\n \n public final class QueryParameterValues {\n   private static final int MICROSECONDS_IN_MILLISECOND = 1000;\n \n   // For creating a Timestamp QueryParameterValue, use this formatter.\n   // example error when using the RowToInsert version (below): \"Invalid format:\n-  // \"1989-02-17 00:00:00.000000\" is too short\".\n+  // \"1989-02-17 00:00:00.000000\" is too short\". See https://stackoverflow.com/a/55155067/611672\n+  // for a nice walkthrough of the machinations involved.\n   public static final DateTimeFormatter QPV_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSSZZ\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"uuuu-MM-dd HH:mm:ss.SSSSSSxxx\");\n+\n   // For inserting a Timestamp in a RowToInsert map for an InsertAllRequest, use this format\n   public static final DateTimeFormatter ROW_TO_INSERT_TIMESTAMP_FORMATTER =\n-      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\").withZone(ZoneOffset.UTC);\n+      DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSSSS\")\n+          .withZone(ZoneOffset.UTC)\n+          .withLocale(Locale.US);\n \n   /** Generate a unique parameter name and add it to the parameter map provided. */\n+  @NotNull\n   public static String buildParameter(\n-      Map<String, QueryParameterValue> queryParameterValueMap,\n-      QueryParameterValue queryParameterValue) {\n+      @NotNull Map<String, QueryParameterValue> queryParameterValueMap,\n+      @NotNull QueryParameterValue queryParameterValue) {\n     String parameterName = \"p\" + queryParameterValueMap.size();\n     queryParameterValueMap.put(parameterName, queryParameterValue);\n     return decorateParameterName(parameterName);\n   }\n \n-  public static QueryParameterValue instantToQPValue(Instant instant) {\n-    return QueryParameterValue.timestamp(instant.toEpochMilli() * MICROSECONDS_IN_MILLISECOND);\n+  // QueryParameterValue can have a null value, so no need to return an Optional.\n+  @NotNull\n+  public static QueryParameterValue instantToQPValue(@Nullable Instant instant) {\n+    final Long epochMicros =\n+        Optional.ofNullable(instant)\n+            .map(Instant::toEpochMilli)\n+            .map(milli -> milli * MICROSECONDS_IN_MILLISECOND)\n+            .orElse(null);\n+    return QueryParameterValue.timestamp(epochMicros);\n+  }\n+\n+  // Will return an empty Optional for null input, but parse errors will still throw\n+  // DateTimeParseException.\n+  public static Optional<Instant> timestampStringToInstant(@Nullable String timestamp) {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI1OTYzNg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492259636", "bodyText": "The other way to operate would have been to convert to string when writing the DTO, but I felt like that was losing type safety too early in the process.", "author": "jaycarlton", "createdAt": "2020-09-21T18:22:36Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -65,35 +103,83 @@ public static String replaceNamedParameters(QueryJobConfiguration queryJobConfig\n     return Matchers.replaceAllInMap(patternToReplacement, result);\n   }\n \n-  public static String formatQuery(String query) {\n+  @NotNull\n+  public static String formatQuery(@NotNull String query) {\n     return new BasicFormatterImpl().format(query);\n   }\n \n   // use lookbehind for non-word character, since \"'\"(@\" or \" @\" don't represent left-side word\n   // boundaries.\n-  private static Pattern buildParameterRegex(String parameterName) {\n+  @NotNull\n+  private static Pattern buildParameterRegex(@NotNull String parameterName) {\n     return Pattern.compile(String.format(\"(?<=\\\\W)%s\\\\b\", decorateParameterName(parameterName)));\n   }\n \n-  public static String decorateParameterName(String parameterName) {\n+  @NotNull\n+  public static String decorateParameterName(@NotNull String parameterName) {\n     return \"@\" + parameterName;\n   }\n \n-  private static String getReplacementString(QueryParameterValue parameterValue) {\n+  @Nullable\n+  public static QueryParameterValue toTimestampQpv(@Nullable OffsetDateTime offsetDateTime) {\n+    final String arg =\n+        Optional.ofNullable(offsetDateTime).map(QPV_TIMESTAMP_FORMATTER::format).orElse(null);\n+    return QueryParameterValue.timestamp(arg);\n+  }\n+\n+  // Return null instead of Optional.empty() so the return value can go directly into\n+  // the content map of an InsertAllRequest.RowToInsert.\n+  @Nullable\n+  public static String toInsertRowString(@Nullable OffsetDateTime offsetDateTime) {\n+    return Optional.ofNullable(offsetDateTime)\n+        .map(ROW_TO_INSERT_TIMESTAMP_FORMATTER::format)\n+        .orElse(null);\n+  }\n+\n+  // BigQuery TIMESTAMP types don't include a zone or offset, but are always UTC.\n+  public static Optional<OffsetDateTime> rowToInsertStringToOffsetTimestamp(\n+      @Nullable String bqTimeString) {\n+    return Optional.ofNullable(bqTimeString)\n+        .filter(s -> s.length() > 0)\n+        .map(ROW_TO_INSERT_TIMESTAMP_FORMATTER::parse)\n+        .map(LocalDateTime::from)\n+        .map(ldt -> OffsetDateTime.of(ldt, ZoneOffset.UTC));\n+  }\n+\n+  @NotNull", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MDQwNw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492260407", "bodyText": "This join is easy to forget. We agreed to export information on active workspaces only, as inactive workspaces aren't a fully-defined, user-facing feature.", "author": "jaycarlton", "createdAt": "2020-09-21T18:24:01Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/UserDao.java", "diffHunk": "@@ -71,4 +70,55 @@\n \n     Long getUserCount();\n   }\n+\n+  @Query(\n+      \"SELECT\\n\"\n+          + \"  u.aboutYou,\\n\"\n+          + \"  u.areaOfResearch,\\n\"\n+          + \"  u.betaAccessBypassTime,\\n\"\n+          + \"  u.betaAccessRequestTime,\\n\"\n+          + \"  u.complianceTrainingBypassTime,\\n\"\n+          + \"  u.complianceTrainingCompletionTime,\\n\"\n+          + \"  u.complianceTrainingExpirationTime,\\n\"\n+          + \"  u.contactEmail,\\n\"\n+          + \"  u.creationTime,\\n\"\n+          + \"  u.currentPosition,\\n\"\n+          + \"  u.dataAccessLevel,\\n\"\n+          + \"  u.dataUseAgreementBypassTime,\\n\"\n+          + \"  u.dataUseAgreementCompletionTime,\\n\"\n+          + \"  u.dataUseAgreementSignedVersion,\\n\"\n+          + \"  u.demographicSurveyCompletionTime,\\n\"\n+          + \"  u.disabled,\\n\"\n+          + \"  u.emailVerificationBypassTime,\\n\"\n+          + \"  u.emailVerificationCompletionTime,\\n\"\n+          + \"  u.emailVerificationStatus,\\n\"\n+          + \"  u.eraCommonsBypassTime,\\n\"\n+          + \"  u.eraCommonsCompletionTime,\\n\"\n+          + \"  u.eraCommonsLinkExpireTime,\\n\"\n+          + \"  u.familyName,\\n\"\n+          + \"  u.firstRegistrationCompletionTime,\\n\"\n+          + \"  u.firstSignInTime,\\n\"\n+          + \"  u.freeTierCreditsLimitDaysOverride,\\n\"\n+          + \"  u.freeTierCreditsLimitDollarsOverride,\\n\"\n+          + \"  u.givenName,\\n\"\n+          + \"  u.idVerificationBypassTime,\\n\"\n+          + \"  u.idVerificationCompletionTime,\\n\"\n+          + \"  u.lastModifiedTime,\\n\"\n+          + \"  u.organization,\\n\"\n+          + \"  u.phoneNumber,\\n\"\n+          + \"  u.professionalUrl,\\n\"\n+          + \"  u.twoFactorAuthBypassTime,\\n\"\n+          + \"  u.twoFactorAuthCompletionTime,\\n\"\n+          + \"  u.userId,\\n\"\n+          + \"  u.username,\\n\"\n+          + \"  a.city,\\n\"\n+          + \"  a.country,\\n\"\n+          + \"  a.state,\\n\"\n+          + \"  a.streetAddress1,\\n\"\n+          + \"  a.streetAddress2,\\n\"\n+          + \"  a.zipCode\\n\"\n+          + \"FROM DbUser u\\n\"\n+          + \"  LEFT OUTER JOIN DbAddress AS a ON u.userId = a.user.userId\\n\"", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTA2NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492261064", "bodyText": "The projection, BigQuery, and DTO column names always agree with the MySql column name. Frequently that doesn't agree with the property exposed on the entity object so we have these as conversions.", "author": "jaycarlton", "createdAt": "2020-09-21T18:25:09Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/WorkspaceDao.java", "diffHunk": "@@ -85,4 +86,44 @@ default void updateBillingStatus(long workspaceId, BillingStatus status) {\n \n     Long getWorkspaceCount();\n   }\n+\n+  @Query(\n+      \"SELECT\\n\"\n+          + \"  w.billingAccountType,\\n\"\n+          + \"  w.billingStatus,\\n\"\n+          + \"  w.cdrVersion.cdrVersionId AS cdrVersionId,\\n\"\n+          + \"  w.creationTime,\\n\"\n+          + \"  w.creator.userId AS creatorId,\\n\"\n+          + \"  w.disseminateResearchOther,\\n\"\n+          + \"  w.lastAccessedTime,\\n\"\n+          + \"  w.lastModifiedTime,\\n\"\n+          + \"  w.name,\\n\"\n+          + \"  w.needsResearchPurposeReviewPrompt AS needsRpReviewPrompt,\\n\"", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MjMxOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492262318", "bodyText": "Institution membership and a couple of other things are going to be denormalized into this BQ table soon.", "author": "jaycarlton", "createdAt": "2020-09-21T18:27:26Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/projection/PrjUser.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package org.pmiops.workbench.db.dao.projection;\n+\n+import java.sql.Timestamp;\n+\n+public interface PrjUser {\n+  /*\n+   * User columns\n+   */\n+  String getAboutYou();\n+\n+  String getAreaOfResearch();\n+\n+  Timestamp getBetaAccessBypassTime();\n+\n+  Timestamp getBetaAccessRequestTime();\n+\n+  Timestamp getComplianceTrainingBypassTime();\n+\n+  Timestamp getComplianceTrainingCompletionTime();\n+\n+  Timestamp getComplianceTrainingExpirationTime();\n+\n+  String getContactEmail();\n+\n+  Timestamp getCreationTime();\n+\n+  String getCurrentPosition();\n+\n+  Short getDataAccessLevel(); // manual type override\n+\n+  Timestamp getDataUseAgreementBypassTime();\n+\n+  Timestamp getDataUseAgreementCompletionTime();\n+\n+  Integer getDataUseAgreementSignedVersion();\n+\n+  Timestamp getDemographicSurveyCompletionTime();\n+\n+  Boolean getDisabled();\n+\n+  Timestamp getEmailVerificationBypassTime();\n+\n+  Timestamp getEmailVerificationCompletionTime();\n+\n+  Short getEmailVerificationStatus(); // manual type override\n+\n+  Timestamp getEraCommonsBypassTime();\n+\n+  Timestamp getEraCommonsCompletionTime();\n+\n+  Timestamp getEraCommonsLinkExpireTime();\n+\n+  String getFamilyName();\n+\n+  Timestamp getFirstRegistrationCompletionTime();\n+\n+  Timestamp getFirstSignInTime();\n+\n+  Short getFreeTierCreditsLimitDaysOverride();\n+\n+  Double getFreeTierCreditsLimitDollarsOverride();\n+\n+  String getGivenName();\n+\n+  Timestamp getIdVerificationBypassTime();\n+\n+  Timestamp getIdVerificationCompletionTime();\n+\n+  Timestamp getLastModifiedTime();\n+\n+  String getOrganization();\n+\n+  String getPhoneNumber();\n+\n+  String getProfessionalUrl();\n+\n+  Timestamp getTwoFactorAuthBypassTime();\n+\n+  Timestamp getTwoFactorAuthCompletionTime();\n+\n+  Long getUserId();\n+\n+  String getUsername();\n+  /*\n+   * Address columns\n+   */\n+  String getCity();\n+\n+  String getCountry();\n+\n+  String getState();\n+\n+  String getStreetAddress1();\n+\n+  String getStreetAddress2();\n+\n+  String getZipCode();\n+}", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MzM2OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492263369", "bodyText": "Some of the enum columns \"just work\", but others (like data_access_level) don't. I believe it had to do with which attribute on the entity class was annotated with the column name.\nI wound up leaving DataAccessLevel out as it was one of those stubborn columns and the representation is slated to change in the multiple tiers project.", "author": "jaycarlton", "createdAt": "2020-09-21T18:29:20Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/projection/PrjWorkspace.java", "diffHunk": "@@ -0,0 +1,75 @@\n+package org.pmiops.workbench.db.dao.projection;\n+\n+import java.sql.Timestamp;\n+import org.pmiops.workbench.model.BillingAccountType;\n+import org.pmiops.workbench.model.BillingStatus;\n+\n+public interface PrjWorkspace {\n+  BillingAccountType getBillingAccountType();", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NTg2NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492265864", "bodyText": "Renamed researcher table to user.", "author": "jaycarlton", "createdAt": "2020-09-21T18:33:45Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/ReportingMapper.java", "diffHunk": "@@ -3,28 +3,23 @@\n import java.util.Collection;\n import java.util.List;\n import org.mapstruct.Mapper;\n-import org.mapstruct.Mapping;\n-import org.pmiops.workbench.db.model.DbUser;\n-import org.pmiops.workbench.db.model.DbWorkspace;\n-import org.pmiops.workbench.model.ReportingResearcher;\n-import org.pmiops.workbench.model.ReportingWorkspace;\n+import org.pmiops.workbench.db.dao.projection.PrjUser;\n+import org.pmiops.workbench.db.dao.projection.PrjWorkspace;\n+import org.pmiops.workbench.db.model.DbStorageEnums;\n+import org.pmiops.workbench.model.BqDtoUser;\n+import org.pmiops.workbench.model.BqDtoWorkspace;\n import org.pmiops.workbench.utils.mappers.CommonMappers;\n import org.pmiops.workbench.utils.mappers.MapStructConfig;\n \n @Mapper(\n     config = MapStructConfig.class,\n-    uses = {CommonMappers.class})\n+    uses = {CommonMappers.class, DbStorageEnums.class})\n public interface ReportingMapper {\n-  @Mapping(source = \"givenName\", target = \"firstName\")\n-  @Mapping(source = \"userId\", target = \"researcherId\")\n-  @Mapping(source = \"disabled\", target = \"isDisabled\")\n-  ReportingResearcher toModel(DbUser dbUser);\n+  BqDtoUser toDto(PrjUser prjUser);\n \n-  List<ReportingResearcher> toReportingResearcherList(Collection<DbUser> dbUsers);\n+  List<BqDtoUser> toReportingUserList(Collection<PrjUser> users);", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjc3NDAxMQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492774011", "bodyText": "@calbach thoughts on a better naming convention here? I think technically a data transfer object would be an aggregate of the whole payload, rather than an individual row. Just going back to ReportingUser for now.", "author": "jaycarlton", "createdAt": "2020-09-22T14:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NTg2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk0NjQ2MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492946460", "bodyText": "Reporting seems clear to me", "author": "calbach", "createdAt": "2020-09-22T18:26:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NTg2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NjA5MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492266091", "bodyText": "Static methods are fair game for MapStruct, so all I have to do to get enum maps is include DbStorageEnums", "author": "jaycarlton", "createdAt": "2020-09-21T18:34:13Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/ReportingMapper.java", "diffHunk": "@@ -3,28 +3,23 @@\n import java.util.Collection;\n import java.util.List;\n import org.mapstruct.Mapper;\n-import org.mapstruct.Mapping;\n-import org.pmiops.workbench.db.model.DbUser;\n-import org.pmiops.workbench.db.model.DbWorkspace;\n-import org.pmiops.workbench.model.ReportingResearcher;\n-import org.pmiops.workbench.model.ReportingWorkspace;\n+import org.pmiops.workbench.db.dao.projection.PrjUser;\n+import org.pmiops.workbench.db.dao.projection.PrjWorkspace;\n+import org.pmiops.workbench.db.model.DbStorageEnums;\n+import org.pmiops.workbench.model.BqDtoUser;\n+import org.pmiops.workbench.model.BqDtoWorkspace;\n import org.pmiops.workbench.utils.mappers.CommonMappers;\n import org.pmiops.workbench.utils.mappers.MapStructConfig;\n \n @Mapper(\n     config = MapStructConfig.class,\n-    uses = {CommonMappers.class})\n+    uses = {CommonMappers.class, DbStorageEnums.class})", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2Nzc0NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492267744", "bodyText": "This was bundled into one function primarily to support timing.", "author": "jaycarlton", "createdAt": "2020-09-21T18:37:08Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/ReportingSnapshotServiceImpl.java", "diffHunk": "@@ -67,37 +62,27 @@ public ReportingSnapshotServiceImpl(\n   @Transactional(readOnly = true)\n   @Override\n   public ReportingSnapshot takeSnapshot() {\n-    final EntityBundle entityBundle = getApplicationDbData();\n+    final QueryResultBundle queryResultBundle = getApplicationDbData();\n     final Stopwatch stopwatch = stopwatchProvider.get().start();\n \n-    final List<ReportingWorkspace> workspaces =\n-        reportingMapper.toReportingWorkspaceList(entityBundle.getWorkspaces());\n-    for (ReportingWorkspace model : workspaces) {\n-      model.setFakeSize(\n-          getFakeSize()); // TODO(jaycarlton): remove after initial query & view testing\n-    }\n-\n     final ReportingSnapshot result =\n         new ReportingSnapshot()\n             .captureTimestamp(clock.millis())\n-            .researchers(reportingMapper.toReportingResearcherList(entityBundle.getUsers()))\n-            .workspaces(workspaces);\n+            .users(reportingMapper.toReportingUserList(queryResultBundle.getUsers()))\n+            .workspaces(\n+                reportingMapper.toReportingWorkspaceList(queryResultBundle.getWorkspaces()));\n     stopwatch.stop();\n     log.info(LogFormatters.duration(\"Conversion to ReportingSnapshot\", stopwatch.elapsed()));\n     return result;\n   }\n \n-  private EntityBundle getApplicationDbData() {\n+  private QueryResultBundle getApplicationDbData() {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2ODY3MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492268671", "bodyText": "We need an instance of the DmlInsertJobBuilder for each table. There's no constructor, but this line overrides the single remaining abstract method, providing all the enum instances.\nJava is the only language where I'd have to do this. It does not appear possible to express that a class should be both an enum type and extend a generic interface.", "author": "jaycarlton", "createdAt": "2020-09-21T18:38:58Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/ReportingUploadServiceDmlImpl.java", "diffHunk": "@@ -15,28 +15,28 @@\n import javax.inject.Provider;\n import org.pmiops.workbench.api.BigQueryService;\n import org.pmiops.workbench.config.WorkbenchConfig;\n-import org.pmiops.workbench.model.ReportingResearcher;\n+import org.pmiops.workbench.model.BqDtoUser;\n+import org.pmiops.workbench.model.BqDtoWorkspace;\n import org.pmiops.workbench.model.ReportingSnapshot;\n-import org.pmiops.workbench.model.ReportingWorkspace;\n import org.pmiops.workbench.reporting.insertion.DmlInsertJobBuilder;\n-import org.pmiops.workbench.reporting.insertion.ResearcherParameter;\n-import org.pmiops.workbench.reporting.insertion.WorkspaceParameter;\n+import org.pmiops.workbench.reporting.insertion.UserParameterColumn;\n+import org.pmiops.workbench.reporting.insertion.WorkspaceParameterColumn;\n import org.springframework.context.annotation.Primary;\n import org.springframework.stereotype.Service;\n \n @Service(\"REPORTING_UPLOAD_SERVICE_DML_IMPL\")\n @Primary\n public class ReportingUploadServiceDmlImpl implements ReportingUploadService {\n+\n   private static final Logger logger = Logger.getLogger(\"ReportingUploadServiceInsertQueryImpl\");\n   private static final long MAX_WAIT_TIME = Duration.ofSeconds(60).toMillis();\n \n   private final BigQueryService bigQueryService;\n   private final Provider<WorkbenchConfig> workbenchConfigProvider;\n \n-  private static final DmlInsertJobBuilder<ReportingResearcher> researcherJobBuilder =\n-      ResearcherParameter::values;\n-  private static final DmlInsertJobBuilder<ReportingWorkspace> workspaceJobBuilder =\n-      WorkspaceParameter::values;\n+  private static final DmlInsertJobBuilder<BqDtoUser> userJobBuilder = UserParameterColumn::values;", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e6f0d5014024efbdc31219201948f994399d9833", "url": "https://github.com/all-of-us/workbench/commit/e6f0d5014024efbdc31219201948f994399d9833", "message": "review fixes", "committedDate": "2020-09-21T19:15:14Z", "type": "commit"}, {"oid": "ab663267e77cba832cf0846d3cfc9754aed9d483", "url": "https://github.com/all-of-us/workbench/commit/ab663267e77cba832cf0846d3cfc9754aed9d483", "message": "fix import and turn off sql debug", "committedDate": "2020-09-21T22:20:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI3MTE2Mw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492271163", "bodyText": "I wasn't happy with the random class living here statically. It means I can't inject it because this is just an interface.", "author": "jaycarlton", "createdAt": "2020-09-21T18:43:37Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/insertion/InsertAllRequestBuilder.java", "diffHunk": "@@ -4,42 +4,49 @@\n import com.google.cloud.bigquery.InsertAllRequest.RowToInsert;\n import com.google.cloud.bigquery.TableId;\n import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Maps;\n+import com.google.common.collect.ImmutableMap;\n+import java.util.AbstractMap;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import org.pmiops.workbench.utils.RandomUtils;\n \n public interface InsertAllRequestBuilder<T> extends ColumnDrivenBuilder<T> {\n   String INSERT_ID_CHARS = \"abcdefghijklmnopqrstuvwxyz\";\n+  int INSERT_ID_LENGTH = 16;\n \n   default InsertAllRequest build(TableId tableId, List<T> models, Map<String, Object> fixedValues) {\n     return InsertAllRequest.newBuilder(tableId)\n         .setIgnoreUnknownValues(true)\n-        .setRows(modelsToRows(models, fixedValues))\n+        .setRows(dtosToRowsToInsert(models, fixedValues))\n         .build();\n   }\n \n-  default List<RowToInsert> modelsToRows(Collection<T> models, Map<String, Object> fixedValues) {\n+  default List<RowToInsert> dtosToRowsToInsert(\n+      Collection<T> models, Map<String, Object> fixedValues) {\n     return models.stream()\n-        .map(m -> modelToRow(m, fixedValues))\n+        .map(m -> dtoToRowToInsert(m, fixedValues))\n         .collect(ImmutableList.toImmutableList());\n   }\n \n-  default RowToInsert modelToRow(T model, Map<String, Object> fixedValues) {\n-    // First N columns are same for all rows (e.g. a partition key column)\n-    final Map<String, Object> rowMap = Maps.newHashMap(fixedValues);\n-\n-    // can't stream/collect here because that uses HashMap.merge() which surprisingly does not\n-    // allow null values although they are valid for HashMap.  We do use null values.\n-    for (QueryParameterColumn<T> qpc : getQueryParameterColumns()) {\n-      rowMap.put(qpc.getParameterName(), qpc.getRowToInsertValue(model));\n-    }\n-\n-    return RowToInsert.of(generateInsertId(), rowMap);\n+  // Null values are supposed to be omitted from the map (or have @Value or @NullValue annotations).\n+  default RowToInsert dtoToRowToInsert(T model, Map<String, Object> fixedValues) {\n+    final ImmutableMap.Builder<String, Object> columnToValueBuilder = ImmutableMap.builder();\n+    columnToValueBuilder.putAll(fixedValues);\n+    Arrays.stream(getQueryParameterColumns())\n+        .map(\n+            col ->\n+                new AbstractMap.SimpleImmutableEntry<>(\n+                    col.getParameterName(), col.getRowToInsertValue(model)))\n+        .filter(e -> Objects.nonNull(e.getValue()))\n+        .forEach(columnToValueBuilder::put);\n+\n+    return RowToInsert.of(generateInsertId(), columnToValueBuilder.build());\n   }\n \n   default String generateInsertId() {\n-    return RandomUtils.generateRandomChars(INSERT_ID_CHARS, 16);\n+    return RandomUtils.generateRandomChars(INSERT_ID_CHARS, INSERT_ID_LENGTH);", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI3MTgzOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492271838", "bodyText": "This (generated/scaffolded) enum knows how to extract column names, object values, and QueryParameterValue instances from a DTO model. It must be kept in sync with the project and the DTO.\nThis feels a bit like overkill, and if I had done the codegen bit first, it might have been rolled into that, with each column having a class that exposes getObject() and getQpv(), for example. But then you'd need a fixed, static list of instances of those, and you're back at an enum.", "author": "jaycarlton", "createdAt": "2020-09-21T18:44:51Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/insertion/UserParameterColumn.java", "diffHunk": "@@ -0,0 +1,182 @@\n+package org.pmiops.workbench.reporting.insertion;\n+\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.enumToQpv;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.enumToString;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.toInsertRowString;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.toTimestampQpv;\n+\n+import com.google.cloud.bigquery.QueryParameterValue;\n+import java.util.function.Function;\n+import org.pmiops.workbench.model.BqDtoUser;\n+\n+public enum UserParameterColumn implements QueryParameterColumn<BqDtoUser> {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI3MzgyMQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492273821", "bodyText": "Descriptions for these properties will be filled in from the same source that fills in the BigQuery schema.", "author": "jaycarlton", "createdAt": "2020-09-21T18:48:25Z", "path": "api/src/main/resources/workbench-api.yaml", "diffHunk": "@@ -7942,69 +7956,204 @@ definitions:\n         description: Time at which snapshot is retrieved. Epoch millis.\n         type: integer\n         format: int64\n-      researchers:\n+      users:\n         type: array\n         items:\n-          \"$ref\": \"#/definitions/ReportingResearcher\"\n+          \"$ref\": \"#/definitions/BqDtoUser\"\n       workspaces:\n         type: array\n         items:\n-          \"$ref\": \"#/definitions/ReportingWorkspace\"\n-  ReportingWorkspace:\n-    description: >\n+          \"$ref\": \"#/definitions/BqDtoWorkspace\"\n+  BqDtoUser:\n     type: object\n     properties:\n-      workspaceId:\n-        description: PK for workspace table.\n-        type: integer\n-        format: int64\n-      creatorId:\n-        description: FK researcher ID for creator of workspace.\n+      aboutYou:\n+        type: string", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4MzU4Nw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492283587", "bodyText": "Everything here is generated in Ruby and pasted in place. If I ever want to do anything else with the generator, I need to switch to a template-based solution that generates things at build time. It's really tedious to copy and paste to ten different files if something changes.", "author": "jaycarlton", "createdAt": "2020-09-21T19:06:27Z", "path": "api/src/test/java/org/pmiops/workbench/testconfig/ReportingTestUtils.java", "diffHunk": "@@ -0,0 +1,526 @@\n+package org.pmiops.workbench.testconfig;\n+\n+import static com.google.common.truth.Truth.assertThat;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.pmiops.workbench.utils.TimeAssertions.assertTimeApprox;\n+import static org.pmiops.workbench.utils.mappers.CommonMappers.offsetDateTimeUtc;\n+\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import org.pmiops.workbench.db.dao.projection.PrjUser;\n+import org.pmiops.workbench.db.dao.projection.PrjWorkspace;\n+import org.pmiops.workbench.db.model.DbCdrVersion;\n+import org.pmiops.workbench.db.model.DbStorageEnums;\n+import org.pmiops.workbench.db.model.DbUser;\n+import org.pmiops.workbench.db.model.DbWorkspace;\n+import org.pmiops.workbench.model.BillingAccountType;\n+import org.pmiops.workbench.model.BillingStatus;\n+import org.pmiops.workbench.model.BqDtoUser;\n+import org.pmiops.workbench.model.BqDtoWorkspace;\n+import org.pmiops.workbench.model.DataAccessLevel;\n+import org.pmiops.workbench.model.EmailVerificationStatus;\n+\n+public class ReportingTestUtils {", "originalCommit": "41864b64e330baa307c8e3dfb4a72886821b5ca1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3MzQyMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492373422", "bodyText": "The basic rule for what to exclude is: is the information surfaced in a user-facing area of the product. If not, it doesn't really belong in the reporting dataset.\nThe user and workspace tables cover maybe 80% of what's being asked for, so I'm starting with those and punting on the rest for now.", "author": "jaycarlton", "createdAt": "2020-09-21T22:08:05Z", "path": "api/reporting/schemas/input/excluded_columns/user.txt", "diffHunk": "@@ -1,9 +1,11 @@\n-version\n-id_verification_is_valid\n-cluster_create_retries\n billing_project_retries", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3MzcwMw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492373703", "bodyText": "BQ timestamps don't have zones; it's always assumed to be UTC (or whatever timezone you want if you're consistent).", "author": "jaycarlton", "createdAt": "2020-09-21T22:08:44Z", "path": "api/reporting/schemas/latest/user.json", "diffHunk": "@@ -0,0 +1,175 @@\n+[\n+  {\n+    \"description\": \"Time snapshot was taken. Same across all rows in the snapshot, and can be used for partitioning.\",\n+    \"name\": \"snapshot_timestamp\",\n+    \"type\": \"INTEGER\"\n+  },\n+  {\n+    \"name\": \"about_you\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"area_of_research\",\n+    \"type\": \"STRING\"\n+  },\n+  {\n+    \"name\": \"beta_access_bypass_time\",\n+    \"type\": \"TIMESTAMP\"", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NDEyOQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492374129", "bodyText": "These schemas are generated and regenerated with the script. This is fine for pre-release, but once we have a production dataset, there are many restrictions. For example, renaming or deleting columns isn't allowed.", "author": "jaycarlton", "createdAt": "2020-09-21T22:09:39Z", "path": "api/reporting/schemas/latest/workspace.json", "diffHunk": "@@ -0,0 +1,143 @@\n+[\n+  {\n+    \"description\": \"Time snapshot was taken. Same across all rows in the snapshot, and can be used for partitioning.\",\n+    \"name\": \"snapshot_timestamp\",\n+    \"type\": \"INTEGER\"\n+  },\n+  {\n+    \"name\": \"billing_account_type\",", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NDg0OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492374849", "bodyText": "I had the query string as a constant in the projection class. That approach is certainly tidier, but you lose IntelliJ's highlighting that understands this SQL.", "author": "jaycarlton", "createdAt": "2020-09-21T22:11:15Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/UserDao.java", "diffHunk": "@@ -71,4 +70,55 @@\n \n     Long getUserCount();\n   }\n+\n+  @Query(\n+      \"SELECT\\n\"", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NTMxNQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492375315", "bodyText": "The Prj prefix is unfortunate, but luckily we only need to refer to these classes from a couple of places.", "author": "jaycarlton", "createdAt": "2020-09-21T22:12:17Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/projection/PrjUser.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package org.pmiops.workbench.db.dao.projection;\n+\n+import java.sql.Timestamp;\n+\n+public interface PrjUser {", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTMzNA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492421334", "bodyText": "Is there a reason to make it short like this? e.g. instead of ProjectedUser", "author": "calbach", "createdAt": "2020-09-22T00:43:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcxNTU4Nw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492715587", "bodyText": "We have like 4 types now.\n\ndb\nprojection\ndto\napi model\n\nI figure DbUser has a short prefix so maybe I should follow that pattern. In prractice, we could have several different projection interfaces pointing to DbUser with more or fewer fields, so we might need ProjectedReportingUser or something.", "author": "jaycarlton", "createdAt": "2020-09-22T13:04:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3NzY5Ng==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492377696", "bodyText": "for tests running the actual DAOs, we need real, dynamic IDs.", "author": "jaycarlton", "createdAt": "2020-09-21T22:18:10Z", "path": "api/src/test/java/org/pmiops/workbench/testconfig/ReportingTestUtils.java", "diffHunk": "@@ -0,0 +1,526 @@\n+package org.pmiops.workbench.testconfig;\n+\n+import static com.google.common.truth.Truth.assertThat;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.pmiops.workbench.utils.TimeAssertions.assertTimeApprox;\n+import static org.pmiops.workbench.utils.mappers.CommonMappers.offsetDateTimeUtc;\n+\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import org.pmiops.workbench.db.dao.projection.PrjUser;\n+import org.pmiops.workbench.db.dao.projection.PrjWorkspace;\n+import org.pmiops.workbench.db.model.DbCdrVersion;\n+import org.pmiops.workbench.db.model.DbStorageEnums;\n+import org.pmiops.workbench.db.model.DbUser;\n+import org.pmiops.workbench.db.model.DbWorkspace;\n+import org.pmiops.workbench.model.BillingAccountType;\n+import org.pmiops.workbench.model.BillingStatus;\n+import org.pmiops.workbench.model.BqDtoUser;\n+import org.pmiops.workbench.model.BqDtoWorkspace;\n+import org.pmiops.workbench.model.DataAccessLevel;\n+import org.pmiops.workbench.model.EmailVerificationStatus;\n+\n+public class ReportingTestUtils {\n+  // The code generator should ensure that no  values repeat\n+  // across columns. This was done to ensure nothing goes wrong\n+  // with the projection interface or mapper, since all nulls look\n+  // alike.\n+  public static final String USER__ABOUT_YOU = \"foo_0\";\n+  public static final String USER__AREA_OF_RESEARCH = \"foo_1\";\n+  public static final Timestamp USER__BETA_ACCESS_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-07T00:00:00.00Z\"));\n+  public static final Timestamp USER__BETA_ACCESS_REQUEST_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-08T00:00:00.00Z\"));\n+  public static final Timestamp USER__COMPLIANCE_TRAINING_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-09T00:00:00.00Z\"));\n+  public static final Timestamp USER__COMPLIANCE_TRAINING_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-10T00:00:00.00Z\"));\n+  public static final Timestamp USER__COMPLIANCE_TRAINING_EXPIRATION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-11T00:00:00.00Z\"));\n+  public static final String USER__CONTACT_EMAIL = \"foo_7\";\n+  public static final Timestamp USER__CREATION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-13T00:00:00.00Z\"));\n+  public static final String USER__CURRENT_POSITION = \"foo_9\";\n+  public static final Short USER__DATA_ACCESS_LEVEL =\n+      DbStorageEnums.dataAccessLevelToStorage(DataAccessLevel.REGISTERED);\n+  public static final Timestamp USER__DATA_USE_AGREEMENT_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-16T00:00:00.00Z\"));\n+  public static final Timestamp USER__DATA_USE_AGREEMENT_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-17T00:00:00.00Z\"));\n+  public static final Integer USER__DATA_USE_AGREEMENT_SIGNED_VERSION = 13;\n+  public static final Timestamp USER__DEMOGRAPHIC_SURVEY_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-19T00:00:00.00Z\"));\n+  public static final Boolean USER__DISABLED = false;\n+  public static final Timestamp USER__EMAIL_VERIFICATION_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-21T00:00:00.00Z\"));\n+  public static final Timestamp USER__EMAIL_VERIFICATION_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-22T00:00:00.00Z\"));\n+  public static final EmailVerificationStatus USER__EMAIL_VERIFICATION_STATUS =\n+      EmailVerificationStatus.SUBSCRIBED;\n+  public static final Timestamp USER__ERA_COMMONS_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-24T00:00:00.00Z\"));\n+  public static final Timestamp USER__ERA_COMMONS_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-25T00:00:00.00Z\"));\n+  public static final Timestamp USER__ERA_COMMONS_LINK_EXPIRE_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-26T00:00:00.00Z\"));\n+  public static final String USER__FAMILY_NAME = \"foo_22\";\n+  public static final Timestamp USER__FIRST_REGISTRATION_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-28T00:00:00.00Z\"));\n+  public static final Timestamp USER__FIRST_SIGN_IN_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-29T00:00:00.00Z\"));\n+  public static final Short USER__FREE_TIER_CREDITS_LIMIT_DAYS_OVERRIDE = 25;\n+  public static final Double USER__FREE_TIER_CREDITS_LIMIT_DOLLARS_OVERRIDE = 26.500000;\n+  public static final String USER__GIVEN_NAME = \"foo_27\";\n+  public static final Timestamp USER__ID_VERIFICATION_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-06-02T00:00:00.00Z\"));\n+  public static final Timestamp USER__ID_VERIFICATION_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-06-03T00:00:00.00Z\"));\n+  public static final Timestamp USER__LAST_MODIFIED_TIME =\n+      Timestamp.from(Instant.parse(\"2015-06-04T00:00:00.00Z\"));\n+  public static final String USER__ORGANIZATION = \"foo_31\";\n+  public static final String USER__PHONE_NUMBER = \"foo_32\";\n+  public static final String USER__PROFESSIONAL_URL = \"foo_33\";\n+  public static final Timestamp USER__TWO_FACTOR_AUTH_BYPASS_TIME =\n+      Timestamp.from(Instant.parse(\"2015-06-08T00:00:00.00Z\"));\n+  public static final Timestamp USER__TWO_FACTOR_AUTH_COMPLETION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-06-09T00:00:00.00Z\"));\n+  public static final Long USER__USER_ID = 36L;\n+  public static final String USER__USERNAME = \"foo_37\";\n+  public static final String USER__CITY = \"foo_0\";\n+  public static final String USER__COUNTRY = \"foo_1\";\n+  public static final String USER__STATE = \"foo_2\";\n+  public static final String USER__STREET_ADDRESS_1 = \"foo_3\";\n+  public static final String USER__STREET_ADDRESS_2 = \"foo_4\";\n+  public static final String USER__ZIP_CODE = \"foo_5\";\n+\n+  public static final BillingAccountType WORKSPACE__BILLING_ACCOUNT_TYPE =\n+      BillingAccountType.FREE_TIER;\n+  public static final BillingStatus WORKSPACE__BILLING_STATUS = BillingStatus.ACTIVE;\n+  public static final Long WORKSPACE__CDR_VERSION_ID = 2L;\n+  public static final Timestamp WORKSPACE__CREATION_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-08T00:00:00.00Z\"));\n+  public static final Long WORKSPACE__CREATOR_ID = 4L;\n+  public static final String WORKSPACE__DISSEMINATE_RESEARCH_OTHER = \"foo_6\";\n+  public static final Timestamp WORKSPACE__LAST_ACCESSED_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-12T00:00:00.00Z\"));\n+  public static final Timestamp WORKSPACE__LAST_MODIFIED_TIME =\n+      Timestamp.from(Instant.parse(\"2015-05-13T00:00:00.00Z\"));\n+  public static final String WORKSPACE__NAME = \"foo_9\";\n+  public static final Short WORKSPACE__NEEDS_RP_REVIEW_PROMPT = 10;\n+  public static final Boolean WORKSPACE__PUBLISHED = false;\n+  public static final String WORKSPACE__RP_ADDITIONAL_NOTES = \"foo_12\";\n+  public static final Boolean WORKSPACE__RP_ANCESTRY = false;\n+  public static final String WORKSPACE__RP_ANTICIPATED_FINDINGS = \"foo_14\";\n+  public static final Boolean WORKSPACE__RP_APPROVED = false;\n+  public static final Boolean WORKSPACE__RP_COMMERCIAL_PURPOSE = true;\n+  public static final Boolean WORKSPACE__RP_CONTROL_SET = false;\n+  public static final Boolean WORKSPACE__RP_DISEASE_FOCUSED_RESEARCH = true;\n+  public static final String WORKSPACE__RP_DISEASE_OF_FOCUS = \"foo_19\";\n+  public static final Boolean WORKSPACE__RP_DRUG_DEVELOPMENT = true;\n+  public static final Boolean WORKSPACE__RP_EDUCATIONAL = false;\n+  public static final Boolean WORKSPACE__RP_ETHICS = true;\n+  public static final String WORKSPACE__RP_INTENDED_STUDY = \"foo_23\";\n+  public static final Boolean WORKSPACE__RP_METHODS_DEVELOPMENT = true;\n+  public static final String WORKSPACE__RP_OTHER_POPULATION_DETAILS = \"foo_25\";\n+  public static final Boolean WORKSPACE__RP_OTHER_PURPOSE = true;\n+  public static final String WORKSPACE__RP_OTHER_PURPOSE_DETAILS = \"foo_27\";\n+  public static final Boolean WORKSPACE__RP_POPULATION_HEALTH = true;\n+  public static final String WORKSPACE__RP_REASON_FOR_ALL_OF_US = \"foo_29\";\n+  public static final Boolean WORKSPACE__RP_REVIEW_REQUESTED = true;\n+  public static final String WORKSPACE__RP_SCIENTIFIC_APPROACH = \"foo_31\";\n+  public static final Boolean WORKSPACE__RP_SOCIAL_BEHAVIORAL = true;\n+  public static final Timestamp WORKSPACE__RP_TIME_REQUESTED =\n+      Timestamp.from(Instant.parse(\"2015-06-07T00:00:00.00Z\"));\n+  public static final Long WORKSPACE__WORKSPACE_ID = 34L;\n+\n+  public static void assertDtoUserFields(BqDtoUser user) {\n+    assertThat(user.getAboutYou()).isEqualTo(USER__ABOUT_YOU);\n+    assertThat(user.getAreaOfResearch()).isEqualTo(USER__AREA_OF_RESEARCH);\n+    assertTimeApprox(user.getBetaAccessBypassTime(), USER__BETA_ACCESS_BYPASS_TIME);\n+    assertTimeApprox(user.getBetaAccessRequestTime(), USER__BETA_ACCESS_REQUEST_TIME);\n+    assertTimeApprox(user.getComplianceTrainingBypassTime(), USER__COMPLIANCE_TRAINING_BYPASS_TIME);\n+    assertTimeApprox(\n+        user.getComplianceTrainingCompletionTime(), USER__COMPLIANCE_TRAINING_COMPLETION_TIME);\n+    assertTimeApprox(\n+        user.getComplianceTrainingExpirationTime(), USER__COMPLIANCE_TRAINING_EXPIRATION_TIME);\n+    assertThat(user.getContactEmail()).isEqualTo(USER__CONTACT_EMAIL);\n+    assertTimeApprox(user.getCreationTime(), USER__CREATION_TIME);\n+    assertThat(user.getCurrentPosition()).isEqualTo(USER__CURRENT_POSITION);\n+    assertThat(user.getDataAccessLevel())\n+        .isEqualTo(DbStorageEnums.dataAccessLevelFromStorage(USER__DATA_ACCESS_LEVEL));\n+    assertTimeApprox(user.getDataUseAgreementBypassTime(), USER__DATA_USE_AGREEMENT_BYPASS_TIME);\n+    assertTimeApprox(\n+        user.getDataUseAgreementCompletionTime(), USER__DATA_USE_AGREEMENT_COMPLETION_TIME);\n+    assertThat(user.getDataUseAgreementSignedVersion())\n+        .isEqualTo(USER__DATA_USE_AGREEMENT_SIGNED_VERSION);\n+    assertTimeApprox(\n+        user.getDemographicSurveyCompletionTime(), USER__DEMOGRAPHIC_SURVEY_COMPLETION_TIME);\n+    assertThat(user.getDisabled()).isEqualTo(USER__DISABLED);\n+    assertTimeApprox(user.getEmailVerificationBypassTime(), USER__EMAIL_VERIFICATION_BYPASS_TIME);\n+    assertTimeApprox(\n+        user.getEmailVerificationCompletionTime(), USER__EMAIL_VERIFICATION_COMPLETION_TIME);\n+    assertThat(user.getEmailVerificationStatus()).isEqualTo(USER__EMAIL_VERIFICATION_STATUS);\n+    assertTimeApprox(user.getEraCommonsBypassTime(), USER__ERA_COMMONS_BYPASS_TIME);\n+    assertTimeApprox(user.getEraCommonsCompletionTime(), USER__ERA_COMMONS_COMPLETION_TIME);\n+    assertTimeApprox(user.getEraCommonsLinkExpireTime(), USER__ERA_COMMONS_LINK_EXPIRE_TIME);\n+    assertThat(user.getFamilyName()).isEqualTo(USER__FAMILY_NAME);\n+    assertTimeApprox(\n+        user.getFirstRegistrationCompletionTime(), USER__FIRST_REGISTRATION_COMPLETION_TIME);\n+    assertTimeApprox(user.getFirstSignInTime(), USER__FIRST_SIGN_IN_TIME);\n+    assertThat(user.getFreeTierCreditsLimitDaysOverride())\n+        .isEqualTo(USER__FREE_TIER_CREDITS_LIMIT_DAYS_OVERRIDE);\n+    assertThat(user.getFreeTierCreditsLimitDollarsOverride())\n+        .isEqualTo(USER__FREE_TIER_CREDITS_LIMIT_DOLLARS_OVERRIDE);\n+    assertThat(user.getGivenName()).isEqualTo(USER__GIVEN_NAME);\n+    assertTimeApprox(user.getIdVerificationBypassTime(), USER__ID_VERIFICATION_BYPASS_TIME);\n+    assertTimeApprox(user.getLastModifiedTime(), USER__LAST_MODIFIED_TIME);\n+    assertThat(user.getOrganization()).isEqualTo(USER__ORGANIZATION);\n+    assertThat(user.getPhoneNumber()).isEqualTo(USER__PHONE_NUMBER);\n+    assertThat(user.getProfessionalUrl()).isEqualTo(USER__PROFESSIONAL_URL);\n+    assertTimeApprox(user.getTwoFactorAuthBypassTime(), USER__TWO_FACTOR_AUTH_BYPASS_TIME);\n+    assertTimeApprox(user.getTwoFactorAuthCompletionTime(), USER__TWO_FACTOR_AUTH_COMPLETION_TIME);\n+    assertThat(user.getUserId()).isEqualTo(USER__USER_ID);\n+    assertThat(user.getUsername()).isEqualTo(USER__USERNAME);\n+  }\n+\n+  public static void assertDtoWorkspaceFields(\n+      BqDtoWorkspace workspace,\n+      long expectedWorkspaceId,", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM3ODA2NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492378064", "bodyText": "This one is a little special, but somewhere I had a whole bunch of assertions that needed this combination of arguments.", "author": "jaycarlton", "createdAt": "2020-09-21T22:19:14Z", "path": "api/src/test/java/org/pmiops/workbench/utils/TimeAssertions.java", "diffHunk": "@@ -14,29 +16,51 @@\n \n   public static final Duration DEFAULT_TOLERANCE = Duration.ofMillis(100);\n \n-  public static void assertTimeWithinTolerance(OffsetDateTime actual, OffsetDateTime expected) {\n-    assertTimeWithinTolerance(actual.toInstant(), expected.toInstant(), DEFAULT_TOLERANCE);\n+  public static void assertTimeApprox(OffsetDateTime actual, OffsetDateTime expected) {\n+    assertTimeApprox(actual.toInstant(), expected.toInstant(), DEFAULT_TOLERANCE);\n   }\n \n-  public static void assertTimeWithinTolerance(\n+  public static void assertTimeApprox(\n       OffsetDateTime actual, OffsetDateTime expected, Duration tolerance) {\n-    assertTimeWithinTolerance(actual.toInstant(), expected.toInstant(), tolerance);\n+    assertTimeApprox(actual.toInstant(), expected.toInstant(), tolerance);\n   }\n \n-  public static void assertTimeWithinTolerance(Instant actual, Instant expected) {\n-    assertTimeWithinTolerance(actual, expected, DEFAULT_TOLERANCE);\n+  public static void assertTimeApprox(Timestamp actual, Timestamp expected) {\n+    assertTimeApprox(actual.toInstant(), expected.toInstant());\n   }\n \n-  public static void assertTimeWithinTolerance(\n-      Instant actual, Instant expected, Duration tolerance) {\n-    assertTimeWithinTolerance(actual.toEpochMilli(), expected.toEpochMilli(), tolerance.toMillis());\n+  public static void assertTimeApprox(Timestamp actual, Timestamp expected, Duration tolerance) {\n+    assertTimeApprox(actual.toInstant(), expected.toInstant(), tolerance);\n   }\n \n-  public static void assertTimeWithinTolerance(long actualEpochMillis, long expectedEpochMillis) {\n-    assertTimeWithinTolerance(actualEpochMillis, expectedEpochMillis, DEFAULT_TOLERANCE.toMillis());\n+  public static void assertTimeApprox(OffsetDateTime actual, Timestamp expected) {\n+    // TODO: move to TimeMappers class after merging that\n+    assertTimeApprox(actual, OffsetDateTime.ofInstant(expected.toInstant(), ZoneOffset.UTC));\n   }\n \n-  public static void assertTimeWithinTolerance(\n+  public static void assertTimeApprox(", "originalCommit": "e6f0d5014024efbdc31219201948f994399d9833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNTczOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492415738", "bodyText": "The following fields are obsolete and I would suggest simply omitting them from the first cut:\n\nbetaAccess*\nemailVerification*\nidVerification*\nphoneNumber\norganization\n\nAnother field I'd prefer not to export because of ambiguous semantics (technically, we don't have a concept of eRA commons expiration today):\n\neraCommonsLinkExpireTime", "author": "calbach", "createdAt": "2020-09-22T00:19:51Z", "path": "api/src/main/resources/workbench-api.yaml", "diffHunk": "@@ -7942,69 +7956,204 @@ definitions:\n         description: Time at which snapshot is retrieved. Epoch millis.\n         type: integer\n         format: int64\n-      researchers:\n+      users:\n         type: array\n         items:\n-          \"$ref\": \"#/definitions/ReportingResearcher\"\n+          \"$ref\": \"#/definitions/BqDtoUser\"\n       workspaces:\n         type: array\n         items:\n-          \"$ref\": \"#/definitions/ReportingWorkspace\"\n-  ReportingWorkspace:\n-    description: >\n+          \"$ref\": \"#/definitions/BqDtoWorkspace\"\n+  BqDtoUser:\n     type: object\n     properties:\n-      workspaceId:\n-        description: PK for workspace table.\n-        type: integer\n-        format: int64\n-      creatorId:\n-        description: FK researcher ID for creator of workspace.\n+      aboutYou:\n+        type: string\n+      areaOfResearch:\n+        type: string\n+      betaAccessBypassTime:", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk1MDUzOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492950538", "bodyText": "I'm assuming these changes should be integrated before this PR is merged. Please let me know if you're planning otherwise.", "author": "calbach", "createdAt": "2020-09-22T18:33:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNTczOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNjQ0Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492416442", "bodyText": "Reminder to revert", "author": "calbach", "createdAt": "2020-09-22T00:22:43Z", "path": "api/src/main/webapp/WEB-INF/cron_default.yaml", "diffHunk": "@@ -1,90 +1,4 @@\n cron:\n-- description: 'Periodic notebook runtime checks'", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY4OTY2NA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492689664", "bodyText": "good catch", "author": "jaycarlton", "createdAt": "2020-09-22T12:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNjQ0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzMyMw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492417323", "bodyText": "Was this capitalization intentional?", "author": "calbach", "createdAt": "2020-09-22T00:26:26Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5MDAwOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492690008", "bodyText": "no, I just didn't want to fix it and have to find the dir to delete.", "author": "jaycarlton", "createdAt": "2020-09-22T12:24:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk0MzczNw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492943737", "bodyText": "I don't understand. The inputs in the PR are in folder excluded_columns. And also, aren't you done generating code at this point? I'm not understanding why columns has been capitalized in this file several times", "author": "calbach", "createdAt": "2020-09-22T18:22:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzYzNg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492417636", "bodyText": "There's a lot going on in this script. Where is the package-level description of what this tool is doing? Details like inputs, outputs, and how Swagger fits in.\nIn terms of usage of the tool: is this meant to be run once and then likely never again? Or are we expecting to run this continually as the database and/or reporting schemas evolve? If so - is there a reproducible mechanism for doing so? I'm noticing seemingly a lot of manual modifications post-codegen, which would make this challenging.", "author": "calbach", "createdAt": "2020-09-22T00:27:42Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5MTg0OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492691849", "bodyText": "The short answer is that I needed it to keep things consistent, and would like to re-use it for schema modifications. There are still a couple of special cases, but I was prioritizing getting somethign shipped on the reporting side.\nFor this to be a long-lived, clean tool, I'd probably want to port it to use a legitimate templating system, and have it occur at build time rather than just once.", "author": "jaycarlton", "createdAt": "2020-09-22T12:26:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzYzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjc5NTUxMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492795512", "bodyText": "There's this README for the whole thing, which would need to be updated if we stick with this structure. I'm scrambling to get a dataset delivered this week, so haven't gotten to update it  yet.", "author": "jaycarlton", "createdAt": "2020-09-22T14:46:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzg3MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492417870", "bodyText": "what is :constant? Is this a default value ?", "author": "calbach", "createdAt": "2020-09-22T00:28:46Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5Mjg3NQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492692875", "bodyText": "It's something for the right hand side in unit test constant declarations; nothing to do with the schema.", "author": "jaycarlton", "createdAt": "2020-09-22T12:28:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzg3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzMzUyMQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492933521", "bodyText": "I still don't understand, but please add a comment to the code", "author": "calbach", "createdAt": "2020-09-22T18:05:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxNzg3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxODgwNQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492418805", "bodyText": "Where is this input format documented?", "author": "calbach", "createdAt": "2020-09-22T00:32:29Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -26,64 +27,187 @@ def to_output_path(dir_name, table_name, suffix)\n   File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n end\n \n-dto_class_name = \"BqDto#{to_camel_case(table_name, true)}\"\n-\n-inputs = {\n+INPUTS = {\n     :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n-    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n-}\n+    :excluded_columns => to_input_path(File.join(input_dir, 'excluded_COLUMNS'), table_name,'txt')\n+}.freeze\n \n-outputs = {\n+OUTPUTS = {\n     :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n     :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n     :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n-    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n-}\n-\n-MYSQL_TO_BIGQUERY_TYPE = {\n-    'varchar' => 'STRING',\n-    'datetime' => 'TIMESTAMP',\n-    'bigint' => 'INT64',\n-    'smallint' => 'INT64',\n-    'longtext' => 'STRING',\n-    'int' => 'INT64',\n-    'tinyint' => 'INT64',\n-    'bit' => 'BOOLEAN',\n-    'double' => 'FLOAT64',\n-    'text' => 'STRING',\n-    'mediumblob' => 'STRING'\n-}\n-\n-def to_bq_type(mysql_type)\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java'),\n+    :unit_test_constants => to_output_path(File.join(output_dir, 'unit_test_constants'), table_name, 'java'),\n+    :unit_test_mocks => to_output_path(File.join(output_dir, 'unit_test_mocks'), table_name, 'java'),\n+    :dto_assertions => to_output_path(File.join(output_dir, 'dto_assertions'), table_name, 'java'),\n+    :query_parameter_COLUMNS => to_output_path(File.join(output_dir, 'query_parameter_COLUMNS'), table_name, 'java'),\n+    :dto_decl => to_output_path(File.join(output_dir, 'dto_decl'), table_name, 'java'),\n+    :entity_decl => to_output_path(File.join(output_dir, 'entity_decl'), table_name, 'java'),\n+}.freeze\n+\n+# This is the canonical type map, but there are places where we assign a tinyint MySql column a Long Entity field, etc.\n+MYSQL_TO_TYPES = {\n+    'varchar' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'datetime' => {\n+        :bigquery => 'TIMESTAMP',\n+        :java => 'Timestamp'\n+    },\n+    'bigint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Long'\n+    },\n+    'smallint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'longtext' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'int' => {\n+        :bigquery => 'INT64',\n+        :java => 'Integer'\n+    },\n+    'tinyint' => {\n+        :bigquery => 'INT64',\n+        :java => 'Short'\n+    },\n+    'bit' => {\n+        :bigquery => 'BOOLEAN',\n+        :java => 'Boolean'\n+    },\n+    'double' => {\n+        :bigquery =>  'FLOAT64',\n+        :java => 'Double'\n+    },\n+    'text' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    },\n+    'mediumblob' => {\n+        :bigquery => 'STRING',\n+        :java => 'String'\n+    }\n+}.freeze\n+\n+# Enumerated types should be the enum in Java and the DTO, but a STRING in BQ.\n+# We could either try to remap to the enum type when writing to BQ, but we still\n+# have to handle getting the value into the projection, and the only way I've been\n+# able to do that is to use the exact same type.\n+ENUM_TYPES = {\n+     'workspace' => {\n+        'billing_status' => {\n+            :java => 'BillingStatus',\n+            :constant => 'BillingStatus.ACTIVE'\n+        },\n+        'billing_account_type' => {\n+            :java => 'BillingAccountType',\n+            :constant => 'BillingAccountType.FREE_TIER'\n+        }\n+     },\n+     'user' => {\n+         'data_access_level' => {\n+             :java => 'DataAccessLevel',\n+             :constant => 'DataAccessLevel.REGISTERED'\n+         },\n+         'email_verification_status' => {\n+             :java => 'EmailVerificationStatus',\n+             :constant => 'EmailVerificationStatus.SUBSCRIBED'\n+         }\n+     }\n+}.freeze\n+\n+# strip size/kind\n+def simple_mysql_type(mysql_type)\n   type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n   match_data = mysql_type.match(type_pattern)\n-  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  result = match_data[:type]\n   raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n   result\n end\n \n-excluded_fields = File.exist?(inputs[:exclude_columns]) \\\n-  ? File.readlines(inputs[:exclude_columns]) \\\n+excluded_fields = File.exist?(INPUTS[:excluded_columns]) \\\n+  ? File.readlines(INPUTS[:excluded_columns]) \\\n       .map{ |c| c.strip } \\\n   : []\n \n def include_field?(excluded_fields, field)\n   !excluded_fields.include?(field)\n end\n \n+ENTITY_MODIFIED_COLUMNS = {\n+    'workspace' => {\n+        'cdr_version_id' => 'cdrVersion.cdrVersionId AS cdrVersionId',\n+        'creator_id' => 'creator.userId AS creatorId',\n+        'needs_rp_review_prompt' => 'needsResearchPurposeReviewPrompt AS needsRpReviewPrompt'\n+    }\n+}\n+\n ## BigQuery schema\n-describe_rows = CSV.new(File.read(inputs[:describe_csv])).sort_by { |row| row[0]  }\n+describe_rows = CSV.new(File.read(INPUTS[:describe_csv])).sort_by { |row| row[0]  }", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5MzU0Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492693542", "bodyText": "It's just this: https://dev.mysql.com/doc/refman/8.0/en/explain.html", "author": "jaycarlton", "createdAt": "2020-09-22T12:29:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxODgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzNDEwMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492934102", "bodyText": "This was less a question of personal curiosity and more: \"how is the next user or reader of this code going to know what the expected input format is? Please add a comment or documentation\"", "author": "calbach", "createdAt": "2020-09-22T18:06:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQxODgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTE3MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492421170", "bodyText": "based on your codegen, I suspect this SQL is generated. If so, how is one meant to update this to match changes in the codegen inputs? Also, should there be some kind of autogenerated comment that comes with it to indicate that this should not be modified manually?", "author": "calbach", "createdAt": "2020-09-22T00:42:42Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/WorkspaceDao.java", "diffHunk": "@@ -85,4 +86,44 @@ default void updateBillingStatus(long workspaceId, BillingStatus status) {\n \n     Long getWorkspaceCount();\n   }\n+\n+  @Query(\n+      \"SELECT\\n\"\n+          + \"  w.billingAccountType,\\n\"\n+          + \"  w.billingStatus,\\n\"\n+          + \"  w.cdrVersion.cdrVersionId AS cdrVersionId,\\n\"\n+          + \"  w.creationTime,\\n\"\n+          + \"  w.creator.userId AS creatorId,\\n\"\n+          + \"  w.disseminateResearchOther,\\n\"\n+          + \"  w.lastAccessedTime,\\n\"\n+          + \"  w.lastModifiedTime,\\n\"\n+          + \"  w.name,\\n\"\n+          + \"  w.needsResearchPurposeReviewPrompt AS needsRpReviewPrompt,\\n\"\n+          + \"  w.published,\\n\"\n+          + \"  w.additionalNotes AS rpAdditionalNotes,\\n\"", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5NDQ3Nw==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492694477", "bodyText": "I should've called it a wizard and not a codegen tool, but you're right. It was on my list to do something like that, but I was worried I'd follow that rabbit too far and end up restructuring the whole thing.\nIn practice, using a graphical merge tool like Beyond Compare worked for me for now.", "author": "jaycarlton", "createdAt": "2020-09-22T12:31:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTE3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjc5NzE2Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492797162", "bodyText": "Standardizing column naming practices will help here, as well as best practices for entity classes.", "author": "jaycarlton", "createdAt": "2020-09-22T14:48:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTE3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTQ0OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492421449", "bodyText": "If this code is generate, please add a generated comment with provenance", "author": "calbach", "createdAt": "2020-09-22T00:44:01Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/projection/PrjUser.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package org.pmiops.workbench.db.dao.projection;", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5NjYxMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492696612", "bodyText": "Good idea.\nWith respect to the name PrjUser above, I was trying to draw a parallel with DbUser, but I like ProjectedUser better than UserProjection.", "author": "jaycarlton", "createdAt": "2020-09-22T12:34:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMTQ0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMzU2Ng==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492423566", "bodyText": "Are these generating code snippets to be pasted into other tests? Based on my reading here I though there would be new tests e.g. for each model, but I don't see any such generated test files.", "author": "calbach", "createdAt": "2020-09-22T00:52:47Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -136,68 +254,190 @@ def to_swagger_property(column)\n   .reject{ |line| '---'.eql?(line)} \\\n   .map{ |line| '  ' + line } \\\n   .join(\"\\n\")\n-IO.write(outputs[:swagger_yaml], indented_yaml)\n-puts \"  DTO Swagger Definition to #{outputs[:swagger_yaml]}\"\n \n-### Projection Interface\n-\n-BIGQUERY_TYPE_TO_JAVA  = {\n-    'STRING' => 'String',\n-    'INT64' => 'long',\n-    'TIMESTAMP' =>  'Timestamp',\n-    'BOOLEAN' =>  'boolean',\n-    'FLOAT64' => 'double'\n-}\n+write_output(OUTPUTS[:swagger_yaml], indented_yaml, 'DTO Swagger Definition')\n \n+### Projection Interface\n def to_getter(field)\n-  \"  #{BIGQUERY_TYPE_TO_JAVA[field[:big_query_type]]} get#{to_camel_case(field[:name], true)}();\"\n+  property_type = field[:java_type]\n+  \"  #{property_type} #{field[:getter]}();\"\n end\n \n-getters = columns.map { |field|\n+projection_decl = \"public interface #{TABLE_INFO[:projection_interface]} {\\n\"\n+projection_decl << COLUMNS.map { |field|\n   to_getter(field)\n-}\n-\n-def projection_name(table_name)\n-  \"Prj#{to_camel_case(table_name, true )}\"\n-end\n-\n-java = \"public interface #{projection_name(table_name)} {\\n\"\n-java << getters.join(\"\\n\")\n-java << \"\\n}\\n\"\n+}.join(\"\\n\")\n+projection_decl << \"\\n}\\n\"\n \n-IO.write(outputs[:projection_interface], java)\n-puts \"  Spring Data Projection Interface: #{outputs[:projection_interface]}\"\n+write_output(OUTPUTS[:projection_interface], projection_decl, 'Spring Data Projection Interface')\n \n ### Projection query\n def hibernate_column_name(field)\n   to_camel_case(field[:name], false)\n end\n \n # Fix up research purpose entity fields, which don't match the column names (i.e. there's no 'rp' prefix)\n-def adjust_rp_col(field, table_alias)\n+def adjust_col(field)\n   md = field.match(/^rp_(?<root>\\w+$)/)\n   projection_field = to_camel_case(field, false)\n   entity_property = projection_field\n   if md and md['root']\n     entity_property = to_camel_case(md['root'], false)\n-    \"#{table_alias}.#{entity_property} AS #{projection_field}\"\n-  else\n-    \"#{table_alias}.#{entity_property}\"\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property} AS #{projection_field}\"\n+  elsif TABLE_INFO[:entity_modified_columns][field]\n+     \"#{TABLE_INFO[:sql_alias]}.#{TABLE_INFO[:entity_modified_columns][field]}\"\n+   else\n+    \"#{TABLE_INFO[:sql_alias]}.#{entity_property}\"\n   end\n end\n \n-  def to_query(table_name, schema)\n-  table_alias = table_name[0].downcase\n+def to_query()\n   \"@Query(\\\"SELECT\\\\n\\\"\\n\" \\\n-    + schema.map do |field|\n-      \"+ \\\"  #{adjust_rp_col(field[:name], table_alias)}\"\n+    + COLUMNS.map do |field|\n+      \"+ \\\"  #{adjust_col(field[:name])}\"\n     end \\\n     .join(\",\\\\n\\\"\\n\") \\\n     + \"\\\\n\\\"\\n\" \\\n-    + \"+ \\\"FROM Db#{to_camel_case(table_name, true)} #{table_alias}\\\")\\n\" \\\n-    + \"  List<#{projection_name(table_name)}> getReporting#{to_camel_case(table_name, true)}s();\"\n+    + \"+ \\\"FROM #{TABLE_INFO[:entity_class]} #{TABLE_INFO[:sql_alias]}\\\")\\n\" \\\n+    + \"  List<#{TABLE_INFO[:projection_interface]}> getReporting#{to_camel_case(TABLE_INFO[:name], true)}s();\"\n+end\n+\n+sql = to_query\n+\n+write_output(OUTPUTS[:projection_query], sql, 'Projection Query')\n+\n+# Unit Test Constants\n+#\n+BASE_TIMESTAMP = Time.new(2015, 5, 5).freeze\n+TIMESTAMP_DELTA_SECONDS = 24 * 60 * 60 # .freeze # seconds in day\n+\n+# N.B. some Short fields are only valid up to the number of associated enum values - 1. Fixing these\n+# up by hand for now.\n+def to_constant_declaration(column, index)\n+  value = case column[:java_type]\n+          when 'String'\n+            \"\\\"foo_#{index}\\\"\"\n+          when 'Integer'\n+            \"%d\" % [index]\n+          when 'Long'\n+            \"%dL\" % [index]\n+          when 'Double'\n+            \"%f\" % [index + 0.5]\n+          when 'Boolean'\n+            index.even? # just flip it every time\n+          when 'Timestamp'\n+            # add a day times the index to base timestamp\n+            timestamp = BASE_TIMESTAMP + TIMESTAMP_DELTA_SECONDS * index\n+            \"Timestamp.from(Instant.parse(\\\"#{timestamp.strftime(\"%Y-%m-%dT00:00:00.00Z\")}\\\"))\"\n+          else\n+            if column[:is_enum]\n+              column[:default_enum]\n+            else\n+              index.to_s\n+            end\n+          end\n+  \"public static final #{column[:java_type]} #{column[:java_constant_name]} = #{value};\"\n+end\n+\n+constants = COLUMNS.enum_for(:each_with_index) \\\n+  .map { |col, index| to_constant_declaration(col, index) } \\\n+  .join(\"\\n\")\n+\n+write_output(OUTPUTS[:unit_test_constants], constants, 'Unit Test Constants')\n+\n+### Mock Instantiation\n+# Mock the projection interface for testing with mock services exposing them\n+mocks = COLUMNS.map { |col|\n+  \"doReturn(#{col[:java_constant_name]}).when(#{TABLE_INFO[:mock]}).#{col[:getter]}();\"\n+}\n+\n+lines = []\n+lines << \"final #{TABLE_INFO[:projection_interface]} #{TABLE_INFO[:mock]} = mock(#{TABLE_INFO[:projection_interface]}.class);\"\n+lines << mocks\n+lines.flatten!\n+\n+write_output(OUTPUTS[:unit_test_mocks], lines.join(\"\\n\"), 'Unit Test Mocks')", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjY5NTU0MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492695541", "bodyText": "Sorry. In the interest of time, I just used a hacky wrapper script to run this against every table description file in the directory. So this script only deals with one model at a time.\nIt would be good to try to run it on your machine to make sure there are no surprises.", "author": "jaycarlton", "createdAt": "2020-09-22T12:33:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMzU2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjc5ODUyOA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492798528", "bodyText": "Re-read your actual question after having some coffee.\nYes, replacing whole files is preferable to replacing chunks of them, but I felt like at that point I should abandon rolling my own system and use something like Swagger class templates.", "author": "jaycarlton", "createdAt": "2020-09-22T14:50:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQyMzU2Ng=="}], "type": "inlineReview"}, {"oid": "d83c6413f72160e6b68fd240097d6a47b3d532ac", "url": "https://github.com/all-of-us/workbench/commit/d83c6413f72160e6b68fd240097d6a47b3d532ac", "message": "restore cron", "committedDate": "2020-09-22T13:18:27Z", "type": "commit"}, {"oid": "596780805a75c479784a08e0f48a047952ea35fd", "url": "https://github.com/all-of-us/workbench/commit/596780805a75c479784a08e0f48a047952ea35fd", "message": "Merge branch 'master' into jaycarlton/projectionTest2", "committedDate": "2020-09-22T13:18:33Z", "type": "commit"}, {"oid": "9e54e6abb023dc580ab1f37fadaf4d62c40eecfd", "url": "https://github.com/all-of-us/workbench/commit/9e54e6abb023dc580ab1f37fadaf4d62c40eecfd", "message": "remove unneeded fields and start adding provenance comments", "committedDate": "2020-09-22T15:19:00Z", "type": "commit"}, {"oid": "dfd2a6644fd2fb09f434144184a2164542a802e0", "url": "https://github.com/all-of-us/workbench/commit/dfd2a6644fd2fb09f434144184a2164542a802e0", "message": "start commenting on things", "committedDate": "2020-09-22T17:23:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzNDk3Mg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492934972", "bodyText": "What is the significance of calling this directory \"latest\"? Does this imply that we're going to have divergent schemas across snapshots? I wouldn't think this works in BigQuery", "author": "calbach", "createdAt": "2020-09-22T18:08:03Z", "path": "api/reporting/schemas/latest/workspace.json", "diffHunk": "@@ -0,0 +1,143 @@\n+[", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc1Nzc5MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r493757790", "bodyText": "We can add columns, and all prior rows just default to null. There's no removing columns without replacing tables.\nOne of the things we can do with views or scheduled queries is hide deprecated columns, though.", "author": "jaycarlton", "createdAt": "2020-09-23T17:15:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzNDk3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMxMDA4MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r494310081", "bodyText": "An open question for RW-5195 is where to host these schema files and any docs for the users. Keeping them in this directory would likely work, but it feels sloppy to have them rely on something buried in our repo and not at the top level someplace. On the other hand, making a new repo just for this purpose seems like overkill.", "author": "jaycarlton", "createdAt": "2020-09-24T13:19:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzNDk3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkzNjMzOQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492936339", "bodyText": "nit: this header should be unnecessary - crons do not require user creds, else they wouldn't work when called by GAE", "author": "calbach", "createdAt": "2020-09-22T18:10:18Z", "path": "api/reporting/curl/upload-snapshot-local.sh", "diffHunk": "@@ -0,0 +1,7 @@\n+#!/bin/bash\n+\n+# Start hit the reporting snapshot & upload cron endpoint  locally.\n+curl -X GET 'http://localhost:8081/v1/cron/uploadReportingSnapshot' \\\n+  --header \"X-AppEngine-Cron: true\" \\\n+  --header \"Authorization: Bearer `gcloud auth print-access-token`\" \\", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk0NTYwMg==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492945602", "bodyText": "Why is expectedType nullable?", "author": "calbach", "createdAt": "2020-09-22T18:25:38Z", "path": "api/src/main/java/org/pmiops/workbench/cohortbuilder/util/QueryParameterValues.java", "diffHunk": "@@ -65,35 +103,83 @@ public static String replaceNamedParameters(QueryJobConfiguration queryJobConfig\n     return Matchers.replaceAllInMap(patternToReplacement, result);\n   }\n \n-  public static String formatQuery(String query) {\n+  @NotNull\n+  public static String formatQuery(@NotNull String query) {\n     return new BasicFormatterImpl().format(query);\n   }\n \n   // use lookbehind for non-word character, since \"'\"(@\" or \" @\" don't represent left-side word\n   // boundaries.\n-  private static Pattern buildParameterRegex(String parameterName) {\n+  @NotNull\n+  private static Pattern buildParameterRegex(@NotNull String parameterName) {\n     return Pattern.compile(String.format(\"(?<=\\\\W)%s\\\\b\", decorateParameterName(parameterName)));\n   }\n \n-  public static String decorateParameterName(String parameterName) {\n+  @NotNull\n+  public static String decorateParameterName(@NotNull String parameterName) {\n     return \"@\" + parameterName;\n   }\n \n-  private static String getReplacementString(QueryParameterValue parameterValue) {\n+  @Nullable\n+  public static QueryParameterValue toTimestampQpv(@Nullable OffsetDateTime offsetDateTime) {\n+    final String arg =\n+        Optional.ofNullable(offsetDateTime).map(QPV_TIMESTAMP_FORMATTER::format).orElse(null);\n+    return QueryParameterValue.timestamp(arg);\n+  }\n+\n+  // Return null instead of Optional.empty() so the return value can go directly into\n+  // the content map of an InsertAllRequest.RowToInsert.\n+  @Nullable\n+  public static String toInsertRowString(@Nullable OffsetDateTime offsetDateTime) {\n+    return Optional.ofNullable(offsetDateTime)\n+        .map(ROW_TO_INSERT_TIMESTAMP_FORMATTER::format)\n+        .orElse(null);\n+  }\n+\n+  // BigQuery TIMESTAMP types don't include a zone or offset, but are always UTC.\n+  public static Optional<OffsetDateTime> rowToInsertStringToOffsetTimestamp(\n+      @Nullable String bqTimeString) {\n+    return Optional.ofNullable(bqTimeString)\n+        .filter(s -> s.length() > 0)\n+        .map(ROW_TO_INSERT_TIMESTAMP_FORMATTER::parse)\n+        .map(LocalDateTime::from)\n+        .map(ldt -> OffsetDateTime.of(ldt, ZoneOffset.UTC));\n+  }\n+\n+  @NotNull\n+  public static <T extends Enum<T>> QueryParameterValue enumToQpv(@Nullable T enumValue) {\n+    return QueryParameterValue.string(enumToString(enumValue));\n+  }\n+\n+  // RowToInsert enum string or null (to be omitted)\n+  @Nullable\n+  public static <T extends Enum<T>> String enumToString(@Nullable T enumValue) {\n+    return Optional.ofNullable(enumValue).map(T::toString).orElse(null);\n+  }\n+\n+  @NotNull\n+  private static String getReplacementString(@Nullable QueryParameterValue parameterValue) {\n     final String value =\n         Optional.ofNullable(parameterValue).map(QueryParameterValue::getValue).orElse(\"NULL\");\n \n-    if (isTimestampQpv(parameterValue)) {\n+    if (isQpvTimestamp(parameterValue)) {\n       return String.format(\"TIMESTAMP '%s'\", value);\n     } else {\n       return value;\n     }\n   }\n \n-  private static boolean isTimestampQpv(QueryParameterValue parameterValue) {\n+  @NotNull\n+  private static boolean isQpvTimestamp(@Nullable QueryParameterValue parameterValue) {\n+    return matchesQpvType(parameterValue, StandardSQLTypeName.TIMESTAMP);\n+  }\n+\n+  // return false if teh parameterValue is non-null does not match the expected type.\n+  private static boolean matchesQpvType(\n+      @Nullable QueryParameterValue parameterValue, @Nullable StandardSQLTypeName expectedType) {", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc1ODc4OQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r493758789", "bodyText": "Good catch; neither of these should be nullable.", "author": "jaycarlton", "createdAt": "2020-09-23T17:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk0NTYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk1MjE2MA==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r492952160", "bodyText": "reporting", "author": "calbach", "createdAt": "2020-09-22T18:36:18Z", "path": "api/src/main/java/org/pmiops/workbench/db/dao/UserServiceImpl.java", "diffHunk": "@@ -1029,4 +1030,9 @@ public boolean hasAuthority(long userId, Authority required) {\n   public Set<DbUser> findAllUsersWithAuthoritiesAndPageVisits() {\n     return userDao.findAllUsersWithAuthoritiesAndPageVisits();\n   }\n+\n+  @Override\n+  public List<PrjUser> getRepotingUsers() {", "originalCommit": "ab663267e77cba832cf0846d3cfc9754aed9d483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8142c31a3aac84212974351a576c71f4da628f04", "url": "https://github.com/all-of-us/workbench/commit/8142c31a3aac84212974351a576c71f4da628f04", "message": "more fixes", "committedDate": "2020-09-22T22:27:53Z", "type": "commit"}, {"oid": "18054785d8e9b303fe4822bb981df09ae851e0e7", "url": "https://github.com/all-of-us/workbench/commit/18054785d8e9b303fe4822bb981df09ae851e0e7", "message": "test update", "committedDate": "2020-09-23T00:08:52Z", "type": "commit"}, {"oid": "b8ee4ffd1dab96ba084b6ba816c683bc31f17644", "url": "https://github.com/all-of-us/workbench/commit/b8ee4ffd1dab96ba084b6ba816c683bc31f17644", "message": "doc improvements", "committedDate": "2020-09-23T17:42:46Z", "type": "commit"}, {"oid": "19f58ba9062671c182cfa09290db6058eea237dd", "url": "https://github.com/all-of-us/workbench/commit/19f58ba9062671c182cfa09290db6058eea237dd", "message": "fixes & cleanup on  wizard, especially around enum handling", "committedDate": "2020-09-23T19:00:00Z", "type": "commit"}, {"oid": "e4479d1057193149186641cbccf4ae35965b2aac", "url": "https://github.com/all-of-us/workbench/commit/e4479d1057193149186641cbccf4ae35965b2aac", "message": "type fixes and comment improvements", "committedDate": "2020-09-23T21:17:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0MDU2MQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r493940561", "bodyText": "This file appears to have now been duplicated - probably the one at api/curl/upload-snapshot-local.sh should be removed", "author": "calbach", "createdAt": "2020-09-23T22:58:37Z", "path": "api/reporting/curl/upload-snapshot-local.sh", "diffHunk": "@@ -0,0 +1,7 @@\n+#!/bin/bash", "originalCommit": "e4479d1057193149186641cbccf4ae35965b2aac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0MjcxMQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r493942711", "bodyText": "opt: Having a long generic type name like this looks a bit unusual to me. We don't have an official style ruling on this, but FWIW Oracle's guides recommend a single character here.", "author": "calbach", "createdAt": "2020-09-23T23:05:09Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/insertion/BigQueryInsertionPayloadTransformer.java", "diffHunk": "@@ -0,0 +1,14 @@\n+package org.pmiops.workbench.reporting.insertion;\n+\n+/*\n+ * Base interface for BigQuery payload builders. Parameterized class is a payload model type,\n+ * and the intent is to pair each implementation with an enum type inheriting from\n+ * QueryParameterColumn<T>.\n+ *\n+ * Builder interfaces for separate BigQuery upload paths are possible.\n+ */\n+public interface BigQueryInsertionPayloadTransformer<MODEL_TYPE> {", "originalCommit": "e4479d1057193149186641cbccf4ae35965b2aac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwODU4Ng==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r494308586", "bodyText": "Regarding this one, yes single-character names have been standard forever. I thought it would make things easier to follow particularly in places where there are multiple type parameters). How about MODEL_T? Or is that worse?", "author": "jaycarlton", "createdAt": "2020-09-24T13:17:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk0MjcxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwNTcwOQ==", "url": "https://github.com/all-of-us/workbench/pull/3949#discussion_r493905709", "bodyText": "The enum entries are all generated, but it would be easy enough to generate the constructor and accessors, too.", "author": "jaycarlton", "createdAt": "2020-09-23T21:28:07Z", "path": "api/src/main/java/org/pmiops/workbench/reporting/insertion/WorkspaceColumnValueExtractor.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.pmiops.workbench.reporting.insertion;\n+\n+import static com.google.cloud.bigquery.QueryParameterValue.bool;\n+import static com.google.cloud.bigquery.QueryParameterValue.int64;\n+import static com.google.cloud.bigquery.QueryParameterValue.string;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.enumToQpv;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.enumToString;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.toInsertRowString;\n+import static org.pmiops.workbench.cohortbuilder.util.QueryParameterValues.toTimestampQpv;\n+\n+import com.google.cloud.bigquery.QueryParameterValue;\n+import java.util.function.Function;\n+import org.pmiops.workbench.model.ReportingWorkspace;\n+\n+public enum WorkspaceColumnValueExtractor implements ColumnValueExtractor<ReportingWorkspace> {\n+  BILLING_ACCOUNT_TYPE(\n+      \"billing_account_type\",\n+      w -> enumToString(w.getBillingAccountType()),\n+      w -> enumToQpv(w.getBillingAccountType())),\n+  BILLING_STATUS(\n+      \"billing_status\",\n+      w -> enumToString(w.getBillingStatus()),\n+      w -> enumToQpv(w.getBillingStatus())),\n+  CDR_VERSION_ID(\n+      \"cdr_version_id\", ReportingWorkspace::getCdrVersionId, w -> int64(w.getCdrVersionId())),\n+  CREATION_TIME(\n+      \"creation_time\",\n+      w -> toInsertRowString(w.getCreationTime()),\n+      w -> toTimestampQpv(w.getCreationTime())),\n+  CREATOR_ID(\"creator_id\", ReportingWorkspace::getCreatorId, w -> int64(w.getCreatorId())),\n+  DISSEMINATE_RESEARCH_OTHER(\n+      \"disseminate_research_other\",\n+      ReportingWorkspace::getDisseminateResearchOther,\n+      w -> string(w.getDisseminateResearchOther())),\n+  LAST_ACCESSED_TIME(\n+      \"last_accessed_time\",\n+      w -> toInsertRowString(w.getLastAccessedTime()),\n+      w -> toTimestampQpv(w.getLastAccessedTime())),\n+  LAST_MODIFIED_TIME(\n+      \"last_modified_time\",\n+      w -> toInsertRowString(w.getLastModifiedTime()),\n+      w -> toTimestampQpv(w.getLastModifiedTime())),\n+  NAME(\"name\", ReportingWorkspace::getName, w -> string(w.getName())),\n+  NEEDS_RP_REVIEW_PROMPT(\n+      \"needs_rp_review_prompt\",\n+      ReportingWorkspace::getNeedsRpReviewPrompt,\n+      w -> int64(w.getNeedsRpReviewPrompt())),\n+  PUBLISHED(\"published\", ReportingWorkspace::getPublished, w -> bool(w.getPublished())),\n+  RP_ADDITIONAL_NOTES(\n+      \"rp_additional_notes\",\n+      ReportingWorkspace::getRpAdditionalNotes,\n+      w -> string(w.getRpAdditionalNotes())),\n+  RP_ANCESTRY(\"rp_ancestry\", ReportingWorkspace::getRpAncestry, w -> bool(w.getRpAncestry())),\n+  RP_ANTICIPATED_FINDINGS(\n+      \"rp_anticipated_findings\",\n+      ReportingWorkspace::getRpAnticipatedFindings,\n+      w -> string(w.getRpAnticipatedFindings())),\n+  RP_APPROVED(\"rp_approved\", ReportingWorkspace::getRpApproved, w -> bool(w.getRpApproved())),\n+  RP_COMMERCIAL_PURPOSE(\n+      \"rp_commercial_purpose\",\n+      ReportingWorkspace::getRpCommercialPurpose,\n+      w -> bool(w.getRpCommercialPurpose())),\n+  RP_CONTROL_SET(\n+      \"rp_control_set\", ReportingWorkspace::getRpControlSet, w -> bool(w.getRpControlSet())),\n+  RP_DISEASE_FOCUSED_RESEARCH(\n+      \"rp_disease_focused_research\",\n+      ReportingWorkspace::getRpDiseaseFocusedResearch,\n+      w -> bool(w.getRpDiseaseFocusedResearch())),\n+  RP_DISEASE_OF_FOCUS(\n+      \"rp_disease_of_focus\",\n+      ReportingWorkspace::getRpDiseaseOfFocus,\n+      w -> string(w.getRpDiseaseOfFocus())),\n+  RP_DRUG_DEVELOPMENT(\n+      \"rp_drug_development\",\n+      ReportingWorkspace::getRpDrugDevelopment,\n+      w -> bool(w.getRpDrugDevelopment())),\n+  RP_EDUCATIONAL(\n+      \"rp_educational\", ReportingWorkspace::getRpEducational, w -> bool(w.getRpEducational())),\n+  RP_ETHICS(\"rp_ethics\", ReportingWorkspace::getRpEthics, w -> bool(w.getRpEthics())),\n+  RP_INTENDED_STUDY(\n+      \"rp_intended_study\",\n+      ReportingWorkspace::getRpIntendedStudy,\n+      w -> string(w.getRpIntendedStudy())),\n+  RP_METHODS_DEVELOPMENT(\n+      \"rp_methods_development\",\n+      ReportingWorkspace::getRpMethodsDevelopment,\n+      w -> bool(w.getRpMethodsDevelopment())),\n+  RP_OTHER_POPULATION_DETAILS(\n+      \"rp_other_population_details\",\n+      ReportingWorkspace::getRpOtherPopulationDetails,\n+      w -> string(w.getRpOtherPopulationDetails())),\n+  RP_OTHER_PURPOSE(\n+      \"rp_other_purpose\", ReportingWorkspace::getRpOtherPurpose, w -> bool(w.getRpOtherPurpose())),\n+  RP_OTHER_PURPOSE_DETAILS(\n+      \"rp_other_purpose_details\",\n+      ReportingWorkspace::getRpOtherPurposeDetails,\n+      w -> string(w.getRpOtherPurposeDetails())),\n+  RP_POPULATION_HEALTH(\n+      \"rp_population_health\",\n+      ReportingWorkspace::getRpPopulationHealth,\n+      w -> bool(w.getRpPopulationHealth())),\n+  RP_REASON_FOR_ALL_OF_US(\n+      \"rp_reason_for_all_of_us\",\n+      ReportingWorkspace::getRpReasonForAllOfUs,\n+      w -> string(w.getRpReasonForAllOfUs())),\n+  RP_REVIEW_REQUESTED(\n+      \"rp_review_requested\",\n+      ReportingWorkspace::getRpReviewRequested,\n+      w -> bool(w.getRpReviewRequested())),\n+  RP_SCIENTIFIC_APPROACH(\n+      \"rp_scientific_approach\",\n+      ReportingWorkspace::getRpScientificApproach,\n+      w -> string(w.getRpScientificApproach())),\n+  RP_SOCIAL_BEHAVIORAL(\n+      \"rp_social_behavioral\",\n+      ReportingWorkspace::getRpSocialBehavioral,\n+      w -> bool(w.getRpSocialBehavioral())),\n+  RP_TIME_REQUESTED(\n+      \"rp_time_requested\",\n+      w -> toInsertRowString(w.getRpTimeRequested()),\n+      w -> toTimestampQpv(w.getRpTimeRequested())),\n+  WORKSPACE_ID(\"workspace_id\", ReportingWorkspace::getWorkspaceId, w -> int64(w.getWorkspaceId()));\n+\n+  public static final String TABLE_NAME = \"workspace\";", "originalCommit": "18054785d8e9b303fe4822bb981df09ae851e0e7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "713b9059127ced52e89bfba66b954b907def3c47", "url": "https://github.com/all-of-us/workbench/commit/713b9059127ced52e89bfba66b954b907def3c47", "message": "pr fixes", "committedDate": "2020-09-24T13:23:05Z", "type": "commit"}]}