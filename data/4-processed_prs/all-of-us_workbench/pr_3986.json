{"pr_number": 3986, "pr_title": "[RW-5267] Tooling for translating table schemas into code", "pr_createdAt": "2020-09-10T15:55:46Z", "pr_url": "https://github.com/all-of-us/workbench/pull/3986", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyOTIxMQ==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486529211", "bodyText": "TODO: link here from top-level README.", "author": "jaycarlton", "createdAt": "2020-09-10T17:55:33Z", "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process", "originalCommit": "217f964ebb09e965d1a7aba90ca91ba5a655020b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNjM0MQ==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486536341", "bodyText": "A prototype example of this is in this PR (changing rapidly).", "author": "jaycarlton", "createdAt": "2020-09-10T18:08:04Z", "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.\n+\n+For example, the output of the `explain address;` is shown below.\n+\n+| Field | Type | Null | Key | Default | Extra |\n+| :--- | :--- | :--- | :--- | :--- | :--- |\n+| id | bigint\\(20\\) | NO | PRI | NULL | auto\\_increment |\n+| street\\_address\\_1 | varchar\\(95\\) | NO |  | NULL |  |\n+| street\\_address\\_2 | varchar\\(95\\) | YES |  | NULL |  |\n+| zip\\_code | varchar\\(10\\) | YES |  | NULL |  |\n+| city | varchar\\(95\\) | NO |  | NULL |  |\n+| state | varchar\\(95\\) | NO |  | NULL |  |\n+| country | varchar\\(95\\) | NO |  | NULL |  |\n+| user\\_id | bigint\\(20\\) | NO | MUL | NULL |  |\n+\n+## Mapping to BigQuery Schema JSON Format\n+The next task is to translate the MySql table description into the [BigQuery schema format](https://cloud.google.com/bigquery/docs/schemas#specifying_a_json_schema_file). Given this CSV input, we only really care about the field name and types. The\n+relational constraints won't be preserved in BigQuery, so they're basically ignored,\n+but we do want the primary key (which would be renamed to `address_id` in this example,\n+though in this case we actually merge `address` columns into the\n+`user` table). The script generates the for each table, with the example\n+of the address table shown. All fields are marked nullable, as to do otherwise would involve\n+adding constraints on the source data and/or its snapshot & upload processes.\n+\n+```json\n+[\n+  {\n+    \"name\": \"id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_1\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_2\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"zip_code\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"city\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"state\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"country\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"user_id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  }\n+]\n+```\n+\n+## Spring Data Projection Interface", "originalCommit": "217f964ebb09e965d1a7aba90ca91ba5a655020b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8", "url": "https://github.com/all-of-us/workbench/commit/a55490889d0741363394bf2e1ffea68f922c04b8", "message": "rebased", "committedDate": "2020-09-10T18:13:07Z", "type": "commit"}, {"oid": "a55490889d0741363394bf2e1ffea68f922c04b8", "url": "https://github.com/all-of-us/workbench/commit/a55490889d0741363394bf2e1ffea68f922c04b8", "message": "rebased", "committedDate": "2020-09-10T18:13:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0MTU5Mg==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486541592", "bodyText": "I'm not clear on what is intended by this sentence.  Also, could you rename this script to include \"local\" somewhere?", "author": "jmthibault79", "createdAt": "2020-09-10T18:16:17Z", "path": "api/curl/upload-snapshot-cron.sh", "diffHunk": "@@ -0,0 +1,7 @@\n+#!/bin/bash\n+\n+# Start hit the reporting snapshot & upload cron endpoint  locally.", "originalCommit": "a55490889d0741363394bf2e1ffea68f922c04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NTYwMQ==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486545601", "bodyText": "I don't like the name \"valid\" here.  Maybe should_include_field?", "author": "jmthibault79", "createdAt": "2020-09-10T18:21:36Z", "path": "api/reporting/schemas/reporting-codegen.rb", "diffHunk": "@@ -0,0 +1,178 @@\n+#!/usr/bin/ruby\n+require 'csv'\n+require 'json'\n+require 'yaml'\n+\n+table_name = ARGV[0]\n+input_dir = File.expand_path(ARGV[1])\n+output_dir = File.expand_path(ARGV[2])\n+puts \"Generate types for #{table_name}...\"\n+Dir.mkdir(output_dir) unless Dir.exist?(output_dir)\n+\n+def to_camel_case(snake_case, capitalize_initial)\n+  result =  snake_case.split('_').collect(&:capitalize).join\n+  unless capitalize_initial\n+    result[0] = result[0].downcase\n+  end\n+  result\n+end\n+\n+def to_input_path(dir_name, table_name, suffix)\n+  File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n+end\n+\n+def to_output_path(dir_name, table_name, suffix)\n+  Dir.mkdir(dir_name) unless Dir.exist?(dir_name)\n+  File.expand_path(File.join(dir_name, \"#{table_name}.#{suffix}\"))\n+end\n+\n+dto_class_name = \"Reporting#{to_camel_case(table_name, true)}\"\n+\n+inputs = {\n+    :describe_csv => to_input_path(File.join(input_dir, 'mysql_describe_csv'), table_name,'csv'),\n+    :exclude_columns => to_input_path(File.join(input_dir, 'excluded_columns'), table_name,'txt')\n+}\n+\n+outputs = {\n+    :big_query_json => to_output_path(File.join(output_dir, 'big_query_json'), table_name,'json'),\n+    :swagger_yaml => to_output_path(File.join(output_dir, 'swagger_yaml'), table_name,'yaml'),\n+    :projection_interface => to_output_path(File.join(output_dir, 'projection_interface'), table_name, 'java'),\n+    :projection_query => to_output_path(File.join(output_dir, 'projection_query'), table_name,'java')\n+}\n+\n+MYSQL_TO_BIGQUERY_TYPE = {\n+    'varchar' => 'STRING',\n+    'datetime' => 'TIMESTAMP',\n+    'bigint' => 'INT64',\n+    'smallint' => 'INT64',\n+    'longtext' => 'STRING',\n+    'int' => 'INT64',\n+    'tinyint' => 'INT64',\n+    'bit' => 'BOOLEAN',\n+    'double' => 'FLOAT64',\n+    'text' => 'STRING',\n+    'mediumblob' => 'STRING'\n+}\n+\n+def to_bq_type(mysql_type)\n+  type_pattern = Regexp.new(\"(?<type>\\\\w+)(\\\\(\\\\d+\\\\))?\")\n+  match_data = mysql_type.match(type_pattern)\n+  result = MYSQL_TO_BIGQUERY_TYPE[match_data[:type]]\n+  raise \"MySQL type #{mysql_type} not recognized.\" if result.nil?\n+  result\n+end\n+\n+excluded_fields = File.exist?(inputs[:exclude_columns]) ? File.readlines(inputs[:exclude_columns]) : []\n+\n+def is_valid_field?(excluded_fields, field)", "originalCommit": "a55490889d0741363394bf2e1ffea68f922c04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NzI0NA==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486547244", "bodyText": "How do you connect an address to a user without this?\nMore generally, how did you choose which columns to exclude?", "author": "jmthibault79", "createdAt": "2020-09-10T18:23:50Z", "path": "api/reporting/schemas/input/excluded_columns/address.txt", "diffHunk": "@@ -0,0 +1,2 @@\n+id\n+user_id", "originalCommit": "a55490889d0741363394bf2e1ffea68f922c04b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIyOTI2Nw==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r487229267", "bodyText": "I'm manually putting it in the JOIN statement We only use the DbAddress class in UserDao.\nThe schema doesn't include columns that are implementation details and don't have user-facing or admin-facing reality.", "author": "jaycarlton", "createdAt": "2020-09-11T18:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NzI0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0ODE1Ng==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486548156", "bodyText": "missing end of comment", "author": "jmthibault79", "createdAt": "2020-09-10T18:25:04Z", "path": "api/reporting/sql/query-users.sql", "diffHunk": "@@ -0,0 +1,10 @@\n+-- Fetch all users from this environment who are not", "originalCommit": "a55490889d0741363394bf2e1ffea68f922c04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "url": "https://github.com/all-of-us/workbench/commit/6fad502fc7ec6f2b9735c9062cdaa1e7447b2030", "message": "alphabetize columns and print output from subprorcess", "committedDate": "2020-09-10T18:33:29Z", "type": "commit"}, {"oid": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "url": "https://github.com/all-of-us/workbench/commit/bf7327da516c8bed19c634bd5e9846866ff82c2d", "message": "fixup exclude processing", "committedDate": "2020-09-10T18:39:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMTE2OA==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486631168", "bodyText": "Where are these sql files used?", "author": "freemabd", "createdAt": "2020-09-10T21:01:39Z", "path": "api/reporting/sql/researcher_stats_view.sql", "diffHunk": "@@ -0,0 +1,4 @@\n+CREATE MATERIALIZED VIEW reporting_local.researcher_stats AS", "originalCommit": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzMDA4OA==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r487230088", "bodyText": "Sorry, that's confusing. They're for ad-hoc manual queries I was doing in building the system. I was going to put them into the devops repo, but they came in on this branch since I just moved over all the applicable stuff under reporting.", "author": "jaycarlton", "createdAt": "2020-09-11T18:55:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMTE2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMjY3MA==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486632670", "bodyText": "Is the csv generation manual or is there a ruby script that does this? Do the csv file names have to match the table name?", "author": "freemabd", "createdAt": "2020-09-10T21:04:41Z", "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.", "originalCommit": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzMDMyOA==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r487230328", "bodyText": "It's manual at the moment. Yes, it needs to match the table name.", "author": "jaycarlton", "createdAt": "2020-09-11T18:55:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYzMjY3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NTg0NQ==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486645845", "bodyText": "Should put something in the readme about how the excluded columns work", "author": "freemabd", "createdAt": "2020-09-10T21:32:44Z", "path": "api/reporting/schemas/REPORTING-SCHEMA-TOOLS.md", "diffHunk": "@@ -0,0 +1,177 @@\n+# Reporting Schema Tools & Process\n+This directory and subdirectories archive schema changes\n+and tools to generate and translate them into code. There\n+are a few different stages, but the grunt work is mostly automated\n+for repeatability and consistency.\n+\n+The ruby script `generate_all_tables.rb` will operate on all of the input CSV files (described below)\n+and produce output files in this directory structure \n+```\n+$ ls ~/scratch/reportingOut5\n+big_query_json       projection_interface projection_query     swagger_yaml\n+```\n+\n+## EXPLAIN Application DB Table\n+The `explain table` MySql [statement](https://dev.mysql.com/doc/refman/8.0/en/explain.html) generates a tabular\n+set of attributes for table. These are then saved as CSV files and placed into the `mysql_describe_csv` directory.\n+\n+For example, the output of the `explain address;` is shown below.\n+\n+| Field | Type | Null | Key | Default | Extra |\n+| :--- | :--- | :--- | :--- | :--- | :--- |\n+| id | bigint\\(20\\) | NO | PRI | NULL | auto\\_increment |\n+| street\\_address\\_1 | varchar\\(95\\) | NO |  | NULL |  |\n+| street\\_address\\_2 | varchar\\(95\\) | YES |  | NULL |  |\n+| zip\\_code | varchar\\(10\\) | YES |  | NULL |  |\n+| city | varchar\\(95\\) | NO |  | NULL |  |\n+| state | varchar\\(95\\) | NO |  | NULL |  |\n+| country | varchar\\(95\\) | NO |  | NULL |  |\n+| user\\_id | bigint\\(20\\) | NO | MUL | NULL |  |\n+\n+## Mapping to BigQuery Schema JSON Format\n+The next task is to translate the MySql table description into the [BigQuery schema format](https://cloud.google.com/bigquery/docs/schemas#specifying_a_json_schema_file). Given this CSV input, we only really care about the field name and types. The\n+relational constraints won't be preserved in BigQuery, so they're basically ignored,\n+but we do want the primary key (which would be renamed to `address_id` in this example,\n+though in this case we actually merge `address` columns into the\n+`user` table). The script generates the for each table, with the example\n+of the address table shown. All fields are marked nullable, as to do otherwise would involve\n+adding constraints on the source data and/or its snapshot & upload processes.\n+\n+```json\n+[\n+  {\n+    \"name\": \"id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_1\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"street_address_2\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"zip_code\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"city\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"state\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"country\",\n+    \"description\": \"\",\n+    \"type\": \"STRING\",\n+    \"mode\": \"NULLABLE\"\n+  },\n+  {\n+    \"name\": \"user_id\",\n+    \"description\": \"\",\n+    \"type\": \"INT64\",\n+    \"mode\": \"NULLABLE\"\n+  }\n+]\n+```\n+\n+## Spring Data Projection Interface\n+The Ruby script builds projection interface definitions and query\n+annotations for the DAO method to fetch the projection. Since these must match exactly, it makes sense\n+to generate them in the same script.\n+\n+We use a projection for reporting snapshots for a couple of reasons. First, it allows the schema used\n+in reporting to evolve independently of the main application schema (up to a point). Second, when doing\n+very large reads, avoiding the overhead of constructing full-fledged entities is useful. Finally, it\n+allows us to decouple the upload snapshot types (DTOs) from the MySQL return values. MapStruct should\n+allow clean convergence from the projection types to the DTOs, though the reverse isn't possible, as\n+we can't actually instantiate the projection interfaces.\n+\n+### Projection Interface\n+The output for the Address table (assuming it's a stand-alone query) is below. In practice, these fields\n+are appended (by hand) to the `PrjUser` interface and query to denormalize it.\n+```java\n+interface PrjAddress {\n+  String getCity();\n+  String getCountry();\n+  long getId();\n+  String getState();\n+  String getStreetAddress1();\n+  String getStreetAddress2();\n+  long getUserId();\n+  String getZipCode();\n+}\n+```\n+\n+### Projection Query Annotation\n+The projection interface query must match exactly in terms of type, name, and order of columns referenced.\n+Failing to do this seems to lead to runtime exceptions but rarely any compile-time warnings. Thus, it's important\n+to unit test these.\n+\n+The following annotation is generated and should be attached to a DAO method such as `List<PrjAddress> findAllAddresses();`\n+```java\n+@Query(\"SELECT\n++ \"  a.city,\"\n++ \"  a.country,\"\n++ \"  a.id,\"\n++ \"  a.state,\"\n++ \"  a.streetAddress1,\"\n++ \"  a.streetAddress2,\"\n++ \"  a.userId,\"\n++ \"  a.zipCode\"\n++ \"FROM DbAddress a\")\n+```\n+\n+## Swagger DTO Classes\n+Currently the classes for the `ReportingSnapshot` object used\n+buy the upload service are specified in the API Swagger file. This was expedient,\n+but it means we're exposing internal details of the application in the API documentation.\n+\n+The output should look something like this:\n+```yaml\n+---\n+ReportingAddress:\n+  type: object\n+  properties:\n+    id:\n+      description: ''\n+      type: integer\n+      format: int64\n+    streetAddress1:\n+      description: ''\n+      type: string\n+    streetAddress2:\n+      description: ''\n+      type: string\n+    zipCode:\n+      description: ''\n+      type: string\n+    city:\n+      description: ''\n+      type: string\n+    state:\n+      description: ''\n+      type: string\n+    country:\n+      description: ''\n+      type: string\n+    userId:\n+      description: ''\n+      type: integer\n+      format: int64\n+```", "originalCommit": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NzIzNw==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r486647237", "bodyText": "Should the csv/text files be generated somehow, instead of committing them into the project. Won't the files become stale over time as the schema evolves?", "author": "freemabd", "createdAt": "2020-09-10T21:35:54Z", "path": "api/reporting/schemas/input/mysql_describe_csv/user.csv", "diffHunk": "@@ -0,0 +1,47 @@\n+user_id,bigint(20),NO,PRI,,auto_increment", "originalCommit": "bf7327da516c8bed19c634bd5e9846866ff82c2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzMTc5Mg==", "url": "https://github.com/all-of-us/workbench/pull/3986#discussion_r487231792", "bodyText": "Eh, there's some awkwardness here, and I'm trying to resist over-building this tool. In general, adding a new column to an app DB table does not mean it needs to show up in the reporting dataset. They're only 90% the same, so there is still some manual fixup.\nI think a good compromise would be to remove this input directory upon scripting the sql calls, but not before. That's not something that needs to happen just yet IMO, as it doesn't affect my stakeholders.", "author": "jaycarlton", "createdAt": "2020-09-11T18:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NzIzNw=="}], "type": "inlineReview"}, {"oid": "1ff4b81feac7cf58005dfc391bad623e425fdd14", "url": "https://github.com/all-of-us/workbench/commit/1ff4b81feac7cf58005dfc391bad623e425fdd14", "message": "cleanup output, indent yaml, and make interface public", "committedDate": "2020-09-10T21:37:42Z", "type": "commit"}, {"oid": "23b9f7394a33cf2c888ca22666f9307b43e31bf0", "url": "https://github.com/all-of-us/workbench/commit/23b9f7394a33cf2c888ca22666f9307b43e31bf0", "message": "email -> username at source & use BqDto prefix", "committedDate": "2020-09-10T21:41:26Z", "type": "commit"}, {"oid": "67330d68aeb791423abaeb912b1dd441c6de880b", "url": "https://github.com/all-of-us/workbench/commit/67330d68aeb791423abaeb912b1dd441c6de880b", "message": "fix newlines in queries and add projection fucntion decl", "committedDate": "2020-09-10T22:30:07Z", "type": "commit"}, {"oid": "ffef953e334f4967bd7040820d39f80f81548415", "url": "https://github.com/all-of-us/workbench/commit/ffef953e334f4967bd7040820d39f80f81548415", "message": "patch for research purpose entity naming convention", "committedDate": "2020-09-11T15:50:22Z", "type": "commit"}, {"oid": "6ba36379bf720f3e7132c3098c63eb0351e11f60", "url": "https://github.com/all-of-us/workbench/commit/6ba36379bf720f3e7132c3098c63eb0351e11f60", "message": "fixes", "committedDate": "2020-09-11T18:47:27Z", "type": "commit"}, {"oid": "6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "url": "https://github.com/all-of-us/workbench/commit/6791e38b83661e2cb42bffd3fb977bfb5f09c8be", "message": "PR comments", "committedDate": "2020-09-11T19:22:12Z", "type": "commit"}]}