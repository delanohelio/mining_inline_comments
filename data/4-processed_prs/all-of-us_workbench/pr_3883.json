{"pr_number": 3883, "pr_title": "[RW-5035] Documenting how to randomize a VCF sample and upload to BQ", "pr_createdAt": "2020-08-14T18:08:53Z", "pr_url": "https://github.com/all-of-us/workbench/pull/3883", "timeline": [{"oid": "2cab7b0676f7545bb722c4d720a3fe9933d5045a", "url": "https://github.com/all-of-us/workbench/commit/2cab7b0676f7545bb722c4d720a3fe9933d5045a", "message": "documenting how to randomize a VCF sample and upload to BQ", "committedDate": "2020-08-14T18:07:57Z", "type": "commit"}, {"oid": "cb9bc0940227c0fa49ccded9e1115602ee78ff6e", "url": "https://github.com/all-of-us/workbench/commit/cb9bc0940227c0fa49ccded9e1115602ee78ff6e", "message": "convert to readme and add profiling", "committedDate": "2020-08-19T22:29:21Z", "type": "commit"}, {"oid": "20e67aa7fde324073041ae335ac21d49c6525d92", "url": "https://github.com/all-of-us/workbench/commit/20e67aa7fde324073041ae335ac21d49c6525d92", "message": "typos", "committedDate": "2020-08-19T22:33:57Z", "type": "commit"}, {"oid": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "url": "https://github.com/all-of-us/workbench/commit/7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "message": "add profiling specs", "committedDate": "2020-08-19T22:37:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTc3Mw==", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399773", "bodyText": "Adding a direct link here to the Java file in the gatk branch would be good", "author": "calbach", "createdAt": "2020-08-19T22:44:32Z", "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time", "originalCommit": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTk4Nw==", "url": "https://github.com/all-of-us/workbench/pull/3883#discussion_r473399987", "bodyText": "Adding  a link here to this shell script would be great.", "author": "calbach", "createdAt": "2020-08-19T22:44:49Z", "path": "api/genomics/docs/uploading_test_microarray_data.md", "diffHunk": "@@ -0,0 +1,73 @@\n+## Creating a randomized microarray dataset and uploading to BigQuery in test environment\n+- The following commands are not meant to be run as is. Treat it more as documentation of what I ran to get the data uploaded.\n+- All profiling was done on a laptop with an Intel Core i7-9850H CPU @ 2.60GHz, 32GB RAM, and an SSD\n+### Generate Randomized VCF samples\n+- Run from workbench/api  \n+`./project.rb randomize-vcf --vcf ~/broad/variantstore/NA12878_204126160130_R01C01.vcf --number-of-copies 1000 --output-path /mnt/genomics/randomized1000.vcf`\n+\n+| Number of Samples | Clock time | Output file size |\n+| ----------------  | ---------- | ---------------- |\n+|1 | 53s | 1.4 GB |\n+|10|1m 26s|2.3 GB|\n+|100|6m 38s| 11 GB|\n+|500|27m|49 GB|\n+|1,000|59m|95 GB|\n+|10,000**|~ 10h|~ 1 TB|\n+|100,000**|~ 100h|~ 10 TB|\n+** Estimates assuming linear growth\n+\n+### Compress VCF file \n+- `bgzip -c randomized1000.vcf > randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time | Compression |\n+| ----------------- | ---------- | ----------- |\n+|100|52s|11 G -> 544 M|\n+|500|4m 18s|49 G -> 1.3 G|\n+|1000|5m 44s|95G -> 2.1 G|\n+\n+### Index VCF file\n+- `bcftools index randomized1000.vcf.gz`\n+\n+| Number of Samples | Clock time |\n+| ----------------- | ---------- |\n+|100|20s|\n+|500|1m 20s|\n+|1000|2m 40s|\n+\n+### Extracting a single sample VCF from multi-sample VCF\n+- `bcftools view -s 204126160130_R01C01.908 randomized1000.vcf.gz > 1000_sample.vcf`\n+\n+| Number of Sample | Clock time |\n+| ---------------  | ---------- |\n+|100|3m 16s|\n+|500|14m|\n+|1,000|28m|\n+|10,000**|~ 4h 40m|\n+|100,000**|~ 46h 40m|\n+** Estimates assuming linear growth\n+\n+### Generate ingest files and upload to GCS\n+```\n+  # Run from variantstore repo, https://github.com/broadinstitute/variantstore\n+  # rm/cp *.tsv is a hack I used to clean up already uploaded files while running the code in a loop\n+  rm *.tsv\n+  ~/broad/gatk/gatk CreateArrayIngestFiles --sample-id 908 -V 1000_sample.vcf --probe-info-file probe_info.csv --ref-version 37\n+  gsutil cp *.tsv gs://all-of-us-workbench-test-genomics/eric/import/3/ready/\n+```\n+- Every import/# folder can only contain up to 4k samples\n+- CreateArrayIngestFiles takes ~1m 20s and operates 1 sample at a time\n+- gsutil cp depends on upload speed\n+\n+\n+### GCS -> BigQuery \n+```\n+  # Run from variantstore repo\n+  cd ingest\n+  ./bq_ingest_arrays.sh all-of-us-workbench-test microarray_data gs://all-of-us-workbench-test-genomics/eric/import 3\n+```\n+- This will have to be changed for >4k samples since a BigQuery table can only have 4k partitions", "originalCommit": "7369aeb03fe6bd4f81ce4e3585c43011ddcc58d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9ca687dd637dab57fd5917b119f3f2e61b5f0e74", "url": "https://github.com/all-of-us/workbench/commit/9ca687dd637dab57fd5917b119f3f2e61b5f0e74", "message": "Add links", "committedDate": "2020-08-20T15:08:56Z", "type": "commit"}]}