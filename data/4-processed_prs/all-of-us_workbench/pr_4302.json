{"pr_number": 4302, "pr_title": "[RW-5938][risk=no] Dueling crons for Reporting", "pr_createdAt": "2020-11-17T17:18:53Z", "pr_url": "https://github.com/all-of-us/workbench/pull/4302", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMjY4MA==", "url": "https://github.com/all-of-us/workbench/pull/4302#discussion_r527202680", "bodyText": "Please update the comments to describe the differences between them", "author": "jmthibault79", "createdAt": "2020-11-19T21:14:24Z", "path": "api/reporting/curl/upload-snapshot-local-query.sh", "diffHunk": "@@ -0,0 +1,6 @@\n+#!/bin/bash\n+\n+# Start hit the reporting snapshot & upload cron endpoint  locally.", "originalCommit": "6d1daf5dae59667af9f7f090b90baf8ec68ae0f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgwOA==", "url": "https://github.com/all-of-us/workbench/pull/4302#discussion_r527203808", "bodyText": "Add \"- streaming mode\" or other distinguisher here", "author": "jmthibault79", "createdAt": "2020-11-19T21:16:16Z", "path": "api/src/main/resources/workbench-api.yaml", "diffHunk": "@@ -1616,7 +1616,38 @@ paths:\n           description: Internal Error\n           schema:\n             \"$ref\": \"#/definitions/ErrorResponse\"\n-\n+  \"/v1/cron/uploadReportingSnapshotStreaming\":\n+    get:\n+      tags:\n+        - offlineReporting\n+        - cron\n+      description: >\n+        Capture a reporting snapshot and beging the upload task to BigQuery reporting dataset.", "originalCommit": "6d1daf5dae59667af9f7f090b90baf8ec68ae0f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0NTg0Nw==", "url": "https://github.com/all-of-us/workbench/pull/4302#discussion_r527945847", "bodyText": "@calbach ISTR discussing this with you once. If I want the test environment to have a small change from the default, do I need to past the entire default cron here, or can I include it somehow?", "author": "jaycarlton", "createdAt": "2020-11-20T20:11:47Z", "path": "api/src/main/webapp/WEB-INF/cron_test.yaml", "diffHunk": "@@ -0,0 +1,107 @@\n+cron:", "originalCommit": "6d1daf5dae59667af9f7f090b90baf8ec68ae0f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d0f09e9f876640ef3be74f43097d695b57d2b698", "url": "https://github.com/all-of-us/workbench/commit/d0f09e9f876640ef3be74f43097d695b57d2b698", "message": "dueling cron jobs", "committedDate": "2020-11-23T15:35:19Z", "type": "forcePushed"}, {"oid": "2ea87625a035323f3baed7aa342886975d06b4e4", "url": "https://github.com/all-of-us/workbench/commit/2ea87625a035323f3baed7aa342886975d06b4e4", "message": "update description", "committedDate": "2020-11-25T16:40:44Z", "type": "forcePushed"}, {"oid": "b66cb0acb9b7b203024223f365e0e485dffe6e58", "url": "https://github.com/all-of-us/workbench/commit/b66cb0acb9b7b203024223f365e0e485dffe6e58", "message": "temporary extra cron methods to test perf of streaming vs insert query upload", "committedDate": "2020-12-01T19:46:01Z", "type": "commit"}, {"oid": "b66cb0acb9b7b203024223f365e0e485dffe6e58", "url": "https://github.com/all-of-us/workbench/commit/b66cb0acb9b7b203024223f365e0e485dffe6e58", "message": "temporary extra cron methods to test perf of streaming vs insert query upload", "committedDate": "2020-12-01T19:46:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY4MDM4OA==", "url": "https://github.com/all-of-us/workbench/pull/4302#discussion_r533680388", "bodyText": "\"via the insert query\" or similar", "author": "jmthibault79", "createdAt": "2020-12-01T19:52:24Z", "path": "api/src/main/resources/workbench-api.yaml", "diffHunk": "@@ -1616,7 +1616,39 @@ paths:\n           description: Internal Error\n           schema:\n             \"$ref\": \"#/definitions/ErrorResponse\"\n-\n+  \"/v1/cron/uploadReportingSnapshotStreaming\":\n+    get:\n+      tags:\n+        - offlineReporting\n+        - cron\n+      description: >\n+        Capture a reporting snapshot and begin the upload task to BigQuery reporting dataset via\n+        the streaming implementation.\n+      operationId: uploadReportingSnapshotStreaming\n+      security: []\n+      responses:\n+        204:\n+          description: No content\n+        500:\n+          description: Internal Error\n+          schema:\n+            \"$ref\": \"#/definitions/ErrorResponse\"\n+  \"/v1/cron/uploadReportingSnapshotQuery\":\n+    get:\n+      tags:\n+        - offlineReporting\n+        - cron\n+      description: >\n+        Capture a reporting snapshot and beging the upload task to BigQuery reporting dataset.", "originalCommit": "b66cb0acb9b7b203024223f365e0e485dffe6e58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY4NDkyMA==", "url": "https://github.com/all-of-us/workbench/pull/4302#discussion_r533684920", "bodyText": "confirmed that these 2 are the only difference from the default", "author": "jmthibault79", "createdAt": "2020-12-01T20:00:01Z", "path": "api/src/main/webapp/WEB-INF/cron_test.yaml", "diffHunk": "@@ -0,0 +1,107 @@\n+cron:\n+  - description: 'Periodic notebook runtime checks'\n+    url: /v1/cron/checkRuntimes\n+    schedule: every 3 hours\n+    timezone: UTC\n+    target: api\n+  - description: |\n+      Daily sync of compliance training status (via Moodle API). This is just a backup to the primary\n+      flow. It should catch expired compliance training, fixup compliance status independent of user\n+      navigation, and update on removal of Moodle course completion for some reason.\n+    url: /v1/cron/bulkSyncComplianceTrainingStatus\n+    schedule: every 24 hours\n+    timezone: UTC\n+    target: api\n+  - description: >\n+      Daily sync of eRA Commons linkage status (via FireCloud API). Records changes in the log,\n+      but currently does not drive any downstream processes.\n+    url: /v1/cron/bulkSyncEraCommonsStatus\n+    schedule: every 24 hours\n+    timezone: UTC\n+    target: api\n+  - description: >\n+      Update our database cache of users' two-factor authentication settings on their GSuite accounts\n+      via Google Directory Service.\n+    url: /v1/cron/bulkSyncTwoFactorAuthStatus\n+    schedule: every 24 hours\n+    timezone: UTC\n+    target: api\n+  - description: 'Daily audit of gcp resources that users have access to'\n+    url: /v1/cron/bulkAuditProjectAccess\n+    schedule: every 1 hours\n+    timezone: UTC\n+    target: api\n+  - description: 'If the AoU Billing Project buffer is not full, refill with one or more projects.'\n+    url: /v1/cron/bufferBillingProjects\n+    schedule: every 1 minutes\n+    timezone: UTC\n+    target: api\n+  - description: 'Fetch a BillingProjectBufferEntry that is in the CREATING state and check its status on Firecloud'\n+    url: /v1/cron/syncBillingProjectStatus\n+    schedule: every 1 minutes\n+    timezone: UTC\n+    target: api\n+  - description: 'Find BillingProjectBufferEntries that have failed during the creation or assignment step and set their statuses to ERROR'\n+    url: /v1/cron/cleanBillingBuffer\n+    schedule: every 1 hours\n+    timezone: UTC\n+    target: api\n+  - description: 'Find and alert users that have exceeded their free tier billing usage'\n+    url: /v1/cron/checkFreeTierBillingUsage\n+    schedule: every 30 minutes\n+    timezone: UTC\n+    target: api\n+  - description: >\n+      Find billing projects associated with deleted workspaces and transfer ownership to\n+      designated \"Garbage Collection\" Service Accounts. This legacy process works around the lack of\n+      API support for deleting these billing projects directly.\n+\n+      The Terra delete Account API is now available, and this cron job is deprecated. To be removed\n+      as part of RW-3627.\n+    url: /v1/cron/billingProjectGarbageCollection\n+    schedule: every 1 hours\n+    timezone: UTC\n+    target: api\n+  - description: >\n+      Sample all gauge metrics for OpenCensus Monitoring and record them. The one-minute interval is\n+      the highest granularity for Stackdriver Monitoring and the lowest interval for an AppEngine\n+      cron job.\n+    url: /v1/cron/monitoring/updateGaugeMetrics\n+    schedule: every 1 minutes\n+    timezone: UTC\n+    target: api\n+  - description: >\n+      Export user and workspace data to RDR.\n+      RDR export is hard-coded to 9pm CT, to align with VUMC expectations that the daily export is run at a time\n+      that is (1) after the close of normal business working hours, and (2) early enough in the evening that the\n+      entire export (and downstream data flows) can complete before start of the next business day.\n+    url: /v1/cron/exportToRdr\n+    schedule: every day 22:00\n+    timezone: America/Chicago\n+    target: api\n+  - description: >\n+      For each workspace update the attribute need_rp_prompt if it has been created 15 days/an Year earlier\n+    url: /v1/cron/updateResearchPurposeReviewPrompt\n+    schedule: every day 21:00\n+    timezone: America/Chicago\n+    target: api\n+  - description: >", "originalCommit": "b66cb0acb9b7b203024223f365e0e485dffe6e58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c973ba52a4e1a1aea3e8ac6f4e92d30daf13e74a", "url": "https://github.com/all-of-us/workbench/commit/c973ba52a4e1a1aea3e8ac6f4e92d30daf13e74a", "message": "clarify comment", "committedDate": "2020-12-01T20:09:15Z", "type": "commit"}, {"oid": "9bd3321ca543b6b7a34c3d037669ae9b0c875488", "url": "https://github.com/all-of-us/workbench/commit/9bd3321ca543b6b7a34c3d037669ae9b0c875488", "message": "fix comment", "committedDate": "2020-12-01T20:16:45Z", "type": "commit"}]}