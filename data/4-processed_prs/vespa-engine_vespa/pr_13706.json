{"pr_number": 13706, "pr_title": "Create generic B-tree bucket DB and content node DB implementation", "pr_createdAt": "2020-06-25T10:54:08Z", "pr_url": "https://github.com/vespa-engine/vespa/pull/13706", "timeline": [{"oid": "2185218c0bb44843bb0f3d33abea3b116b35160a", "url": "https://github.com/vespa-engine/vespa/commit/2185218c0bb44843bb0f3d33abea3b116b35160a", "message": "Create generic B-tree bucket DB and content node DB implementation\n\nThis is the first stage of removing the legacy DB implementation.\nSupport for B-tree specific functionality such as lock-free snapshot\nreads will be added soon. This commit is just for feature parity.\n\nAbstract away actual database implementation to allow it to\nbe chosen dynamically at startup. This abstraction does incur\nsome overhead via call indirections and type erasures of callbacks,\nso it's likely it will be removed once the transition to the\nnew B-tree DB has been completed.\n\nSince the algorithms used for bucket key operations is so similar\nbetween the content node and distributor, a generic B-tree backed\nbucket database has been created. The distributor DB will be rewritten\naround this code very soon.\n\nDue to the strong coupling between bucket locking and actual DB\nimplementation details, the new bucket DB has a fairly significant\ncode overlap with the legacy implementation. This is to avoid\nspending time abstracting away and factoring out code for a\nlegacy implementation that is to be removed entirely anyway.\n\nRemove existing LockableMap functionality not used or that's\nonly used by tests.", "committedDate": "2020-06-25T10:40:10Z", "type": "commit"}, {"oid": "3f7ba13c24d6ac5b4ee77f61580a9bbf867cbd06", "url": "https://github.com/vespa-engine/vespa/commit/3f7ba13c24d6ac5b4ee77f61580a9bbf867cbd06", "message": "Wire config for enabling content node B-tree bucket DB", "committedDate": "2020-06-25T10:48:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5MzA3OQ==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446193079", "bodyText": "Please add class comment.", "author": "geirst", "createdAt": "2020-06-26T13:44:49Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.h", "diffHunk": "@@ -0,0 +1,158 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#pragma once\n+\n+#include \"abstract_bucket_map.h\"\n+#include \"storagebucketinfo.h\"\n+#include <vespa/document/bucket/bucketid.h>\n+#include <vespa/vespalib/util/time.h>\n+#include <vespa/vespalib/stllike/hash_map.h>\n+#include <vespa/vespalib/stllike/hash_set.h>\n+#include <map>\n+#include <memory>\n+#include <mutex>\n+#include <condition_variable>\n+#include <cassert>\n+#include <iosfwd>\n+\n+namespace storage::bucketdb {\n+\n+template <typename DataStoreTraitsT> class GenericBTreeBucketDatabase;\n+\n+template <typename T>\n+class BTreeLockableMap : public AbstractBucketMap<T> {", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5NzIyOQ==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446197229", "bodyText": "Typo? 'such such'", "author": "geirst", "createdAt": "2020-06-26T13:52:03Z", "path": "storage/src/vespa/storage/bucketdb/abstract_bucket_map.h", "diffHunk": "@@ -0,0 +1,244 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#pragma once\n+\n+#include <vespa/document/bucket/bucketid.h>\n+#include <vespa/vespalib/stllike/hash_map.h>\n+#include <vespa/vespalib/stllike/hash_set.h>\n+#include <vespa/vespalib/util/time.h>\n+#include <cassert>\n+#include <functional>\n+#include <iosfwd>\n+#include <map>\n+\n+namespace storage::bucketdb {\n+\n+/*\n+ * Interface for content node bucket database implementations.\n+ *\n+ * Allows for multiple divergent implementations to exist of the\n+ * bucket database in a transition period.\n+ */\n+template <typename ValueT>\n+class AbstractBucketMap {\n+public:\n+    using key_type    = uint64_t; // Always a raw u64 bucket key.\n+    using mapped_type = ValueT;\n+    using size_type   = size_t;\n+    using BucketId    = document::BucketId;\n+    struct WrappedEntry;\n+\n+    // Responsible for releasing lock in map when out of scope.\n+    class LockKeeper {\n+        friend struct WrappedEntry;\n+        AbstractBucketMap& _map;\n+        key_type _key;\n+        bool _locked;\n+\n+        LockKeeper(AbstractBucketMap& map, key_type key)\n+            : _map(map), _key(key), _locked(true) {}\n+        void unlock() { _map.unlock(_key); _locked = false; }\n+    public:\n+        ~LockKeeper() { if (_locked) unlock(); }\n+    };\n+\n+    struct WrappedEntry {\n+        WrappedEntry()\n+            : _exists(false),\n+              _preExisted(false),\n+              _lockKeeper(),\n+              _value(),\n+              _clientId(nullptr)\n+        {}\n+        WrappedEntry(AbstractBucketMap& map,\n+                     const key_type& key, const mapped_type& val,\n+                     const char* clientId, bool preExisted_)\n+            : _exists(true),\n+              _preExisted(preExisted_),\n+              _lockKeeper(new LockKeeper(map, key)),\n+              _value(val),\n+              _clientId(clientId) {}\n+        WrappedEntry(AbstractBucketMap& map, const key_type& key,\n+                     const char* clientId)\n+            : _exists(false),\n+              _preExisted(false),\n+              _lockKeeper(new LockKeeper(map, key)),\n+              _value(),\n+              _clientId(clientId) {}\n+        // TODO noexcept on these:\n+        WrappedEntry(WrappedEntry&&) = default;\n+        WrappedEntry& operator=(WrappedEntry&&) = default;\n+        ~WrappedEntry();\n+\n+        mapped_type* operator->() { return &_value; }\n+        const mapped_type* operator->() const { return &_value; }\n+        mapped_type& operator*() { return _value; }\n+        const mapped_type& operator*() const { return _value; }\n+\n+        const mapped_type *get() const { return &_value; }\n+        mapped_type *get() { return &_value; }\n+\n+        void write();\n+        void remove();\n+        void unlock();\n+        [[nodiscard]] bool exist() const { return _exists; } // TODO rename to exists()\n+        [[nodiscard]] bool preExisted() const { return _preExisted; }\n+        [[nodiscard]] bool locked() const { return _lockKeeper.get(); }\n+        const key_type& getKey() const { return _lockKeeper->_key; };\n+\n+        BucketId getBucketId() const {\n+            return BucketId(BucketId::keyToBucketId(getKey()));\n+        }\n+    protected:\n+        bool _exists;\n+        bool _preExisted;\n+        std::unique_ptr<LockKeeper> _lockKeeper;\n+        mapped_type _value;\n+        const char* _clientId;\n+        friend class AbstractLockableMap;\n+    };\n+\n+    struct LockId {\n+        key_type _key;\n+        const char* _owner;\n+\n+        LockId() : _key(0), _owner(\"none - empty token\") {}\n+        LockId(key_type key, const char* owner)\n+            : _key(key), _owner(owner)\n+        {\n+            assert(_owner);\n+        }\n+\n+        size_t hash() const { return _key; }\n+        size_t operator%(size_t val) const { return _key % val; }\n+        bool operator==(const LockId& id) const { return (_key == id._key); }\n+        operator key_type() const { return _key; }\n+    };\n+\n+    using EntryMap = std::map<BucketId, WrappedEntry>; // TODO ordered std::vector instead? map interface needed?\n+\n+    enum Decision { ABORT, UPDATE, REMOVE, CONTINUE, DECISION_COUNT };\n+\n+    AbstractBucketMap() = default;\n+    virtual ~AbstractBucketMap() = default;\n+\n+    virtual void insert(const key_type& key, const mapped_type& value,\n+                        const char* client_id, bool has_lock,\n+                        bool& pre_existed) = 0;\n+    virtual bool erase(const key_type& key, const char* clientId, bool has_lock) = 0;\n+\n+    virtual WrappedEntry get(const key_type& key, const char* clientId, bool createIfNonExisting) = 0;\n+    WrappedEntry get(const key_type& key, const char* clientId) {\n+        return get(key, clientId, false);\n+    }\n+    /**\n+     * Returns all buckets in the bucket database that can contain the given\n+     * bucket, and all buckets that that bucket contains.\n+     */\n+    virtual EntryMap getAll(const BucketId& bucketId, const char* clientId) = 0;\n+    /**\n+     * Returns all buckets in the bucket database that can contain the given\n+     * bucket. Usually, there should be only one such bucket, but in the case\n+     * of inconsistent splitting, there may be more than one.\n+     */\n+    virtual EntryMap getContained(const BucketId& bucketId, const char* clientId) = 0;\n+    /**\n+     * Returns true iff bucket has no superbuckets or sub-buckets in the\n+     * database. Usage assumption is that any operation that can cause the\n+     * bucket to become inconsistent will require taking its lock, so by\n+     * requiring the lock to be provided here we avoid race conditions.\n+     */\n+    virtual bool isConsistent(const WrappedEntry& entry) = 0; // TODO const\n+\n+    static constexpr uint32_t DEFAULT_CHUNK_SIZE = 1000;\n+\n+\n+    /**\n+     * Iterate over the entire database contents, holding the global database\n+     * mutex for `chunkSize` processed entries at a time, yielding the current\n+     * thread between each such such to allow other threads to get a chance", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwOTgyMQ==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446209821", "bodyText": "Should we use holdElem instead? @toregge FYI", "author": "geirst", "createdAt": "2020-06-26T14:13:20Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.hpp", "diffHunk": "@@ -0,0 +1,509 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#include \"btree_lockable_map.h\"\n+#include \"generic_btree_bucket_database.hpp\"\n+#include <vespa/vespalib/btree/btreebuilder.h>\n+#include <vespa/vespalib/btree/btreenodeallocator.hpp>\n+#include <vespa/vespalib/btree/btreenode.hpp>\n+#include <vespa/vespalib/btree/btreenodestore.hpp>\n+#include <vespa/vespalib/btree/btreeiterator.hpp>\n+#include <vespa/vespalib/btree/btreeroot.hpp>\n+#include <vespa/vespalib/btree/btreebuilder.hpp>\n+#include <vespa/vespalib/btree/btree.hpp>\n+#include <vespa/vespalib/btree/btreestore.hpp>\n+#include <vespa/vespalib/datastore/datastore.h>\n+#include <vespa/vespalib/stllike/hash_map.hpp>\n+#include <vespa/vespalib/stllike/hash_set.hpp>\n+#include <thread>\n+#include <sstream>\n+\n+// Major TODOs in the short term:\n+//  - Introduce snapshotting for readers\n+//  - Greatly improve performance for DB iteration for readers by avoiding\n+//    requirement to lock individual buckets and perform O(n) lbound seeks\n+//    just to do a sweep.\n+\n+namespace storage::bucketdb {\n+\n+using vespalib::datastore::EntryRef;\n+using vespalib::ConstArrayRef;\n+using document::BucketId;\n+\n+template <typename T>\n+struct BTreeLockableMap<T>::ValueTraits {\n+    using ValueType     = T;\n+    using ConstValueRef = const T&;\n+    using DataStoreType = vespalib::datastore::DataStore<ValueType>;\n+\n+    static EntryRef entry_ref_from_value(uint64_t value) {\n+        return EntryRef(value & 0xffffffffULL);\n+    }\n+    static ValueType make_invalid_value() {\n+        return ValueType();\n+    }\n+    static uint64_t store_and_wrap_value(DataStoreType& store, const ValueType& value) noexcept {\n+        return store.addEntry(value).ref();\n+    }\n+    static void remove_by_wrapped_value(DataStoreType& store, uint64_t value) noexcept {\n+        store.freeElem(entry_ref_from_value(value), 1);", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjU1NTU5Mg==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446555592", "bodyText": "Use holdElem to ensure it goes through hold cycle (exists with stable value while any readers might access it).", "author": "toregge", "createdAt": "2020-06-27T18:49:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwOTgyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMzIyNg==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446213226", "bodyText": "Only allocatedBytes should be returned. It subsumes the other values.", "author": "geirst", "createdAt": "2020-06-26T14:19:23Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.hpp", "diffHunk": "@@ -0,0 +1,509 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#include \"btree_lockable_map.h\"\n+#include \"generic_btree_bucket_database.hpp\"\n+#include <vespa/vespalib/btree/btreebuilder.h>\n+#include <vespa/vespalib/btree/btreenodeallocator.hpp>\n+#include <vespa/vespalib/btree/btreenode.hpp>\n+#include <vespa/vespalib/btree/btreenodestore.hpp>\n+#include <vespa/vespalib/btree/btreeiterator.hpp>\n+#include <vespa/vespalib/btree/btreeroot.hpp>\n+#include <vespa/vespalib/btree/btreebuilder.hpp>\n+#include <vespa/vespalib/btree/btree.hpp>\n+#include <vespa/vespalib/btree/btreestore.hpp>\n+#include <vespa/vespalib/datastore/datastore.h>\n+#include <vespa/vespalib/stllike/hash_map.hpp>\n+#include <vespa/vespalib/stllike/hash_set.hpp>\n+#include <thread>\n+#include <sstream>\n+\n+// Major TODOs in the short term:\n+//  - Introduce snapshotting for readers\n+//  - Greatly improve performance for DB iteration for readers by avoiding\n+//    requirement to lock individual buckets and perform O(n) lbound seeks\n+//    just to do a sweep.\n+\n+namespace storage::bucketdb {\n+\n+using vespalib::datastore::EntryRef;\n+using vespalib::ConstArrayRef;\n+using document::BucketId;\n+\n+template <typename T>\n+struct BTreeLockableMap<T>::ValueTraits {\n+    using ValueType     = T;\n+    using ConstValueRef = const T&;\n+    using DataStoreType = vespalib::datastore::DataStore<ValueType>;\n+\n+    static EntryRef entry_ref_from_value(uint64_t value) {\n+        return EntryRef(value & 0xffffffffULL);\n+    }\n+    static ValueType make_invalid_value() {\n+        return ValueType();\n+    }\n+    static uint64_t store_and_wrap_value(DataStoreType& store, const ValueType& value) noexcept {\n+        return store.addEntry(value).ref();\n+    }\n+    static void remove_by_wrapped_value(DataStoreType& store, uint64_t value) noexcept {\n+        store.freeElem(entry_ref_from_value(value), 1);\n+    }\n+    static ValueType unwrap_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+    static ConstValueRef unwrap_const_ref_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+};\n+\n+template <typename T>\n+BTreeLockableMap<T>::BTreeLockableMap()\n+    : _impl(std::make_unique<GenericBTreeBucketDatabase<ValueTraits>>())\n+{}\n+\n+template <typename T>\n+BTreeLockableMap<T>::~BTreeLockableMap() = default;\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::LockIdSet() : Hash() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::~LockIdSet() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockIdSet::getMemoryUsage() const {\n+    return Hash::getMemoryConsumption();\n+}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::LockWaiters() : _id(0), _map() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::~LockWaiters() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockWaiters::insert(const LockId & lid) {\n+    Key id(_id++);\n+    _map.insert(typename WaiterMap::value_type(id, lid));\n+    return id;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator==(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    if (_impl->size() != other._impl->size()) {\n+        return false;\n+    }\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid(); ++lhs, ++rhs) {\n+        assert(rhs.valid());\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return false;\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return false;\n+        }\n+    }\n+    return true;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator<(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid() && rhs.valid(); ++lhs, ++rhs) {\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return (lhs.getKey() < rhs.getKey());\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return (_impl->const_value_ref_from_valid_iterator(lhs)\n+                    < other._impl->const_value_ref_from_valid_iterator(rhs));\n+        }\n+    }\n+    if (lhs.valid() == rhs.valid()) {\n+        return false; // All keys are equal in maps of equal size.\n+    }\n+    return rhs.valid(); // Rhs still valid, lhs is not; ergo lhs is \"less\".\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::size() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->size();\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::getMemoryUsage() const noexcept {\n+    std::lock_guard guard(_lock);\n+    const auto impl_usage = _impl->memory_usage();\n+    return (impl_usage.allocatedBytes() + impl_usage.usedBytes() +", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxNjI2Ng==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446216266", "bodyText": "Consider removing 'std::mutex'.", "author": "geirst", "createdAt": "2020-06-26T14:24:43Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.hpp", "diffHunk": "@@ -0,0 +1,509 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#include \"btree_lockable_map.h\"\n+#include \"generic_btree_bucket_database.hpp\"\n+#include <vespa/vespalib/btree/btreebuilder.h>\n+#include <vespa/vespalib/btree/btreenodeallocator.hpp>\n+#include <vespa/vespalib/btree/btreenode.hpp>\n+#include <vespa/vespalib/btree/btreenodestore.hpp>\n+#include <vespa/vespalib/btree/btreeiterator.hpp>\n+#include <vespa/vespalib/btree/btreeroot.hpp>\n+#include <vespa/vespalib/btree/btreebuilder.hpp>\n+#include <vespa/vespalib/btree/btree.hpp>\n+#include <vespa/vespalib/btree/btreestore.hpp>\n+#include <vespa/vespalib/datastore/datastore.h>\n+#include <vespa/vespalib/stllike/hash_map.hpp>\n+#include <vespa/vespalib/stllike/hash_set.hpp>\n+#include <thread>\n+#include <sstream>\n+\n+// Major TODOs in the short term:\n+//  - Introduce snapshotting for readers\n+//  - Greatly improve performance for DB iteration for readers by avoiding\n+//    requirement to lock individual buckets and perform O(n) lbound seeks\n+//    just to do a sweep.\n+\n+namespace storage::bucketdb {\n+\n+using vespalib::datastore::EntryRef;\n+using vespalib::ConstArrayRef;\n+using document::BucketId;\n+\n+template <typename T>\n+struct BTreeLockableMap<T>::ValueTraits {\n+    using ValueType     = T;\n+    using ConstValueRef = const T&;\n+    using DataStoreType = vespalib::datastore::DataStore<ValueType>;\n+\n+    static EntryRef entry_ref_from_value(uint64_t value) {\n+        return EntryRef(value & 0xffffffffULL);\n+    }\n+    static ValueType make_invalid_value() {\n+        return ValueType();\n+    }\n+    static uint64_t store_and_wrap_value(DataStoreType& store, const ValueType& value) noexcept {\n+        return store.addEntry(value).ref();\n+    }\n+    static void remove_by_wrapped_value(DataStoreType& store, uint64_t value) noexcept {\n+        store.freeElem(entry_ref_from_value(value), 1);\n+    }\n+    static ValueType unwrap_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+    static ConstValueRef unwrap_const_ref_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+};\n+\n+template <typename T>\n+BTreeLockableMap<T>::BTreeLockableMap()\n+    : _impl(std::make_unique<GenericBTreeBucketDatabase<ValueTraits>>())\n+{}\n+\n+template <typename T>\n+BTreeLockableMap<T>::~BTreeLockableMap() = default;\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::LockIdSet() : Hash() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::~LockIdSet() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockIdSet::getMemoryUsage() const {\n+    return Hash::getMemoryConsumption();\n+}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::LockWaiters() : _id(0), _map() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::~LockWaiters() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockWaiters::insert(const LockId & lid) {\n+    Key id(_id++);\n+    _map.insert(typename WaiterMap::value_type(id, lid));\n+    return id;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator==(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    if (_impl->size() != other._impl->size()) {\n+        return false;\n+    }\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid(); ++lhs, ++rhs) {\n+        assert(rhs.valid());\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return false;\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return false;\n+        }\n+    }\n+    return true;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator<(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid() && rhs.valid(); ++lhs, ++rhs) {\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return (lhs.getKey() < rhs.getKey());\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return (_impl->const_value_ref_from_valid_iterator(lhs)\n+                    < other._impl->const_value_ref_from_valid_iterator(rhs));\n+        }\n+    }\n+    if (lhs.valid() == rhs.valid()) {\n+        return false; // All keys are equal in maps of equal size.\n+    }\n+    return rhs.valid(); // Rhs still valid, lhs is not; ergo lhs is \"less\".\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::size() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->size();\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::getMemoryUsage() const noexcept {\n+    std::lock_guard guard(_lock);\n+    const auto impl_usage = _impl->memory_usage();\n+    return (impl_usage.allocatedBytes() + impl_usage.usedBytes() +\n+            impl_usage.deadBytes() + impl_usage.allocatedBytesOnHold() +\n+            _lockedKeys.getMemoryUsage() +\n+            sizeof(std::mutex) + sizeof(std::condition_variable));\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::empty() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->empty();\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::swap(BTreeLockableMap& other) {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    _impl.swap(other._impl);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::acquireKey(const LockId& lid, std::unique_lock<std::mutex>& guard) {\n+    if (_lockedKeys.exists(lid)) {\n+        auto waitId = _lockWaiters.insert(lid);\n+        while (_lockedKeys.exists(lid)) {\n+            _cond.wait(guard);\n+        }\n+        _lockWaiters.erase(waitId);\n+    }\n+}\n+\n+template <typename T>\n+typename BTreeLockableMap<T>::WrappedEntry\n+BTreeLockableMap<T>::get(const key_type& key, const char* clientId, bool createIfNonExisting) {\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    acquireKey(lid, guard);\n+    auto iter = _impl->find(key);\n+    bool preExisted = iter.valid();\n+\n+    if (!preExisted && createIfNonExisting) {\n+        _impl->update_by_raw_key(key, mapped_type());\n+        // TODO avoid double lookup, though this is in an unlikely branch so shouldn't matter much.\n+        iter = _impl->find(key);\n+        assert(iter.valid());\n+    }\n+    if (!iter.valid()) {\n+        return WrappedEntry();\n+    }\n+    _lockedKeys.insert(lid);\n+    return WrappedEntry(*this, key, _impl->entry_from_iterator(iter), clientId, preExisted);\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::erase(const key_type& key, const char* client_id, bool has_lock) {\n+    LockId lid(key, client_id);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    return _impl->remove_by_raw_key(key);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::insert(const key_type& key, const mapped_type& value,\n+                                 const char* clientId, bool has_lock, bool& pre_existed)\n+{\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    pre_existed = _impl->update_by_raw_key(key, value);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::clear() {\n+    std::lock_guard<std::mutex> guard(_lock);", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxOTY0MQ==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446219641", "bodyText": "Consider removing old const_cast.", "author": "geirst", "createdAt": "2020-06-26T14:30:19Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.hpp", "diffHunk": "@@ -0,0 +1,509 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#include \"btree_lockable_map.h\"\n+#include \"generic_btree_bucket_database.hpp\"\n+#include <vespa/vespalib/btree/btreebuilder.h>\n+#include <vespa/vespalib/btree/btreenodeallocator.hpp>\n+#include <vespa/vespalib/btree/btreenode.hpp>\n+#include <vespa/vespalib/btree/btreenodestore.hpp>\n+#include <vespa/vespalib/btree/btreeiterator.hpp>\n+#include <vespa/vespalib/btree/btreeroot.hpp>\n+#include <vespa/vespalib/btree/btreebuilder.hpp>\n+#include <vespa/vespalib/btree/btree.hpp>\n+#include <vespa/vespalib/btree/btreestore.hpp>\n+#include <vespa/vespalib/datastore/datastore.h>\n+#include <vespa/vespalib/stllike/hash_map.hpp>\n+#include <vespa/vespalib/stllike/hash_set.hpp>\n+#include <thread>\n+#include <sstream>\n+\n+// Major TODOs in the short term:\n+//  - Introduce snapshotting for readers\n+//  - Greatly improve performance for DB iteration for readers by avoiding\n+//    requirement to lock individual buckets and perform O(n) lbound seeks\n+//    just to do a sweep.\n+\n+namespace storage::bucketdb {\n+\n+using vespalib::datastore::EntryRef;\n+using vespalib::ConstArrayRef;\n+using document::BucketId;\n+\n+template <typename T>\n+struct BTreeLockableMap<T>::ValueTraits {\n+    using ValueType     = T;\n+    using ConstValueRef = const T&;\n+    using DataStoreType = vespalib::datastore::DataStore<ValueType>;\n+\n+    static EntryRef entry_ref_from_value(uint64_t value) {\n+        return EntryRef(value & 0xffffffffULL);\n+    }\n+    static ValueType make_invalid_value() {\n+        return ValueType();\n+    }\n+    static uint64_t store_and_wrap_value(DataStoreType& store, const ValueType& value) noexcept {\n+        return store.addEntry(value).ref();\n+    }\n+    static void remove_by_wrapped_value(DataStoreType& store, uint64_t value) noexcept {\n+        store.freeElem(entry_ref_from_value(value), 1);\n+    }\n+    static ValueType unwrap_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+    static ConstValueRef unwrap_const_ref_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+};\n+\n+template <typename T>\n+BTreeLockableMap<T>::BTreeLockableMap()\n+    : _impl(std::make_unique<GenericBTreeBucketDatabase<ValueTraits>>())\n+{}\n+\n+template <typename T>\n+BTreeLockableMap<T>::~BTreeLockableMap() = default;\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::LockIdSet() : Hash() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::~LockIdSet() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockIdSet::getMemoryUsage() const {\n+    return Hash::getMemoryConsumption();\n+}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::LockWaiters() : _id(0), _map() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::~LockWaiters() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockWaiters::insert(const LockId & lid) {\n+    Key id(_id++);\n+    _map.insert(typename WaiterMap::value_type(id, lid));\n+    return id;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator==(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    if (_impl->size() != other._impl->size()) {\n+        return false;\n+    }\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid(); ++lhs, ++rhs) {\n+        assert(rhs.valid());\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return false;\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return false;\n+        }\n+    }\n+    return true;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator<(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid() && rhs.valid(); ++lhs, ++rhs) {\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return (lhs.getKey() < rhs.getKey());\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return (_impl->const_value_ref_from_valid_iterator(lhs)\n+                    < other._impl->const_value_ref_from_valid_iterator(rhs));\n+        }\n+    }\n+    if (lhs.valid() == rhs.valid()) {\n+        return false; // All keys are equal in maps of equal size.\n+    }\n+    return rhs.valid(); // Rhs still valid, lhs is not; ergo lhs is \"less\".\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::size() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->size();\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::getMemoryUsage() const noexcept {\n+    std::lock_guard guard(_lock);\n+    const auto impl_usage = _impl->memory_usage();\n+    return (impl_usage.allocatedBytes() + impl_usage.usedBytes() +\n+            impl_usage.deadBytes() + impl_usage.allocatedBytesOnHold() +\n+            _lockedKeys.getMemoryUsage() +\n+            sizeof(std::mutex) + sizeof(std::condition_variable));\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::empty() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->empty();\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::swap(BTreeLockableMap& other) {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    _impl.swap(other._impl);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::acquireKey(const LockId& lid, std::unique_lock<std::mutex>& guard) {\n+    if (_lockedKeys.exists(lid)) {\n+        auto waitId = _lockWaiters.insert(lid);\n+        while (_lockedKeys.exists(lid)) {\n+            _cond.wait(guard);\n+        }\n+        _lockWaiters.erase(waitId);\n+    }\n+}\n+\n+template <typename T>\n+typename BTreeLockableMap<T>::WrappedEntry\n+BTreeLockableMap<T>::get(const key_type& key, const char* clientId, bool createIfNonExisting) {\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    acquireKey(lid, guard);\n+    auto iter = _impl->find(key);\n+    bool preExisted = iter.valid();\n+\n+    if (!preExisted && createIfNonExisting) {\n+        _impl->update_by_raw_key(key, mapped_type());\n+        // TODO avoid double lookup, though this is in an unlikely branch so shouldn't matter much.\n+        iter = _impl->find(key);\n+        assert(iter.valid());\n+    }\n+    if (!iter.valid()) {\n+        return WrappedEntry();\n+    }\n+    _lockedKeys.insert(lid);\n+    return WrappedEntry(*this, key, _impl->entry_from_iterator(iter), clientId, preExisted);\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::erase(const key_type& key, const char* client_id, bool has_lock) {\n+    LockId lid(key, client_id);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    return _impl->remove_by_raw_key(key);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::insert(const key_type& key, const mapped_type& value,\n+                                 const char* clientId, bool has_lock, bool& pre_existed)\n+{\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    pre_existed = _impl->update_by_raw_key(key, value);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::clear() {\n+    std::lock_guard<std::mutex> guard(_lock);\n+    _impl->clear();\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::findNextKey(key_type& key, mapped_type& val,\n+                                      const char* clientId,\n+                                      std::unique_lock<std::mutex> &guard)\n+{\n+    // Wait for next value to unlock.\n+    auto it = _impl->lower_bound(key);\n+    while (it.valid() && _lockedKeys.exists(LockId(it.getKey(), \"\"))) {\n+        auto wait_id = _lockWaiters.insert(LockId(it.getKey(), clientId));\n+        _cond.wait(guard);\n+        _lockWaiters.erase(wait_id);\n+        it = _impl->lower_bound(key);\n+    }\n+    if (!it.valid()) {\n+        return true;\n+    }\n+    key = it.getKey();\n+    val = _impl->entry_from_iterator(it);\n+    return false;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::handleDecision(key_type& key, mapped_type& val,\n+                                         Decision decision)\n+{\n+    switch (decision) {\n+    case Decision::UPDATE:\n+        _impl->update_by_raw_key(key, val);\n+        break;\n+    case Decision::REMOVE:\n+        // Invalidating is fine, since the caller doesn't hold long-lived iterators.\n+        _impl->remove_by_raw_key(key);\n+        break;\n+    case Decision::ABORT:\n+        return true;\n+    case Decision::CONTINUE:\n+        break;\n+    default:\n+        HDR_ABORT(\"should not be reached\");\n+    }\n+    return false;\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::do_for_each_mutable(std::function<Decision(uint64_t, mapped_type&)> func,\n+                                              const char* clientId,\n+                                              const key_type& first,\n+                                              const key_type& last)\n+{\n+    key_type key = first;\n+    mapped_type val;\n+    std::unique_lock guard(_lock);\n+    while (true) {\n+        if (findNextKey(key, val, clientId, guard) || key > last) {\n+            return;\n+        }\n+        Decision d(func(const_cast<const key_type&>(key), val));", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyMDcxNA==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446220714", "bodyText": "Consider removing old const_cast.", "author": "geirst", "createdAt": "2020-06-26T14:31:57Z", "path": "storage/src/vespa/storage/bucketdb/btree_lockable_map.hpp", "diffHunk": "@@ -0,0 +1,509 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#include \"btree_lockable_map.h\"\n+#include \"generic_btree_bucket_database.hpp\"\n+#include <vespa/vespalib/btree/btreebuilder.h>\n+#include <vespa/vespalib/btree/btreenodeallocator.hpp>\n+#include <vespa/vespalib/btree/btreenode.hpp>\n+#include <vespa/vespalib/btree/btreenodestore.hpp>\n+#include <vespa/vespalib/btree/btreeiterator.hpp>\n+#include <vespa/vespalib/btree/btreeroot.hpp>\n+#include <vespa/vespalib/btree/btreebuilder.hpp>\n+#include <vespa/vespalib/btree/btree.hpp>\n+#include <vespa/vespalib/btree/btreestore.hpp>\n+#include <vespa/vespalib/datastore/datastore.h>\n+#include <vespa/vespalib/stllike/hash_map.hpp>\n+#include <vespa/vespalib/stllike/hash_set.hpp>\n+#include <thread>\n+#include <sstream>\n+\n+// Major TODOs in the short term:\n+//  - Introduce snapshotting for readers\n+//  - Greatly improve performance for DB iteration for readers by avoiding\n+//    requirement to lock individual buckets and perform O(n) lbound seeks\n+//    just to do a sweep.\n+\n+namespace storage::bucketdb {\n+\n+using vespalib::datastore::EntryRef;\n+using vespalib::ConstArrayRef;\n+using document::BucketId;\n+\n+template <typename T>\n+struct BTreeLockableMap<T>::ValueTraits {\n+    using ValueType     = T;\n+    using ConstValueRef = const T&;\n+    using DataStoreType = vespalib::datastore::DataStore<ValueType>;\n+\n+    static EntryRef entry_ref_from_value(uint64_t value) {\n+        return EntryRef(value & 0xffffffffULL);\n+    }\n+    static ValueType make_invalid_value() {\n+        return ValueType();\n+    }\n+    static uint64_t store_and_wrap_value(DataStoreType& store, const ValueType& value) noexcept {\n+        return store.addEntry(value).ref();\n+    }\n+    static void remove_by_wrapped_value(DataStoreType& store, uint64_t value) noexcept {\n+        store.freeElem(entry_ref_from_value(value), 1);\n+    }\n+    static ValueType unwrap_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+    static ConstValueRef unwrap_const_ref_from_key_value(const DataStoreType& store, [[maybe_unused]] uint64_t key, uint64_t value) {\n+        return store.getEntry(entry_ref_from_value(value));\n+    }\n+};\n+\n+template <typename T>\n+BTreeLockableMap<T>::BTreeLockableMap()\n+    : _impl(std::make_unique<GenericBTreeBucketDatabase<ValueTraits>>())\n+{}\n+\n+template <typename T>\n+BTreeLockableMap<T>::~BTreeLockableMap() = default;\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::LockIdSet() : Hash() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockIdSet::~LockIdSet() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockIdSet::getMemoryUsage() const {\n+    return Hash::getMemoryConsumption();\n+}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::LockWaiters() : _id(0), _map() {}\n+\n+template <typename T>\n+BTreeLockableMap<T>::LockWaiters::~LockWaiters() = default;\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::LockWaiters::insert(const LockId & lid) {\n+    Key id(_id++);\n+    _map.insert(typename WaiterMap::value_type(id, lid));\n+    return id;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator==(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    if (_impl->size() != other._impl->size()) {\n+        return false;\n+    }\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid(); ++lhs, ++rhs) {\n+        assert(rhs.valid());\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return false;\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return false;\n+        }\n+    }\n+    return true;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::operator<(const BTreeLockableMap& other) const {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    auto lhs = _impl->begin();\n+    auto rhs = other._impl->begin();\n+    for (; lhs.valid() && rhs.valid(); ++lhs, ++rhs) {\n+        if (lhs.getKey() != rhs.getKey()) {\n+            return (lhs.getKey() < rhs.getKey());\n+        }\n+        if (_impl->const_value_ref_from_valid_iterator(lhs)\n+            != other._impl->const_value_ref_from_valid_iterator(rhs))\n+        {\n+            return (_impl->const_value_ref_from_valid_iterator(lhs)\n+                    < other._impl->const_value_ref_from_valid_iterator(rhs));\n+        }\n+    }\n+    if (lhs.valid() == rhs.valid()) {\n+        return false; // All keys are equal in maps of equal size.\n+    }\n+    return rhs.valid(); // Rhs still valid, lhs is not; ergo lhs is \"less\".\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::size() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->size();\n+}\n+\n+template <typename T>\n+size_t BTreeLockableMap<T>::getMemoryUsage() const noexcept {\n+    std::lock_guard guard(_lock);\n+    const auto impl_usage = _impl->memory_usage();\n+    return (impl_usage.allocatedBytes() + impl_usage.usedBytes() +\n+            impl_usage.deadBytes() + impl_usage.allocatedBytesOnHold() +\n+            _lockedKeys.getMemoryUsage() +\n+            sizeof(std::mutex) + sizeof(std::condition_variable));\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::empty() const noexcept {\n+    std::lock_guard guard(_lock);\n+    return _impl->empty();\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::swap(BTreeLockableMap& other) {\n+    std::lock_guard guard(_lock);\n+    std::lock_guard guard2(other._lock);\n+    _impl.swap(other._impl);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::acquireKey(const LockId& lid, std::unique_lock<std::mutex>& guard) {\n+    if (_lockedKeys.exists(lid)) {\n+        auto waitId = _lockWaiters.insert(lid);\n+        while (_lockedKeys.exists(lid)) {\n+            _cond.wait(guard);\n+        }\n+        _lockWaiters.erase(waitId);\n+    }\n+}\n+\n+template <typename T>\n+typename BTreeLockableMap<T>::WrappedEntry\n+BTreeLockableMap<T>::get(const key_type& key, const char* clientId, bool createIfNonExisting) {\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    acquireKey(lid, guard);\n+    auto iter = _impl->find(key);\n+    bool preExisted = iter.valid();\n+\n+    if (!preExisted && createIfNonExisting) {\n+        _impl->update_by_raw_key(key, mapped_type());\n+        // TODO avoid double lookup, though this is in an unlikely branch so shouldn't matter much.\n+        iter = _impl->find(key);\n+        assert(iter.valid());\n+    }\n+    if (!iter.valid()) {\n+        return WrappedEntry();\n+    }\n+    _lockedKeys.insert(lid);\n+    return WrappedEntry(*this, key, _impl->entry_from_iterator(iter), clientId, preExisted);\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::erase(const key_type& key, const char* client_id, bool has_lock) {\n+    LockId lid(key, client_id);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    return _impl->remove_by_raw_key(key);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::insert(const key_type& key, const mapped_type& value,\n+                                 const char* clientId, bool has_lock, bool& pre_existed)\n+{\n+    LockId lid(key, clientId);\n+    std::unique_lock guard(_lock);\n+    if (!has_lock) {\n+        acquireKey(lid, guard);\n+    }\n+    pre_existed = _impl->update_by_raw_key(key, value);\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::clear() {\n+    std::lock_guard<std::mutex> guard(_lock);\n+    _impl->clear();\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::findNextKey(key_type& key, mapped_type& val,\n+                                      const char* clientId,\n+                                      std::unique_lock<std::mutex> &guard)\n+{\n+    // Wait for next value to unlock.\n+    auto it = _impl->lower_bound(key);\n+    while (it.valid() && _lockedKeys.exists(LockId(it.getKey(), \"\"))) {\n+        auto wait_id = _lockWaiters.insert(LockId(it.getKey(), clientId));\n+        _cond.wait(guard);\n+        _lockWaiters.erase(wait_id);\n+        it = _impl->lower_bound(key);\n+    }\n+    if (!it.valid()) {\n+        return true;\n+    }\n+    key = it.getKey();\n+    val = _impl->entry_from_iterator(it);\n+    return false;\n+}\n+\n+template <typename T>\n+bool BTreeLockableMap<T>::handleDecision(key_type& key, mapped_type& val,\n+                                         Decision decision)\n+{\n+    switch (decision) {\n+    case Decision::UPDATE:\n+        _impl->update_by_raw_key(key, val);\n+        break;\n+    case Decision::REMOVE:\n+        // Invalidating is fine, since the caller doesn't hold long-lived iterators.\n+        _impl->remove_by_raw_key(key);\n+        break;\n+    case Decision::ABORT:\n+        return true;\n+    case Decision::CONTINUE:\n+        break;\n+    default:\n+        HDR_ABORT(\"should not be reached\");\n+    }\n+    return false;\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::do_for_each_mutable(std::function<Decision(uint64_t, mapped_type&)> func,\n+                                              const char* clientId,\n+                                              const key_type& first,\n+                                              const key_type& last)\n+{\n+    key_type key = first;\n+    mapped_type val;\n+    std::unique_lock guard(_lock);\n+    while (true) {\n+        if (findNextKey(key, val, clientId, guard) || key > last) {\n+            return;\n+        }\n+        Decision d(func(const_cast<const key_type&>(key), val));\n+        if (handleDecision(key, val, d)) {\n+            return;\n+        }\n+        ++key;\n+    }\n+}\n+\n+template <typename T>\n+void BTreeLockableMap<T>::do_for_each(std::function<Decision(uint64_t, const mapped_type&)> func,\n+                                      const char* clientId,\n+                                      const key_type& first,\n+                                      const key_type& last)\n+{\n+    key_type key = first;\n+    mapped_type val;\n+    std::unique_lock guard(_lock);\n+    while (true) {\n+        if (findNextKey(key, val, clientId, guard) || key > last) {\n+            return;\n+        }\n+        Decision d(func(const_cast<const key_type&>(key), val));", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNTA1OA==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446235058", "bodyText": "Consider returning const entry ref from valid iterator instead.", "author": "geirst", "createdAt": "2020-06-26T14:56:05Z", "path": "storage/src/vespa/storage/bucketdb/generic_btree_bucket_database.hpp", "diffHunk": "@@ -0,0 +1,494 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#pragma once\n+\n+#include \"generic_btree_bucket_database.h\"\n+\n+namespace storage::bucketdb {\n+\n+using document::BucketId;\n+\n+template <typename DataStoreTraitsT>\n+BucketId GenericBTreeBucketDatabase<DataStoreTraitsT>::bucket_from_valid_iterator(const BTreeConstIterator& iter) {\n+    return BucketId(BucketId::keyToBucketId(iter.getKey()));\n+}\n+\n+template <typename DataStoreTraitsT>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::commit_tree_changes() {\n+    // TODO break up and refactor\n+    // TODO verify semantics and usage\n+    // TODO make BTree wrapping API which abstracts away all this stuff via reader/writer interfaces\n+    _tree.getAllocator().freeze();\n+\n+    auto current_gen = _generation_handler.getCurrentGeneration();\n+    _store.transferHoldLists(current_gen);\n+    _tree.getAllocator().transferHoldLists(current_gen);\n+\n+    _generation_handler.incGeneration();\n+\n+    auto used_gen = _generation_handler.getFirstUsedGeneration();\n+    _store.trimHoldLists(used_gen);\n+    _tree.getAllocator().trimHoldLists(used_gen);\n+}\n+\n+template <typename DataStoreTraitsT>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::clear() noexcept {\n+    _tree.clear();\n+    commit_tree_changes();\n+}\n+\n+template <typename DataStoreTraitsT>\n+size_t GenericBTreeBucketDatabase<DataStoreTraitsT>::size() const noexcept {\n+    return _tree.size();\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::empty() const noexcept {\n+    return !_tree.begin().valid();\n+}\n+\n+template <typename DataStoreTraitsT>\n+vespalib::MemoryUsage GenericBTreeBucketDatabase<DataStoreTraitsT>::memory_usage() const noexcept {\n+    auto mem_usage = _tree.getMemoryUsage();\n+    mem_usage.merge(_store.getMemoryUsage());\n+    return mem_usage;\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ValueType\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::entry_from_iterator(const BTreeConstIterator& iter) const {\n+    if (!iter.valid()) {\n+        return DataStoreTraitsT::make_invalid_value();\n+    }\n+    const auto value = iter.getData();\n+    std::atomic_thread_fence(std::memory_order_acquire);\n+    return DataStoreTraitsT::unwrap_from_key_value(_store, iter.getKey(), value);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ConstValueRef\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::const_value_ref_from_valid_iterator(const BTreeConstIterator& iter) const {\n+    const auto value = iter.getData();\n+    std::atomic_thread_fence(std::memory_order_acquire);\n+    return DataStoreTraitsT::unwrap_const_ref_from_key_value(_store, iter.getKey(), value);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::lower_bound(uint64_t key) const noexcept {\n+    return _tree.lowerBound(key);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::find(uint64_t key) const noexcept {\n+    return _tree.find(key);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::begin() const noexcept {\n+    return _tree.begin();\n+}\n+\n+/*\n+ * Finding the complete set of parents of a given bucket is not obvious how to\n+ * do efficiently, as we only know that the parents are ordered before their\n+ * children, but we do not a-priori know if any exist at all. The Judy DB impl\n+ * does O(b) explicit point lookups (where b is the number of used bits in the\n+ * bucket), starting at the leaf bit and working towards the root. To avoid\n+ * having to re-create iterators and perform a full tree search every time, we\n+ * turn this on its head and start from the root, progressing towards the leaf.\n+ * This allows us to reuse a single iterator and to continue seeking forwards\n+ * from its current position.\n+ *\n+ * To speed up the process of converging on the target bucket without needing\n+ * to check many unrelated subtrees, we let the underlying B-tree automatically\n+ * aggregate the min/max range of the used-bits of all contained bucket keys.\n+ * If we e.g. know that the minimum number of used bits in the DB is 16, we can\n+ * immediately seek to this level in the tree instead of working our way down\n+ * one bit at a time. By definition, no parents can exist above this level.\n+ * This is a very important optimization, as bucket trees are usually very well\n+ * balanced due to randomized distribution of data (combined with a cluster-wide\n+ * minimum tree level imposed by distribution bits). It is common that the minimum\n+ * number of used bits == max number of used bits, i.e. a totally even split.\n+ * This means that for a system without inconsistently split buckets (i.e. no\n+ * parents) we're highly likely to converge on the target bucket in a single seek.\n+ *\n+ * Algorithm:\n+ *\n+ *   Core invariant: every subsequent iterator seek performed in this algorithm\n+ *   is for a key that is strictly higher than the one the iterator is currently at.\n+ *\n+ *   1. Lbound seek to the lowest key that is known to exclude all already visited\n+ *      parents. On the first iteration we use a bit count equal to the minimum number\n+ *      of key used-bits in the entire DB, allowing us to potentially skip most subtrees.\n+ *   2. If the current node's key is greater than that of the requested bucket's key,\n+ *      we've either descended to--or beyond--it in its own subtree or we've entered\n+ *      a disjoint subtree. Since we know that all parents must sort before any given\n+ *      child bucket, no more parents may be found at this point. Algorithm terminates.\n+ *   3. As the main body of the loop is entered, we know one of following must hold:\n+ *      3.1 The current node is an explicitly present parent of our bucket.\n+ *      3.2 The current node is contained in a left subtree branch of a parent that\n+ *          does not have a bucket explicitly present in the tree. It cannot be in\n+ *          a right subtree of any parent, as that would imply the node is ordered\n+ *          _after_ our own bucket in an in-order traversal, which would contradict\n+ *          the check in step 2 above.\n+ *   4. If the current node contains the requested bucket, we're at a parent\n+ *      node of the bucket; add it to the result set.\n+ *      If this is _not_ the case, we're in a different subtree. Example: the\n+ *      requested bucket has a key whose MSB is 1 but the first bucket in the\n+ *      tree has a key with an MSB of 0. Either way we need to update our search\n+ *      key to home in on the target subtree where more parents may be found;\n+ *   5. Update the seek key to find the next possible parent. To ensure this key is\n+ *      strictly greater than the iterator's current key we find the largest shared\n+ *      prefix of bits in common between the current node's key and the requested\n+ *      bucket's key. The prefix length + 1 is then the depth in the tree at which the\n+ *      two subtrees branch off and diverge.\n+ *      The new key is then the MSB prefix length + 1 requested bucket's key with a\n+ *      matching number of used-bits set. Forward lbound-seek the iterator to this key.\n+ *      `--> TODO elaborate on prefix semantics when they are equal wrt. min used bits\n+ *   6. Iff iterator is still valid, go to step 2\n+ *\n+ * This algorithm is able to skip through large parts of the tree in a sparsely populated\n+ * tree, but the number of seeks will trend towards O(b - min_bits) as with the legacy\n+ * implementation when a tree is densely populated (where `b` is the used-bits count of the\n+ * most specific node in the tree for the target bucket, and min_bits is the minimum number\n+ * of used-bits for any key in the database). This because all logical inner nodes in the tree\n+ * will have subtrees under them. Even in the worst case we should be more efficient than the\n+ * legacy Judy-based implementation since we've cut any dense search space in half for each\n+ * invocation of seek() on the iterator.\n+ */\n+template <typename DataStoreTraitsT>\n+template <typename Func>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::find_parents_internal(\n+        const typename BTree::FrozenView& frozen_view,\n+        const BucketId& bucket,\n+        Func func) const\n+{\n+    const uint64_t bucket_key = bucket.toKey();\n+    if (frozen_view.empty()) {\n+        return frozen_view.begin(); // Will be invalid.\n+    }\n+    const auto min_db_bits = frozen_view.getAggregated().getMin();\n+    assert(min_db_bits >= static_cast<int32_t>(BucketId::minNumBits));\n+    assert(min_db_bits <= static_cast<int32_t>(BucketId::maxNumBits));\n+    // Start at the lowest possible tree level no parents can exist above,\n+    // descending towards the bucket itself.\n+    // Note: important to use getId() rather than getRawId(), as min_db_bits may be\n+    // greater than the used bits of the queried bucket. If we used the raw ID, we'd\n+    // end up looking at undefined bits.\n+    const auto first_key = BucketId(min_db_bits, bucket.getId()).toKey();\n+    auto iter = frozen_view.lowerBound(first_key);\n+    // Try skipping as many levels of the tree as possible as we go.\n+    uint32_t bits = min_db_bits;\n+    while (iter.valid() && (iter.getKey() < bucket_key)) {\n+        auto candidate = BucketId(BucketId::keyToBucketId(iter.getKey()));\n+        if (candidate.contains(bucket)) {\n+            assert(candidate.getUsedBits() >= bits);\n+            func(iter.getKey(), entry_from_iterator(iter));", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzOTA0OA==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446239048", "bodyText": "Consider rename to wrap_and_store_value.", "author": "geirst", "createdAt": "2020-06-26T15:02:36Z", "path": "storage/src/vespa/storage/bucketdb/generic_btree_bucket_database.hpp", "diffHunk": "@@ -0,0 +1,494 @@\n+// Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n+#pragma once\n+\n+#include \"generic_btree_bucket_database.h\"\n+\n+namespace storage::bucketdb {\n+\n+using document::BucketId;\n+\n+template <typename DataStoreTraitsT>\n+BucketId GenericBTreeBucketDatabase<DataStoreTraitsT>::bucket_from_valid_iterator(const BTreeConstIterator& iter) {\n+    return BucketId(BucketId::keyToBucketId(iter.getKey()));\n+}\n+\n+template <typename DataStoreTraitsT>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::commit_tree_changes() {\n+    // TODO break up and refactor\n+    // TODO verify semantics and usage\n+    // TODO make BTree wrapping API which abstracts away all this stuff via reader/writer interfaces\n+    _tree.getAllocator().freeze();\n+\n+    auto current_gen = _generation_handler.getCurrentGeneration();\n+    _store.transferHoldLists(current_gen);\n+    _tree.getAllocator().transferHoldLists(current_gen);\n+\n+    _generation_handler.incGeneration();\n+\n+    auto used_gen = _generation_handler.getFirstUsedGeneration();\n+    _store.trimHoldLists(used_gen);\n+    _tree.getAllocator().trimHoldLists(used_gen);\n+}\n+\n+template <typename DataStoreTraitsT>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::clear() noexcept {\n+    _tree.clear();\n+    commit_tree_changes();\n+}\n+\n+template <typename DataStoreTraitsT>\n+size_t GenericBTreeBucketDatabase<DataStoreTraitsT>::size() const noexcept {\n+    return _tree.size();\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::empty() const noexcept {\n+    return !_tree.begin().valid();\n+}\n+\n+template <typename DataStoreTraitsT>\n+vespalib::MemoryUsage GenericBTreeBucketDatabase<DataStoreTraitsT>::memory_usage() const noexcept {\n+    auto mem_usage = _tree.getMemoryUsage();\n+    mem_usage.merge(_store.getMemoryUsage());\n+    return mem_usage;\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ValueType\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::entry_from_iterator(const BTreeConstIterator& iter) const {\n+    if (!iter.valid()) {\n+        return DataStoreTraitsT::make_invalid_value();\n+    }\n+    const auto value = iter.getData();\n+    std::atomic_thread_fence(std::memory_order_acquire);\n+    return DataStoreTraitsT::unwrap_from_key_value(_store, iter.getKey(), value);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ConstValueRef\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::const_value_ref_from_valid_iterator(const BTreeConstIterator& iter) const {\n+    const auto value = iter.getData();\n+    std::atomic_thread_fence(std::memory_order_acquire);\n+    return DataStoreTraitsT::unwrap_const_ref_from_key_value(_store, iter.getKey(), value);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::lower_bound(uint64_t key) const noexcept {\n+    return _tree.lowerBound(key);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::find(uint64_t key) const noexcept {\n+    return _tree.find(key);\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::begin() const noexcept {\n+    return _tree.begin();\n+}\n+\n+/*\n+ * Finding the complete set of parents of a given bucket is not obvious how to\n+ * do efficiently, as we only know that the parents are ordered before their\n+ * children, but we do not a-priori know if any exist at all. The Judy DB impl\n+ * does O(b) explicit point lookups (where b is the number of used bits in the\n+ * bucket), starting at the leaf bit and working towards the root. To avoid\n+ * having to re-create iterators and perform a full tree search every time, we\n+ * turn this on its head and start from the root, progressing towards the leaf.\n+ * This allows us to reuse a single iterator and to continue seeking forwards\n+ * from its current position.\n+ *\n+ * To speed up the process of converging on the target bucket without needing\n+ * to check many unrelated subtrees, we let the underlying B-tree automatically\n+ * aggregate the min/max range of the used-bits of all contained bucket keys.\n+ * If we e.g. know that the minimum number of used bits in the DB is 16, we can\n+ * immediately seek to this level in the tree instead of working our way down\n+ * one bit at a time. By definition, no parents can exist above this level.\n+ * This is a very important optimization, as bucket trees are usually very well\n+ * balanced due to randomized distribution of data (combined with a cluster-wide\n+ * minimum tree level imposed by distribution bits). It is common that the minimum\n+ * number of used bits == max number of used bits, i.e. a totally even split.\n+ * This means that for a system without inconsistently split buckets (i.e. no\n+ * parents) we're highly likely to converge on the target bucket in a single seek.\n+ *\n+ * Algorithm:\n+ *\n+ *   Core invariant: every subsequent iterator seek performed in this algorithm\n+ *   is for a key that is strictly higher than the one the iterator is currently at.\n+ *\n+ *   1. Lbound seek to the lowest key that is known to exclude all already visited\n+ *      parents. On the first iteration we use a bit count equal to the minimum number\n+ *      of key used-bits in the entire DB, allowing us to potentially skip most subtrees.\n+ *   2. If the current node's key is greater than that of the requested bucket's key,\n+ *      we've either descended to--or beyond--it in its own subtree or we've entered\n+ *      a disjoint subtree. Since we know that all parents must sort before any given\n+ *      child bucket, no more parents may be found at this point. Algorithm terminates.\n+ *   3. As the main body of the loop is entered, we know one of following must hold:\n+ *      3.1 The current node is an explicitly present parent of our bucket.\n+ *      3.2 The current node is contained in a left subtree branch of a parent that\n+ *          does not have a bucket explicitly present in the tree. It cannot be in\n+ *          a right subtree of any parent, as that would imply the node is ordered\n+ *          _after_ our own bucket in an in-order traversal, which would contradict\n+ *          the check in step 2 above.\n+ *   4. If the current node contains the requested bucket, we're at a parent\n+ *      node of the bucket; add it to the result set.\n+ *      If this is _not_ the case, we're in a different subtree. Example: the\n+ *      requested bucket has a key whose MSB is 1 but the first bucket in the\n+ *      tree has a key with an MSB of 0. Either way we need to update our search\n+ *      key to home in on the target subtree where more parents may be found;\n+ *   5. Update the seek key to find the next possible parent. To ensure this key is\n+ *      strictly greater than the iterator's current key we find the largest shared\n+ *      prefix of bits in common between the current node's key and the requested\n+ *      bucket's key. The prefix length + 1 is then the depth in the tree at which the\n+ *      two subtrees branch off and diverge.\n+ *      The new key is then the MSB prefix length + 1 requested bucket's key with a\n+ *      matching number of used-bits set. Forward lbound-seek the iterator to this key.\n+ *      `--> TODO elaborate on prefix semantics when they are equal wrt. min used bits\n+ *   6. Iff iterator is still valid, go to step 2\n+ *\n+ * This algorithm is able to skip through large parts of the tree in a sparsely populated\n+ * tree, but the number of seeks will trend towards O(b - min_bits) as with the legacy\n+ * implementation when a tree is densely populated (where `b` is the used-bits count of the\n+ * most specific node in the tree for the target bucket, and min_bits is the minimum number\n+ * of used-bits for any key in the database). This because all logical inner nodes in the tree\n+ * will have subtrees under them. Even in the worst case we should be more efficient than the\n+ * legacy Judy-based implementation since we've cut any dense search space in half for each\n+ * invocation of seek() on the iterator.\n+ */\n+template <typename DataStoreTraitsT>\n+template <typename Func>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::BTreeConstIterator\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::find_parents_internal(\n+        const typename BTree::FrozenView& frozen_view,\n+        const BucketId& bucket,\n+        Func func) const\n+{\n+    const uint64_t bucket_key = bucket.toKey();\n+    if (frozen_view.empty()) {\n+        return frozen_view.begin(); // Will be invalid.\n+    }\n+    const auto min_db_bits = frozen_view.getAggregated().getMin();\n+    assert(min_db_bits >= static_cast<int32_t>(BucketId::minNumBits));\n+    assert(min_db_bits <= static_cast<int32_t>(BucketId::maxNumBits));\n+    // Start at the lowest possible tree level no parents can exist above,\n+    // descending towards the bucket itself.\n+    // Note: important to use getId() rather than getRawId(), as min_db_bits may be\n+    // greater than the used bits of the queried bucket. If we used the raw ID, we'd\n+    // end up looking at undefined bits.\n+    const auto first_key = BucketId(min_db_bits, bucket.getId()).toKey();\n+    auto iter = frozen_view.lowerBound(first_key);\n+    // Try skipping as many levels of the tree as possible as we go.\n+    uint32_t bits = min_db_bits;\n+    while (iter.valid() && (iter.getKey() < bucket_key)) {\n+        auto candidate = BucketId(BucketId::keyToBucketId(iter.getKey()));\n+        if (candidate.contains(bucket)) {\n+            assert(candidate.getUsedBits() >= bits);\n+            func(iter.getKey(), entry_from_iterator(iter));\n+        }\n+        bits = next_parent_bit_seek_level(bits, candidate, bucket);\n+        const auto parent_key = BucketId(bits, bucket.getRawId()).toKey();\n+        assert(parent_key > iter.getKey());\n+        iter.seek(parent_key);\n+    }\n+    return iter;\n+}\n+\n+template <typename DataStoreTraitsT>\n+template <typename Func>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::find_parents_and_self_internal(\n+        const typename BTree::FrozenView& frozen_view,\n+        const BucketId& bucket,\n+        Func func) const\n+{\n+    auto iter = find_parents_internal(frozen_view, bucket, func);\n+    if (iter.valid() && iter.getKey() == bucket.toKey()) {\n+        func(iter.getKey(), entry_from_iterator(iter));\n+    }\n+}\n+\n+template <typename DataStoreTraitsT>\n+template <typename Func>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::find_parents_and_self(\n+        const document::BucketId& bucket,\n+        Func func) const\n+{\n+    auto view = _tree.getFrozenView();\n+    find_parents_and_self_internal(view, bucket, std::move(func));\n+}\n+\n+template <typename DataStoreTraitsT>\n+template <typename Func>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::find_parents_self_and_children(\n+        const BucketId& bucket,\n+        Func func) const\n+{\n+    auto view = _tree.getFrozenView();\n+    auto iter = find_parents_internal(view, bucket, func);\n+    // `iter` is already pointing at, or beyond, one of the bucket's subtrees.\n+    for (; iter.valid(); ++iter) {\n+        auto candidate = BucketId(BucketId::keyToBucketId(iter.getKey()));\n+        if (bucket.contains(candidate)) {\n+            func(iter.getKey(), entry_from_iterator(iter));\n+        } else {\n+            break;\n+        }\n+    }\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ValueType\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::get(const BucketId& bucket) const {\n+    return entry_from_iterator(_tree.find(bucket.toKey()));\n+}\n+\n+template <typename DataStoreTraitsT>\n+typename GenericBTreeBucketDatabase<DataStoreTraitsT>::ValueType\n+GenericBTreeBucketDatabase<DataStoreTraitsT>::get_by_raw_key(uint64_t key) const {\n+    return entry_from_iterator(_tree.find(key));\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::remove_by_raw_key(uint64_t key) {\n+    auto iter = _tree.find(key);\n+    if (!iter.valid()) {\n+        return false;\n+    }\n+    const auto value = iter.getData();\n+    DataStoreTraitsT::remove_by_wrapped_value(_store, value);\n+    _tree.remove(iter);\n+    commit_tree_changes();\n+    return true;\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::remove(const BucketId& bucket) {\n+    return remove_by_raw_key(bucket.toKey());\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::update_by_raw_key(uint64_t bucket_key,\n+                                                                     const ValueType& new_entry)\n+{\n+    const auto new_value = DataStoreTraitsT::store_and_wrap_value(_store, new_entry);\n+    auto iter = _tree.lowerBound(bucket_key);\n+    const bool pre_existed = (iter.valid() && (iter.getKey() == bucket_key));\n+    if (pre_existed) {\n+        DataStoreTraitsT::remove_by_wrapped_value(_store, iter.getData());\n+        // In-place update of value; does not require tree structure modification\n+        std::atomic_thread_fence(std::memory_order_release); // Must ensure visibility when new array ref is observed\n+        iter.writeData(new_value);\n+    } else {\n+        _tree.insert(iter, bucket_key, new_value);\n+    }\n+    commit_tree_changes(); // TODO does publishing a new root imply an implicit memory fence?\n+    return pre_existed;\n+}\n+\n+template <typename DataStoreTraitsT>\n+bool GenericBTreeBucketDatabase<DataStoreTraitsT>::update(const BucketId& bucket,\n+                                                          const ValueType& new_entry)\n+{\n+    return update_by_raw_key(bucket.toKey(), new_entry);\n+}\n+\n+// TODO need snapshot read with guarding\n+// FIXME semantics of for-each in judy and bit tree DBs differ, former expects lbound, latter ubound..!\n+// FIXME but bit-tree code says \"lowerBound\" in impl and \"after\" in declaration???\n+template <typename DataStoreTraitsT>\n+void GenericBTreeBucketDatabase<DataStoreTraitsT>::for_each(EntryProcessor& proc, const BucketId& after) const {\n+    for (auto iter = _tree.upperBound(after.toKey()); iter.valid(); ++iter) {\n+        // TODO memory fencing once we use snapshots!\n+        if (!proc.process(DataStoreTraitsT::unwrap_const_ref_from_key_value(_store, iter.getKey(), iter.getData()))) {\n+            break;\n+        }\n+    }\n+}\n+\n+/*\n+ * Returns the bucket ID which, based on the buckets already existing in the DB,\n+ * is the most specific location in the tree in which it should reside. This may\n+ * or may not be a bucket that already exists.\n+ *\n+ * Example: if there is a single bucket (1, 1) in the tree, a query for (1, 1) or\n+ * (1, 3) will return (1, 1) as that is the most specific leaf in that subtree.\n+ * A query for (1, 0) will return (1, 0) even though this doesn't currently exist,\n+ * as there is no existing bucket that can contain the queried bucket. It is up to\n+ * the caller to create this bucket according to its needs.\n+ *\n+ * Usually this function will be called with an ID whose used-bits is at max (58), in\n+ * order to find a leaf bucket to route an incoming document operation to.\n+ *\n+ * TODO rename this function, it's very much _not_ obvious what an \"appropriate\" bucket is..!\n+ * TODO this should be possible to do concurrently\n+ */\n+template <typename DataStoreTraitsT>\n+BucketId GenericBTreeBucketDatabase<DataStoreTraitsT>::getAppropriateBucket(uint16_t minBits, const BucketId& bid) const {\n+    // The bucket tree is ordered in such a way that it represents a\n+    // natural in-order traversal of all buckets, with inner nodes being\n+    // visited before leaf nodes. This means that a lower bound seek will\n+    // never return a parent of a seeked bucket. The iterator will be pointing\n+    // to a bucket that is either the actual bucket given as the argument to\n+    // lowerBound() or the next in-order bucket (or end() if none exists).\n+    // TODO snapshot\n+    auto iter = _tree.lowerBound(bid.toKey());\n+    if (iter.valid()) {\n+        // Find the first level in the tree where the paths through the bucket tree\n+        // diverge for the target bucket and the current bucket.\n+        minBits = getMinDiffBits(minBits, bucket_from_valid_iterator(iter), bid);\n+    }\n+    // TODO is it better to copy original iterator and do begin() on the copy?\n+    auto first_iter = _tree.begin();\n+    // Original iterator might be in a different subtree than that of our\n+    // target bucket. If possible, rewind one node to discover any parent or\n+    // leftmost sibling of our node. If there's no such node, we'll still\n+    // discover the greatest equal bit prefix.\n+    if (iter != first_iter) {\n+        --iter;\n+        minBits = getMinDiffBits(minBits, bucket_from_valid_iterator(iter), bid);\n+    }\n+    return BucketId(minBits, bid.getRawId());\n+}\n+\n+/*\n+ * Enumerate the number of child subtrees under `bucket`. The value returned is in the\n+ * range [0, 2] regardless of how many subtrees are present further down in the tree.\n+ *\n+ * Finding this number is reasonably straight forward; we construct two buckets that\n+ * represent the key ranges for the left and right subtrees under `bucket` and check\n+ * if there are any ranges in the tree's keyspace that are contained in these.\n+ */\n+template <typename DataStoreTraitsT>\n+uint32_t GenericBTreeBucketDatabase<DataStoreTraitsT>::child_subtree_count(const BucketId& bucket) const {\n+    assert(bucket.getUsedBits() < BucketId::maxNumBits);\n+    BucketId lhs_bucket(bucket.getUsedBits() + 1, bucket.getId());\n+    BucketId rhs_bucket(bucket.getUsedBits() + 1, (1ULL << bucket.getUsedBits()) | bucket.getId());\n+\n+    auto iter = _tree.lowerBound(lhs_bucket.toKey());\n+    if (!iter.valid()) {\n+        return 0;\n+    }\n+    if (lhs_bucket.contains(bucket_from_valid_iterator(iter))) {\n+        iter.seek(rhs_bucket.toKey());\n+        if (!iter.valid()) {\n+            return 1; // lhs subtree only\n+        }\n+        return (rhs_bucket.contains(bucket_from_valid_iterator(iter)) ? 2 : 1);\n+    } else if (rhs_bucket.contains(bucket_from_valid_iterator(iter))) {\n+        return 1; // rhs subtree only\n+    }\n+    return 0;\n+}\n+\n+template <typename DataStoreTraitsT>\n+struct BTreeBuilderMerger final : Merger<typename DataStoreTraitsT::ValueType> {\n+    using DBType           = GenericBTreeBucketDatabase<DataStoreTraitsT>;\n+    using ValueType        = typename DataStoreTraitsT::ValueType;\n+    using BTreeBuilderType = typename DBType::BTree::Builder;\n+\n+    DBType&           _db;\n+    BTreeBuilderType& _builder;\n+    uint64_t          _current_key;\n+    uint64_t          _current_value;\n+    ValueType         _cached_value;\n+    bool              _valid_cached_value;\n+\n+    BTreeBuilderMerger(DBType& db, BTreeBuilderType& builder)\n+        : _db(db),\n+          _builder(builder),\n+          _current_key(0),\n+          _current_value(0),\n+          _cached_value(),\n+          _valid_cached_value(false)\n+    {}\n+    ~BTreeBuilderMerger() override = default;\n+\n+    uint64_t bucket_key() const noexcept override {\n+        return _current_key;\n+    }\n+    BucketId bucket_id() const noexcept override {\n+        return BucketId(BucketId::keyToBucketId(_current_key));\n+    }\n+    ValueType& current_entry() override {\n+        if (!_valid_cached_value) {\n+            _cached_value = DataStoreTraitsT::unwrap_from_key_value(_db.store(), _current_key, _current_value);\n+            _valid_cached_value = true;\n+        }\n+        return _cached_value;\n+    }\n+    void insert_before_current(const BucketId& bucket_id, const ValueType& e) override {\n+        const uint64_t bucket_key = bucket_id.toKey();\n+        assert(bucket_key < _current_key);\n+        const auto new_value = DataStoreTraitsT::store_and_wrap_value(_db.store(), e);\n+        _builder.insert(bucket_key, new_value);\n+    }\n+\n+    void update_iteration_state(uint64_t key, uint64_t value) {\n+        _current_key = key;\n+        _current_value = value;\n+        _valid_cached_value = false;\n+    }\n+};\n+\n+template <typename DataStoreTraitsT>\n+struct BTreeTrailingInserter final : TrailingInserter<typename DataStoreTraitsT::ValueType> {\n+    using DBType           = GenericBTreeBucketDatabase<DataStoreTraitsT>;\n+    using ValueType        = typename DataStoreTraitsT::ValueType;\n+    using BTreeBuilderType = typename DBType::BTree::Builder;\n+\n+    DBType&           _db;\n+    BTreeBuilderType& _builder;\n+\n+    BTreeTrailingInserter(DBType& db, BTreeBuilderType& builder)\n+        : _db(db),\n+          _builder(builder)\n+    {}\n+\n+    ~BTreeTrailingInserter() override = default;\n+\n+    void insert_at_end(const BucketId& bucket_id, const ValueType& e) override {\n+        const uint64_t bucket_key = bucket_id.toKey();\n+        const auto new_value = DataStoreTraitsT::store_and_wrap_value(_db.store(), e);", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NzYwMQ==", "url": "https://github.com/vespa-engine/vespa/pull/13706#discussion_r446247601", "bodyText": "Consider fixing function name (move 'not').", "author": "geirst", "createdAt": "2020-06-26T15:17:34Z", "path": "storage/src/tests/bucketdb/lockablemaptest.cpp", "diffHunk": "@@ -641,24 +703,64 @@ TEST(LockableMapTest, find_all_inconsistent_below_16_bits) {\n     EXPECT_EQ(A(3,4,5), *results[id3.stripUnused()]); // sub bucket\n }\n \n-TEST(LockableMapTest, is_consistent) {\n-    Map map;\n+TYPED_TEST(LockableMapTest, is_consistent) {\n+    TypeParam map;\n     document::BucketId id1(16, 0x00001); // contains id2-id3\n     document::BucketId id2(17, 0x00001);\n \n     bool preExisted;\n     map.insert(id1.stripUnused().toKey(), A(1,2,3), \"foo\", preExisted);\n     {\n-        Map::WrappedEntry entry(\n-                map.get(id1.stripUnused().toKey(), \"foo\", true));\n+        auto entry = map.get(id1.stripUnused().toKey(), \"foo\", true);\n         EXPECT_TRUE(map.isConsistent(entry));\n     }\n     map.insert(id2.stripUnused().toKey(), A(1,2,3), \"foo\", preExisted);\n     {\n-        Map::WrappedEntry entry(\n-                map.get(id1.stripUnused().toKey(), \"foo\", true));\n+        auto entry = map.get(id1.stripUnused().toKey(), \"foo\", true);\n         EXPECT_FALSE(map.isConsistent(entry));\n     }\n }\n \n+TYPED_TEST(LockableMapTest, get_without_auto_create_does_implicitly_not_lock_bucket) {", "originalCommit": "2185218c0bb44843bb0f3d33abea3b116b35160a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "947982bec5303debd8b4ec2b90d9f59df7f64131", "url": "https://github.com/vespa-engine/vespa/commit/947982bec5303debd8b4ec2b90d9f59df7f64131", "message": "Address review comments\n\nAlso rewrite some GMock macros that triggered Valgrind warnings\ndue to default test object printers accessing uninitialized memory.", "committedDate": "2020-06-29T13:01:28Z", "type": "commit"}]}