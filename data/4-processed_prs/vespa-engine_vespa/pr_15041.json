{"pr_number": 15041, "pr_title": "Add unit test for bucket merge with partially filled diff", "pr_createdAt": "2020-10-26T19:38:13Z", "pr_url": "https://github.com/vespa-engine/vespa/pull/15041", "timeline": [{"oid": "243a0602d6d7d6c6cf4ab759fe9f394631f2ba0c", "url": "https://github.com/vespa-engine/vespa/commit/243a0602d6d7d6c6cf4ab759fe9f394631f2ba0c", "message": "Add unit test for bucket merge with partially filled diff from last source only node.", "committedDate": "2020-10-26T19:25:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY0Mjg4MA==", "url": "https://github.com/vespa-engine/vespa/pull/15041#discussion_r512642880", "bodyText": "Consider adding a comment that we only fill the first diff entry here to simulate max chunk size being exceeded when filling on the source node", "author": "vekterli", "createdAt": "2020-10-27T12:14:38Z", "path": "storage/src/tests/persistence/mergehandlertest.cpp", "diffHunk": "@@ -1176,4 +1176,182 @@ TEST_F(MergeHandlerTest, remove_put_on_existing_timestamp) {\n     EXPECT_TRUE(foundTimestamp);\n }\n \n+namespace {\n+\n+storage::api::GetBucketDiffCommand::Entry\n+make_entry(uint64_t timestamp, uint16_t mask) {\n+    storage::api::GetBucketDiffCommand::Entry entry;\n+    entry._timestamp = timestamp;\n+    entry._gid = document::GlobalId();\n+    entry._headerSize = 0;\n+    entry._bodySize = 0;\n+    entry._flags = MergeHandler::StateFlag::IN_USE;\n+    entry._hasMask = mask;\n+    return entry;\n+}\n+\n+void\n+fill_entry(storage::api::ApplyBucketDiffCommand::Entry &e, const document::Document& doc, const document::DocumentTypeRepo &repo)\n+{\n+    e._docName = doc.getId().toString();\n+    vespalib::nbostream stream;\n+    doc.serialize(stream);\n+    e._headerBlob.resize(stream.size());\n+    memcpy(&e._headerBlob[0], stream.peek(), stream.size());\n+    e._repo = &repo;\n+}\n+\n+/*\n+ * Helper class to check both timestamp and mask at once.\n+ */\n+struct EntryCheck\n+{\n+    uint64_t _timestamp;\n+    uint16_t _hasMask;\n+\n+    EntryCheck(uint64_t timestamp, uint16_t hasMask)\n+        : _timestamp(timestamp),\n+          _hasMask(hasMask)\n+    {\n+    }\n+    bool operator==(const api::GetBucketDiffCommand::Entry &rhs) const {\n+        return _timestamp == rhs._timestamp && _hasMask == rhs._hasMask;\n+    }\n+};\n+\n+std::ostream &operator<<(std::ostream &os, const EntryCheck &entry)\n+{\n+    os << \"EntryCheck(timestamp=\" << entry._timestamp << \", hasMask=\" << entry._hasMask << \")\";\n+    return os;\n+}\n+\n+}\n+\n+namespace api {\n+\n+std::ostream &operator<<(std::ostream &os, const MergeBucketCommand::Node &node)\n+{\n+    os << \"Node(\" << node.index << \",\" << (node.sourceOnly ? \"true\" : \"false\") << \")\";\n+    return os;\n+}\n+\n+std::ostream &operator<<(std::ostream &os, const GetBucketDiffCommand::Entry &entry)\n+{\n+    os << \"Entry(timestamp=\" << entry._timestamp << \", hasMask=\" << entry._hasMask << \")\";\n+    return os;\n+}\n+\n+}\n+\n+TEST_F(MergeHandlerTest, partially_filled_apply_bucket_diff_reply)\n+{\n+    using NodeList = decltype(_nodes);\n+    // Redundancy is 2 and source only nodes 3 and 4 have doc1 and doc2\n+    _nodes.clear();\n+    _nodes.emplace_back(0, false);\n+    _nodes.emplace_back(1, false);\n+    _nodes.emplace_back(2, true);\n+    _nodes.emplace_back(3, true);\n+    _nodes.emplace_back(4, true);\n+    _maxTimestamp = 30000;  // Extend timestamp range to include doc1 and doc2\n+\n+    auto doc1 = _env->_testDocMan.createRandomDocumentAtLocation(_location, 1);\n+    auto doc2 = _env->_testDocMan.createRandomDocumentAtLocation(_location, 2);\n+    \n+    MergeHandler handler = createHandler();\n+    auto cmd = std::make_shared<api::MergeBucketCommand>(_bucket, _nodes, _maxTimestamp);\n+    cmd->setSourceIndex(1234);\n+    MessageTracker::UP tracker = handler.handleMergeBucket(*cmd, createTracker(cmd, _bucket));\n+    ASSERT_EQ(1u, messageKeeper()._msgs.size());\n+    ASSERT_EQ(api::MessageType::GETBUCKETDIFF, messageKeeper()._msgs[0]->getType());\n+    size_t baseline_diff_size = 0;\n+    {\n+        LOG(debug, \"checking GetBucketDiff command\");\n+        auto& cmd2 = dynamic_cast<api::GetBucketDiffCommand&>(*messageKeeper()._msgs[0]);\n+        EXPECT_THAT(_nodes, ContainerEq(cmd2.getNodes()));\n+        EXPECT_EQ(1, cmd2.getAddress()->getIndex());\n+        EXPECT_EQ(1234, cmd2.getSourceIndex());\n+        EXPECT_TRUE(getEnv()._fileStorHandler.isMerging(_bucket));\n+        auto &s = getEnv()._fileStorHandler.editMergeStatus(_bucket);\n+        EXPECT_EQ((NodeList{{0, false}, {1, false}, {2, true}, {3, true}, {4, true}}), s.nodeList);\n+        baseline_diff_size = cmd2.getDiff().size();\n+        auto reply = std::make_unique<api::GetBucketDiffReply>(cmd2);\n+        auto &diff = reply->getDiff();\n+        // doc1 and doc2 is present on nodes 3 and 4.\n+        diff.push_back(make_entry(20000, ((1 << 3) | (1 << 4))));\n+        diff.push_back(make_entry(20100, ((1 << 3) | (1 << 4))));\n+        EXPECT_EQ(baseline_diff_size + 2u, reply->getDiff().size());\n+        handler.handleGetBucketDiffReply(*reply, messageKeeper());\n+        LOG(debug, \"sent handleGetBucketDiffReply\");\n+    }\n+    ASSERT_EQ(2u, messageKeeper()._msgs.size());\n+    ASSERT_EQ(api::MessageType::APPLYBUCKETDIFF, messageKeeper()._msgs[1]->getType());\n+    {\n+        LOG(debug, \"checking first ApplyBucketDiff command\");\n+        EXPECT_TRUE(getEnv()._fileStorHandler.isMerging(_bucket));\n+        auto &s = getEnv()._fileStorHandler.editMergeStatus(_bucket);\n+        // Node 4 has been eliminated before the first ApplyBucketDiff command\n+        EXPECT_EQ((NodeList{{0, false}, {1, false}, {2, true}, {3, true}}), s.nodeList);\n+        EXPECT_EQ(baseline_diff_size + 2u, s.diff.size());\n+        EXPECT_EQ(EntryCheck(20000, 8u), s.diff[baseline_diff_size]);\n+        EXPECT_EQ(EntryCheck(20100, 8u), s.diff[baseline_diff_size + 1]);\n+        auto& cmd3 = dynamic_cast<api::ApplyBucketDiffCommand&>(*messageKeeper()._msgs[1]);\n+        // ApplyBucketDiffCommand has a shorter node list, node 2 is not present\n+        EXPECT_EQ((NodeList{{0, false}, {1, false}, {3, true}}), cmd3.getNodes());\n+        auto reply = std::make_unique<api::ApplyBucketDiffReply>(cmd3);\n+        auto& diff = reply->getDiff();\n+        EXPECT_EQ(2u, diff.size());\n+        EXPECT_EQ(EntryCheck(20000u, 4u), diff[0]._entry);\n+        EXPECT_EQ(EntryCheck(20100u, 4u), diff[1]._entry);\n+        fill_entry(diff[0], *doc1, getEnv().getDocumentTypeRepo());\n+        diff[0]._entry._hasMask |= 2u;", "originalCommit": "243a0602d6d7d6c6cf4ab759fe9f394631f2ba0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4MTAxMQ==", "url": "https://github.com/vespa-engine/vespa/pull/15041#discussion_r512681011", "bodyText": "Consider adding a function comment that clarifies that this is the per-request mask, not the global mask (and maybe also why that is why we need to additionally provide the original node set)", "author": "vekterli", "createdAt": "2020-10-27T13:15:46Z", "path": "storage/src/vespa/storage/persistence/filestorage/mergestatus.cpp", "diffHunk": "@@ -21,12 +80,14 @@ MergeStatus::~MergeStatus() = default;\n bool\n MergeStatus::removeFromDiff(\n         const std::vector<api::ApplyBucketDiffCommand::Entry>& part,\n-        uint16_t hasMask)\n+        uint16_t hasMask,", "originalCommit": "243a0602d6d7d6c6cf4ab759fe9f394631f2ba0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4MTI0MQ==", "url": "https://github.com/vespa-engine/vespa/pull/15041#discussion_r512681241", "bodyText": "Consider adding a comment on why we have to remap the masks here", "author": "vekterli", "createdAt": "2020-10-27T13:16:04Z", "path": "storage/src/vespa/storage/persistence/filestorage/mergestatus.cpp", "diffHunk": "@@ -62,11 +123,14 @@ MergeStatus::removeFromDiff(\n                 }\n                 it = diff.erase(it);\n                 altered = true;\n-            } else if (it2->_entry._hasMask != it->_hasMask) {\n-                // Hasmasks have changed, meaning bucket contents changed on\n-                // one or more of the nodes during merging.\n-                altered = true;\n-                it->_hasMask = it2->_entry._hasMask;\n+            } else {\n+                uint16_t mask = remap_mask(it2->_entry._hasMask);", "originalCommit": "243a0602d6d7d6c6cf4ab759fe9f394631f2ba0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "91eb5d5e3a8ab31803569e0e5d68ff1d217d3853", "url": "https://github.com/vespa-engine/vespa/commit/91eb5d5e3a8ab31803569e0e5d68ff1d217d3853", "message": "Add comments about per-reply masks.", "committedDate": "2020-10-27T14:16:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjgzMTUxNg==", "url": "https://github.com/vespa-engine/vespa/pull/15041#discussion_r512831516", "bodyText": "@vekterli and @toregge : Consider moving this out of the cpp file and then creating explicit unit tests for it.", "author": "geirst", "createdAt": "2020-10-27T16:14:52Z", "path": "storage/src/vespa/storage/persistence/filestorage/mergestatus.cpp", "diffHunk": "@@ -3,11 +3,70 @@\n #include \"mergestatus.h\"\n #include <ostream>\n #include <vespa/log/log.h>\n+#include <vespa/vespalib/stllike/hash_map.h>\n+#include <cassert>\n \n LOG_SETUP(\".mergestatus\");\n \n namespace storage {\n \n+namespace {\n+\n+/*\n+ * Class for remapping bit masks from a partial set of nodes to a full\n+ * set of nodes.\n+ */\n+class MaskRemapper", "originalCommit": "91eb5d5e3a8ab31803569e0e5d68ff1d217d3853", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}