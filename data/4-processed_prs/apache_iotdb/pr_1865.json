{"pr_number": 1865, "pr_title": "[IOTDB-915]Add raft log persist mechanism and use persist log to catch up", "pr_createdAt": "2020-10-26T11:34:21Z", "pr_url": "https://github.com/apache/iotdb/pull/1865", "timeline": [{"oid": "3382d6b943c4c77dc49babb68d724b5b18f9bb36", "url": "https://github.com/apache/iotdb/commit/3382d6b943c4c77dc49babb68d724b5b18f9bb36", "message": "add raft log mechanism and use persist log to catch up", "committedDate": "2020-10-27T12:26:26Z", "type": "commit"}, {"oid": "b7cf81a002bafb87ca22471b8b04075ccff3b0cf", "url": "https://github.com/apache/iotdb/commit/b7cf81a002bafb87ca22471b8b04075ccff3b0cf", "message": "add test", "committedDate": "2020-10-27T12:26:26Z", "type": "commit"}, {"oid": "7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "url": "https://github.com/apache/iotdb/commit/7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "message": "add tests", "committedDate": "2020-10-27T12:26:26Z", "type": "commit"}, {"oid": "7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "url": "https://github.com/apache/iotdb/commit/7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "message": "add tests", "committedDate": "2020-10-27T12:26:26Z", "type": "forcePushed"}, {"oid": "851ef636efbdece6da8e610f718f95bb3f18f577", "url": "https://github.com/apache/iotdb/commit/851ef636efbdece6da8e610f718f95bb3f18f577", "message": "add use persist log to snapshot", "committedDate": "2020-10-27T12:58:52Z", "type": "commit"}, {"oid": "40b08e18fe45d78eaf750e119464698bfaa8b4bb", "url": "https://github.com/apache/iotdb/commit/40b08e18fe45d78eaf750e119464698bfaa8b4bb", "message": "remove useless lines", "committedDate": "2020-10-28T01:41:49Z", "type": "commit"}, {"oid": "7949b1d5c7e8d2e14d64935858b8d90f78b2c847", "url": "https://github.com/apache/iotdb/commit/7949b1d5c7e8d2e14d64935858b8d90f78b2c847", "message": "fix fristLogIndex bug", "committedDate": "2020-10-28T03:50:44Z", "type": "commit"}, {"oid": "6b83b80af4b0621269e5f4ec0fbd48806836b57b", "url": "https://github.com/apache/iotdb/commit/6b83b80af4b0621269e5f4ec0fbd48806836b57b", "message": "add switch of whether use persist logs on disk to catchup", "committedDate": "2020-10-28T04:01:39Z", "type": "commit"}, {"oid": "106afb2d03a113ff516811a802aa6a4395b69ee2", "url": "https://github.com/apache/iotdb/commit/106afb2d03a113ff516811a802aa6a4395b69ee2", "message": "fix log manager print messags", "committedDate": "2020-10-28T05:18:26Z", "type": "commit"}, {"oid": "b66f25dc47edcde947157629408c845ae7678151", "url": "https://github.com/apache/iotdb/commit/b66f25dc47edcde947157629408c845ae7678151", "message": "add getLogs UT", "committedDate": "2020-10-28T11:21:28Z", "type": "commit"}, {"oid": "8779750209db47695e176bae9f2e7bae210c4759", "url": "https://github.com/apache/iotdb/commit/8779750209db47695e176bae9f2e7bae210c4759", "message": "change forceRaftLogPeriodInMS to 1000", "committedDate": "2020-10-28T11:55:24Z", "type": "commit"}, {"oid": "482e36bab7b37fadb0350433262ae186c7578daa", "url": "https://github.com/apache/iotdb/commit/482e36bab7b37fadb0350433262ae186c7578daa", "message": "use persist log when getTerm not found in memory", "committedDate": "2020-10-28T13:01:15Z", "type": "commit"}, {"oid": "29de7ef7ce1b2e924fa278274083c8aeeef3d804", "url": "https://github.com/apache/iotdb/commit/29de7ef7ce1b2e924fa278274083c8aeeef3d804", "message": "add recovery log data file", "committedDate": "2020-10-29T05:36:06Z", "type": "commit"}, {"oid": "bb5ba169a5262fecff40cd79cd26d1bb5b2599ab", "url": "https://github.com/apache/iotdb/commit/bb5ba169a5262fecff40cd79cd26d1bb5b2599ab", "message": "revert the test log level to warn", "committedDate": "2020-10-29T08:24:47Z", "type": "commit"}, {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95", "url": "https://github.com/apache/iotdb/commit/2463c422b62d9e6f728cbaef67214845bd792c95", "message": "fix get log term", "committedDate": "2020-10-29T12:20:08Z", "type": "commit"}, {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "url": "https://github.com/apache/iotdb/commit/30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "message": "fix ut on windows", "committedDate": "2020-10-30T06:27:00Z", "type": "commit"}, {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "url": "https://github.com/apache/iotdb/commit/9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "message": "fix ut", "committedDate": "2020-10-30T09:03:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyNDQ1Nw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514724457", "bodyText": "It's better to add a default comment.such as 'default: 1G'", "author": "LebronAl", "createdAt": "2020-10-30T02:30:57Z", "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group", "originalCommit": "2463c422b62d9e6f728cbaef67214845bd792c95", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAzMjI4OQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515032289", "bodyText": "sure", "author": "neuyilan", "createdAt": "2020-10-30T11:29:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyNDQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyNDkyOA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514724928", "bodyText": "Please fix typo", "author": "LebronAl", "createdAt": "2020-10-30T02:31:28Z", "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group\n+max_raft_log_persist_data_size_per_file=1073741824\n+\n+# The maximum number of persistent raft log files on disk per raft group, So each raft group's", "originalCommit": "2463c422b62d9e6f728cbaef67214845bd792c95", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyOTU2NA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514729564", "bodyText": "same as above", "author": "LebronAl", "createdAt": "2020-10-30T02:36:26Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/config/ClusterConfig.java", "diffHunk": "@@ -127,6 +127,32 @@\n \n   private int pullSnapshotRetryIntervalMs = 5 * 1000;\n \n+  /**\n+   * The maximum value of the raft log index stored in the memory per raft group, These indexes are\n+   * used to index the location of the log on the disk\n+   */\n+  private int maxRaftLogIndexSizeInMemory = 10000;\n+\n+  /**\n+   * The maximum value of the raft log persisted on disk per file(in byte) per raft group", "originalCommit": "2463c422b62d9e6f728cbaef67214845bd792c95", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3Mzc4Mg==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514773782", "bodyText": "I thought these code should be in the function maybeTerm in committedEntryManager, which should manager all persisted entries, it will throw a EntryCompactedException or get log from disk if isEnableRaftLogPersistence  is enabled.\nBTW, I'm a little confused about whether we should get log from disk in function getTerm,If so, then maybe we should change isEnableUsePersistLogOnDiskToCatchUp's name", "author": "LebronAl", "createdAt": "2020-10-30T03:24:11Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/RaftLogManager.java", "diffHunk": "@@ -292,15 +292,29 @@ public long getLastLogIndex() {\n   public long getTerm(long index) throws EntryCompactedException {\n     long dummyIndex = getFirstIndex() - 1;\n     if (index < dummyIndex) {\n+      // search in disk", "originalCommit": "2463c422b62d9e6f728cbaef67214845bd792c95", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU3MjQwOA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515572408", "bodyText": "if we manage all the persisted entries(committed or persisted) on the committedEntryManager, it's better to move the code in committedEntryManager. but now, the committedEntryManager only manager the commit log in memory. and the LogManager  managers the UnCommittedEntryManager CommittedEntryManager and StableEntryManager, I think CommittedEntryManager is as logs in memory,  StableEntryManager is as logs in the disk,  so I think it's ok to leave the code here, the log manager knows where to get the logs\nbesides, I argue with you that, when get term, we should not care the isEnableUsePersistLogOnDiskToCatchUp  parpermeter, if isEnableRaftLogPersistence is enabled is ok", "author": "neuyilan", "createdAt": "2020-11-01T04:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3Mzc4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYyMzE3MA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515623170", "bodyText": "OK.As the logmanager has been given more functionality, I will try to merge UnCommittedEntryManager and CommittedEntryManager to a MemoryLogmanager to avoid redundant entry shift laterly.\nI'm OK with here~", "author": "LebronAl", "createdAt": "2020-11-01T13:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3Mzc4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyOTgxNA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514929814", "bodyText": "change the logSizeDeque", "author": "LebronAl", "createdAt": "2020-10-30T08:11:14Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;\n+\n+  private static final int logDeleteCheckIntervalSecond = 1;\n \n   /**\n    * the lock uses when change the logSizeDeque", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkzNzIyNg==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514937226", "bodyText": "It seems that this function will return empty list when maxHaveAppliedCommitIndex == commitLogIndex ,So how can we handle redo log when restart as we have not merged wal and raft logs?", "author": "LebronAl", "createdAt": "2020-10-30T08:27:21Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -160,31 +216,28 @@ public LogManagerMeta getMeta() {\n    * Recover all the logs in disk. This function will be called once this instance is created.\n    */\n   @Override\n-  public List<Log> getAllEntries() {\n-    List<Log> logs = recoverLog();\n-    int size = logs.size();\n-    if (size != 0 && meta.getLastLogIndex() <= logs.get(size - 1).getCurrLogIndex()) {\n-      meta.setLastLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setLastLogIndex(logs.get(size - 1).getCurrLogIndex());\n-      meta.setCommitLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setCommitLogIndex(logs.get(size - 1).getCurrLogIndex());\n+  public List<Log> getAllEntriesBeforeAppliedIndex() {", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MzM0Nw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514953347", "bodyText": "It seems all the callers(close (),checkCloseCurrentFile())  to this function's has got the writelock?", "author": "LebronAl", "createdAt": "2020-10-30T08:59:28Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -203,47 +256,113 @@ public void append(List<Log> entries) throws IOException {\n    */\n   private void putLogs(List<Log> entries) {\n     for (Log log : entries) {\n-      logBuffer.mark();\n+      logDataBuffer.mark();\n+      logIndexBuffer.mark();\n       ByteBuffer logData = log.serialize();\n       int size = logData.capacity() + Integer.BYTES;\n       try {\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n       } catch (BufferOverflowException e) {\n         logger.info(\"Raft log buffer overflow!\");\n-        logBuffer.reset();\n+        logDataBuffer.reset();\n+        logIndexBuffer.reset();\n         flushLogBuffer();\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        checkCloseCurrentFile(log.getCurrLogIndex() - 1);\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n+      }\n+    }\n+  }\n+\n+  private void checkCloseCurrentFile(long commitIndex) {\n+    if (offsetOfTheCurrentLogDataOutputStream > maxRaftLogPersistDataSizePerFile) {\n+      try {\n+        closeCurrentFile(commitIndex);\n+        serializeMeta(meta);\n+        createNewLogFile(logDir, commitIndex + 1);\n+      } catch (IOException e) {\n+        logger.error(\"check close current file failed\", e);\n       }\n     }\n   }\n \n+  private void closeCurrentFile(long commitIndex) throws IOException {\n+    lock.writeLock().lock();", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA0MTQ3OQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515041479", "bodyText": "sure, I will remove the lock here", "author": "neuyilan", "createdAt": "2020-10-30T11:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MzM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDkyNw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514960927", "bodyText": "I doubt if this parameter is necessary, given that you have already prevented storage abuse by taking two parameters: max_number_of_persist_raft_log_files and max_raft_log_persist_data_size_per_file", "author": "LebronAl", "createdAt": "2020-10-30T09:14:43Z", "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group\n+max_raft_log_persist_data_size_per_file=1073741824\n+\n+# The maximum number of persistent raft log files on disk per raft group, So each raft group's\n+# So each raft group's log takes up disk space approximately equals\n+# max_raft_log_persist_data_size_per_file*max_number_of_persist_raft_log_files\n+max_number_of_persist_raft_log_files=5\n+\n+# The maximum number of logs saved on the disk\n+max_persist_raft_log_number_on_disk=1000000", "originalCommit": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2NzIyNQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515567225", "bodyText": "This parameter is used in this case: a log is small, and the number of logs a file holds is greater than this value, so in this case, only one log is needed", "author": "neuyilan", "createdAt": "2020-11-01T02:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2NjY2OA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514966668", "bodyText": "It seems that the max_unsnapshoted_log_size can be deleted now", "author": "LebronAl", "createdAt": "2020-10-30T09:25:52Z", "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n ", "originalCommit": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MDUzMQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514970531", "bodyText": "As the function checkDeletePersistRaftLog  will get the write lock, I doubt whether 1 is too small?", "author": "LebronAl", "createdAt": "2020-10-30T09:32:41Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;\n+\n+  private static final int logDeleteCheckIntervalSecond = 1;", "originalCommit": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA0NTM0Ng==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515045346", "bodyText": "sure, the default log size is 1gb, the disk write speed approximately 50MB/s, so I think 5 second is enough", "author": "neuyilan", "createdAt": "2020-10-30T11:54:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MDUzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ1NTYxOA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515455618", "bodyText": "Agree with this", "author": "LebronAl", "createdAt": "2020-10-31T04:49:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MDUzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDg2Mg==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514974862", "bodyText": "As you have used logIndexOffsetList to record offset in putLog, I doubt whether it's necessary to maintain this buffer in putLog.Maybe You can generate index buffer in flushLogBuffer .", "author": "LebronAl", "createdAt": "2020-10-30T09:40:25Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer", "originalCommit": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2NzYxMw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515567613", "bodyText": "I designed it on purpose. If you set a temporary buffer, GC will occur. I think it is better to set a buffer that is not GC?", "author": "neuyilan", "createdAt": "2020-11-01T02:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYyMTkzNw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515621937", "bodyText": "OK~", "author": "LebronAl", "createdAt": "2020-11-01T13:23:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4MTg1MA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514981850", "bodyText": "As currently the  default maxSize of each data file is 1G,it's seems a Integer is able to record the offset.", "author": "LebronAl", "createdAt": "2020-10-30T09:52:52Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;", "originalCommit": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA1ODkwMw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515058903", "bodyText": "sure, but the parameter of the max size of the log data file is left to the user to set, so if the user config larger than an integer, it would occur errors, or we could limit the range of user configuration for the maxSize parameter.", "author": "neuyilan", "createdAt": "2020-10-30T12:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4MTg1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ1NTQ2Mw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515455463", "bodyText": "OK, Just let it go~Long is OK for me ~", "author": "LebronAl", "createdAt": "2020-10-31T04:47:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4MTg1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk0OTcyMQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514949721", "bodyText": "Replace request.toString() with simply request.", "author": "jt2594838", "createdAt": "2020-10-30T08:52:43Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/catchup/LogCatchUpTask.java", "diffHunk": "@@ -191,12 +191,11 @@ private AppendEntriesRequest prepareRequest(List<ByteBuffer> logList, int startP\n         logger.error(\"getTerm failed for newly append entries\", e);\n       }\n     }\n+    logger.debug(\"{}, node={} catchup request={}\", raftMember.getName(), node, request.toString());", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1Mjk4OA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514952988", "bodyText": "It seems more like getAllEntriesAfterAppliedIndex instead of  getAllEntriesBeforeAppliedIndex.", "author": "jt2594838", "createdAt": "2020-10-30T08:58:49Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -160,31 +216,28 @@ public LogManagerMeta getMeta() {\n    * Recover all the logs in disk. This function will be called once this instance is created.\n    */\n   @Override\n-  public List<Log> getAllEntries() {\n-    List<Log> logs = recoverLog();\n-    int size = logs.size();\n-    if (size != 0 && meta.getLastLogIndex() <= logs.get(size - 1).getCurrLogIndex()) {\n-      meta.setLastLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setLastLogIndex(logs.get(size - 1).getCurrLogIndex());\n-      meta.setCommitLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setCommitLogIndex(logs.get(size - 1).getCurrLogIndex());\n+  public List<Log> getAllEntriesBeforeAppliedIndex() {\n+    logger.debug(\"getAllEntriesBeforeAppliedIndex, maxHaveAppliedCommitIndex={}, commitLogIndex={}\",\n+        meta.getMaxHaveAppliedCommitIndex(), meta.getCommitLogIndex());\n+    if (meta.getMaxHaveAppliedCommitIndex() >= meta.getCommitLogIndex()) {\n+      return Collections.emptyList();\n     }\n-    return logs;\n+    return getLogs(meta.getMaxHaveAppliedCommitIndex(), meta.getCommitLogIndex());", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA2NjgzMQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515066831", "bodyText": "Sure, thanks", "author": "neuyilan", "createdAt": "2020-10-30T12:35:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1Mjk4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDU3OQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514960579", "bodyText": "Maybe list.set() is enough for this.", "author": "jt2594838", "createdAt": "2020-10-30T09:14:10Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -203,47 +256,113 @@ public void append(List<Log> entries) throws IOException {\n    */\n   private void putLogs(List<Log> entries) {\n     for (Log log : entries) {\n-      logBuffer.mark();\n+      logDataBuffer.mark();\n+      logIndexBuffer.mark();\n       ByteBuffer logData = log.serialize();\n       int size = logData.capacity() + Integer.BYTES;\n       try {\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n       } catch (BufferOverflowException e) {\n         logger.info(\"Raft log buffer overflow!\");\n-        logBuffer.reset();\n+        logDataBuffer.reset();\n+        logIndexBuffer.reset();\n         flushLogBuffer();\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        checkCloseCurrentFile(log.getCurrLogIndex() - 1);\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n+      }\n+    }\n+  }\n+\n+  private void checkCloseCurrentFile(long commitIndex) {\n+    if (offsetOfTheCurrentLogDataOutputStream > maxRaftLogPersistDataSizePerFile) {\n+      try {\n+        closeCurrentFile(commitIndex);\n+        serializeMeta(meta);\n+        createNewLogFile(logDir, commitIndex + 1);\n+      } catch (IOException e) {\n+        logger.error(\"check close current file failed\", e);\n       }\n     }\n   }\n \n+  private void closeCurrentFile(long commitIndex) throws IOException {\n+    lock.writeLock().lock();\n+    try {\n+      if (currentLogDataOutputStream != null) {\n+        currentLogDataOutputStream.close();\n+        currentLogDataOutputStream = null;\n+      }\n+\n+      if (currentLogIndexOutputStream != null) {\n+        currentLogIndexOutputStream.close();\n+        currentLogIndexOutputStream = null;\n+      }\n+      File currentLogDataFile = getCurrentLogDataFile();\n+      String newDataFileName = currentLogDataFile.getName()\n+          .replaceAll(String.valueOf(Long.MAX_VALUE), String.valueOf(commitIndex));\n+      File newCurrentLogDatFile = SystemFileFactory.INSTANCE\n+          .getFile(currentLogDataFile.getParent() + File.separator + newDataFileName);\n+      if (!currentLogDataFile.renameTo(newCurrentLogDatFile)) {\n+        logger.error(\"rename log data file={} failed\", currentLogDataFile.getAbsoluteFile());\n+      }\n+      logDataFileList.remove(logDataFileList.size() - 1);\n+      logDataFileList.add(newCurrentLogDatFile);", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA2NzAyNQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515067025", "bodyText": "Got it", "author": "neuyilan", "createdAt": "2020-10-30T12:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2NDM2Mw==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514964363", "bodyText": "Is it really startTime? I think it should more clear here.", "author": "jt2594838", "createdAt": "2020-10-30T09:21:30Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA3MDMwMQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515070301", "bodyText": "its typo, i'll fix it", "author": "neuyilan", "createdAt": "2020-10-30T12:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2NDM2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MTM2Mg==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514971362", "bodyText": "Better to use buffered stream.", "author": "jt2594838", "createdAt": "2020-10-30T09:34:10Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -349,9 +495,136 @@ private void checkLogFile(File file) {\n       } catch (IOException e) {\n         logger.warn(\"Cannot delete outdated log file {}\", file);\n       }\n+      return false;\n+    }\n+\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    // start index should be smaller than end index\n+    if (Long.parseLong(splits[0]) > Long.parseLong(splits[1])) {\n+      try {\n+        Files.delete(file.toPath());\n+      } catch (IOException e) {\n+        logger.warn(\"Cannot delete incorrect log file {}\", file);\n+      }\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private void recoverTheLastLogFile() {\n+    if (logIndexFileList.isEmpty()) {\n+      logger.info(\"no log index file to recover\");\n+      return;\n+    }\n+\n+    File lastIndexFile = logIndexFileList.get(logIndexFileList.size() - 1);\n+    long endIndex = Long.parseLong(lastIndexFile.getName().split(FILE_NAME_SEPARATOR)[1]);\n+    boolean success = true;\n+    if (endIndex != Long.MAX_VALUE) {\n+      logger.info(\"last log index file={} no need to recover\", lastIndexFile.getAbsoluteFile());\n+    } else {\n+      success = recoverTheLastLogIndexFile(lastIndexFile);\n+    }\n+\n+    if (!success) {\n+      logger.error(\"recover log index file failed, clear all logs in disk, {}\",\n+          lastIndexFile.getAbsoluteFile());\n+      for (int i = 0; i < logIndexFileList.size(); i++) {\n+        deleteLogDataAndIndexFile(i);\n+      }\n+      clearFirstLogIndex();\n+\n+      return;\n+    }\n+\n+    File lastDataFile = logDataFileList.get(logDataFileList.size() - 1);\n+    endIndex = Long.parseLong(lastDataFile.getName().split(FILE_NAME_SEPARATOR)[1]);\n+    if (endIndex != Long.MAX_VALUE) {\n+      logger.info(\"last log data file={} no need to recover\", lastDataFile.getAbsoluteFile());\n+      return;\n+    }\n+\n+    success = recoverTheLastLogDataFile(logDataFileList.get(logDataFileList.size() - 1));\n+    if (!success) {\n+      logger.error(\"recover log data file failed, clear all logs in disk,{}\",\n+          lastDataFile.getAbsoluteFile());\n+      for (int i = 0; i < logIndexFileList.size(); i++) {\n+        deleteLogDataAndIndexFile(i);\n+      }\n+      clearFirstLogIndex();\n+    }\n+  }\n+\n+  private boolean recoverTheLastLogDataFile(File file) {\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    long startIndex = Long.parseLong(splits[0]);\n+    Pair<File, Pair<Long, Long>> fileStartAndEndIndex = getLogIndexFile(startIndex);\n+    if (fileStartAndEndIndex.right.left == startIndex) {\n+      long endIndex = fileStartAndEndIndex.right.right;\n+      String newDataFileName = file.getName()\n+          .replaceAll(String.valueOf(Long.MAX_VALUE), String.valueOf(endIndex));\n+      File newLogDataFile = SystemFileFactory.INSTANCE\n+          .getFile(file.getParent() + File.separator + newDataFileName);\n+      if (!file.renameTo(newLogDataFile)) {\n+        logger.error(\"rename log data file={} failed when recover\", file.getAbsoluteFile());\n+      }\n+      logDataFileList.remove(logDataFileList.size() - 1);\n+      logDataFileList.add(newLogDataFile);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  private boolean recoverTheLastLogIndexFile(File file) {\n+    logger.debug(\"start to recover the last log index file={}\", file.getAbsoluteFile());\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    long startIndex = Long.parseLong(splits[0]);\n+    int longLength = 8;\n+    byte[] bytes = new byte[longLength];\n+\n+    int totalCount = 0;\n+    long offset = 0;\n+    try (FileInputStream inputStream = new FileInputStream(file)) {", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDU2OQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514974569", "bodyText": "If maxPersistRaftLogNumberOnDisk is too small, is it possible that the current file will be deleted here?", "author": "jt2594838", "createdAt": "2020-10-30T09:39:55Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -593,75 +815,385 @@ public void close() {\n   }\n \n   /**\n-   * adjust maxRemovedLogSize to the first log file\n+   * get file version from file The file name structure is as follows\uff1a\n+   * {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file file\n+   * @return version from file\n+   */\n+  private long getFileVersion(File file) {\n+    return Long.parseLong(file.getName().split(FILE_NAME_SEPARATOR)[2]);\n+  }\n+\n+  public void checkDeletePersistRaftLog() {\n+    // 1. check the log index offset list size\n+    try {\n+      lock.writeLock().lock();\n+      if (logIndexOffsetList.size() > maxRaftLogIndexSizeInMemory) {\n+        int compactIndex = logIndexOffsetList.size() - maxRaftLogIndexSizeInMemory;\n+        logIndexOffsetList.subList(0, compactIndex).clear();\n+        firstLogIndex += compactIndex;\n+      }\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+\n+    // 2. check the persist log file number\n+    while (logDataFileList.size() > maxNumberOfPersistRaftLogFiles) {\n+      deleteLogDataAndIndexFile(0);\n+    }\n+\n+    // 3. check the persist log index number\n+    while (!logDataFileList.isEmpty()) {\n+      File firstFile = logDataFileList.get(0);\n+      String[] splits = firstFile.getName().split(FILE_NAME_SEPARATOR);\n+      if (meta.getCommitLogIndex() - Long.parseLong(splits[1]) > maxPersistRaftLogNumberOnDisk) {\n+        deleteLogDataAndIndexFile(0);\n+      } else {\n+        return;\n+      }\n+    }", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2NjMyNQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515566325", "bodyText": "Thank you for reminding me that this could happen. I'll fix it", "author": "neuyilan", "createdAt": "2020-11-01T02:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NjE3MQ==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514976171", "bodyText": "I think we should enforce a limit on this method to avoid out ot memory when the range is too long.", "author": "jt2594838", "createdAt": "2020-10-30T09:42:46Z", "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -593,75 +815,385 @@ public void close() {\n   }\n \n   /**\n-   * adjust maxRemovedLogSize to the first log file\n+   * get file version from file The file name structure is as follows\uff1a\n+   * {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file file\n+   * @return version from file\n+   */\n+  private long getFileVersion(File file) {\n+    return Long.parseLong(file.getName().split(FILE_NAME_SEPARATOR)[2]);\n+  }\n+\n+  public void checkDeletePersistRaftLog() {\n+    // 1. check the log index offset list size\n+    try {\n+      lock.writeLock().lock();\n+      if (logIndexOffsetList.size() > maxRaftLogIndexSizeInMemory) {\n+        int compactIndex = logIndexOffsetList.size() - maxRaftLogIndexSizeInMemory;\n+        logIndexOffsetList.subList(0, compactIndex).clear();\n+        firstLogIndex += compactIndex;\n+      }\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+\n+    // 2. check the persist log file number\n+    while (logDataFileList.size() > maxNumberOfPersistRaftLogFiles) {\n+      deleteLogDataAndIndexFile(0);\n+    }\n+\n+    // 3. check the persist log index number\n+    while (!logDataFileList.isEmpty()) {\n+      File firstFile = logDataFileList.get(0);\n+      String[] splits = firstFile.getName().split(FILE_NAME_SEPARATOR);\n+      if (meta.getCommitLogIndex() - Long.parseLong(splits[1]) > maxPersistRaftLogNumberOnDisk) {\n+        deleteLogDataAndIndexFile(0);\n+      } else {\n+        return;\n+      }\n+    }\n+  }\n+\n+  private void deleteLogDataAndIndexFile(int index) {\n+    File logDataFile = null;\n+    File logIndexFile = null;\n+    try {\n+      lock.writeLock().lock();\n+      logDataFile = logDataFileList.get(index);\n+      logIndexFile = logIndexFileList.get(index);\n+      Files.delete(logDataFile.toPath());\n+      Files.delete(logIndexFile.toPath());\n+      logDataFileList.remove(index);\n+      logIndexFileList.remove(index);\n+      logger.debug(\"delete date file={}, index file={}\", logDataFile.getAbsoluteFile(),\n+          logIndexFile.getAbsoluteFile());\n+    } catch (IOException e) {\n+      logger.error(\"delete file failed, index={}, data file={}, index file={}\", index,\n+          logDataFile == null ? null : logDataFile.getAbsoluteFile(),\n+          logIndexFile == null ? null : logIndexFile.getAbsoluteFile());\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * The file name structure is as follows\uff1a {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file1 File to compare\n+   * @param file2 File to compare\n    */\n-  private void adjustNextThreshold() {\n-    if (!logFileList.isEmpty()) {\n-      maxRemovedLogSize = logFileList.get(0).length();\n+  private int comparePersistLogFileName(File file1, File file2) {\n+    String[] items1 = file1.getName().split(FILE_NAME_SEPARATOR);\n+    String[] items2 = file2.getName().split(FILE_NAME_SEPARATOR);\n+    if (items1.length != FILE_NAME_PART_LENGTH || items2.length != FILE_NAME_PART_LENGTH) {\n+      logger.error(\n+          \"file1={}, file2={} name should be in the following format: startLogIndex-endLogIndex-version-data\",\n+          file1.getAbsoluteFile(), file2.getAbsoluteFile());\n+    }\n+    long startLogIndex1 = Long.parseLong(items1[0]);\n+    long startLogIndex2 = Long.parseLong(items2[0]);\n+    int res = Long.compare(startLogIndex1, startLogIndex2);\n+    if (res == 0) {\n+      return Long.compare(Long.parseLong(items1[1]), Long.parseLong(items2[1]));\n     }\n+    return res;\n   }\n \n   /**\n-   * actually delete the data file which only contains removed data\n+   * @param startIndex the log start index\n+   * @param endIndex   the log end index\n+   * @return the raft log which index between [startIndex, endIndex] or empty if not found\n    */\n-  private void actuallyDeleteFile() {\n-    Iterator<File> logFileIterator = logFileList.iterator();\n-    while (logFileIterator.hasNext()) {\n-      File logFile = logFileIterator.next();\n-      if (logger.isDebugEnabled()) {\n-        logger.debug(\"Examining file for removal, file: {}, len: {}, removedLogSize: {}\", logFile\n-            , logFile.length(), removedLogSize);\n-      }\n-      if (logFile.length() > removedLogSize) {\n-        break;\n-      }\n-\n-      logger.info(\"Removing a log file {}, len: {}, removedLogSize: {}\", logFile,\n-          logFile.length(), removedLogSize);\n-      removedLogSize -= logFile.length();\n-      // if system down before delete, we can use this to delete file during recovery\n-      minAvailableVersion = getFileVersion(logFile);\n-      serializeMeta(meta);\n+  @Override\n+  public List<Log> getLogs(long startIndex, long endIndex) {", "originalCommit": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcxMjAzOA==", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r515712038", "bodyText": "I add one parameter to limit the max number of logs per fetch", "author": "neuyilan", "createdAt": "2020-11-02T02:26:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NjE3MQ=="}], "type": "inlineReview"}, {"oid": "bc72be14931a04e0cfce7b575d226840f8524d2b", "url": "https://github.com/apache/iotdb/commit/bc72be14931a04e0cfce7b575d226840f8524d2b", "message": "fix windows ut", "committedDate": "2020-10-30T11:26:50Z", "type": "commit"}, {"oid": "fa7f863325e18177cb0827eccdb2c420a9f53d64", "url": "https://github.com/apache/iotdb/commit/fa7f863325e18177cb0827eccdb2c420a9f53d64", "message": "fix revoew", "committedDate": "2020-11-01T02:38:48Z", "type": "commit"}, {"oid": "e8a950a9571fc3397ef540a366f9b934d42a0417", "url": "https://github.com/apache/iotdb/commit/e8a950a9571fc3397ef540a366f9b934d42a0417", "message": "fix review and add per fetch limit when get logs from disks", "committedDate": "2020-11-01T04:17:55Z", "type": "commit"}, {"oid": "6e38d4b3e0e5676820ceba712da1a1b0bbe286b3", "url": "https://github.com/apache/iotdb/commit/6e38d4b3e0e5676820ceba712da1a1b0bbe286b3", "message": "fix ci", "committedDate": "2020-11-01T04:58:26Z", "type": "commit"}, {"oid": "b6e598395f377cf74f54a60f8af3432f226ca4eb", "url": "https://github.com/apache/iotdb/commit/b6e598395f377cf74f54a60f8af3432f226ca4eb", "message": "fix review", "committedDate": "2020-11-01T05:37:35Z", "type": "commit"}, {"oid": "70f8abc6edd45f9006ac9b4ec973e8456c24eaa4", "url": "https://github.com/apache/iotdb/commit/70f8abc6edd45f9006ac9b4ec973e8456c24eaa4", "message": "try to fix ci", "committedDate": "2020-11-02T04:12:19Z", "type": "commit"}, {"oid": "777d8d7e1b654b438b3d8519173efa3a0c22560d", "url": "https://github.com/apache/iotdb/commit/777d8d7e1b654b438b3d8519173efa3a0c22560d", "message": "try to fix windows ci", "committedDate": "2020-11-02T06:17:52Z", "type": "commit"}, {"oid": "69561c255886cd65aad93435372815d9d4114254", "url": "https://github.com/apache/iotdb/commit/69561c255886cd65aad93435372815d9d4114254", "message": "fix exception not catch", "committedDate": "2020-11-02T07:02:48Z", "type": "commit"}, {"oid": "6de89e2c58e4794ec37adbad5b8894cd14a8739f", "url": "https://github.com/apache/iotdb/commit/6de89e2c58e4794ec37adbad5b8894cd14a8739f", "message": "fix null exception", "committedDate": "2020-11-02T07:20:50Z", "type": "commit"}, {"oid": "671194506a53bd0fa6bcf435d1cf3bbc2456ae04", "url": "https://github.com/apache/iotdb/commit/671194506a53bd0fa6bcf435d1cf3bbc2456ae04", "message": "resolve confilict", "committedDate": "2020-11-02T08:55:58Z", "type": "commit"}, {"oid": "d19f3b7fb93eb49299b62207c506a09d50cb043f", "url": "https://github.com/apache/iotdb/commit/d19f3b7fb93eb49299b62207c506a09d50cb043f", "message": "fix IT bug", "committedDate": "2020-11-03T02:05:11Z", "type": "commit"}, {"oid": "abf53f677a41790f4956541d38bbdd50a28802ea", "url": "https://github.com/apache/iotdb/commit/abf53f677a41790f4956541d38bbdd50a28802ea", "message": "Merge branch 'apache_cluster_new' into apache_cluster_new_1023_raft_log_catch_up", "committedDate": "2020-11-03T04:08:38Z", "type": "commit"}, {"oid": "c7450996bb79d1a5b58aad50f4d406512a99c190", "url": "https://github.com/apache/iotdb/commit/c7450996bb79d1a5b58aad50f4d406512a99c190", "message": "fix IT win bug", "committedDate": "2020-11-03T06:25:16Z", "type": "commit"}]}