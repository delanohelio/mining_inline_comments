{"pr_number": 820, "pr_title": "Table config cache", "pr_createdAt": "2020-05-08T23:12:29Z", "pr_url": "https://github.com/NationalSecurityAgency/datawave/pull/820", "timeline": [{"oid": "8b08c2022c7e6c9029c8ae6e4879ecaa1390f256", "url": "https://github.com/NationalSecurityAgency/datawave/commit/8b08c2022c7e6c9029c8ae6e4879ecaa1390f256", "message": "WIP: table configuration cache", "committedDate": "2020-04-24T22:14:05Z", "type": "commit"}, {"oid": "2a17c238b150a50cc8fb2d9038a6ec2bd00d2895", "url": "https://github.com/NationalSecurityAgency/datawave/commit/2a17c238b150a50cc8fb2d9038a6ec2bd00d2895", "message": "WIP: more table cache refinements", "committedDate": "2020-05-08T23:08:04Z", "type": "commit"}, {"oid": "4e742a6affd8dab3899f13929c95922eb08e6218", "url": "https://github.com/NationalSecurityAgency/datawave/commit/4e742a6affd8dab3899f13929c95922eb08e6218", "message": "Update IngestJob.java", "committedDate": "2020-05-08T23:15:30Z", "type": "commit"}, {"oid": "85e3a0f1f849a3d26b0644b0b6fb76c0c150e228", "url": "https://github.com/NationalSecurityAgency/datawave/commit/85e3a0f1f849a3d26b0644b0b6fb76c0c150e228", "message": "PR feedback: add enable flag, rename DIR to PATH", "committedDate": "2020-06-01T20:25:23Z", "type": "commit"}, {"oid": "000084f54693f7998ad79294dec8a971bae85a81", "url": "https://github.com/NationalSecurityAgency/datawave/commit/000084f54693f7998ad79294dec8a971bae85a81", "message": "Merge branch 'release/version2.9' into tableConfigCache", "committedDate": "2020-06-01T20:29:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA3MDI3Ng==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r434070276", "bodyText": "is this really still a \"TODO\"?", "author": "ivakegg", "createdAt": "2020-06-02T18:02:12Z", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/OptionsParser.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package datawave.ingest;\n+\n+import datawave.ingest.config.TableConfigCache;\n+import datawave.ingest.data.config.ingest.AccumuloHelper;\n+import datawave.ingest.util.ConfigurationFileHelper;\n+import datawave.util.cli.PasswordConverter;\n+import org.apache.hadoop.conf.Configuration;\n+\n+public class OptionsParser {\n+    \n+    // todo: we parse these same arguments across several different classes in slightly different ways with very similar flags (e.g. zookeepers, zooKeepers,", "originalCommit": "000084f54693f7998ad79294dec8a971bae85a81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMjM5MA==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r434132390", "bodyText": "there are other options still to add, but to minimize scope creep, I've left the todo", "author": "hlgp", "createdAt": "2020-06-02T19:40:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA3MDI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwMjY0OQ==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r438102649", "bodyText": "Should this use a try with resources, or a finally block to do the cleanup?", "author": "alerman", "createdAt": "2020-06-10T13:00:57Z", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/config/BaseHdfsFileCacheUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package datawave.ingest.config;\n+\n+import datawave.ingest.data.config.ingest.AccumuloHelper;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.commons.lang.Validate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+\n+public abstract class BaseHdfsFileCacheUtil {\n+    \n+    protected Path cacheFilePath;\n+    protected final Configuration conf;\n+    protected AccumuloHelper accumuloHelper;\n+    private static String delimiter = \"\\t\";\n+    \n+    private static final Logger log = Logger.getLogger(BaseHdfsFileCacheUtil.class);\n+    \n+    public BaseHdfsFileCacheUtil(Configuration conf) {\n+        Validate.notNull(conf, \"Configuration object passed in null\");\n+        this.conf = conf;\n+        setCacheFilePath(conf);\n+    }\n+    \n+    public Path getCacheFilePath() {\n+        return this.cacheFilePath;\n+    }\n+    \n+    public abstract void setCacheFilePath(Configuration conf);\n+    \n+    public void read() throws IOException {\n+        try (BufferedReader in = new BufferedReader(new InputStreamReader(FileSystem.get(this.cacheFilePath.toUri(), conf).open(this.cacheFilePath)))) {\n+            readCache(in, delimiter);\n+        } catch (IOException ex) {\n+            if (shouldRefreshCache(this.conf)) {\n+                update();\n+            } else {\n+                throw new IOException(\"Unable to read cache file at \" + this.cacheFilePath, ex);\n+            }\n+            \n+        }\n+        \n+    }\n+    \n+    public abstract void writeCacheFile(FileSystem fs, Path tempFile) throws IOException;\n+    \n+    public void update() {\n+        FileSystem fs = null;\n+        Path tempFile = null;\n+        try {", "originalCommit": "000084f54693f7998ad79294dec8a971bae85a81", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwNjc0Mg==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r438106742", "bodyText": "I am not sure I like this logic. If for some reason there are hundreds of files here we could have problems. Would it be better to grab a sorted list of this.cacheFilePath.getName() + \".*\" and parse the number an increment? Might be a moot point but something to consider", "author": "alerman", "createdAt": "2020-06-10T13:07:10Z", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/config/BaseHdfsFileCacheUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package datawave.ingest.config;\n+\n+import datawave.ingest.data.config.ingest.AccumuloHelper;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.commons.lang.Validate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+\n+public abstract class BaseHdfsFileCacheUtil {\n+    \n+    protected Path cacheFilePath;\n+    protected final Configuration conf;\n+    protected AccumuloHelper accumuloHelper;\n+    private static String delimiter = \"\\t\";\n+    \n+    private static final Logger log = Logger.getLogger(BaseHdfsFileCacheUtil.class);\n+    \n+    public BaseHdfsFileCacheUtil(Configuration conf) {\n+        Validate.notNull(conf, \"Configuration object passed in null\");\n+        this.conf = conf;\n+        setCacheFilePath(conf);\n+    }\n+    \n+    public Path getCacheFilePath() {\n+        return this.cacheFilePath;\n+    }\n+    \n+    public abstract void setCacheFilePath(Configuration conf);\n+    \n+    public void read() throws IOException {\n+        try (BufferedReader in = new BufferedReader(new InputStreamReader(FileSystem.get(this.cacheFilePath.toUri(), conf).open(this.cacheFilePath)))) {\n+            readCache(in, delimiter);\n+        } catch (IOException ex) {\n+            if (shouldRefreshCache(this.conf)) {\n+                update();\n+            } else {\n+                throw new IOException(\"Unable to read cache file at \" + this.cacheFilePath, ex);\n+            }\n+            \n+        }\n+        \n+    }\n+    \n+    public abstract void writeCacheFile(FileSystem fs, Path tempFile) throws IOException;\n+    \n+    public void update() {\n+        FileSystem fs = null;\n+        Path tempFile = null;\n+        try {\n+            fs = FileSystem.get(cacheFilePath.toUri(), conf);\n+            tempFile = createTempFile(fs);\n+            writeCacheFile(fs, tempFile);\n+            createCacheFile(fs, tempFile);\n+        } catch (IOException e) {\n+            cleanup(fs, tempFile);\n+            \n+            log.error(\"Unable to update cache file \" + cacheFilePath + \" \" + e.getMessage());\n+        }\n+        \n+    }\n+    \n+    protected void initAccumuloHelper() {\n+        if (accumuloHelper == null) {\n+            accumuloHelper = new AccumuloHelper();\n+            accumuloHelper.setup(conf);\n+        }\n+    }\n+    \n+    public void createCacheFile(FileSystem fs, Path tmpCacheFile) {\n+        try {\n+            fs.delete(this.cacheFilePath, false);\n+            if (!fs.rename(tmpCacheFile, this.cacheFilePath)) {\n+                throw new IOException(\"Failed to rename temporary cache file\");\n+            }\n+            \n+        } catch (Exception e) {\n+            log.warn(\"Unable to rename \" + tmpCacheFile + \" to \" + this.cacheFilePath + \"probably because somebody else replaced it \", e);\n+            cleanup(fs, tmpCacheFile);\n+        }\n+        log.info(\"Updated \" + cacheFilePath);\n+        \n+    }\n+    \n+    protected void cleanup(FileSystem fs, Path tmpCacheFile) {\n+        try {\n+            fs.delete(tmpCacheFile, false);\n+        } catch (Exception e) {\n+            log.error(\"Unable to clean up \" + tmpCacheFile, e);\n+        }\n+    }\n+    \n+    protected void readCache(BufferedReader in, String delimiter) throws IOException {\n+        String line;\n+        while ((line = in.readLine()) != null) {\n+            String[] parts = StringUtils.split(line, delimiter);\n+            if (parts.length == 2) {\n+                conf.set(parts[0], parts[1]);\n+            }\n+        }\n+        in.close();\n+    }\n+    \n+    public Path createTempFile(FileSystem fs) throws IOException {\n+        int count = 1;\n+        Path tmpCacheFile = null;\n+        try {\n+            do {\n+                Path parentDirectory = this.cacheFilePath.getParent();\n+                String fileName = this.cacheFilePath.getName() + \".\" + count;", "originalCommit": "000084f54693f7998ad79294dec8a971bae85a81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ1NTQwOQ==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r440455409", "bodyText": "This has been there for a long time and, occasionally, we would see a handful of files hang around.  They should never be left around in the first place so I questioned this, too.  I found a place where a delete was not called but should have been and added it as part of this PR.  I didn't want to change the logic too much since it's mainly a refactor.  I will come back after more vetting and remove this in a separate PR.", "author": "hlgp", "createdAt": "2020-06-15T21:23:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwNjc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MzgzNg==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r440473836", "bodyText": "Ok I can accept that", "author": "alerman", "createdAt": "2020-06-15T22:07:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwNjc0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE1MDg1Mw==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r438150853", "bodyText": "No soup for you!", "author": "alerman", "createdAt": "2020-06-10T14:08:35Z", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/job/reduce/AggregatingReducer.java", "diffHunk": "@@ -14,8 +14,10 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import com.google.common.collect.*;", "originalCommit": "000084f54693f7998ad79294dec8a971bae85a81", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE1MTI1MQ==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r438151251", "bodyText": "I thought you knew better. Sad panda.", "author": "alerman", "createdAt": "2020-06-10T14:09:03Z", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/job/writer/AbstractContextWriter.java", "diffHunk": "@@ -3,10 +3,7 @@\n import com.google.common.collect.ArrayListMultimap;\n import com.google.common.collect.Multimap;\n import datawave.ingest.data.config.ingest.BaseIngestHelper;\n-import datawave.ingest.mapreduce.job.BulkIngestCounters;\n-import datawave.ingest.mapreduce.job.BulkIngestKey;\n-import datawave.ingest.mapreduce.job.ConstraintChecker;\n-import datawave.ingest.mapreduce.job.IngestJob;\n+import datawave.ingest.mapreduce.job.*;", "originalCommit": "000084f54693f7998ad79294dec8a971bae85a81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ0OTY1Nw==", "url": "https://github.com/NationalSecurityAgency/datawave/pull/820#discussion_r440449657", "bodyText": "reconfigured my IDE not to do this anymore!", "author": "hlgp", "createdAt": "2020-06-15T21:11:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE1MTI1MQ=="}], "type": "inlineReview"}, {"oid": "348e7fe16c3c5f671b2c78cd4809c1f91839d743", "url": "https://github.com/NationalSecurityAgency/datawave/commit/348e7fe16c3c5f671b2c78cd4809c1f91839d743", "message": "remove star imports", "committedDate": "2020-06-15T18:13:53Z", "type": "commit"}, {"oid": "4f0b7f0a5e0ff78a809b05d9de58821084779eae", "url": "https://github.com/NationalSecurityAgency/datawave/commit/4f0b7f0a5e0ff78a809b05d9de58821084779eae", "message": "Merge branch 'release/version2.9' into tableConfigCache", "committedDate": "2020-07-29T22:06:34Z", "type": "commit"}, {"oid": "83ca330f55a592f7275c4d4f3f3cfffdb4a249af", "url": "https://github.com/NationalSecurityAgency/datawave/commit/83ca330f55a592f7275c4d4f3f3cfffdb4a249af", "message": "Merge branch 'release/version2.9' into tableConfigCache", "committedDate": "2020-07-31T12:48:22Z", "type": "commit"}, {"oid": "f6836e2867468fd2ed0e9702fd5484df855d6794", "url": "https://github.com/NationalSecurityAgency/datawave/commit/f6836e2867468fd2ed0e9702fd5484df855d6794", "message": "Merge branch 'release/version2.9' into tableConfigCache", "committedDate": "2020-07-31T14:00:17Z", "type": "commit"}]}