{"pr_number": 13577, "pr_title": "[FLINK-16579][table] Upgrade Calcite version to 1.26 for Flink SQL", "pr_createdAt": "2020-10-10T02:05:38Z", "pr_url": "https://github.com/apache/flink/pull/13577", "timeline": [{"oid": "cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "url": "https://github.com/apache/flink/commit/cde6b367bcfc5c76ee9004a6399c734ec890c6d1", "message": "Fix test failure", "committedDate": "2020-10-10T06:47:22Z", "type": "forcePushed"}, {"oid": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "url": "https://github.com/apache/flink/commit/66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "message": "Fix test failure", "committedDate": "2020-10-10T07:44:02Z", "type": "forcePushed"}, {"oid": "039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f", "url": "https://github.com/apache/flink/commit/039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient", "committedDate": "2020-10-10T08:41:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MTgyNA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502761824", "bodyText": "Why we have to depend on this dependency now?", "author": "wuchong", "createdAt": "2020-10-10T08:02:25Z", "path": "flink-table/flink-table-planner-blink/pom.xml", "diffHunk": "@@ -189,10 +194,6 @@ under the License.\n \t\t\t\t\t<groupId>org.apache.commons</groupId>\n \t\t\t\t\t<artifactId>commons-dbcp2</artifactId>\n \t\t\t\t</exclusion>\n-\t\t\t\t<exclusion>\n-\t\t\t\t\t<groupId>com.esri.geometry</groupId>\n-\t\t\t\t\t<artifactId>esri-geometry-api</artifactId>", "originalCommit": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3MDE2MQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502770161", "bodyText": "This is a required dependency now, the calcite code can not compile without this jar.", "author": "danny0405", "createdAt": "2020-10-10T09:34:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MTgyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzU5Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502763597", "bodyText": "What about changing to use @Ignore +\t@Test annotations and also update the comment above?\nThis can avoid the IDEA warns this method is never used.", "author": "wuchong", "createdAt": "2020-10-10T08:21:48Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -35,15 +35,12 @@ protected SqlParserImplFactory parserImplFactory() {\n \t}\n \n \t// overrides test methods that we don't support\n-\t@Override", "originalCommit": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzY0Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502763647", "bodyText": "Use @Test instead?", "author": "wuchong", "createdAt": "2020-10-10T08:22:23Z", "path": "flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java", "diffHunk": "@@ -994,7 +992,6 @@ public void testShowViews() {\n \n \t// Override the test because our ROW field type default is nullable,\n \t// which is different with Calcite.\n-\t@Override", "originalCommit": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE1MTE0Mw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r503151143", "bodyText": "This is missed to update?", "author": "wuchong", "createdAt": "2020-10-12T09:10:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzY0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NDc3Mg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502764772", "bodyText": "It seems that dynamicOptions is never used in this class, should we remove it?", "author": "wuchong", "createdAt": "2020-10-10T08:35:13Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/schema/TableSourceTable.scala", "diffHunk": "@@ -65,15 +65,8 @@ class TableSourceTable(\n     statistic) {\n \n   override def getQualifiedName: util.List[String] = {\n-    val names = super.getQualifiedName\n     val builder = ImmutableList.builder[String]()\n-    builder.addAll(names)\n-    if (dynamicOptions.size() != 0) {", "originalCommit": "66f4d1a8a98fd2a0736b0f8e14dbe765cedf33f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3MTE3NQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502771175", "bodyText": "Yes, it can be removed.", "author": "danny0405", "createdAt": "2020-10-10T09:44:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NDc3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502765363", "bodyText": "Is it a temporary solution? Why we have to override the default value in interface?\nBut this change is only needed in legacy planner?", "author": "wuchong", "createdAt": "2020-10-10T08:42:27Z", "path": "flink-table/flink-table-planner/src/main/java/org/apache/calcite/schema/Statistic.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.schema;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelReferentialConstraint;\n+import org.apache.calcite.util.ImmutableBitSet;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a {@link Table}.\n+ *\n+ * <p>Each of the methods may return {@code null} meaning \"not known\".</p>\n+ *\n+ * <p>Changes:\n+ *\n+ * <ul>\n+ *     <li>Line 61: default collations change from null to empty list.</li>", "originalCommit": "039b08f4bdd8a60a89fa324cc0213f4fb99a9e8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3MTM2OQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r502771369", "bodyText": "Because the default value changes to null, the legacy planner does not have custom Statistic so there is no good way to override the default value, e.g. (the Statistics.UNKNOWN).", "author": "danny0405", "createdAt": "2020-10-10T09:46:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2OTA3Ng==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504369076", "bodyText": "legacy planner also has customer statistic class: FlinkStatistic", "author": "godfreyhe", "createdAt": "2020-10-14T02:51:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzMzM3OQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505133379", "bodyText": "Yes, but because the legacy planner use the RelOptTableImpl as the SqlValidatorTable during sql-to-rel conversion, and there is no common base preparing table like FlinkPreparingTableBase in blink planner, we need to override each AbstractTable sub-class for the legacy planner.\nHere it is QueryOperationCatalogViewTable that causes the problem.", "author": "danny0405", "createdAt": "2020-10-15T02:50:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NTM2Mw=="}], "type": "inlineReview"}, {"oid": "25608fcfaf896c97019360e45ae4a2c9fe96ae63", "url": "https://github.com/apache/flink/commit/25608fcfaf896c97019360e45ae4a2c9fe96ae63", "message": "Fix the test error", "committedDate": "2020-10-12T11:30:42Z", "type": "forcePushed"}, {"oid": "7a38dfe2a8668cc543609d5e9f4a8887b8230b0e", "url": "https://github.com/apache/flink/commit/7a38dfe2a8668cc543609d5e9f4a8887b8230b0e", "message": "Fix test failure", "committedDate": "2020-10-13T04:12:43Z", "type": "forcePushed"}, {"oid": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "url": "https://github.com/apache/flink/commit/3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "message": "Fix test failure\n\nRe-trigger the Azure tests", "committedDate": "2020-10-14T07:14:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2NzU2OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504367568", "bodyText": "missed to update?", "author": "godfreyhe", "createdAt": "2020-10-14T02:45:36Z", "path": "flink-table/flink-sql-parser/src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java", "diffHunk": "@@ -188,7 +190,6 @@ public void testDescribeTable() {\n \t/**\n \t * Here we override the super method to avoid test error from `describe statement` supported in original calcite.\n \t */\n-\t@Override", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MDY4Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504380687", "bodyText": "nit: util.Collections", "author": "godfreyhe", "createdAt": "2020-10-14T03:35:31Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -165,18 +165,28 @@ object AggSqlFunction {\n     }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n       aggregateFunction: ImperativeAggregateFunction[_, _],\n       externalAccType: DataType)\n-    : SqlOperandTypeChecker = {\n+    : SqlOperandMetadata = {\n \n     val methods = checkAndExtractMethods(aggregateFunction, \"accumulate\")\n \n     /**\n       * Operand type checker based on [[AggregateFunction]] given information.\n       */\n-    new SqlOperandTypeChecker {\n+    new SqlOperandMetadata {\n+      override def paramNames(): util.List[String] = {\n+        // Does not support named parameters.\n+        Collections.emptyList()", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzNDYwMw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505134603", "bodyText": "What's the difference ?", "author": "danny0405", "createdAt": "2020-10-15T02:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MDY4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzEyNzU5NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507127594", "bodyText": "we had introduced java.util.", "author": "godfreyhe", "createdAt": "2020-10-18T12:30:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MDY4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzM1MTI2NQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507351265", "bodyText": "Thanks, java.util import removed.", "author": "danny0405", "createdAt": "2020-10-19T02:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MDY4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MTgwOA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504381808", "bodyText": "is this necessary?", "author": "godfreyhe", "createdAt": "2020-10-14T03:39:48Z", "path": "flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -40,12 +44,19 @@ class TableSqlFunction(\n     functionImpl: FlinkTableFunctionImpl[_])\n   extends SqlUserDefinedTableFunction(\n     new SqlIdentifier(name, SqlParserPos.ZERO),\n+    SqlKind.OTHER_FUNCTION,\n     ReturnTypes.CURSOR,\n     createEvalOperandTypeInference(name, tableFunction, typeFactory),\n-    createEvalOperandTypeChecker(name, tableFunction),\n-    null,\n+    createEvalOperandMetadata(name, tableFunction),\n     functionImpl) {\n \n+  override def getRowTypeInference: SqlReturnTypeInference = new SqlReturnTypeInference {\n+    override def inferReturnType(opBinding: SqlOperatorBinding): RelDataType = {\n+      // The arguments should never be used.\n+      functionImpl.getRowType(opBinding.getTypeFactory, Collections.emptyList())\n+    }\n+  }", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzNTEyNg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505135126", "bodyText": "Yes, all the sql table function needs to implement getRowTypeInference now.", "author": "danny0405", "createdAt": "2020-10-15T02:57:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MTgwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzEyODQ1NQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507128455", "bodyText": "TableSqlFunction extends from SqlUserDefinedTableFunction, and SqlUserDefinedTableFunction has provided the default implementation", "author": "godfreyhe", "createdAt": "2020-10-18T12:33:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MTgwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzM1NDY3MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507354670", "bodyText": "Yes, it can be removed.", "author": "danny0405", "createdAt": "2020-10-19T02:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4MTgwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4ODQ5MQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504388491", "bodyText": "does null has special meaning ? if not, return empty list to avoid NPE", "author": "godfreyhe", "createdAt": "2020-10-14T04:06:20Z", "path": "flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/functions/utils/UserDefinedFunctionUtils.scala", "diffHunk": "@@ -372,6 +373,9 @@ object UserDefinedFunctionUtils {\n \n       override def getConsistency: Consistency = Consistency.NONE\n \n+      override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = null", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzNTI5NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505135294", "bodyText": "It is never used, if there is NPE, it should be a bug.", "author": "danny0405", "createdAt": "2020-10-15T02:58:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4ODQ5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzEyODg5OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507128898", "bodyText": "throw exception directly", "author": "godfreyhe", "createdAt": "2020-10-18T12:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM4ODQ5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzNjgzOA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504436838", "bodyText": "can we simplify this result express ?  I think (1L:BIGINT..2L:BIGINT) ... (29L:BIGINT..30L:BIGINT) is meaningless, because b is bigint type, not double type.\nI find the result of another CalcTest does not contain type (BIGINT), can we also simply it ?", "author": "godfreyhe", "createdAt": "2020-10-14T06:42:02Z", "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/CalcTest.scala", "diffHunk": "@@ -73,12 +78,23 @@ class CalcTest extends TableTestBase {\n     val util = batchTestUtil()\n     val table = util.addTable[(Int, Long, String)](\"MyTable\", 'a, 'b, 'c)\n \n-    val resultStr = (1 to 30).map(i => s\"$i:BIGINT\").mkString(\", \")\n+    val resultStr = \"Sarg[(-\u221e..1L:BIGINT), (1L:BIGINT..2L:BIGINT), \" +\n+        \"(2L:BIGINT..3L:BIGINT), (3L:BIGINT..4L:BIGINT), \" +\n+        \"(4L:BIGINT..5L:BIGINT), (5L:BIGINT..6L:BIGINT), \" +\n+        \"(6L:BIGINT..7L:BIGINT), (7L:BIGINT..8L:BIGINT), \" +\n+        \"(8L:BIGINT..9L:BIGINT), (9L:BIGINT..10L:BIGINT), \" +\n+        \"(10L:BIGINT..11L:BIGINT), (11L:BIGINT..12L:BIGINT), (12L:BIGINT..13L:BIGINT), \" +\n+        \"(13L:BIGINT..14L:BIGINT), (14L:BIGINT..15L:BIGINT), (15L:BIGINT..16L:BIGINT), \" +\n+        \"(16L:BIGINT..17L:BIGINT), (17L:BIGINT..18L:BIGINT), (18L:BIGINT..19L:BIGINT), \" +\n+        \"(19L:BIGINT..20L:BIGINT), (20L:BIGINT..21L:BIGINT), (21L:BIGINT..22L:BIGINT), \" +\n+        \"(22L:BIGINT..23L:BIGINT), (23L:BIGINT..24L:BIGINT), (24L:BIGINT..25L:BIGINT), \" +\n+        \"(25L:BIGINT..26L:BIGINT), (26L:BIGINT..27L:BIGINT), (27L:BIGINT..28L:BIGINT), \" +\n+        \"(28L:BIGINT..29L:BIGINT), (29L:BIGINT..30L:BIGINT), (30L:BIGINT..+\u221e)]:BIGINT\"", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzNzgxNQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504437815", "bodyText": "it can be simplified as (1 to 30).map(i => s\"${i}L:BIGINT\").mkString(\"Sarg[\", \", \", \"]:BIGINT\")", "author": "godfreyhe", "createdAt": "2020-10-14T06:44:20Z", "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/batch/sql/CalcTest.scala", "diffHunk": "@@ -54,12 +54,17 @@ class CalcTest extends TableTestBase {\n     val util = batchTestUtil()\n     val table = util.addTable[(Int, Long, String)](\"MyTable\", 'a, 'b, 'c)\n \n-    val resultStr = (1 to 30).map(i => s\"$i:BIGINT\").mkString(\", \")\n+    val resultStr = \"Sarg[1L:BIGINT, 2L:BIGINT, 3L:BIGINT, 4L:BIGINT, 5L:BIGINT, \" +\n+        \"6L:BIGINT, 7L:BIGINT, 8L:BIGINT, 9L:BIGINT, 10L:BIGINT, 11L:BIGINT, \" +\n+        \"12L:BIGINT, 13L:BIGINT, 14L:BIGINT, 15L:BIGINT, 16L:BIGINT, \" +\n+        \"17L:BIGINT, 18L:BIGINT, 19L:BIGINT, 20L:BIGINT, 21L:BIGINT, \" +\n+        \"22L:BIGINT, 23L:BIGINT, 24L:BIGINT, 25L:BIGINT, 26L:BIGINT, \" +\n+        \"27L:BIGINT, 28L:BIGINT, 29L:BIGINT, 30L:BIGINT]:BIGINT\"", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MDMxNQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504440315", "bodyText": "can SEARCH(c, Sarg['xx']:CHAR(2) be simplified as =(c, 'xx')", "author": "godfreyhe", "createdAt": "2020-10-14T06:50:10Z", "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/stream/table/CalcTest.scala", "diffHunk": "@@ -114,11 +114,13 @@ class CalcTest extends TableTestBase {\n     val resultTable = sourceTable.select('a, 'b, 'c)\n       .where((1 to 30).map($\"b\" === _).reduce((ex1, ex2) => ex1 || ex2) && ($\"c\" === \"xx\"))\n \n+    val operands = \"Sarg[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \" +", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTE0MTIyMg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505141222", "bodyText": "They are equivalent, we can use the RexUtil.expandSearch to convert it to =(c, 'xx').", "author": "danny0405", "createdAt": "2020-10-15T03:21:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MDMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0Mjk1OQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504442959", "bodyText": "Is it within our expectations? what changes (or bug fixs) cause this ?", "author": "godfreyhe", "createdAt": "2020-10-14T06:55:58Z", "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/expressions/ScalarFunctionsTest.scala", "diffHunk": "@@ -2679,15 +2679,15 @@ class ScalarFunctionsTest extends ScalarTypesTestBase {\n       \"1017-11-29 22:58:58.998\")\n \n     val QUARTER = Seq(\n-      \"2018-03-01 22:58:58.998\",\n-      \"2018-08-31 22:58:58.998\",\n+      \"2018-02-28 22:58:58.998\",", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTE0MTc3NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505141774", "bodyText": "See CALCITE-3881 and FLINK-16823.", "author": "danny0405", "createdAt": "2020-10-15T03:24:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0Mjk1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDg0Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504444847", "bodyText": "the plan is worse than before, because more fields need to shuffle.", "author": "godfreyhe", "createdAt": "2020-10-14T06:59:56Z", "path": "flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/plan/TimeIndicatorConversionTest.scala", "diffHunk": "@@ -204,20 +204,16 @@ class TimeIndicatorConversionTest extends TableTestBase {\n \n     val result = t.unionAll(t).select('rowtime)\n \n-    val expected = binaryNode(\n-      \"DataStreamUnion\",\n-      unaryNode(\n-        \"DataStreamCalc\",\n+    val expected = unaryNode(\n+      \"DataStreamCalc\",\n+      binaryNode(\n+        \"DataStreamUnion\",\n         streamTableNode(t),\n-        term(\"select\", \"rowtime\")\n-      ),\n-      unaryNode(\n-        \"DataStreamCalc\",\n         streamTableNode(t),\n-        term(\"select\", \"rowtime\")\n+        term(\"all\", \"true\"),\n+        term(\"union all\", \"rowtime, long, int\")\n       ),\n-      term(\"all\", \"true\"),\n-      term(\"union all\", \"rowtime\")\n+      term(\"select\", \"rowtime\")", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTIwNzQxMA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505207410", "bodyText": "After some debug, i found that the 2 plan has the same cost:", "author": "danny0405", "createdAt": "2020-10-15T06:38:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTIzMzEzOQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505233139", "bodyText": "I would fire a issue to track this ~", "author": "danny0405", "createdAt": "2020-10-15T07:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk4OTcxNQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505989715", "bodyText": "See FLINK-19668", "author": "danny0405", "createdAt": "2020-10-16T02:19:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1MTAyMA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504451020", "bodyText": "nit: Line 671 ~ 681", "author": "godfreyhe", "createdAt": "2020-10-14T07:12:50Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql2rel/RelDecorrelator.java", "diffHunk": "@@ -0,0 +1,2955 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.sql2rel;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.ImmutableSortedMap;\n+import com.google.common.collect.ImmutableSortedSet;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Multimap;\n+import com.google.common.collect.MultimapBuilder;\n+import com.google.common.collect.Sets;\n+import com.google.common.collect.SortedSetMultimap;\n+import org.apache.calcite.linq4j.Ord;\n+import org.apache.calcite.linq4j.function.Function2;\n+import org.apache.calcite.plan.Context;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelOptCostImpl;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.plan.RelRule;\n+import org.apache.calcite.plan.hep.HepPlanner;\n+import org.apache.calcite.plan.hep.HepProgram;\n+import org.apache.calcite.plan.hep.HepRelVertex;\n+import org.apache.calcite.rel.BiRel;\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelHomogeneousShuttle;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Correlate;\n+import org.apache.calcite.rel.core.CorrelationId;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.core.RelFactories;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rel.core.Values;\n+import org.apache.calcite.rel.logical.LogicalAggregate;\n+import org.apache.calcite.rel.logical.LogicalCorrelate;\n+import org.apache.calcite.rel.logical.LogicalFilter;\n+import org.apache.calcite.rel.logical.LogicalJoin;\n+import org.apache.calcite.rel.logical.LogicalProject;\n+import org.apache.calcite.rel.logical.LogicalSnapshot;\n+import org.apache.calcite.rel.logical.LogicalTableFunctionScan;\n+import org.apache.calcite.rel.metadata.RelMdUtil;\n+import org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.calcite.rel.rules.CoreRules;\n+import org.apache.calcite.rel.rules.FilterCorrelateRule;\n+import org.apache.calcite.rel.rules.FilterJoinRule;\n+import org.apache.calcite.rel.rules.FilterProjectTransposeRule;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexCorrelVariable;\n+import org.apache.calcite.rex.RexFieldAccess;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexLiteral;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexShuttle;\n+import org.apache.calcite.rex.RexSubQuery;\n+import org.apache.calcite.rex.RexUtil;\n+import org.apache.calcite.rex.RexVisitorImpl;\n+import org.apache.calcite.sql.SqlExplainFormat;\n+import org.apache.calcite.sql.SqlExplainLevel;\n+import org.apache.calcite.sql.SqlFunction;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlOperator;\n+import org.apache.calcite.sql.fun.SqlCountAggFunction;\n+import org.apache.calcite.sql.fun.SqlSingleValueAggFunction;\n+import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n+import org.apache.calcite.tools.RelBuilder;\n+import org.apache.calcite.tools.RelBuilderFactory;\n+import org.apache.calcite.util.Holder;\n+import org.apache.calcite.util.ImmutableBeans;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.apache.calcite.util.Litmus;\n+import org.apache.calcite.util.Pair;\n+import org.apache.calcite.util.ReflectUtil;\n+import org.apache.calcite.util.ReflectiveVisitor;\n+import org.apache.calcite.util.Util;\n+import org.apache.calcite.util.mapping.Mappings;\n+import org.apache.calcite.util.trace.CalciteTrace;\n+import org.slf4j.Logger;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.math.BigDecimal;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Copied to fix CALCITE-4333, should be removed for the next Calcite upgrade.\n+ *\n+ * <p>Changes: Line 672 ~ Line 682, Line 430 ~ Line 441.", "originalCommit": "4c7d37a3b02ae9221b3f9a798ba9fd0523ec4bb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NTMxMQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504455311", "bodyText": "please also remove the TODO in SqlDateTimeUtils class around line 1488 since CALCITE-3199 is fixed", "author": "godfreyhe", "createdAt": "2020-10-14T07:21:00Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/BuiltInMethods.scala", "diffHunk": "@@ -458,19 +458,15 @@ object BuiltInMethods {\n   val TRUNCATE_DEC = Types.lookupMethod(classOf[SqlFunctionUtils], \"struncate\",\n     classOf[DecimalData], classOf[Int])\n \n-  // TODO: remove if CALCITE-3199 fixed\n-  //  https://issues.apache.org/jira/browse/CALCITE-3199\n-  val UNIX_DATE_CEIL = Types.lookupMethod(classOf[SqlDateTimeUtils], \"unixDateCeil\",\n+  val UNIX_DATE_CEIL = Types.lookupMethod(classOf[DateTimeUtils], \"unixDateCeil\",", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NTc0OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504455748", "bodyText": "return empty list to avoid NPE or throw an exception here directly", "author": "godfreyhe", "createdAt": "2020-10-14T07:21:53Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -165,18 +165,28 @@ object AggSqlFunction {\n     }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n       aggregateFunction: ImperativeAggregateFunction[_, _],\n       externalAccType: DataType)\n-    : SqlOperandTypeChecker = {\n+    : SqlOperandMetadata = {\n \n     val methods = checkAndExtractMethods(aggregateFunction, \"accumulate\")\n \n     /**\n       * Operand type checker based on [[AggregateFunction]] given information.\n       */\n-    new SqlOperandTypeChecker {\n+    new SqlOperandMetadata {\n+      override def paramNames(): util.List[String] = {\n+        // Does not support named parameters.\n+        Collections.emptyList()\n+      }\n+\n+      override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = {\n+        // This should be never invoked.\n+        null", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1Njg2NQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504456865", "bodyText": "never use", "author": "godfreyhe", "createdAt": "2020-10-14T07:24:07Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -129,20 +138,46 @@ object TableSqlFunction {\n         }\n   }\n \n-  private[flink] def createOperandTypeChecker(\n+  private[flink] def createOperandMetadata(\n       name: String,\n-      udtf: TableFunction[_]): SqlOperandTypeChecker = {\n-    new OperandTypeChecker(name, udtf, checkAndExtractMethods(udtf, \"eval\"))\n+      udtf: TableFunction[_]): SqlOperandMetadata = {\n+    new OperandMetadata(name, udtf, checkAndExtractMethods(udtf, \"eval\"))\n+  }\n+\n+  /**\n+   * Converts arguments from [[org.apache.calcite.sql.SqlNode]] to\n+   * java object format.\n+   *\n+   * @param callBinding Operator bound to arguments\n+   * @param function target function to get parameter types from\n+   * @param opName name of the operator to use in error message\n+   * @return converted list of arguments\n+   */\n+  private[flink] def convertArguments(\n+      callBinding: SqlOperatorBinding,\n+      function: org.apache.calcite.schema.Function,\n+      opName: SqlIdentifier): util.List[Object] = {\n+    val arguments = new util.ArrayList[Object](callBinding.getOperandCount)\n+    0 until callBinding.getOperandCount foreach { i =>\n+      val operandType = callBinding.getOperandType(i)", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1NzMxNw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504457317", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T07:24:49Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -191,4 +226,8 @@ class OperandTypeChecker(\n   override def isOptional(i: Int): Boolean = false\n \n   override def getConsistency: Consistency = Consistency.NONE\n+\n+  override def paramTypes(typeFactory: RelDataTypeFactory): util.List[RelDataType] = null", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1OTY5OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504459698", "bodyText": "is this necessary in this pr or support it in another pr ?\nbtw, please add related test in FlinkRelMdUniqueKeysTest", "author": "godfreyhe", "createdAt": "2020-10-14T07:29:14Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -70,9 +70,29 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n   private def getTableUniqueKeys(\n       tableSource: TableSource[_],\n       relOptTable: RelOptTable): JSet[ImmutableBitSet] = {\n-    // TODO get uniqueKeys from TableSchema of TableSource\n \n     relOptTable match {\n+      case sourceTable: TableSourceTable =>", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI0NTg4Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505245887", "bodyText": "Because this pr is huge, i would give tests in another PR.", "author": "danny0405", "createdAt": "2020-10-15T07:12:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1OTY5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2MDY1Mw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504460653", "bodyText": "ditto.\nnit, please move this close to TableScan part, this could make the code easier to read.", "author": "godfreyhe", "createdAt": "2020-10-14T07:30:56Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -539,6 +559,25 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n     }\n   }\n \n+  def getUniqueKeys(\n+      rel: TableFunctionScan,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    if (rel.getInputs.size() == 1\n+        && rel.getCall.asInstanceOf[RexCall].getOperator.isInstanceOf[SqlWindowTableFunction]) {\n+      mq.getUniqueKeys(rel.getInput(0), ignoreNulls)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def getUniqueKeys(\n+      rel: WatermarkAssigner,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    mq.getUniqueKeys(rel.getInput, ignoreNulls)\n+  }", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2MjUxMg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504462512", "bodyText": "can we make sure all INs have been converted to SEARCH ?", "author": "godfreyhe", "createdAt": "2020-10-14T07:34:19Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/SelectivityEstimator.scala", "diffHunk": "@@ -187,8 +188,8 @@ class SelectivityEstimator(rel: RelNode, mq: FlinkRelMetadataQuery)\n       case IS_NOT_NULL =>\n         estimateIsNotNull(operands.head)\n \n-      case IN =>\n-        estimateIn(operands.head, operands.slice(1, operands.size))\n+      case SEARCH =>", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI3Mzk2OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505273968", "bodyText": "Yes, since 1.26. the IN constant rex call is not allowed, only in sub-query.", "author": "danny0405", "createdAt": "2020-10-15T07:36:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2MjUxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2NTc5Mg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504465792", "bodyText": "do we meet some corner cases ?", "author": "godfreyhe", "createdAt": "2020-10-14T07:40:04Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala", "diffHunk": "@@ -94,7 +94,13 @@ class BatchExecNestedLoopJoin(\n       (buildRowSize + BinaryRowDataSerializer.LENGTH_SIZE_IN_BYTES) * shuffleBuildCount(mq)\n     val cpuCost = leftRowCnt * rightRowCnt\n     val costFactory = planner.getCostFactory.asInstanceOf[FlinkCostFactory]\n-    costFactory.makeCost(mq.getRowCount(this), cpuCost, 0, 0, memoryCost)\n+    val cost = costFactory.makeCost(mq.getRowCount(this), cpuCost, 0, 0, memoryCost)\n+    if (singleRowJoin) {\n+      // Make single row join more preferable than non-single row join.\n+      cost.multiplyBy(0.99)\n+    } else {\n+      cost\n+    }", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI0NDM3OQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505244379", "bodyText": "Without this change, the single row join plan can not be picked.", "author": "danny0405", "createdAt": "2020-10-15T07:11:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2NTc5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2NzY2NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504467664", "bodyText": "remove this directly?", "author": "godfreyhe", "createdAt": "2020-10-14T07:43:31Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkStreamRuleSets.scala", "diffHunk": "@@ -108,15 +108,16 @@ object FlinkStreamRuleSets {\n       REDUCE_EXPRESSION_RULES.asScala ++\n       List(\n         //removes constant keys from an Agg\n-        AggregateProjectPullUpConstantsRule.INSTANCE,\n+        CoreRules.AGGREGATE_PROJECT_PULL_UP_CONSTANTS,\n         // fix: FLINK-17553 unsupported call error when constant exists in group window key\n         // this rule will merge the project generated by AggregateProjectPullUpConstantsRule and\n         // make sure window aggregate can be correctly rewritten by StreamLogicalWindowAggregateRule\n-        ProjectMergeRule.INSTANCE,\n+        CoreRules.PROJECT_MERGE,\n         StreamLogicalWindowAggregateRule.INSTANCE,\n+//        WindowJoinRewriteRule.INSTANCE,", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ2OTY1MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504469650", "bodyText": "please also update the param name in java doc", "author": "godfreyhe", "createdAt": "2020-10-14T07:46:58Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala", "diffHunk": "@@ -198,9 +198,14 @@ object ColumnIntervalUtil {\n     */\n   def getColumnIntervalWithFilter(\n       originInterval: Option[ValueInterval],\n-      predicate: RexNode,\n+      oriPred: RexNode,", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3MTI0MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504471240", "bodyText": "provide an utility method in FlinkRexUtil,  there is a lot of similar code", "author": "godfreyhe", "createdAt": "2020-10-14T07:49:51Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ColumnIntervalUtil.scala", "diffHunk": "@@ -198,9 +198,14 @@ object ColumnIntervalUtil {\n     */\n   def getColumnIntervalWithFilter(\n       originInterval: Option[ValueInterval],\n-      predicate: RexNode,\n+      oriPred: RexNode,\n       inputRef: Int,\n       rexBuilder: RexBuilder): ValueInterval = {\n+    val predicate = if (oriPred.getKind == SqlKind.SEARCH) {\n+      RexUtil.expandSearch(rexBuilder, null, oriPred)\n+    } else {\n+      oriPred\n+    }", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3Mjc2Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504472767", "bodyText": "It seems this method is not been used in any where.", "author": "godfreyhe", "createdAt": "2020-10-14T07:52:19Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/FlinkRexUtil.scala", "diffHunk": "@@ -343,6 +344,54 @@ object FlinkRexUtil {\n       }\n     })\n \n+  /**\n+   * Returns whether a given tree contains any {@link RexInputRef} nodes\n+   * with given indices.\n+   *\n+   * @param node a RexNode tree\n+   */\n+  def containsInputRef(node: RexNode, refs: JSet[Integer]): Boolean = try {\n+    val visitor = new RexVisitorImpl[Void](true) {\n+      override def visitInputRef(inputRef: RexInputRef): Void = {\n+        if (refs.contains(inputRef.getIndex)) {\n+          throw new Util.FoundOne(inputRef)\n+        }\n+        null\n+      }\n+    }\n+    node.accept(visitor)\n+    false\n+  } catch {\n+    case e: Util.FoundOne =>\n+      Util.swallow(e, null)\n+      true\n+  }", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI4ODczMA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505288730", "bodyText": "It is used by the following PR, i can remove it now ~", "author": "danny0405", "createdAt": "2020-10-15T07:49:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3Mjc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3MzAyNA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504473024", "bodyText": "revert this ?", "author": "godfreyhe", "createdAt": "2020-10-14T07:52:45Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/FlinkRexUtil.scala", "diffHunk": "@@ -353,7 +402,7 @@ object FlinkRexUtil {\n     */\n   private[flink] def adjustInputRef(\n       expr: RexNode,\n-      fieldsOldToNewIndexMapping: Map[Int, Int]): RexNode = expr.accept(\n+      fieldsOldToNewIndexMapping: Map[Int, Int]) = expr.accept(", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3NDEzMQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504474131", "bodyText": "Very useful abstraction ~", "author": "godfreyhe", "createdAt": "2020-10-14T07:54:42Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RelShuttles.scala", "diffHunk": "@@ -20,15 +20,15 @@ package org.apache.flink.table.planner.plan.utils\n import org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable\n \n import com.google.common.collect.Sets\n-import org.apache.calcite.plan.{RelOptUtil, ViewExpanders}\n-import org.apache.calcite.rel.core.{TableFunctionScan, TableScan}\n+import org.apache.calcite.plan.ViewExpanders\n+import org.apache.calcite.rel.core.TableScan\n import org.apache.calcite.rel.logical._\n-import org.apache.calcite.rel.{RelNode, RelShuttle, RelShuttleImpl}\n+import org.apache.calcite.rel.{RelHomogeneousShuttle, RelNode, RelShuttleImpl}\n import org.apache.calcite.rex.{RexNode, RexShuttle, RexSubQuery}\n \n import scala.collection.JavaConversions._\n \n-class DefaultRelShuttle extends RelShuttle {\n+class DefaultRelShuttle extends RelHomogeneousShuttle {", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3NjA4MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504476080", "bodyText": "RowTableFunction(s) does not work now ?", "author": "godfreyhe", "createdAt": "2020-10-14T07:57:32Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java", "diffHunk": "@@ -768,7 +768,8 @@ public void testRowTableFunction() throws Exception {\n \n \t\ttEnv().createTemporarySystemFunction(\"RowTableFunction\", RowTableFunction.class);\n \t\ttEnv().executeSql(\n-\t\t\t\t\"INSERT INTO SinkTable SELECT t.s, t.sa FROM SourceTable, LATERAL TABLE(RowTableFunction(s)) t\")\n+\t\t\t\t\"INSERT INTO SinkTable SELECT t.s, t.sa FROM SourceTable source, \"", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI5MzkxNQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505293915", "bodyText": "Yes, the same column name s has ambiguity now.", "author": "danny0405", "createdAt": "2020-10-15T07:53:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ3NjA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4MTQyNA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504481424", "bodyText": "this is a bad case, more data will be shuffled.", "author": "godfreyhe", "createdAt": "2020-10-14T08:06:18Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/HashAggregateTest.xml", "diffHunk": "@@ -564,16 +562,14 @@ HashAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS EXPR$0]), rowType\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- LogicalProject($f0=[0]), rowType=[RecordType(INTEGER $f0)]\n-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n HashAggregate(isMerge=[false], select=[COUNT(*) AS EXPR$0]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- Exchange(distribution=[single]), rowType=[RecordType(INTEGER $f0)]\n-   +- Calc(select=[0 AS $f0]), rowType=[RecordType(INTEGER $f0)]\n-      +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]], fields=[byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- Exchange(distribution=[single]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4MjI4MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504482280", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T08:07:42Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/SortAggregateTest.xml", "diffHunk": "@@ -574,16 +572,14 @@ SortAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS EXPR$0]), rowType\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- LogicalProject($f0=[0]), rowType=[RecordType(INTEGER $f0)]\n-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n SortAggregate(isMerge=[false], select=[COUNT(*) AS EXPR$0]), rowType=[RecordType(BIGINT EXPR$0)]\n-+- Exchange(distribution=[single]), rowType=[RecordType(INTEGER $f0)]\n-   +- Calc(select=[0 AS $f0]), rowType=[RecordType(INTEGER $f0)]\n-      +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105)]]], fields=[byte, short, int, long, float, double, boolean, string, date, time, timestamp, decimal3020, decimal105]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]\n++- Exchange(distribution=[single]), rowType=[RecordType(TINYINT byte, SMALLINT short, INTEGER int, BIGINT long, FLOAT float, DOUBLE double, BOOLEAN boolean, VARCHAR(2147483647) string, DATE date, TIME(0) time, TIMESTAMP(3) timestamp, DECIMAL(30, 20) decimal3020, DECIMAL(10, 5) decimal105)]", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4NTI1OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504485258", "bodyText": "AVG does not been rewritten as SUM / COUNT now ?", "author": "godfreyhe", "createdAt": "2020-10-14T08:12:20Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/agg/WindowAggregateTest.xml", "diffHunk": "@@ -699,10 +699,10 @@ LogicalProject(s=[$1], a=[$2], wStart=[TUMBLE_START($0)])\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Calc(select=[CAST(CASE(=($f1, 0), null:INTEGER, s)) AS s, CAST(CAST(/(CASE(=($f1, 0), null:INTEGER, s), $f1))) AS a, w$start AS wStart])\n-+- HashWindowAggregate(window=[TumblingGroupWindow('w$, b, 900000)], properties=[w$start, w$end, w$rowtime], select=[Final_$SUM0(sum$0) AS s, Final_COUNT(count1$1) AS $f1])\n+Calc(select=[CAST(s) AS s, CAST(a) AS a, w$start AS wStart])\n++- HashWindowAggregate(window=[TumblingGroupWindow('w$, b, 900000)], properties=[w$start, w$end, w$rowtime], select=[Final_SUM(sum$0) AS s, Final_AVG(sum$1, count$2) AS a])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzM5Njk0NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507396944", "bodyText": "The cost are the same with the old plan, same cause as the Union project push case.", "author": "danny0405", "createdAt": "2020-10-19T03:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4NTI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzM5NzY5NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507397694", "bodyText": "Tracked by FLINK-19668.", "author": "danny0405", "createdAt": "2020-10-19T03:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ4NTI1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ5NDYzNA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504494634", "bodyText": "bad case!", "author": "godfreyhe", "createdAt": "2020-10-14T08:27:16Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/sql/RankTest.xml", "diffHunk": "@@ -142,19 +171,17 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Calc(select=[a, b, $2])\n-+- Rank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b, c], orderBy=[a ASC], global=[true], select=[a, b, c, $2])\n+Calc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- OverAggregate(partitionBy=[b, c], orderBy=[a ASC], window#0=[RANK(*) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o0])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwNjQ5NQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504506495", "bodyText": "bad case", "author": "godfreyhe", "createdAt": "2020-10-14T08:45:20Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkLogicalRankRuleForConstantRangeTest.xml", "diffHunk": "@@ -178,14 +178,14 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, b, $2])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b,c], orderBy=[a ASC], select=[a, b, c, $2])\n+FlinkLogicalCalc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {1, 2} order by [0 ASC-nulls-first] aggs [RANK()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwNzQyOQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504507429", "bodyText": "bad case", "author": "godfreyhe", "createdAt": "2020-10-14T08:46:42Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/FlinkLogicalRankRuleForRangeEndTest.xml", "diffHunk": "@@ -131,14 +131,14 @@ WHERE rk <= 2 AND rk > -2\n       <![CDATA[\n LogicalProject(a=[$0], b=[$1], rk=[$2])\n +- LogicalFilter(condition=[AND(<=($2, 2), >($2, -2))])\n-   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], rk=[RANK() OVER (PARTITION BY $1, $2 ORDER BY $0 NULLS FIRST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, b, w0$o0])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=-1, rankEnd=2], partitionBy=[b,c], orderBy=[a ASC], select=[a, b, c, w0$o0])\n+FlinkLogicalCalc(select=[a, b, w0$o0 AS $2], where=[SEARCH(w0$o0, Sarg[(-2..2]])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {1, 2} order by [0 ASC-nulls-first] aggs [RANK()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwODcyMg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504508722", "bodyText": "bad case", "author": "godfreyhe", "createdAt": "2020-10-14T08:48:42Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -30,16 +30,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[RANK() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[RANK() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, w0$o0])\n-+- FlinkLogicalRank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime, w0$o0])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] aggs [RANK()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTIzMA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509230", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T08:49:24Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -57,16 +56,15 @@ WHERE rank_num >= 1 AND rank_num < 3\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 3))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, w0$o0])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=2], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime, w0$o0])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..3)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTMzMQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509331", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T08:49:32Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -84,16 +82,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwOTM5Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504509397", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T08:49:38Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/RankNumberColumnRemoveRuleTest.xml", "diffHunk": "@@ -111,16 +108,15 @@ WHERE rank_num >= 1 AND rank_num < 2\n       <![CDATA[\n LogicalProject(a=[$0], rank_num=[$4])\n +- LogicalFilter(condition=[AND(>=($4, 1), <($4, 2))])\n-   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)])\n+   +- LogicalProject(a=[$0], b=[$1], c=[$2], rowtime=[$3], rank_num=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $3 DESC NULLS LAST)])\n       +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-FlinkLogicalCalc(select=[a, 1:BIGINT AS w0$o0])\n-+- FlinkLogicalRank(rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[a], orderBy=[rowtime DESC], select=[a, rowtime])\n-   +- FlinkLogicalCalc(select=[a, rowtime])\n-      +- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]])\n+FlinkLogicalCalc(select=[a, w0$o0 AS rank_num], where=[SEARCH(w0$o0, Sarg[[1..2)])])\n++- FlinkLogicalOverAggregate(window#0=[window(partition {0} order by [3 DESC-nulls-last] rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()])])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDY2Mg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504514662", "bodyText": "do you know which jira causes this join-order change without join-order rule ?", "author": "godfreyhe", "createdAt": "2020-10-14T08:57:15Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/join/SemiAntiJoinTest.xml", "diffHunk": "@@ -516,28 +516,28 @@ LogicalFilter(condition=[<>($cor0.b, $1)])\n       <![CDATA[\n Join(joinType=[LeftAntiJoin], where=[<>(b, e)], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n :- Exchange(distribution=[single])\n-:  +- Join(joinType=[LeftSemiJoin], where=[$f0], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n-:     :- Exchange(distribution=[single])\n-:     :  +- Join(joinType=[LeftAntiJoin], where=[AND(OR(=(b, i), IS NULL(b), IS NULL(i)), =(c, k))], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])\n-:     :     :- Exchange(distribution=[hash[c]])\n+:  +- Join(joinType=[LeftAntiJoin], where=[AND(OR(IS NULL(b), IS NULL(i), =(b, i)), =(c, k))], select=[a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTMyODEwOQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505328109", "bodyText": "Actually i'm not sure.", "author": "danny0405", "createdAt": "2020-10-15T08:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDY2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNzM0Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504517347", "bodyText": "bad case, shuffle more data", "author": "godfreyhe", "createdAt": "2020-10-14T09:01:12Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableScanTest.xml", "diffHunk": "@@ -23,17 +23,16 @@ limitations under the License.\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()])\n-+- LogicalProject($f0=[0])\n-   +- LogicalFilter(condition=[>($1, 1)])\n-      +- LogicalTableScan(table=[[default_catalog, default_database, src]])\n++- LogicalFilter(condition=[>($1, 1)])\n+   +- LogicalTableScan(table=[[default_catalog, default_database, src]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n GroupAggregate(select=[COUNT_RETRACT(*) AS EXPR$0], changelogMode=[I,UA,D])\n +- Exchange(distribution=[single], changelogMode=[I,UB,UA])\n-   +- Calc(select=[0 AS $f0], where=[>(a, 1)], changelogMode=[I,UB,UA])\n-      +- TableSourceScan(table=[[default_catalog, default_database, src, filter=[], project=[a]]], fields=[a], changelogMode=[I,UB,UA])\n+   +- Calc(select=[ts, a, b], where=[>(a, 1)], changelogMode=[I,UB,UA])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNzk4Mg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504517982", "bodyText": "bad case", "author": "godfreyhe", "createdAt": "2020-10-14T09:02:05Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/TableSourceTest.xml", "diffHunk": "@@ -68,15 +68,14 @@ Calc(select=[rtime])\n     <Resource name=\"planBefore\">\n       <![CDATA[\n LogicalAggregate(group=[{}], EXPR$0=[COUNT()])\n-+- LogicalProject($f0=[1])\n-   +- LogicalTableScan(table=[[default_catalog, default_database, T]])\n++- LogicalTableScan(table=[[default_catalog, default_database, T]])\n ]]>\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n GroupAggregate(select=[COUNT(*) AS EXPR$0])\n +- Exchange(distribution=[single])\n-   +- TableSourceScan(table=[[default_catalog, default_database, T, project=[]]], fields=[])\n+   +- TableSourceScan(table=[[default_catalog, default_database, T]], fields=[id, name])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxODg3Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504518877", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-10-14T09:03:30Z", "path": "flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/SetOperatorsTest.xml", "diffHunk": "@@ -100,10 +100,9 @@ LogicalProject(b=[$1], c=[$2])\n     </Resource>\n     <Resource name=\"planAfter\">\n       <![CDATA[\n-Union(all=[true], union=[b, c])\n-:- Calc(select=[b, c])\n-:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, left, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n-+- Calc(select=[b, c])\n+Calc(select=[b, c])", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk5MDI3Mg==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505990272", "bodyText": "Because the cost is the same, tracked by FLINK-19668.", "author": "danny0405", "createdAt": "2020-10-16T02:20:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxODg3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxOTYxMw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504519613", "bodyText": "remove unused imports", "author": "godfreyhe", "createdAt": "2020-10-14T09:04:33Z", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/CorrelateTest.scala", "diffHunk": "@@ -23,7 +23,7 @@ import org.apache.flink.table.api._\n import org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram\n import org.apache.flink.table.planner.utils.{MockPythonTableFunction, TableFunc0, TableFunc1, TableTestBase}\n \n-import org.apache.calcite.rel.rules.{CalcMergeRule, FilterCalcMergeRule, ProjectCalcMergeRule}\n+import org.apache.calcite.rel.rules.{CalcMergeRule, CoreRules, FilterCalcMergeRule, ProjectCalcMergeRule}", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUzNTI3MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504535270", "bodyText": "use assertEquals with delta parameter, we only need to compare two-bit precision.", "author": "godfreyhe", "createdAt": "2020-10-14T09:29:27Z", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdDistinctRowCountTest.scala", "diffHunk": "@@ -91,35 +91,43 @@ class FlinkRelMdDistinctRowCountTest extends FlinkRelMdHandlerTestBase {\n     assertEquals(1.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(), null))\n     assertEquals(50.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(0), null))\n     assertEquals(48.0, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(1), null))\n-    assertEquals(16.96, mq.getDistinctRowCount(logicalProject, ImmutableBitSet.of(2), null), 1e-2)\n+    assertEquals(17.13976902522821,", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwMzk3Nw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504603977", "bodyText": "is this within our expectations?", "author": "godfreyhe", "createdAt": "2020-10-14T11:31:55Z", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdPopulationSizeTest.scala", "diffHunk": "@@ -294,28 +306,28 @@ class FlinkRelMdPopulationSizeTest extends FlinkRelMdHandlerTestBase {\n \n     assertEquals(1.0, mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of()))\n     assertEquals(2.0E7, mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(0)))\n-    assertEquals(505696447.06,\n+    assertEquals(5.056964454581646E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(1)), 1e-2)\n-    assertEquals(799999979.15,\n+    assertEquals(8.0E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(1, 5)), 1e-2)\n-    assertEquals(793772745.78,\n+    assertEquals(7.937719925300186E8,\n       mq.getPopulationSize(logicalLeftJoinNotOnUniqueKeys, ImmutableBitSet.of(0, 6)), 1e-2)\n \n     assertEquals(1.0,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of()))\n-    assertEquals(12642411.178,\n+    assertEquals(1.2642411364806734E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(0)), 1e-2)\n-    assertEquals(19752070.37,\n+    assertEquals(1.9752070270976853E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(1)), 1e-2)\n-    assertEquals(19999999.87,\n+    assertEquals(2.0E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(1, 5)), 1e-2)\n-    assertEquals(19996088.14,\n+    assertEquals(1.9996069026214965E7,\n       mq.getPopulationSize(logicalRightJoinOnLHSUniqueKeys, ImmutableBitSet.of(0, 6)), 1e-2)\n \n     assertEquals(1.0, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of()))\n     assertEquals(2.0E7, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of(0)))\n     assertEquals(8.0E8, mq.getPopulationSize(logicalFullJoinWithoutEquiCond, ImmutableBitSet.of(1)))\n-    assertEquals(6.295509444597865E15,\n+    assertEquals(8.0E15,", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTM2Mzc3Mw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r505363773", "bodyText": "Caused by CALCITE-4132, but i'm not sure if it is better.", "author": "danny0405", "createdAt": "2020-10-15T08:51:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwMzk3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNjMzMA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r504606330", "bodyText": "we can implement a rule to re-add a projection node for one phase aggregation", "author": "godfreyhe", "createdAt": "2020-10-14T11:36:39Z", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoLegacyTableSourceScanRuleTest.scala", "diffHunk": "@@ -111,6 +111,9 @@ class PushProjectIntoLegacyTableSourceScanRuleTest extends TableTestBase {\n \n   @Test\n   def testProjectWithoutInputRef(): Unit = {\n+    // Regression by: CALCITE-4220,", "originalCommit": "3db5dd700d90b99ec0a5d70d9b27a0df41a44164", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "95841cbd815bd7dc9e773e00d164f961fd9bef1e", "url": "https://github.com/apache/flink/commit/95841cbd815bd7dc9e773e00d164f961fd9bef1e", "message": "Fix review comments", "committedDate": "2020-10-16T05:48:51Z", "type": "forcePushed"}, {"oid": "b14e01c94da3395f712b1a7f5f062fef1e367fb2", "url": "https://github.com/apache/flink/commit/b14e01c94da3395f712b1a7f5f062fef1e367fb2", "message": "[FLINK-16579][table] Update Calcite version of pom and NOTICE file\n\nAs a dependency, the Guava version upgrade to 29.0-jre, janino version upgrade to 3.0.11", "committedDate": "2020-10-19T03:15:39Z", "type": "commit"}, {"oid": "1b3686abb009159aa2062b3bf178bae7a9cee08e", "url": "https://github.com/apache/flink/commit/1b3686abb009159aa2062b3bf178bae7a9cee08e", "message": "[FLINK-16579][table] Upgrade SQL parser Calcite version to 1.26.0", "committedDate": "2020-10-19T03:15:39Z", "type": "commit"}, {"oid": "1e6804f7a0f4b3165115feca6dd6bdb66a46baa9", "url": "https://github.com/apache/flink/commit/1e6804f7a0f4b3165115feca6dd6bdb66a46baa9", "message": "[FLINK-16579][table] Fix the API change for planner\n\n* SqlParser, SqlValidator and SqlToRelConverter now have a Config bean for configuration\n* Since CALCITE-2082, all the UDF needs a SqlOperandMetadata for operand type inference\n  (before the change, it is SqlOperandTypeChecker)\n* Since CALCITE-4215, Statistic default collations change from empty list to null", "committedDate": "2020-10-19T03:21:12Z", "type": "commit"}, {"oid": "3d780cebbed6e5301a78543489dc6a7c4ec6d273", "url": "https://github.com/apache/flink/commit/3d780cebbed6e5301a78543489dc6a7c4ec6d273", "message": "[FLINK-16579][table] Replace the rule instance with new one since CALCITE-3923\n\nThe core rules are changed to support new way of parameterization, and moved to CoreRules.", "committedDate": "2020-10-19T03:21:12Z", "type": "commit"}, {"oid": "40f569428531fd0f9a5075e32f0f819b0d0328ea", "url": "https://github.com/apache/flink/commit/40f569428531fd0f9a5075e32f0f819b0d0328ea", "message": "[FLINK-16579][table] Since CALCITE-3877, the default over window bounds does not print out in the plan digest", "committedDate": "2020-10-19T03:21:13Z", "type": "commit"}, {"oid": "4c0252a6db5699cf5505e356aae94ae49bb4fe01", "url": "https://github.com/apache/flink/commit/4c0252a6db5699cf5505e356aae94ae49bb4fe01", "message": "[FLINK-16579][table] Since CALCITE-3877, the default over window bounds does not print out in the plan digest", "committedDate": "2020-10-19T03:21:13Z", "type": "commit"}, {"oid": "d978e4fdfcf24f694bd1eadee0dc3ddcf029fd43", "url": "https://github.com/apache/flink/commit/d978e4fdfcf24f694bd1eadee0dc3ddcf029fd43", "message": "[FLINK-16579][table] Since CALCITE-4220, the aggregate is promoted automatically after sql to rel conversion", "committedDate": "2020-10-19T03:21:13Z", "type": "commit"}, {"oid": "fbeaabe5b9f2f64500f68b38192dbac4cb0c3bfa", "url": "https://github.com/apache/flink/commit/fbeaabe5b9f2f64500f68b38192dbac4cb0c3bfa", "message": "[FLINK-16579][table] The NDV algorithm has been tweaked since CALCITE-4132", "committedDate": "2020-10-19T03:21:13Z", "type": "commit"}, {"oid": "b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "url": "https://github.com/apache/flink/commit/b56c3e8c4176504173eb1bb4dcc312082dabc4bc", "message": "Fix review comments", "committedDate": "2020-10-19T03:21:15Z", "type": "forcePushed"}, {"oid": "cee7bbd18b87444a31483cd90607d9757b043ab6", "url": "https://github.com/apache/flink/commit/cee7bbd18b87444a31483cd90607d9757b043ab6", "message": "Fix review comments", "committedDate": "2020-10-19T03:31:39Z", "type": "forcePushed"}, {"oid": "98165e15f06942ca55663fd9c1de667f8008139e", "url": "https://github.com/apache/flink/commit/98165e15f06942ca55663fd9c1de667f8008139e", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments", "committedDate": "2020-10-19T03:51:50Z", "type": "forcePushed"}, {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "url": "https://github.com/apache/flink/commit/08fada0fcc5270803b39a711b20f519c3e6c7fe4", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments", "committedDate": "2020-10-19T06:41:49Z", "type": "commit"}, {"oid": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "url": "https://github.com/apache/flink/commit/08fada0fcc5270803b39a711b20f519c3e6c7fe4", "message": "[FLINK-16579][table] All kinds of left plan changes\n\n* The predicate normalization now only happens during planning, that means it does not change in the digest anymore\n* The HOP and SESSION window names changes to $HOP and $SESSION, both are deprecated\n* IS NOT DISTINCT FROM is expanded in the plan now\n* Many sort aggregate changes to hash aggregate which are more efficient\n\nFix review comments", "committedDate": "2020-10-19T06:41:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyMjYxMw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507722613", "bodyText": "remove this ?", "author": "godfreyhe", "createdAt": "2020-10-19T12:55:09Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/TableSqlFunction.scala", "diffHunk": "@@ -82,9 +83,16 @@ class TableSqlFunction(\n \n   override def toString: String = displayName\n \n-  override def getRowType(\n+  override def getRowTypeInference: SqlReturnTypeInference = new SqlReturnTypeInference {\n+    override def inferReturnType(opBinding: SqlOperatorBinding): RelDataType = {\n+      val arguments = convertArguments(opBinding, functionImpl, getNameAsId)\n+      getRowType(opBinding.getTypeFactory, arguments)\n+    }\n+  }", "originalCommit": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2MzYwMQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r508163601", "bodyText": "It is needed because getRowType needs to be overridden.", "author": "danny0405", "createdAt": "2020-10-20T02:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyMjYxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyMzE3MA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507723170", "bodyText": "unused import", "author": "godfreyhe", "createdAt": "2020-10-19T12:56:00Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/functions/utils/AggSqlFunction.scala", "diffHunk": "@@ -19,24 +19,25 @@\n package org.apache.flink.table.planner.functions.utils\n \n import org.apache.flink.table.api.ValidationException\n-import org.apache.flink.table.functions.{AggregateFunction, FunctionIdentifier, TableAggregateFunction, ImperativeAggregateFunction}\n+import org.apache.flink.table.functions.{AggregateFunction, FunctionIdentifier, ImperativeAggregateFunction, TableAggregateFunction}\n+import org.apache.flink.table.planner.JList\n import org.apache.flink.table.planner.calcite.FlinkTypeFactory\n import org.apache.flink.table.planner.functions.bridging.BridgingSqlAggFunction\n-import org.apache.flink.table.planner.functions.utils.AggSqlFunction.{createOperandTypeChecker, createOperandTypeInference, createReturnTypeInference}\n+import org.apache.flink.table.planner.functions.utils.AggSqlFunction.{createOperandMetadata, createOperandTypeInference, createReturnTypeInference}\n import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils._\n import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType\n import org.apache.flink.table.types.DataType\n import org.apache.flink.table.types.logical.LogicalType\n \n-import org.apache.calcite.rel.`type`.RelDataType\n+import org.apache.calcite.rel.`type`.{RelDataType, RelDataTypeFactory}\n import org.apache.calcite.sql._\n import org.apache.calcite.sql.`type`.SqlOperandTypeChecker.Consistency\n import org.apache.calcite.sql.`type`._\n import org.apache.calcite.sql.parser.SqlParserPos\n import org.apache.calcite.sql.validate.SqlUserDefinedAggFunction\n import org.apache.calcite.util.Optionality\n \n-import java.util\n+import java.util.Collections", "originalCommit": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyNTgzNA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507725834", "bodyText": "is this change necessary ?", "author": "godfreyhe", "createdAt": "2020-10-19T13:00:02Z", "path": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdUniqueKeys.scala", "diffHunk": "@@ -58,6 +61,18 @@ class FlinkRelMdUniqueKeys private extends MetadataHandler[BuiltInMetadata.Uniqu\n     getTableUniqueKeys(null, rel.getTable)\n   }\n \n+  def getUniqueKeys(\n+      rel: TableFunctionScan,\n+      mq: RelMetadataQuery,\n+      ignoreNulls: Boolean): JSet[ImmutableBitSet] = {\n+    if (rel.getInputs.size() == 1\n+        && rel.getCall.asInstanceOf[RexCall].getOperator.isInstanceOf[SqlWindowTableFunction]) {\n+      mq.getUniqueKeys(rel.getInput(0), ignoreNulls)\n+    } else {\n+      null\n+    }\n+  }", "originalCommit": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2MDY3OA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r508160678", "bodyText": "No, i can remove it.", "author": "danny0405", "createdAt": "2020-10-20T01:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzcyNTgzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzczMDI3OQ==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507730279", "bodyText": "this also need to update", "author": "godfreyhe", "createdAt": "2020-10-19T13:07:17Z", "path": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/metadata/FlinkRelMdPopulationSizeTest.scala", "diffHunk": "@@ -66,19 +66,25 @@ class FlinkRelMdPopulationSizeTest extends FlinkRelMdHandlerTestBase {\n     assertEquals(1.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of()))\n     assertEquals(50.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(0)))\n     assertEquals(48.0, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(1)))\n-    assertEquals(16.22, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(2)), 1e-2)\n-    assertEquals(6.98, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(3)), 1e-2)\n-    assertEquals(20.09, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(4)), 1e-2)\n-    assertEquals(20.09, mq.getPopulationSize(logicalProject, ImmutableBitSet.of(5)), 1e-2)\n+    assertEquals(16.43531528030365,", "originalCommit": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzczMTY2NA==", "url": "https://github.com/apache/flink/pull/13577#discussion_r507731664", "bodyText": "unnecessary change", "author": "godfreyhe", "createdAt": "2020-10-19T13:09:35Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/util/collections/ByteHashSet.java", "diffHunk": "@@ -26,7 +26,7 @@\n \n \tprotected boolean[] used;\n \n-\tpublic ByteHashSet() {\n+\tpublic ByteHashSet(final int expected) {", "originalCommit": "08fada0fcc5270803b39a711b20f519c3e6c7fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2MDMyNw==", "url": "https://github.com/apache/flink/pull/13577#discussion_r508160327", "bodyText": "This is needed because it is a bug.", "author": "danny0405", "createdAt": "2020-10-20T01:50:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzczMTY2NA=="}], "type": "inlineReview"}, {"oid": "ba0ed645a060dc91363fb4334dadff0edcfb0e40", "url": "https://github.com/apache/flink/commit/ba0ed645a060dc91363fb4334dadff0edcfb0e40", "message": "Fix the review comments", "committedDate": "2020-10-20T02:10:08Z", "type": "commit"}, {"oid": "1123f2ba056792dc7dd24feb4d49c6c235f47389", "url": "https://github.com/apache/flink/commit/1123f2ba056792dc7dd24feb4d49c6c235f47389", "message": "Fix review comments", "committedDate": "2020-10-20T03:29:54Z", "type": "commit"}]}