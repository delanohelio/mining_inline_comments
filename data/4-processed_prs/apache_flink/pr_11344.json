{"pr_number": 11344, "pr_title": "[FLINK-16250][python][ml] Add interfaces for PipelineStage and Pipeline", "pr_createdAt": "2020-03-07T14:18:08Z", "pr_url": "https://github.com/apache/flink/pull/11344", "timeline": [{"oid": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "url": "https://github.com/apache/flink/commit/99e47d0ba1b97997531c2bbaee32091ddc1e344e", "message": "[FLINK-16250][python][ml] Add interfaces for PipelineStage and Pipeline", "committedDate": "2020-03-07T14:10:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0MzkyNg==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390143926", "bodyText": "The return type of self.get_params().to_json() is already string and so the casting could be removed?", "author": "dianfu", "createdAt": "2020-03-10T08:02:32Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,270 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        import re\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    def _make_java_value(self, obj):\n+        \"\"\" Convert Python object into Java \"\"\"\n+        if isinstance(obj, list):\n+            obj = [self._make_java_value(x) for x in obj]\n+        return obj\n+\n+    def to_json(self) -> str:\n+        return str(self.get_params().to_json())", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MzM1OQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390153359", "bodyText": "The Java PipeLine has a constructor which accepts a string of pipelineJson,  should it also be supported in the Python Pipeline?", "author": "dianfu", "createdAt": "2020-03-10T08:25:18Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,270 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        import re\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    def _make_java_value(self, obj):\n+        \"\"\" Convert Python object into Java \"\"\"\n+        if isinstance(obj, list):\n+            obj = [self._make_java_value(x) for x in obj]\n+        return obj\n+\n+    def to_json(self) -> str:\n+        return str(self.get_params().to_json())\n+\n+    def load_json(self, json: str) -> None:\n+        self.get_params().load_json(json)\n+\n+\n+class Transformer(PipelineStage):\n+    \"\"\"\n+    A transformer is a PipelineStage that transforms an input Table to a result Table.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    @abstractmethod\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaTransformer(Transformer):\n+    \"\"\"\n+    Base class for Transformer that wrap Java implementations. Subclasses should\n+    ensure they have the transformer Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return Table(self._j_obj.transform(table_env._j_tenv, table._j_table))\n+\n+\n+class Model(Transformer):\n+    \"\"\"\n+    Abstract class for models that are fitted by estimators.\n+\n+    A model is an ordinary Transformer except how it is created. While ordinary transformers\n+    are defined by specifying the parameters directly, a model is usually generated by an Estimator\n+    when Estimator.fit(table_env, table) is invoked.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+\n+class JavaModel(JavaTransformer, Model):\n+    \"\"\"\n+    Base class for JavaTransformer that wrap Java implementations.\n+    Subclasses should ensure they have the model Java object available as j_obj.\n+    \"\"\"\n+\n+\n+class Estimator(PipelineStage):\n+    \"\"\"\n+    Estimators are PipelineStages responsible for training and generating machine learning models.\n+\n+    The implementations are expected to take an input table as training samples and generate a\n+    Model which fits these samples.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> Model:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaEstimator(Estimator):\n+    \"\"\"\n+    Base class for Estimator that wrap Java implementations.\n+    Subclasses should ensure they have the estimator Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> JavaModel:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return JavaModel(self._j_obj.fit(table_env._j_tenv, table._j_table))\n+\n+\n+class Pipeline(Estimator, Model):\n+    \"\"\"\n+    A pipeline is a linear workflow which chains Estimators and Transformers to\n+    execute an algorithm.\n+\n+    A pipeline itself can either act as an Estimator or a Transformer, depending on the stages it\n+    includes. More specifically:\n+\n+\n+    If a Pipeline has an Estimator, one needs to call `Pipeline.fit(TableEnvironment, Table)`\n+    before use the pipeline as a Transformer. In this case the Pipeline is an Estimator and\n+    can produce a Pipeline as a `Model`.\n+\n+    If a Pipeline has noEstimator, it is a Transformer and can be applied to a Table directly.\n+    In this case, `Pipeline#fit(TableEnvironment, Table)` will simply return the pipeline itself.\n+\n+\n+    In addition, a pipeline can also be used as a PipelineStage in another pipeline, just like an\n+    ordinaryEstimator or Transformer as describe above.\n+    \"\"\"\n+\n+    def __init__(self, stages=None):", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIwMDQ5MA==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390200490", "bodyText": "Good catch!", "author": "hequn8128", "createdAt": "2020-03-10T09:53:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MzM1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1Njg0OQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390156849", "bodyText": "+1", "author": "dianfu", "createdAt": "2020-03-10T08:32:56Z", "path": "flink-python/pyflink/ml/api/param/base.py", "diffHunk": "@@ -164,17 +164,16 @@ def to_json(self) -> str:\n         import jsonpickle\n         return str(jsonpickle.encode(self._param_map, keys=True))\n \n-    def load_json(self, json: str) -> 'Params':\n+    def load_json(self, json: str) -> None:", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MzM4MA==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390163380", "bodyText": "trainTable -> train_table", "author": "dianfu", "createdAt": "2020-03-10T08:46:30Z", "path": "flink-python/pyflink/ml/tests/test_pipeline_it_case.py", "diffHunk": "@@ -0,0 +1,154 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from pyflink.table.types import DataTypes\n+from pyflink.testing.test_case_utils import MLTestCase\n+\n+from pyflink.ml.api import JavaTransformer, Transformer, Estimator, Model, \\\n+    MLEnvironmentFactory, Pipeline\n+from pyflink.ml.lib.param.colname import HasSelectedCols, HasVectorCol,\\\n+    HasPredictionCol, HasOutputCol\n+from pyflink import keyword\n+from pyflink.testing import source_sink_utils\n+from pyflink.java_gateway import get_gateway\n+\n+\n+class WrapperTransformer(JavaTransformer, HasSelectedCols):\n+    \"\"\"\n+    A Transformer wrappers Java Transformer.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None):\n+        _j_obj = get_gateway().jvm.org.apache.flink.ml.pipeline.\\\n+            UserDefinedPipelineStages.SelectColumnTransformer()\n+        super().__init__(_j_obj)\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+\n+class PythonAddTransformer(Transformer, HasSelectedCols, HasOutputCol):\n+    \"\"\"\n+    A Transformer which is implemented with Python. Output a column\n+    contains the sum of all columns.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None, output_col=None):\n+        super().__init__()\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+    def transform(self, table_env, table):\n+        input_columns = self.get_selected_cols()\n+        expr = \"+\".join(input_columns)\n+        expr = expr + \" as \" + self.get_output_col()\n+        return table.add_columns(expr)\n+\n+\n+class PythonEstimator(Estimator, HasVectorCol, HasPredictionCol):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def fit(self, table_env, table):\n+        return PythonModel(\n+            table_env,\n+            table.select(\"max(features) as max_sum\"),\n+            self.get_prediction_col())\n+\n+\n+class PythonModel(Model):\n+\n+    def __init__(self, table_env, model_data_table, output_col_name):\n+        self._model_data_table = model_data_table\n+        self._output_col_name = output_col_name\n+        self.max_sum = 0\n+        self.load_model(table_env)\n+\n+    def load_model(self, table_env):\n+        \"\"\"\n+        Train the model to get the max_sum value which is used to predicate data.\n+        \"\"\"\n+        table_sink = source_sink_utils.TestRetractSink([\"max_sum\"], [DataTypes.BIGINT()])\n+        table_env.register_table_sink(\"Model_Results\", table_sink)\n+        self._model_data_table.insert_into(\"Model_Results\")\n+        table_env.execute(\"load model\")\n+        actual = source_sink_utils.results()\n+        self.max_sum = actual.apply(0)\n+\n+    def transform(self, table_env, table):\n+        \"\"\"\n+        Use max_sum to predicate input. Return turn if input value is bigger than max_sum\n+        \"\"\"\n+        return table\\\n+            .add_columns(\"features > {} as {}\".format(self.max_sum, self._output_col_name))\\\n+            .select(\"{}\".format(self._output_col_name))\n+\n+\n+class PythonPipelineTest(MLTestCase):\n+\n+    def test_java_transformer(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+\n+        table_sink = source_sink_utils.TestAppendSink(\n+            ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])\n+        t_env.register_table_sink(\"TransformerResults\", table_sink)\n+\n+        source_table = t_env.from_elements([(1, 2, 3, 4), (4, 3, 2, 1)], ['a', 'b', 'c', 'd'])\n+        transformer = WrapperTransformer(selected_cols=[\"a\", \"b\"])\n+        transformer\\\n+            .transform(t_env, source_table)\\\n+            .insert_into(\"TransformerResults\")\n+\n+        # execute\n+        t_env.execute('JavaPipelineITCase')\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual, [\"1,2\", \"4,3\"])\n+\n+    def test_pipeline(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+        MLEnvironmentFactory().get_default().get_stream_execution_environment().set_parallelism(1)\n+        trainTable = t_env.from_elements(", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MzQ3NQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390163475", "bodyText": "servingTable -> serving_table", "author": "dianfu", "createdAt": "2020-03-10T08:46:41Z", "path": "flink-python/pyflink/ml/tests/test_pipeline_it_case.py", "diffHunk": "@@ -0,0 +1,154 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from pyflink.table.types import DataTypes\n+from pyflink.testing.test_case_utils import MLTestCase\n+\n+from pyflink.ml.api import JavaTransformer, Transformer, Estimator, Model, \\\n+    MLEnvironmentFactory, Pipeline\n+from pyflink.ml.lib.param.colname import HasSelectedCols, HasVectorCol,\\\n+    HasPredictionCol, HasOutputCol\n+from pyflink import keyword\n+from pyflink.testing import source_sink_utils\n+from pyflink.java_gateway import get_gateway\n+\n+\n+class WrapperTransformer(JavaTransformer, HasSelectedCols):\n+    \"\"\"\n+    A Transformer wrappers Java Transformer.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None):\n+        _j_obj = get_gateway().jvm.org.apache.flink.ml.pipeline.\\\n+            UserDefinedPipelineStages.SelectColumnTransformer()\n+        super().__init__(_j_obj)\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+\n+class PythonAddTransformer(Transformer, HasSelectedCols, HasOutputCol):\n+    \"\"\"\n+    A Transformer which is implemented with Python. Output a column\n+    contains the sum of all columns.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None, output_col=None):\n+        super().__init__()\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+    def transform(self, table_env, table):\n+        input_columns = self.get_selected_cols()\n+        expr = \"+\".join(input_columns)\n+        expr = expr + \" as \" + self.get_output_col()\n+        return table.add_columns(expr)\n+\n+\n+class PythonEstimator(Estimator, HasVectorCol, HasPredictionCol):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def fit(self, table_env, table):\n+        return PythonModel(\n+            table_env,\n+            table.select(\"max(features) as max_sum\"),\n+            self.get_prediction_col())\n+\n+\n+class PythonModel(Model):\n+\n+    def __init__(self, table_env, model_data_table, output_col_name):\n+        self._model_data_table = model_data_table\n+        self._output_col_name = output_col_name\n+        self.max_sum = 0\n+        self.load_model(table_env)\n+\n+    def load_model(self, table_env):\n+        \"\"\"\n+        Train the model to get the max_sum value which is used to predicate data.\n+        \"\"\"\n+        table_sink = source_sink_utils.TestRetractSink([\"max_sum\"], [DataTypes.BIGINT()])\n+        table_env.register_table_sink(\"Model_Results\", table_sink)\n+        self._model_data_table.insert_into(\"Model_Results\")\n+        table_env.execute(\"load model\")\n+        actual = source_sink_utils.results()\n+        self.max_sum = actual.apply(0)\n+\n+    def transform(self, table_env, table):\n+        \"\"\"\n+        Use max_sum to predicate input. Return turn if input value is bigger than max_sum\n+        \"\"\"\n+        return table\\\n+            .add_columns(\"features > {} as {}\".format(self.max_sum, self._output_col_name))\\\n+            .select(\"{}\".format(self._output_col_name))\n+\n+\n+class PythonPipelineTest(MLTestCase):\n+\n+    def test_java_transformer(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+\n+        table_sink = source_sink_utils.TestAppendSink(\n+            ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])\n+        t_env.register_table_sink(\"TransformerResults\", table_sink)\n+\n+        source_table = t_env.from_elements([(1, 2, 3, 4), (4, 3, 2, 1)], ['a', 'b', 'c', 'd'])\n+        transformer = WrapperTransformer(selected_cols=[\"a\", \"b\"])\n+        transformer\\\n+            .transform(t_env, source_table)\\\n+            .insert_into(\"TransformerResults\")\n+\n+        # execute\n+        t_env.execute('JavaPipelineITCase')\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual, [\"1,2\", \"4,3\"])\n+\n+    def test_pipeline(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+        MLEnvironmentFactory().get_default().get_stream_execution_environment().set_parallelism(1)\n+        trainTable = t_env.from_elements(\n+            [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], ['a', 'b'])\n+        servingTable = t_env.from_elements([(0, 0), (12, 3)], ['a', 'b'])", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2NTk1Mg==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390165952", "bodyText": "Seems that there is no need to set the parallelism?", "author": "dianfu", "createdAt": "2020-03-10T08:51:35Z", "path": "flink-python/pyflink/ml/tests/test_pipeline_it_case.py", "diffHunk": "@@ -0,0 +1,154 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from pyflink.table.types import DataTypes\n+from pyflink.testing.test_case_utils import MLTestCase\n+\n+from pyflink.ml.api import JavaTransformer, Transformer, Estimator, Model, \\\n+    MLEnvironmentFactory, Pipeline\n+from pyflink.ml.lib.param.colname import HasSelectedCols, HasVectorCol,\\\n+    HasPredictionCol, HasOutputCol\n+from pyflink import keyword\n+from pyflink.testing import source_sink_utils\n+from pyflink.java_gateway import get_gateway\n+\n+\n+class WrapperTransformer(JavaTransformer, HasSelectedCols):\n+    \"\"\"\n+    A Transformer wrappers Java Transformer.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None):\n+        _j_obj = get_gateway().jvm.org.apache.flink.ml.pipeline.\\\n+            UserDefinedPipelineStages.SelectColumnTransformer()\n+        super().__init__(_j_obj)\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+\n+class PythonAddTransformer(Transformer, HasSelectedCols, HasOutputCol):\n+    \"\"\"\n+    A Transformer which is implemented with Python. Output a column\n+    contains the sum of all columns.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None, output_col=None):\n+        super().__init__()\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+    def transform(self, table_env, table):\n+        input_columns = self.get_selected_cols()\n+        expr = \"+\".join(input_columns)\n+        expr = expr + \" as \" + self.get_output_col()\n+        return table.add_columns(expr)\n+\n+\n+class PythonEstimator(Estimator, HasVectorCol, HasPredictionCol):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def fit(self, table_env, table):\n+        return PythonModel(\n+            table_env,\n+            table.select(\"max(features) as max_sum\"),\n+            self.get_prediction_col())\n+\n+\n+class PythonModel(Model):\n+\n+    def __init__(self, table_env, model_data_table, output_col_name):\n+        self._model_data_table = model_data_table\n+        self._output_col_name = output_col_name\n+        self.max_sum = 0\n+        self.load_model(table_env)\n+\n+    def load_model(self, table_env):\n+        \"\"\"\n+        Train the model to get the max_sum value which is used to predicate data.\n+        \"\"\"\n+        table_sink = source_sink_utils.TestRetractSink([\"max_sum\"], [DataTypes.BIGINT()])\n+        table_env.register_table_sink(\"Model_Results\", table_sink)\n+        self._model_data_table.insert_into(\"Model_Results\")\n+        table_env.execute(\"load model\")\n+        actual = source_sink_utils.results()\n+        self.max_sum = actual.apply(0)\n+\n+    def transform(self, table_env, table):\n+        \"\"\"\n+        Use max_sum to predicate input. Return turn if input value is bigger than max_sum\n+        \"\"\"\n+        return table\\\n+            .add_columns(\"features > {} as {}\".format(self.max_sum, self._output_col_name))\\\n+            .select(\"{}\".format(self._output_col_name))\n+\n+\n+class PythonPipelineTest(MLTestCase):\n+\n+    def test_java_transformer(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+\n+        table_sink = source_sink_utils.TestAppendSink(\n+            ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])\n+        t_env.register_table_sink(\"TransformerResults\", table_sink)\n+\n+        source_table = t_env.from_elements([(1, 2, 3, 4), (4, 3, 2, 1)], ['a', 'b', 'c', 'd'])\n+        transformer = WrapperTransformer(selected_cols=[\"a\", \"b\"])\n+        transformer\\\n+            .transform(t_env, source_table)\\\n+            .insert_into(\"TransformerResults\")\n+\n+        # execute\n+        t_env.execute('JavaPipelineITCase')\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual, [\"1,2\", \"4,3\"])\n+\n+    def test_pipeline(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+        MLEnvironmentFactory().get_default().get_stream_execution_environment().set_parallelism(1)", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2OTU0Nw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390169547", "bodyText": "move the import to the header of this file?", "author": "dianfu", "createdAt": "2020-03-10T08:58:25Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,270 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        import re", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE3MTI2MA==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390171260", "bodyText": "Should we make it a staticmethod?", "author": "dianfu", "createdAt": "2020-03-10T09:01:42Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,270 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        import re\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    def _make_java_value(self, obj):", "originalCommit": "99e47d0ba1b97997531c2bbaee32091ddc1e344e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "url": "https://github.com/apache/flink/commit/8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "message": "address comments", "committedDate": "2020-03-10T09:58:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzIwNQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390727205", "bodyText": "was wondering why this java class is created here not earlier. is this only to test how pyflink uses JavaTransformer?\nCan we reuse the FakeTransformer class in TransfomerBaseTest? we can refactor it out or make it public if necessary IMO.", "author": "walterddr", "createdAt": "2020-03-11T03:26:07Z", "path": "flink-ml-parent/flink-ml-lib/src/test/java/org/apache/flink/ml/pipeline/UserDefinedPipelineStages.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.ml.pipeline;\n+\n+import org.apache.flink.ml.api.core.Transformer;\n+import org.apache.flink.ml.api.misc.param.Params;\n+import org.apache.flink.ml.params.shared.colname.HasSelectedCols;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+\n+/**\n+ * Util class for testing {@link org.apache.flink.ml.api.core.PipelineStage}.", "originalCommit": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyODU1MQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390828551", "bodyText": "Yes, it is used to test JavaTransformer including setting/getting params and doing transform(tEnv, table). I find we can't merge these two classes since the transform(tEnv, table) method is different in these two classes. The goal of the two classes is also different. FakeTransFormer is used to test transform(BatchOperator) and transform(StreamOperator) while SelectColumnTransformer is used to test transform(tEnv, table).  Could we keep the SelectColumnTransformer class?", "author": "hequn8128", "createdAt": "2020-03-11T09:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE3MTU2Nw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r391171567", "bodyText": "+1, seems like there can be some improvement in the Java side testing based on this feedback", "author": "walterddr", "createdAt": "2020-03-11T18:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzY1Mw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390727653", "bodyText": "These 2 utils are extremely useful, not just to PipelineStage IMO.", "author": "walterddr", "createdAt": "2020-03-11T03:28:13Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,275 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+import re\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    @staticmethod\n+    def _make_java_value(obj):\n+        \"\"\" Convert Python object into Java \"\"\"\n+        if isinstance(obj, list):\n+            obj = [PipelineStage._make_java_value(x) for x in obj]\n+        return obj", "originalCommit": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgzMDMxMw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390830313", "bodyText": "It is only used by PipelineStage now. We can extract them into a Util class if there are other places using it. What do you think?", "author": "hequn8128", "createdAt": "2020-03-11T09:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE3MDU5Mg==", "url": "https://github.com/apache/flink/pull/11344#discussion_r391170592", "bodyText": "yup. it makes sense. since this is the last of the series of PR for this FLIP. we can follow up later", "author": "walterddr", "createdAt": "2020-03-11T18:15:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyNzY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyOTk1Mw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390729953", "bodyText": "Pipeline should implement Estimator, Model and Transformer I believe", "author": "walterddr", "createdAt": "2020-03-11T03:39:13Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,275 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+import re\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    @staticmethod\n+    def _make_java_value(obj):\n+        \"\"\" Convert Python object into Java \"\"\"\n+        if isinstance(obj, list):\n+            obj = [PipelineStage._make_java_value(x) for x in obj]\n+        return obj\n+\n+    def to_json(self) -> str:\n+        return self.get_params().to_json()\n+\n+    def load_json(self, json: str) -> None:\n+        self.get_params().load_json(json)\n+\n+\n+class Transformer(PipelineStage):\n+    \"\"\"\n+    A transformer is a PipelineStage that transforms an input Table to a result Table.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    @abstractmethod\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaTransformer(Transformer):\n+    \"\"\"\n+    Base class for Transformer that wrap Java implementations. Subclasses should\n+    ensure they have the transformer Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return Table(self._j_obj.transform(table_env._j_tenv, table._j_table))\n+\n+\n+class Model(Transformer):\n+    \"\"\"\n+    Abstract class for models that are fitted by estimators.\n+\n+    A model is an ordinary Transformer except how it is created. While ordinary transformers\n+    are defined by specifying the parameters directly, a model is usually generated by an Estimator\n+    when Estimator.fit(table_env, table) is invoked.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+\n+class JavaModel(JavaTransformer, Model):\n+    \"\"\"\n+    Base class for JavaTransformer that wrap Java implementations.\n+    Subclasses should ensure they have the model Java object available as j_obj.\n+    \"\"\"\n+\n+\n+class Estimator(PipelineStage):\n+    \"\"\"\n+    Estimators are PipelineStages responsible for training and generating machine learning models.\n+\n+    The implementations are expected to take an input table as training samples and generate a\n+    Model which fits these samples.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> Model:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaEstimator(Estimator):\n+    \"\"\"\n+    Base class for Estimator that wrap Java implementations.\n+    Subclasses should ensure they have the estimator Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> JavaModel:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return JavaModel(self._j_obj.fit(table_env._j_tenv, table._j_table))\n+\n+\n+class Pipeline(Estimator, Model):", "originalCommit": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgzMDg5Nw==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390830897", "bodyText": "Good catch. The Model is also a Transformer. It seems we don't need to add the Transformer here?", "author": "hequn8128", "createdAt": "2020-03-11T09:11:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyOTk1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE2OTU2OQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r391169569", "bodyText": "I would be better if we keep it consistent with the Java API definition. adding Transformer here shouldn't change anything, as you said its already included in the Estimator", "author": "walterddr", "createdAt": "2020-03-11T18:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyOTk1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM3Njc3OA==", "url": "https://github.com/apache/flink/pull/11344#discussion_r391376778", "bodyText": "ok. I don't have strong opinions here. Will add the Transformer to make it consistent with Java.", "author": "hequn8128", "createdAt": "2020-03-12T02:57:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDcyOTk1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczMjE4OA==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390732188", "bodyText": "Is this to_json / load_json pair interoperable with java?", "author": "becketqin", "createdAt": "2020-03-11T03:49:47Z", "path": "flink-python/pyflink/ml/api/base.py", "diffHunk": "@@ -0,0 +1,275 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+import re\n+\n+from abc import ABCMeta, abstractmethod\n+\n+from pyflink.table.table_environment import TableEnvironment\n+from pyflink.table.table import Table\n+from pyflink.ml.api.param import WithParams, Params\n+from py4j.java_gateway import get_field\n+\n+\n+class PipelineStage(WithParams):\n+    \"\"\"\n+    Base class for a stage in a pipeline. The interface is only a concept, and does not have any\n+    actual functionality. Its subclasses must be either Estimator or Transformer. No other classes\n+    should inherit this interface directly.\n+\n+    Each pipeline stage is with parameters, and requires a public empty constructor for\n+    restoration in Pipeline.\n+    \"\"\"\n+\n+    def __init__(self, params=None):\n+        if params is None:\n+            self._params = Params()\n+        else:\n+            self._params = params\n+\n+    def get_params(self) -> Params:\n+        return self._params\n+\n+    def _convert_params_to_java(self, j_pipeline_stage):\n+        for param in self._params._param_map:\n+            java_param = self._make_java_param(j_pipeline_stage, param)\n+            java_value = self._make_java_value(self._params._param_map[param])\n+            j_pipeline_stage.set(java_param, java_value)\n+\n+    @staticmethod\n+    def _make_java_param(j_pipeline_stage, param):\n+        # camel case to snake case\n+        name = re.sub(r'(?<!^)(?=[A-Z])', '_', param.name).upper()\n+        return get_field(j_pipeline_stage, name)\n+\n+    @staticmethod\n+    def _make_java_value(obj):\n+        \"\"\" Convert Python object into Java \"\"\"\n+        if isinstance(obj, list):\n+            obj = [PipelineStage._make_java_value(x) for x in obj]\n+        return obj\n+\n+    def to_json(self) -> str:\n+        return self.get_params().to_json()\n+\n+    def load_json(self, json: str) -> None:\n+        self.get_params().load_json(json)\n+\n+\n+class Transformer(PipelineStage):\n+    \"\"\"\n+    A transformer is a PipelineStage that transforms an input Table to a result Table.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    @abstractmethod\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaTransformer(Transformer):\n+    \"\"\"\n+    Base class for Transformer that wrap Java implementations. Subclasses should\n+    ensure they have the transformer Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def transform(self, table_env: TableEnvironment, table: Table) -> Table:\n+        \"\"\"\n+        Applies the transformer on the input table, and returns the result table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table to be transformed\n+        :returns: the transformed table\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return Table(self._j_obj.transform(table_env._j_tenv, table._j_table))\n+\n+\n+class Model(Transformer):\n+    \"\"\"\n+    Abstract class for models that are fitted by estimators.\n+\n+    A model is an ordinary Transformer except how it is created. While ordinary transformers\n+    are defined by specifying the parameters directly, a model is usually generated by an Estimator\n+    when Estimator.fit(table_env, table) is invoked.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+\n+class JavaModel(JavaTransformer, Model):\n+    \"\"\"\n+    Base class for JavaTransformer that wrap Java implementations.\n+    Subclasses should ensure they have the model Java object available as j_obj.\n+    \"\"\"\n+\n+\n+class Estimator(PipelineStage):\n+    \"\"\"\n+    Estimators are PipelineStages responsible for training and generating machine learning models.\n+\n+    The implementations are expected to take an input table as training samples and generate a\n+    Model which fits these samples.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> Model:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+class JavaEstimator(Estimator):\n+    \"\"\"\n+    Base class for Estimator that wrap Java implementations.\n+    Subclasses should ensure they have the estimator Java object available as j_obj.\n+    \"\"\"\n+\n+    def __init__(self, j_obj):\n+        super().__init__()\n+        self._j_obj = j_obj\n+\n+    def fit(self, table_env: TableEnvironment, table: Table) -> JavaModel:\n+        \"\"\"\n+        Train and produce a Model which fits the records in the given Table.\n+\n+        :param table_env: the table environment to which the input table is bound.\n+        :param table: the table with records to train the Model.\n+        :returns: a model trained to fit on the given Table.\n+        \"\"\"\n+        self._convert_params_to_java(self._j_obj)\n+        return JavaModel(self._j_obj.fit(table_env._j_tenv, table._j_table))\n+\n+\n+class Pipeline(Estimator, Model):\n+    \"\"\"\n+    A pipeline is a linear workflow which chains Estimators and Transformers to\n+    execute an algorithm.\n+\n+    A pipeline itself can either act as an Estimator or a Transformer, depending on the stages it\n+    includes. More specifically:\n+\n+\n+    If a Pipeline has an Estimator, one needs to call `Pipeline.fit(TableEnvironment, Table)`\n+    before use the pipeline as a Transformer. In this case the Pipeline is an Estimator and\n+    can produce a Pipeline as a `Model`.\n+\n+    If a Pipeline has noEstimator, it is a Transformer and can be applied to a Table directly.\n+    In this case, `Pipeline#fit(TableEnvironment, Table)` will simply return the pipeline itself.\n+\n+\n+    In addition, a pipeline can also be used as a PipelineStage in another pipeline, just like an\n+    ordinaryEstimator or Transformer as describe above.\n+    \"\"\"\n+\n+    def __init__(self, stages=None, pipeline_json=None):\n+        super().__init__()\n+        self.stages = []\n+        self.last_estimator_index = -1\n+        if stages is not None:\n+            for stage in stages:\n+                self.append_stage(stage)\n+        if pipeline_json is not None:\n+            self.load_json(pipeline_json)\n+\n+    def need_fit(self):\n+        return self.last_estimator_index >= 0\n+\n+    @staticmethod\n+    def _is_stage_need_fit(stage):\n+        return (isinstance(stage, Pipeline) and stage.need_fit()) or \\\n+               ((not isinstance(stage, Pipeline)) and isinstance(stage, Estimator))\n+\n+    def get_stages(self) -> tuple:\n+        # make it immutable by changing to tuple\n+        return tuple(self.stages)\n+\n+    def append_stage(self, stage: PipelineStage) -> 'Pipeline':\n+        if self._is_stage_need_fit(stage):\n+            self.last_estimator_index = len(self.stages)\n+        elif not isinstance(stage, Transformer):\n+            raise RuntimeError(\"All PipelineStages should be Estimator or Transformer!\")\n+        self.stages.append(stage)\n+        return self\n+\n+    def fit(self, t_env: TableEnvironment, input: Table) -> 'Pipeline':\n+        \"\"\"\n+        Train the pipeline to fit on the records in the given Table.\n+\n+        :param t_env: the table environment to which the input table is bound.\n+        :param input: the table with records to train the Pipeline.\n+        :returns: a pipeline with same stages as this Pipeline except all Estimators \\\n+        replaced with their corresponding Models.\n+        \"\"\"\n+        transform_stages = []\n+        for i in range(0, len(self.stages)):\n+            s = self.stages[i]\n+            if i <= self.last_estimator_index:\n+                need_fit = self._is_stage_need_fit(s)\n+                if need_fit:\n+                    t = s.fit(t_env, input)\n+                else:\n+                    t = s\n+                transform_stages.append(t)\n+                input = t.transform(t_env, input)\n+            else:\n+                transform_stages.append(s)\n+        return Pipeline(transform_stages)\n+\n+    def transform(self, t_env: TableEnvironment, input: Table) -> Table:\n+        \"\"\"\n+        Generate a result table by applying all the stages in this pipeline to\n+        the input table in order.\n+\n+        :param t_env: the table environment to which the input table is bound.\n+        :param input: the table to be transformed.\n+        :returns: a result table with all the stages applied to the input tables in order.\n+        \"\"\"\n+        if self.need_fit():\n+            raise RuntimeError(\"Pipeline contains Estimator, need to fit first.\")\n+        for s in self.stages:\n+            input = s.transform(t_env, input)\n+        return input\n+\n+    def to_json(self) -> str:\n+        import jsonpickle\n+        return str(jsonpickle.encode(self, keys=True))", "originalCommit": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg0Mzc3NQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390843775", "bodyText": "Good catch! We can't do it now. The Java side uses ObjectMapper while Python uses JsonPickle. I'm not sure if there are existing frameworks to address this problem.", "author": "hequn8128", "createdAt": "2020-03-11T09:33:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczMjE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDczMzUzNQ==", "url": "https://github.com/apache/flink/pull/11344#discussion_r390733535", "bodyText": "typo -> predictResult", "author": "becketqin", "createdAt": "2020-03-11T03:56:01Z", "path": "flink-python/pyflink/ml/tests/test_pipeline_it_case.py", "diffHunk": "@@ -0,0 +1,171 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+from pyflink.table.types import DataTypes\n+from pyflink.testing.test_case_utils import MLTestCase\n+\n+from pyflink.ml.api import JavaTransformer, Transformer, Estimator, Model, \\\n+    MLEnvironmentFactory, Pipeline\n+from pyflink.ml.api.param import WithParams, ParamInfo, TypeConverters\n+from pyflink.ml.lib.param.colname import HasSelectedCols,\\\n+    HasPredictionCol, HasOutputCol\n+from pyflink import keyword\n+from pyflink.testing import source_sink_utils\n+from pyflink.java_gateway import get_gateway\n+\n+\n+class HasVectorCol(WithParams):\n+    \"\"\"\n+    Trait for parameter vectorColName.\n+    \"\"\"\n+    vector_col = ParamInfo(\n+        \"vectorCol\",\n+        \"Name of a vector column\",\n+        is_optional=False,\n+        type_converter=TypeConverters.to_string)\n+\n+    def set_vector_col(self, v: str) -> 'HasVectorCol':\n+        return super().set(self.vector_col, v)\n+\n+    def get_vector_col(self) -> str:\n+        return super().get(self.vector_col)\n+\n+\n+class WrapperTransformer(JavaTransformer, HasSelectedCols):\n+    \"\"\"\n+    A Transformer wrappers Java Transformer.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None):\n+        _j_obj = get_gateway().jvm.org.apache.flink.ml.pipeline.\\\n+            UserDefinedPipelineStages.SelectColumnTransformer()\n+        super().__init__(_j_obj)\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+\n+class PythonAddTransformer(Transformer, HasSelectedCols, HasOutputCol):\n+    \"\"\"\n+    A Transformer which is implemented with Python. Output a column\n+    contains the sum of all columns.\n+    \"\"\"\n+    @keyword\n+    def __init__(self, *, selected_cols=None, output_col=None):\n+        super().__init__()\n+        kwargs = self._input_kwargs\n+        self._set(**kwargs)\n+\n+    def transform(self, table_env, table):\n+        input_columns = self.get_selected_cols()\n+        expr = \"+\".join(input_columns)\n+        expr = expr + \" as \" + self.get_output_col()\n+        return table.add_columns(expr)\n+\n+\n+class PythonEstimator(Estimator, HasVectorCol, HasPredictionCol):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def fit(self, table_env, table):\n+        return PythonModel(\n+            table_env,\n+            table.select(\"max(features) as max_sum\"),\n+            self.get_prediction_col())\n+\n+\n+class PythonModel(Model):\n+\n+    def __init__(self, table_env, model_data_table, output_col_name):\n+        self._model_data_table = model_data_table\n+        self._output_col_name = output_col_name\n+        self.max_sum = 0\n+        self.load_model(table_env)\n+\n+    def load_model(self, table_env):\n+        \"\"\"\n+        Train the model to get the max_sum value which is used to predicate data.\n+        \"\"\"\n+        table_sink = source_sink_utils.TestRetractSink([\"max_sum\"], [DataTypes.BIGINT()])\n+        table_env.register_table_sink(\"Model_Results\", table_sink)\n+        self._model_data_table.insert_into(\"Model_Results\")\n+        table_env.execute(\"load model\")\n+        actual = source_sink_utils.results()\n+        self.max_sum = actual.apply(0)\n+\n+    def transform(self, table_env, table):\n+        \"\"\"\n+        Use max_sum to predicate input. Return turn if input value is bigger than max_sum\n+        \"\"\"\n+        return table\\\n+            .add_columns(\"features > {} as {}\".format(self.max_sum, self._output_col_name))\\\n+            .select(\"{}\".format(self._output_col_name))\n+\n+\n+class PythonPipelineTest(MLTestCase):\n+\n+    def test_java_transformer(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+\n+        table_sink = source_sink_utils.TestAppendSink(\n+            ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])\n+        t_env.register_table_sink(\"TransformerResults\", table_sink)\n+\n+        source_table = t_env.from_elements([(1, 2, 3, 4), (4, 3, 2, 1)], ['a', 'b', 'c', 'd'])\n+        transformer = WrapperTransformer(selected_cols=[\"a\", \"b\"])\n+        transformer\\\n+            .transform(t_env, source_table)\\\n+            .insert_into(\"TransformerResults\")\n+\n+        # execute\n+        t_env.execute('JavaPipelineITCase')\n+        actual = source_sink_utils.results()\n+        self.assert_equals(actual, [\"1,2\", \"4,3\"])\n+\n+    def test_pipeline(self):\n+        t_env = MLEnvironmentFactory().get_default().get_stream_table_environment()\n+        train_table = t_env.from_elements(\n+            [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], ['a', 'b'])\n+        serving_table = t_env.from_elements([(0, 0), (12, 3)], ['a', 'b'])\n+\n+        table_sink = source_sink_utils.TestAppendSink(\n+            ['predicate_result'],", "originalCommit": "8c0c307475110a20b22fa0e4a270fbe6f5b98e29", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a976d122cca58ccf820360a2bf12a094871d0977", "url": "https://github.com/apache/flink/commit/a976d122cca58ccf820360a2bf12a094871d0977", "message": "fix typo", "committedDate": "2020-03-11T09:37:55Z", "type": "commit"}, {"oid": "95473512ec290d571496fae32da8d33013c8f56e", "url": "https://github.com/apache/flink/commit/95473512ec290d571496fae32da8d33013c8f56e", "message": "add Transformer to make it consistent with Java", "committedDate": "2020-03-12T05:38:13Z", "type": "commit"}]}