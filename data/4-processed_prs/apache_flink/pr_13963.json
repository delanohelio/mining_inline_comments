{"pr_number": 13963, "pr_title": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming", "pr_createdAt": "2020-11-06T10:23:18Z", "pr_url": "https://github.com/apache/flink/pull/13963", "timeline": [{"oid": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "url": "https://github.com/apache/flink/commit/cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming", "committedDate": "2020-11-07T09:45:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIzMTQ2NA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519231464", "bodyText": "I have a slight preference to keep these fields private and add a public getter (public FileSplitAssigner.Provider getAssigner().", "author": "StephanEwen", "createdAt": "2020-11-07T23:12:53Z", "path": "flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/AbstractFileSource.java", "diffHunk": "@@ -75,12 +75,12 @@\n \n \tprivate final FileEnumerator.Provider enumeratorFactory;\n \n-\tprivate final FileSplitAssigner.Provider assignerFactory;\n+\tprotected final FileSplitAssigner.Provider assignerFactory;", "originalCommit": "55caff0edf1e89c58d896468e8b4c8a814726aac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDE4MQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240181", "bodyText": "+1 Some communities directly open mandatory checkstyle checks, providing only setters and getters instead of protected fields.", "author": "JingsongLi", "createdAt": "2020-11-08T00:52:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIzMTQ2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIzMTQ3Mg==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519231472", "bodyText": "Similar as above.", "author": "StephanEwen", "createdAt": "2020-11-07T23:13:00Z", "path": "flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/AbstractFileSource.java", "diffHunk": "@@ -75,12 +75,12 @@\n \n \tprivate final FileEnumerator.Provider enumeratorFactory;\n \n-\tprivate final FileSplitAssigner.Provider assignerFactory;\n+\tprotected final FileSplitAssigner.Provider assignerFactory;\n \n \tprivate final BulkFormat<T, SplitT> readerFormat;\n \n \t@Nullable\n-\tprivate final ContinuousEnumerationSettings continuousEnumerationSettings;\n+\tprotected final ContinuousEnumerationSettings continuousEnumerationSettings;", "originalCommit": "55caff0edf1e89c58d896468e8b4c8a814726aac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDI4OQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240289", "bodyText": "Code style:\npublic ContinuousHivePendingSplitsCheckpoint(\n         Collection<HiveSourceSplit> splits,\n         Comparable<?> currentReadOffset,\n         Collection<List<String>> seenPartitions) {", "author": "JingsongLi", "createdAt": "2020-11-08T00:53:26Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpoint.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * The checkpoint of current state of continuous hive source reading.\n+ */\n+public class ContinuousHivePendingSplitsCheckpoint extends PendingSplitsCheckpoint<HiveSourceSplit> {\n+\n+\tprivate final Comparable<?> currentReadOffset;\n+\tprivate final Collection<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpoint(Collection<HiveSourceSplit> splits,", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDc5NA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240794", "bodyText": "Why not return newSplits and pass to handleNewSplits and call assignSplits?", "author": "JingsongLi", "createdAt": "2020-11-08T01:00:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();\n+\t// the maximum partition read offset seen so far\n+\tprivate volatile T currentReadOffset;\n+\t// the partitions that have been processed for a given read offset\n+\tprivate final Set<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHiveSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumeratorContext,\n+\t\t\tT currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tFileSplitAssigner splitAssigner,\n+\t\t\tlong discoveryInterval,\n+\t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\tthis.enumeratorContext = enumeratorContext;\n+\t\tthis.currentReadOffset = currentReadOffset;\n+\t\tthis.seenPartitions = new HashSet<>(seenPartitions);\n+\t\tthis.splitAssigner = splitAssigner;\n+\t\tthis.discoveryInterval = discoveryInterval;\n+\t\tthis.jobConf = jobConf;\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n+\t\treadersAwaitingSplit = new LinkedHashMap<>();\n+\t}\n+\n+\t@Override\n+\tpublic void start() {\n+\t\ttry {\n+\t\t\tfetcherContext.open();\n+\t\t\tenumeratorContext.callAsync(\n+\t\t\t\t\tthis::monitorAndGetSplits,\n+\t\t\t\t\tthis::handleNewSplits,\n+\t\t\t\t\tdiscoveryInterval,\n+\t\t\t\t\tdiscoveryInterval);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to start continuous split enumerator\", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void handleSourceEvent(int subtaskId, SourceEvent sourceEvent) {\n+\t\tif (sourceEvent instanceof RequestSplitEvent) {\n+\t\t\treadersAwaitingSplit.put(subtaskId, ((RequestSplitEvent) sourceEvent).hostName());\n+\t\t\tassignSplits();\n+\t\t} else {\n+\t\t\tLOG.error(\"Received unrecognized event: {}\", sourceEvent);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addSplitsBack(List<HiveSourceSplit> splits, int subtaskId) {\n+\t\tLOG.debug(\"Continuous Hive Source Enumerator adds splits back: {}\", splits);\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(splits));\n+\t\t} finally {\n+\t\t\tstateLock.writeLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addReader(int subtaskId) {\n+\t\t// this source is purely lazy-pull-based, nothing to do upon registration\n+\t}\n+\n+\t@Override\n+\tpublic PendingSplitsCheckpoint<HiveSourceSplit> snapshotState() throws Exception {\n+\t\tstateLock.readLock().lock();\n+\t\ttry {\n+\t\t\tCollection<HiveSourceSplit> remainingSplits = (Collection<HiveSourceSplit>) (Collection<?>) splitAssigner.remainingSplits();\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpoint(remainingSplits, currentReadOffset, seenPartitions);\n+\t\t} finally {\n+\t\t\tstateLock.readLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\ttry {\n+\t\t\tfetcherContext.close();\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\tprivate Void monitorAndGetSplits() throws Exception {\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n+\t\t\tif (partitions.isEmpty()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\tpartitions.sort(Comparator.comparing(o -> o.f1));\n+\t\t\tList<HiveSourceSplit> newSplits = new ArrayList<>();\n+\t\t\t// the max offset of new partitions\n+\t\t\tT maxOffset = currentReadOffset;\n+\t\t\tSet<List<String>> nextSeen = new HashSet<>();\n+\t\t\tfor (Tuple2<Partition, T> tuple2 : partitions) {\n+\t\t\t\tPartition partition = tuple2.f0;\n+\t\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\t\tif (seenPartitions.add(partSpec)) {\n+\t\t\t\t\tT offset = tuple2.f1;\n+\t\t\t\t\tif (offset.compareTo(currentReadOffset) > 0) {\n+\t\t\t\t\t\tnextSeen.add(partSpec);\n+\t\t\t\t\t}\n+\t\t\t\t\tif (offset.compareTo(maxOffset) > 0) {\n+\t\t\t\t\t\tmaxOffset = offset;\n+\t\t\t\t\t}\n+\t\t\t\t\tLOG.info(\"Found new partition {} of table {}, generating splits for it\",\n+\t\t\t\t\t\t\tpartSpec, tablePath.getFullName());\n+\t\t\t\t\tnewSplits.addAll(HiveSourceFileEnumerator.createInputSplits(\n+\t\t\t\t\t\t\t0, Collections.singletonList(fetcherContext.toHiveTablePartition(partition)), jobConf));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcurrentReadOffset = maxOffset;\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(newSplits));", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MzEyNQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519243125", "bodyText": "Splits are part of the state. If we return newSplits and snapshotState is called in between, it can read inconsistent state.", "author": "lirui-apache", "createdAt": "2020-11-08T01:34:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDc5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDg3OQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240879", "bodyText": "Why not DEFAULT_SPLIT_ASSIGNER ?", "author": "JingsongLi", "createdAt": "2020-11-08T01:02:06Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MzQxOA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519243418", "bodyText": "In streaming read, we would want the splits to be consumed in order. The DEFAULT_SPLIT_ASSIGNER is locality-aware and therefore doesn't meet the requirement. But perhaps I should only do this for partitioned table, because non-partitioned table still reuse super class's enumerator and the splits are not generated in order in the first place.", "author": "lirui-apache", "createdAt": "2020-11-08T01:38:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDg3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1NTc5Mg==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519255792", "bodyText": "I think ideally we need to implement an assigner that assigns splits in order, but is also locality-aware. Guess we can leave that for the future.", "author": "lirui-apache", "createdAt": "2020-11-08T04:42:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDg3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MDk3NQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519240975", "bodyText": "continuousPartitionedEnumerator?", "author": "JingsongLi", "createdAt": "2020-11-08T01:03:51Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,\n \t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedRowType, useMapRedReader, limit),\n-\t\t\t\tnull);\n-\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\t\t\tcontinuousEnumerationSettings);\n+\t\tthis.jobConfWrapper = new JobConfWrapper(jobConf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n \t}\n \n \t@Override\n \tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n \t\treturn HiveSourceSplitSerializer.INSTANCE;\n \t}\n \n+\t@Override\n+\tpublic SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> getEnumeratorCheckpointSerializer() {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpointSerializer(getSplitSerializer());\n+\t\t} else {\n+\t\t\treturn super.getEnumeratorCheckpointSerializer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, fetcherContext.getConsumeStartOffset(), Collections.emptyList(), Collections.emptyList());\n+\t\t} else {\n+\t\t\treturn super.createEnumerator(enumContext);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> restoreEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext, PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\tPreconditions.checkState(checkpoint instanceof ContinuousHivePendingSplitsCheckpoint,\n+\t\t\t\t\t\"Illegal type of splits checkpoint %s for streaming read partitioned table\", checkpoint.getClass().getName());\n+\t\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, hiveCheckpoint.getCurrentReadOffset(), hiveCheckpoint.getSeenPartitions(), hiveCheckpoint.getSplits());\n+\t\t} else {\n+\t\t\treturn super.restoreEnumerator(enumContext, checkpoint);\n+\t\t}\n+\t}\n+\n+\tprivate boolean needContinuousSplitEnumerator() {", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTAwMg==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241002", "bodyText": "Ditto", "author": "JingsongLi", "createdAt": "2020-11-08T01:04:05Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,\n+\t\t\tContinuousPartitionFetcher<Partition, ?> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext,\n \t\t\tRowType producedRowType) {\n \t\tsuper(\n \t\t\t\tnew org.apache.flink.core.fs.Path[1],\n \t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n-\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcontinuousEnumerationSettings == null ? DEFAULT_SPLIT_ASSIGNER : SimpleSplitAssigner::new,\n \t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedRowType, useMapRedReader, limit),\n-\t\t\t\tnull);\n-\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\t\t\tcontinuousEnumerationSettings);\n+\t\tthis.jobConfWrapper = new JobConfWrapper(jobConf);\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.partitionKeys = catalogTable.getPartitionKeys();\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n \t}\n \n \t@Override\n \tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n \t\treturn HiveSourceSplitSerializer.INSTANCE;\n \t}\n \n+\t@Override\n+\tpublic SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> getEnumeratorCheckpointSerializer() {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpointSerializer(getSplitSerializer());\n+\t\t} else {\n+\t\t\treturn super.getEnumeratorCheckpointSerializer();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, fetcherContext.getConsumeStartOffset(), Collections.emptyList(), Collections.emptyList());\n+\t\t} else {\n+\t\t\treturn super.createEnumerator(enumContext);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> restoreEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext, PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) {\n+\t\tif (needContinuousSplitEnumerator()) {\n+\t\t\tPreconditions.checkState(checkpoint instanceof ContinuousHivePendingSplitsCheckpoint,\n+\t\t\t\t\t\"Illegal type of splits checkpoint %s for streaming read partitioned table\", checkpoint.getClass().getName());\n+\t\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\t\treturn createContinuousSplitEnumerator(\n+\t\t\t\t\tenumContext, hiveCheckpoint.getCurrentReadOffset(), hiveCheckpoint.getSeenPartitions(), hiveCheckpoint.getSplits());\n+\t\t} else {\n+\t\t\treturn super.restoreEnumerator(enumContext, checkpoint);\n+\t\t}\n+\t}\n+\n+\tprivate boolean needContinuousSplitEnumerator() {\n+\t\treturn getBoundedness() == Boundedness.CONTINUOUS_UNBOUNDED && !partitionKeys.isEmpty();\n+\t}\n+\n+\tprivate SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> createContinuousSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumContext,\n+\t\t\tComparable<?> currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tCollection<HiveSourceSplit> splits) {\n+\t\treturn new ContinuousHiveSplitEnumerator(", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTM2OQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241369", "bodyText": "It is better to create a builder for HiveSource instead of passing all arguments.", "author": "JingsongLi", "createdAt": "2020-11-08T01:09:11Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -46,29 +58,98 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final ContinuousPartitionFetcher<Partition, ?> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<?> fetcherContext;\n+\tprivate final ObjectPath tablePath;\n+\n \tHiveSource(\n \t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n \t\t\tCatalogTable catalogTable,\n \t\t\tList<HiveTablePartition> partitions,\n \t\t\t@Nullable Long limit,\n \t\t\tString hiveVersion,\n \t\t\tboolean useMapRedReader,\n-\t\t\tboolean isStreamingSource,\n+\t\t\t@Nullable ContinuousEnumerationSettings continuousEnumerationSettings,", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1NjAxOQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519256019", "bodyText": "I added the builder, but the builder constructor still takes quite a few parameters. That's because the BulkFormat needs to be created in the constructor. And since BulkFormat is determined during construction, it makes no sense to allow modifying tablePath or partitions later through the builder.", "author": "lirui-apache", "createdAt": "2020-11-08T04:45:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTM2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTQ2Ng==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241466", "bodyText": "Why not move these check to upper?", "author": "JingsongLi", "createdAt": "2020-11-08T01:10:32Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -151,23 +139,63 @@ public boolean isBounded() {\n \t\t\t\tcatalogTable,\n \t\t\t\thiveShim,\n \t\t\t\tremainingPartitions);\n+\t\tConfiguration configuration = Configuration.fromMap(catalogTable.getOptions());\n \n-\t\t@SuppressWarnings(\"unchecked\")\n-\t\tTypeInformation<RowData> typeInfo =\n-\t\t\t\t(TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());\n+\t\tDuration monitorInterval = null;\n+\t\tContinuousPartitionFetcher<Partition, ?> fetcher = null;\n+\t\tHiveContinuousPartitionFetcherContext<?> fetcherContext = null;\n+\t\tif (isStreamingSource()) {\n+\t\t\tmonitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_SCAN_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tfetcher = new HiveContinuousPartitionFetcher();\n+\n+\t\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\t\t\tfetcherContext = new HiveContinuousPartitionFetcherContext(\n+\t\t\t\t\ttablePath,\n+\t\t\t\t\thiveShim,\n+\t\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\t\tconfiguration,\n+\t\t\t\t\tdefaultPartitionName);\n+\t\t}\n \n-\t\tHiveTableInputFormat inputFormat = getInputFormat(\n+\t\tHiveSource hiveSource = new HiveSource(\n+\t\t\t\tjobConf,\n+\t\t\t\ttablePath,\n+\t\t\t\tcatalogTable,\n \t\t\t\tallHivePartitions,\n-\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\t\t\t\tlimit,\n+\t\t\t\thiveVersion,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER),\n+\t\t\t\tisStreamingSource() ? new ContinuousEnumerationSettings(monitorInterval) : null,\n+\t\t\t\tfetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\t(RowType) getProducedDataType().getLogicalType()\n+\t\t);\n+\t\tDataStreamSource<RowData> source = execEnv.fromSource(\n+\t\t\t\thiveSource, WatermarkStrategy.noWatermarks(), \"HiveSource-\" + tablePath.getFullName());\n \n \t\tif (isStreamingSource()) {", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTYwMw==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241603", "bodyText": "Do we really need this lock? I think thread safety should be ensured by framework", "author": "JingsongLi", "createdAt": "2020-11-08T01:12:28Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0NjMwOA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519246308", "bodyText": "We do. Because the monitorAndGetSplits is scheduled asynchronously by SplitEnumeratorContext::callAsync. So there can be concurrent calls of, say, monitorAndGetSplits and snapshotState.", "author": "lirui-apache", "createdAt": "2020-11-08T02:21:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTgyNQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241825", "bodyText": "seenPartitions vs distinctPartitions\nI prefer the latter, because it is only used to remove duplication, and does not save all the partitions we have seen", "author": "JingsongLi", "createdAt": "2020-11-08T01:15:33Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHiveSplitEnumerator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.connector.source.SourceEvent;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.api.connector.source.event.RequestSplitEvent;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.assigners.FileSplitAssigner;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;\n+\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * A continuously monitoring {@link SplitEnumerator} for hive source.\n+ */\n+public class ContinuousHiveSplitEnumerator<T extends Comparable<T>> implements SplitEnumerator<HiveSourceSplit, PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ContinuousHiveSplitEnumerator.class);\n+\n+\tprivate final SplitEnumeratorContext<HiveSourceSplit> enumeratorContext;\n+\tprivate final LinkedHashMap<Integer, String> readersAwaitingSplit;\n+\tprivate final FileSplitAssigner splitAssigner;\n+\tprivate final long discoveryInterval;\n+\n+\tprivate final JobConf jobConf;\n+\tprivate final ObjectPath tablePath;\n+\n+\tprivate final ContinuousPartitionFetcher<Partition, T> fetcher;\n+\tprivate final HiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext;\n+\n+\tprivate final ReadWriteLock stateLock = new ReentrantReadWriteLock();\n+\t// the maximum partition read offset seen so far\n+\tprivate volatile T currentReadOffset;\n+\t// the partitions that have been processed for a given read offset\n+\tprivate final Set<List<String>> seenPartitions;\n+\n+\tpublic ContinuousHiveSplitEnumerator(\n+\t\t\tSplitEnumeratorContext<HiveSourceSplit> enumeratorContext,\n+\t\t\tT currentReadOffset,\n+\t\t\tCollection<List<String>> seenPartitions,\n+\t\t\tFileSplitAssigner splitAssigner,\n+\t\t\tlong discoveryInterval,\n+\t\t\tJobConf jobConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tContinuousPartitionFetcher<Partition, T> fetcher,\n+\t\t\tHiveTableSource.HiveContinuousPartitionFetcherContext<T> fetcherContext) {\n+\t\tthis.enumeratorContext = enumeratorContext;\n+\t\tthis.currentReadOffset = currentReadOffset;\n+\t\tthis.seenPartitions = new HashSet<>(seenPartitions);\n+\t\tthis.splitAssigner = splitAssigner;\n+\t\tthis.discoveryInterval = discoveryInterval;\n+\t\tthis.jobConf = jobConf;\n+\t\tthis.tablePath = tablePath;\n+\t\tthis.fetcher = fetcher;\n+\t\tthis.fetcherContext = fetcherContext;\n+\t\treadersAwaitingSplit = new LinkedHashMap<>();\n+\t}\n+\n+\t@Override\n+\tpublic void start() {\n+\t\ttry {\n+\t\t\tfetcherContext.open();\n+\t\t\tenumeratorContext.callAsync(\n+\t\t\t\t\tthis::monitorAndGetSplits,\n+\t\t\t\t\tthis::handleNewSplits,\n+\t\t\t\t\tdiscoveryInterval,\n+\t\t\t\t\tdiscoveryInterval);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to start continuous split enumerator\", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void handleSourceEvent(int subtaskId, SourceEvent sourceEvent) {\n+\t\tif (sourceEvent instanceof RequestSplitEvent) {\n+\t\t\treadersAwaitingSplit.put(subtaskId, ((RequestSplitEvent) sourceEvent).hostName());\n+\t\t\tassignSplits();\n+\t\t} else {\n+\t\t\tLOG.error(\"Received unrecognized event: {}\", sourceEvent);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addSplitsBack(List<HiveSourceSplit> splits, int subtaskId) {\n+\t\tLOG.debug(\"Continuous Hive Source Enumerator adds splits back: {}\", splits);\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(splits));\n+\t\t} finally {\n+\t\t\tstateLock.writeLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void addReader(int subtaskId) {\n+\t\t// this source is purely lazy-pull-based, nothing to do upon registration\n+\t}\n+\n+\t@Override\n+\tpublic PendingSplitsCheckpoint<HiveSourceSplit> snapshotState() throws Exception {\n+\t\tstateLock.readLock().lock();\n+\t\ttry {\n+\t\t\tCollection<HiveSourceSplit> remainingSplits = (Collection<HiveSourceSplit>) (Collection<?>) splitAssigner.remainingSplits();\n+\t\t\treturn new ContinuousHivePendingSplitsCheckpoint(remainingSplits, currentReadOffset, seenPartitions);\n+\t\t} finally {\n+\t\t\tstateLock.readLock().unlock();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws IOException {\n+\t\ttry {\n+\t\t\tfetcherContext.close();\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\tprivate Void monitorAndGetSplits() throws Exception {\n+\t\tstateLock.writeLock().lock();\n+\t\ttry {\n+\t\t\tList<Tuple2<Partition, T>> partitions = fetcher.fetchPartitions(fetcherContext, currentReadOffset);\n+\t\t\tif (partitions.isEmpty()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\tpartitions.sort(Comparator.comparing(o -> o.f1));\n+\t\t\tList<HiveSourceSplit> newSplits = new ArrayList<>();\n+\t\t\t// the max offset of new partitions\n+\t\t\tT maxOffset = currentReadOffset;\n+\t\t\tSet<List<String>> nextSeen = new HashSet<>();\n+\t\t\tfor (Tuple2<Partition, T> tuple2 : partitions) {\n+\t\t\t\tPartition partition = tuple2.f0;\n+\t\t\t\tList<String> partSpec = partition.getValues();\n+\t\t\t\tif (seenPartitions.add(partSpec)) {\n+\t\t\t\t\tT offset = tuple2.f1;\n+\t\t\t\t\tif (offset.compareTo(currentReadOffset) > 0) {\n+\t\t\t\t\t\tnextSeen.add(partSpec);\n+\t\t\t\t\t}\n+\t\t\t\t\tif (offset.compareTo(maxOffset) > 0) {\n+\t\t\t\t\t\tmaxOffset = offset;\n+\t\t\t\t\t}\n+\t\t\t\t\tLOG.info(\"Found new partition {} of table {}, generating splits for it\",\n+\t\t\t\t\t\t\tpartSpec, tablePath.getFullName());\n+\t\t\t\t\tnewSplits.addAll(HiveSourceFileEnumerator.createInputSplits(\n+\t\t\t\t\t\t\t0, Collections.singletonList(fetcherContext.toHiveTablePartition(partition)), jobConf));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcurrentReadOffset = maxOffset;\n+\t\t\tsplitAssigner.addSplits(new ArrayList<>(newSplits));\n+\t\t\tif (!nextSeen.isEmpty()) {\n+\t\t\t\tseenPartitions.clear();", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0NzEyNA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519247124", "bodyText": "The field is a Set, so I think calling it distinct is redundant. Besides, seenPartitions is better to indicate the purpose of the filed, which is to filter out partitions that we have processed. Perhaps I can rename it to seenPartitionsForOffset?", "author": "lirui-apache", "createdAt": "2020-11-08T02:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTgyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1NjA3NQ==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519256075", "bodyText": "Renamed to seenPartitionsSinceOffset. Hopefully that makes more sense.", "author": "lirui-apache", "createdAt": "2020-11-08T04:46:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTgyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTk2Mw==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241963", "bodyText": "It is better to pass serializer here", "author": "JingsongLi", "createdAt": "2020-11-08T01:17:26Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpointSerializer.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * SerDe for {@link ContinuousHivePendingSplitsCheckpoint}.\n+ */\n+public class ContinuousHivePendingSplitsCheckpointSerializer implements SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tprivate final PendingSplitsCheckpointSerializer<HiveSourceSplit> superSerDe;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpointSerializer(SimpleVersionedSerializer<HiveSourceSplit> splitSerDe) {\n+\t\tsuperSerDe = new PendingSplitsCheckpointSerializer<>(splitSerDe);\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) throws IOException {\n+\t\tPreconditions.checkArgument(checkpoint.getClass() == ContinuousHivePendingSplitsCheckpoint.class,\n+\t\t\t\t\"Only supports %s\", ContinuousHivePendingSplitsCheckpoint.class.getName());\n+\n+\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\tPendingSplitsCheckpoint<HiveSourceSplit> superCP = PendingSplitsCheckpoint.fromCollectionSnapshot(\n+\t\t\t\thiveCheckpoint.getSplits(), hiveCheckpoint.getAlreadyProcessedPaths());\n+\t\tbyte[] superBytes = superSerDe.serialize(superCP);\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\toutputStream.writeInt(superBytes.length);\n+\t\t\toutputStream.write(superBytes);\n+\t\t\toutputStream.writeObject(hiveCheckpoint.getCurrentReadOffset());", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0OTg2NA==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519249864", "bodyText": "Do you mean we need a SerDe interface for the read offset?", "author": "lirui-apache", "createdAt": "2020-11-08T03:12:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTk2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI0MTk4Mg==", "url": "https://github.com/apache/flink/pull/13963#discussion_r519241982", "bodyText": "It is better to use ListSer", "author": "JingsongLi", "createdAt": "2020-11-08T01:17:47Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ContinuousHivePendingSplitsCheckpointSerializer.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpoint;\n+import org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * SerDe for {@link ContinuousHivePendingSplitsCheckpoint}.\n+ */\n+public class ContinuousHivePendingSplitsCheckpointSerializer implements SimpleVersionedSerializer<PendingSplitsCheckpoint<HiveSourceSplit>> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tprivate final PendingSplitsCheckpointSerializer<HiveSourceSplit> superSerDe;\n+\n+\tpublic ContinuousHivePendingSplitsCheckpointSerializer(SimpleVersionedSerializer<HiveSourceSplit> splitSerDe) {\n+\t\tsuperSerDe = new PendingSplitsCheckpointSerializer<>(splitSerDe);\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(PendingSplitsCheckpoint<HiveSourceSplit> checkpoint) throws IOException {\n+\t\tPreconditions.checkArgument(checkpoint.getClass() == ContinuousHivePendingSplitsCheckpoint.class,\n+\t\t\t\t\"Only supports %s\", ContinuousHivePendingSplitsCheckpoint.class.getName());\n+\n+\t\tContinuousHivePendingSplitsCheckpoint hiveCheckpoint = (ContinuousHivePendingSplitsCheckpoint) checkpoint;\n+\t\tPendingSplitsCheckpoint<HiveSourceSplit> superCP = PendingSplitsCheckpoint.fromCollectionSnapshot(\n+\t\t\t\thiveCheckpoint.getSplits(), hiveCheckpoint.getAlreadyProcessedPaths());\n+\t\tbyte[] superBytes = superSerDe.serialize(superCP);\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\toutputStream.writeInt(superBytes.length);\n+\t\t\toutputStream.write(superBytes);\n+\t\t\toutputStream.writeObject(hiveCheckpoint.getCurrentReadOffset());\n+\t\t\toutputStream.writeInt(hiveCheckpoint.getSeenPartitions().size());", "originalCommit": "cfd0be434e9ef7b045dc0b418d68b1ad0665d186", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a7187f1e884c1ba4fec3d3630a3f157d633d0c48", "url": "https://github.com/apache/flink/commit/a7187f1e884c1ba4fec3d3630a3f157d633d0c48", "message": "[FLINK-19888][connector-files] Make file source more extensible", "committedDate": "2020-11-08T02:12:25Z", "type": "commit"}, {"oid": "873f52f02d69a32b1f78822fa6e0fe850833807f", "url": "https://github.com/apache/flink/commit/873f52f02d69a32b1f78822fa6e0fe850833807f", "message": "address comments for hive connector", "committedDate": "2020-11-08T04:38:55Z", "type": "forcePushed"}, {"oid": "292f7debbab2b02b317b2578421edb9eed5cdb48", "url": "https://github.com/apache/flink/commit/292f7debbab2b02b317b2578421edb9eed5cdb48", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming", "committedDate": "2020-11-08T07:05:14Z", "type": "commit"}, {"oid": "292f7debbab2b02b317b2578421edb9eed5cdb48", "url": "https://github.com/apache/flink/commit/292f7debbab2b02b317b2578421edb9eed5cdb48", "message": "[FLINK-19888][hive] Migrate Hive source to FLIP-27 source interface for streaming", "committedDate": "2020-11-08T07:05:14Z", "type": "forcePushed"}]}