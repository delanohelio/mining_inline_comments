{"pr_number": 11725, "pr_title": "[FLINK-15670][API] Provide a Kafka Source/Sink pair as KafkaShuffle", "pr_createdAt": "2020-04-14T06:24:18Z", "pr_url": "https://github.com/apache/flink/pull/11725", "timeline": [{"oid": "84dd0ae96bf17f9ed3b04fb1077162b2757f21b3", "url": "https://github.com/apache/flink/commit/84dd0ae96bf17f9ed3b04fb1077162b2757f21b3", "message": "[FLINK-15670] Kafka Shuffle and Test Case", "committedDate": "2020-04-15T07:02:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410295578", "bodyText": "Although Properties can be used recursively, it's in a different way than most collections, so I found the name flatten very confusing at first. How about resolveDefaults?\nThen I'd also adjust the javadoc: Flatten the default values of a Properties...?", "author": "AHeise", "createdAt": "2020-04-17T15:23:30Z", "path": "flink-core/src/main/java/org/apache/flink/util/PropertiesUtil.java", "diffHunk": "@@ -108,6 +109,27 @@ public static boolean getBoolean(Properties config, String key, boolean defaultV\n \t\t}\n \t}\n \n+\t/**\n+\t * Flatten a recursive {@link Properties} to a first level property map.\n+\t * In some cases, {KafkaProducer#propsToMap} for example, Properties is used purely as a HashMap\n+\t * without considering its default properties.\n+\t *\n+\t * @param config Properties to be flatten\n+\t * @return Properties without defaults; all properties are put in the first-level\n+\t */\n+\tpublic static Properties flatten(Properties config) {", "originalCommit": "a156e0f5ede214854db0d7e755ba2c96dd0e9e58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NjQ0Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410296442", "bodyText": "On a second thought, wouldn't it make more sense to provide a correct propsToMap implementation? If it's only used in KafkaProducer, then we could fix it there. If not, I'd consider that function more useful than this flatten.", "author": "AHeise", "createdAt": "2020-04-17T15:24:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwMjkyMA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410302920", "bodyText": "Should be covered with one test case.", "author": "AHeise", "createdAt": "2020-04-17T15:35:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU0MTI4NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418541284", "bodyText": "On a second thought, wouldn't it make more sense to provide a correct propsToMap implementation? If it's only used in KafkaProducer, then we could fix it there. If not, I'd consider that function more useful than this flatten.\n\nthis name is still confusing since it is a Properties, not a map.\nThe flattened properties are actually used in the Kafka client lib, not that easy to fix.", "author": "curcur", "createdAt": "2020-05-01T13:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU0MTM2OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418541368", "bodyText": "Should be covered with one test case.\n\nsure, will add one later.", "author": "curcur", "createdAt": "2020-05-01T13:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTI5ODM4Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421298383", "bodyText": "The flattened properties are actually used in the Kafka client lib, not that easy to fix.\n\nDoes that mean that Kafka is actually not processing the recursive Properties correctly? We should probably file a bug report then.", "author": "AHeise", "createdAt": "2020-05-07T07:34:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NTU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5NjgyMA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410296820", "bodyText": "nit: flattened", "author": "AHeise", "createdAt": "2020-04-17T15:25:27Z", "path": "flink-core/src/main/java/org/apache/flink/util/PropertiesUtil.java", "diffHunk": "@@ -108,6 +109,27 @@ public static boolean getBoolean(Properties config, String key, boolean defaultV\n \t\t}\n \t}\n \n+\t/**\n+\t * Flatten a recursive {@link Properties} to a first level property map.\n+\t * In some cases, {KafkaProducer#propsToMap} for example, Properties is used purely as a HashMap\n+\t * without considering its default properties.\n+\t *\n+\t * @param config Properties to be flatten", "originalCommit": "a156e0f5ede214854db0d7e755ba2c96dd0e9e58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwMDExMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410300111", "bodyText": "On commit comment:\n\n{@link Properties} is used purely as a HashMap\nYou probably meant Hashtable.", "author": "AHeise", "createdAt": "2020-04-17T15:30:52Z", "path": "flink-core/src/main/java/org/apache/flink/util/PropertiesUtil.java", "diffHunk": "@@ -19,6 +19,7 @@\n ", "originalCommit": "a156e0f5ede214854db0d7e755ba2c96dd0e9e58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU0MzcxNQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418543715", "bodyText": "Oh, yep, used as a HashTable, translated to a HashMap. Thanks :-)", "author": "curcur", "createdAt": "2020-05-01T13:36:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwMDExMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwNzM2NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410307365", "bodyText": "Except for watermark forwarding, this is a 1:1 copy of StreamSink, correct? So at the very least, this should subclass StreamSink and only have special treatment for that.\nHowever, we can do much better, since you added invoke(mark) to all SinkFunctions why not give it a no-op implementation and always invoke it in StreamSink?", "author": "AHeise", "createdAt": "2020-04-17T15:42:58Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamShuffleSink.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.LatencyMarker;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+\n+/**\n+ * A {@link StreamOperator} for executing {@link SinkFunction} that handle both elements and watermarks.\n+ *\n+ * @param <IN>\n+ */\n+@Internal\n+public class StreamShuffleSink<IN> extends AbstractUdfStreamOperator<Object, SinkFunction<IN>>", "originalCommit": "3e382e517e703d609dcd3a7d09e2e1e426fb7576", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU0NDA4Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418544086", "bodyText": "Mark this as resolved since the entire API part has been re-organized.", "author": "curcur", "createdAt": "2020-05-01T13:37:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwNzM2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMDU0OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410310549", "bodyText": "invoke is not a very good name as that is only associated with the core functionality of a function. I'd probably name it like processWatermark, setWatermark, or onWatermark. Probably a good idea to get more feedback from some API maintainer.", "author": "AHeise", "createdAt": "2020-04-17T15:48:05Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/SinkFunction.java", "diffHunk": "@@ -52,6 +54,20 @@ default void invoke(IN value, Context context) throws Exception {\n \t\tinvoke(value);\n \t}\n \n+\t/**\n+\t * This function is called for every watermark.\n+\t *\n+\t * <p>You have to override this method when implementing a {@code SinkFunction} to handle watermark.\n+\t * This method has to be used together with {@link StreamShuffleSink}\n+\t *\n+\t * @param watermark The watermark to handle.\n+\t * @throws Exception This method may throw exceptions. Throwing an exception will cause the operation\n+\t *                   to fail and may trigger recovery.\n+\t */\n+\tdefault void invoke(Watermark watermark) throws Exception {", "originalCommit": "3e382e517e703d609dcd3a7d09e2e1e426fb7576", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM1NDc4Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r414354783", "bodyText": "If we use a custom operator for sink/source, we can avoid modifying the public API of SinkFunction at all. Stephan also talked about operators in the jira ticket.", "author": "AHeise", "createdAt": "2020-04-24T07:25:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMDU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU0NDMzOQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418544339", "bodyText": "Mark as resolved since the entire API is re-organized.\nSinkFunction is untouched in the new version.", "author": "curcur", "createdAt": "2020-05-01T13:38:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMDU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMTA4Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410311086", "bodyText": "Not necessary, see comment on StreamShuffleSink.", "author": "AHeise", "createdAt": "2020-04-17T15:48:55Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java", "diffHunk": "@@ -1333,6 +1334,32 @@ public ExecutionConfig getExecutionConfig() {\n \t\treturn sink;\n \t}\n \n+\t/**\n+\t * Adds a {@link StreamShuffleSink} to this DataStream. {@link StreamShuffleSink} is attached with\n+\t * {@link SinkFunction} that can manipulate watermarks.\n+\t *\n+\t * @param sinkFunction\n+\t * \t\t\tThe object containing the sink's invoke function for both the element and watermark.\n+\t * @return\tThe closed DataStream.\n+\t */\n+\tpublic DataStreamSink<T> addSinkShuffle(SinkFunction<T> sinkFunction) {", "originalCommit": "3e382e517e703d609dcd3a7d09e2e1e426fb7576", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU5NTM5Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418595393", "bodyText": "Mark as resolved since the entire API is re-organized.\nThis method is wrapped inside FlinkKafkaShuffle", "author": "curcur", "createdAt": "2020-05-01T15:35:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMTA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMTE3MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r410311171", "bodyText": "Not necessary, see comment on StreamShuffleSink.", "author": "AHeise", "createdAt": "2020-04-17T15:49:03Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/transformations/SinkTransformation.java", "diffHunk": "@@ -67,6 +68,22 @@ public SinkTransformation(\n \t\tthis(input, name, SimpleOperatorFactory.of(operator), parallelism);\n \t}\n \n+\t/**\n+\t * Creates a new {@code SinkTransformation} from the given input {@code Transformation}.\n+\t *\n+\t * @param input The input {@code Transformation}\n+\t * @param name The name of the {@code Transformation}, this will be shown in Visualizations and the Log\n+\t * @param operator The sink shuffle operator\n+\t * @param parallelism The parallelism of this {@code SinkTransformation}\n+\t */\n+\tpublic SinkTransformation(", "originalCommit": "3e382e517e703d609dcd3a7d09e2e1e426fb7576", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU5NTU0Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418595542", "bodyText": "Mark as resolved since the entire API is re-organized.", "author": "curcur", "createdAt": "2020-05-01T15:36:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMTE3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2MzI2OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r414363269", "bodyText": "My gut feeling is that we need to forward also some events, such as end of partition.\nI'm also not entirely sure how to deal with checkpoint barriers (see general remark).\nIn any case, we should use StreamElementSerializer instead of implementing it again.", "author": "AHeise", "createdAt": "2020-04-24T07:39:47Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\tString defaultTopicId,\n+\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\tProperties props,\n+\t\tKeySelector<IN, KEY> keySelector,\n+\t\tSemantic semantic,\n+\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\tdefaultTopicId, partitionIndex, timestamp, null, kafkaSerializer.serializeRecord(next, timestamp));\n+\t\tpendingRecords.incrementAndGet();\n+\t\ttransaction.producer.send(record, callback);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each watermark.\n+\t * @param transaction transaction state;\n+\t *                    watermark are written to Kafka (if needed) in transactions\n+\t * @param watermark watermark to handle\n+\t * @throws FlinkKafkaException\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, Watermark watermark) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint subtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t// broadcast watermark\n+\t\tfor (int partition : partitions) {\n+\t\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\t\tdefaultTopicId, partition, watermark.getTimestamp(), null, kafkaSerializer.serializeWatermark(watermark, subtask));\n+\t\t\tpendingRecords.incrementAndGet();\n+\t\t\ttransaction.producer.send(record, callback);\n+\t\t}\n+\t}\n+\n+\tprivate int[] getPartitions(KafkaTransactionState transaction) {\n+\t\tint[] partitions = topicPartitionsMap.get(defaultTopicId);\n+\t\tif (partitions == null) {\n+\t\t\tpartitions = getPartitionsByTopic(defaultTopicId, transaction.producer);\n+\t\t\ttopicPartitionsMap.put(defaultTopicId, partitions);\n+\t\t}\n+\n+\t\tPreconditions.checkArgument(partitions.length == numberOfPartitions);\n+\n+\t\treturn partitions;\n+\t}\n+\n+\t/**\n+\t * Flink Kafka Shuffle Serializer.\n+\t */\n+\tpublic static final class KafkaSerializer<IN> implements Serializable {\n+\t\tpublic static final int TAG_REC_WITH_TIMESTAMP = 0;\n+\t\tpublic static final int TAG_REC_WITHOUT_TIMESTAMP = 1;\n+\t\tpublic static final int TAG_WATERMARK = 2;\n+\n+\t\tprivate final TypeSerializer<IN> serializer;\n+\n+\t\tprivate transient DataOutputSerializer dos;\n+\n+\t\tKafkaSerializer(TypeSerializer<IN> serializer) {\n+\t\t\tthis.serializer = serializer;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Format: TAG, (timestamp), record.\n+\t\t */\n+\t\tbyte[] serializeRecord(IN record, Long timestamp) {\n+\t\t\tif (dos == null) {\n+\t\t\t\tdos = new DataOutputSerializer(16);\n+\t\t\t}\n+\n+\t\t\ttry {\n+\t\t\t\tif (timestamp == null) {\n+\t\t\t\t\tdos.writeInt(TAG_REC_WITHOUT_TIMESTAMP);", "originalCommit": "8b9abb0692d7deb89b98671097faa8d7aef1a673", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU5NjE3MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r418596170", "bodyText": "Mark as resolved since the entire API is re-organized.\nThe issue why StreamElementSerializer is needed is discussed offline and summarized in the following comments.", "author": "curcur", "createdAt": "2020-05-01T15:37:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2MzI2OQ=="}], "type": "inlineReview"}, {"oid": "a9c54057daa5bb907302534b04be5f4742d1b586", "url": "https://github.com/apache/flink/commit/a9c54057daa5bb907302534b04be5f4742d1b586", "message": "[FLINK-15670] Kafka Shuffle and Test Case", "committedDate": "2020-04-30T09:12:43Z", "type": "forcePushed"}, {"oid": "0d84af72bc6f7159452da67f34d8825a0d040d02", "url": "https://github.com/apache/flink/commit/0d84af72bc6f7159452da67f34d8825a0d040d02", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-02T06:32:10Z", "type": "forcePushed"}, {"oid": "066795205734add3b142a92c687c98b25253985e", "url": "https://github.com/apache/flink/commit/066795205734add3b142a92c687c98b25253985e", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-02T07:06:13Z", "type": "forcePushed"}, {"oid": "563a1b12312b1b2de2b8426272e4975f046156d8", "url": "https://github.com/apache/flink/commit/563a1b12312b1b2de2b8426272e4975f046156d8", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-02T09:46:04Z", "type": "forcePushed"}, {"oid": "a03c41aa7757aa950ba2121887a5a9227b6b438d", "url": "https://github.com/apache/flink/commit/a03c41aa7757aa950ba2121887a5a9227b6b438d", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-02T11:44:39Z", "type": "forcePushed"}, {"oid": "18025611a435e08ed6ac626804f3b7529f7344c7", "url": "https://github.com/apache/flink/commit/18025611a435e08ed6ac626804f3b7529f7344c7", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-02T15:56:06Z", "type": "forcePushed"}, {"oid": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "url": "https://github.com/apache/flink/commit/38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-06T07:51:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTI5NjUwNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421296506", "bodyText": "I'd remove these assertions and only keep the last 3.", "author": "AHeise", "createdAt": "2020-05-07T07:31:17Z", "path": "flink-core/src/test/java/org/apache/flink/util/PropertiesUtilTest.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.util;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.util.PropertiesUtil.flatten;\n+\n+/**\n+ * Tests for the {@link PropertiesUtil}.\n+ */\n+public class PropertiesUtilTest {\n+\n+\t@Test\n+\tpublic void testFlatten() {\n+\t\t// default Properties is null\n+\t\tProperties prop1 = new Properties();\n+\t\tprop1.put(\"key1\", \"value1\");\n+\n+\t\t// default Properties is prop1\n+\t\tProperties prop2 = new Properties(prop1);\n+\t\tprop2.put(\"key2\", \"value2\");\n+\n+\t\t// default Properties is prop2\n+\t\tProperties prop3 = new Properties(prop2);\n+\t\tprop3.put(\"key3\", \"value3\");\n+\n+\t\tProperties flattened = flatten(prop3);\n+\t\tAssert.assertEquals(flattened.get(\"key1\"), prop3.getProperty(\"key1\"));\n+\t\tAssert.assertEquals(flattened.get(\"key2\"), prop3.getProperty(\"key2\"));\n+\t\tAssert.assertEquals(flattened.get(\"key3\"), prop3.getProperty(\"key3\"));\n+\t\tAssert.assertNotEquals(flattened.get(\"key1\"), prop3.get(\"key1\"));\n+\t\tAssert.assertNotEquals(flattened.get(\"key2\"), prop3.get(\"key2\"));\n+\t\tAssert.assertEquals(flattened.get(\"key3\"), prop3.get(\"key3\"));", "originalCommit": "7f3e5e0374113e80d0fd0295b8a4e44d6d42bdd5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMwODA2OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421308069", "bodyText": "Not necessary. KafkaShuffleProducer can use the protected currentTransaction().", "author": "AHeise", "createdAt": "2020-05-07T07:51:53Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.java", "diffHunk": "@@ -235,6 +243,10 @@ public final void invoke(\n \t\tinvoke(currentTransactionHolder.handle, value, context);\n \t}\n \n+\tpublic final void invoke(Watermark watermark) throws Exception {\n+\t\tinvoke(currentTransactionHolder.handle, watermark);\n+\t}\n+", "originalCommit": "f42f22edd83854868a6bf57a3a5ff32747ffe773", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMwODExMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421308113", "bodyText": "Not necessary. KafkaShuffleProducer can use the protected currentTransaction().", "author": "AHeise", "createdAt": "2020-05-07T07:51:58Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.java", "diffHunk": "@@ -166,6 +167,13 @@ protected TXN currentTransaction() {\n \t */\n \tprotected abstract void invoke(TXN transaction, IN value, Context context) throws Exception;\n \n+\t/**\n+\t * Handle watermark within a transaction.\n+\t */\n+\tprotected void invoke(TXN transaction, Watermark watermark) throws Exception {\n+\t\tthrow new UnsupportedOperationException(\"invokeWithWatermark should not be invoked\");\n+\t}\n+", "originalCommit": "f42f22edd83854868a6bf57a3a5ff32747ffe773", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMxMDI1Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421310257", "bodyText": "Is this supposed to be public or not? It should probably be package-private.\nI was also thinking of pulling it as top-level class, which then also incorporates the deserializing stuff of the next commit.", "author": "AHeise", "createdAt": "2020-05-07T07:55:35Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\tString defaultTopicId,\n+\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\tProperties props,\n+\t\tKeySelector<IN, KEY> keySelector,\n+\t\tSemantic semantic,\n+\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\tdefaultTopicId, partitionIndex, timestamp, null, kafkaSerializer.serializeRecord(next, timestamp));\n+\t\tpendingRecords.incrementAndGet();\n+\t\ttransaction.getProducer().send(record, callback);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each watermark.\n+\t * @param transaction transaction state;\n+\t *                    watermark are written to Kafka (if needed) in transactions\n+\t * @param watermark watermark to handle\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, Watermark watermark) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint subtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t// broadcast watermark\n+\t\tlong timestamp = watermark.getTimestamp();\n+\t\tfor (int partition : partitions) {\n+\t\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\t\tdefaultTopicId, partition, timestamp, null, kafkaSerializer.serializeWatermark(watermark, subtask));\n+\t\t\tpendingRecords.incrementAndGet();\n+\t\t\ttransaction.getProducer().send(record, callback);\n+\t\t}\n+\t}\n+\n+\tprivate int[] getPartitions(KafkaTransactionState transaction) {\n+\t\tint[] partitions = topicPartitionsMap.get(defaultTopicId);\n+\t\tif (partitions == null) {\n+\t\t\tpartitions = getPartitionsByTopic(defaultTopicId, transaction.getProducer());\n+\t\t\ttopicPartitionsMap.put(defaultTopicId, partitions);\n+\t\t}\n+\n+\t\tPreconditions.checkArgument(partitions.length == numberOfPartitions);\n+\n+\t\treturn partitions;\n+\t}\n+\n+\t/**\n+\t * Flink Kafka Shuffle Serializer.\n+\t */\n+\tpublic static final class KafkaSerializer<IN> implements Serializable {\n+\t\tpublic static final int TAG_REC_WITH_TIMESTAMP = 0;\n+\t\tpublic static final int TAG_REC_WITHOUT_TIMESTAMP = 1;\n+\t\tpublic static final int TAG_WATERMARK = 2;", "originalCommit": "f42f22edd83854868a6bf57a3a5ff32747ffe773", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3ODIzMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423478231", "bodyText": "This is a good idea.\nActually I find the SerDe part might be able to be reused by other persistent storage, not just Kafka, so it is necessary to exact Ser/De to the top level.\nLet's see whether I have time to extract these parts out before", "author": "curcur", "createdAt": "2020-05-12T05:55:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMxMDI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA0NDA2OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425044068", "bodyText": "As discussed offline, maybe extracting it next time when a similar persistent channel is needed.", "author": "curcur", "createdAt": "2020-05-14T10:46:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMxMDI1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMxMjgyOA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421312828", "bodyText": "Nit: Method parameters must be double-indented. Cannot be automatically done with IntelliJ unfortunately :(.", "author": "AHeise", "createdAt": "2020-05-07T07:59:46Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\tString defaultTopicId,", "originalCommit": "f42f22edd83854868a6bf57a3a5ff32747ffe773", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3NzI2MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423477261", "bodyText": "I will do a pass on it....", "author": "curcur", "createdAt": "2020-05-12T05:52:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMxMjgyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyNDM3MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421324371", "bodyText": "nit: indent.", "author": "AHeise", "createdAt": "2020-05-07T08:19:45Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyNTI5NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421325294", "bodyText": "I'd assume so, or else bounded inputs won't work well.", "author": "AHeise", "createdAt": "2020-05-07T08:21:16Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3ODk2Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423478967", "bodyText": "OK, let's discuss these offline.\nOn the other hand, my question is actually whether I need to pass the MAX watermark downstream?", "author": "curcur", "createdAt": "2020-05-12T05:57:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyNTI5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyODQ5Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421328493", "bodyText": "KafkaShuffleElement seems over-engineered. I guess having a holder for timestamp + object is enough and then simply use instanceof checks.", "author": "AHeise", "createdAt": "2020-05-07T08:26:29Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\tprivate abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tboolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tKafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\t}\n+", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTUzMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423479533", "bodyText": "As stated above, the SerDe part can be generalized. it is not just Kafka.", "author": "curcur", "createdAt": "2020-05-12T05:59:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyODQ5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MTA1Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423591053", "bodyText": "do it if having time.", "author": "curcur", "createdAt": "2020-05-12T09:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyODQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyOTQ4NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421329485", "bodyText": "Can you explain me once again, why we store timestamp directly in ProducerRecord and still also serialize it? Seems redundant.", "author": "AHeise", "createdAt": "2020-05-07T08:28:03Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\tString defaultTopicId,\n+\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\tProperties props,\n+\t\tKeySelector<IN, KEY> keySelector,\n+\t\tSemantic semantic,\n+\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(", "originalCommit": "f42f22edd83854868a6bf57a3a5ff32747ffe773", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MDYwMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423480603", "bodyText": "I think during my first test, the timestamp read from ConsumerRecord is always the timestamp when it was created.\nBut i did not spend time looking into this because as stated above, the SerDe part can be generalized. it is not just Kafka. So i would like to make the SerDe less dependant on Kafka.", "author": "curcur", "createdAt": "2020-05-12T06:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyOTQ4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUxNDgyNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423514826", "bodyText": "Okay seems plausible.", "author": "AHeise", "createdAt": "2020-05-12T07:20:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMyOTQ4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDA0Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421330042", "bodyText": "Again, why do we serialize timestamp in the payload and not take it from ConsumerRecord?", "author": "AHeise", "createdAt": "2020-05-07T08:28:54Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\tprivate abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tboolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tKafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\tKafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\tKafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)\n+\t\t\tthrows Exception {\n+\t\t\tbyte[] value = record.value();\n+\t\t\tdis.setBuffer(value);\n+\t\t\tint tag = IntSerializer.INSTANCE.deserialize(dis);\n+\n+\t\t\tif (tag == TAG_REC_WITHOUT_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_REC_WITH_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(LongSerializer.INSTANCE.deserialize(dis), serializer.deserialize(dis));", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MDY5MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423480690", "bodyText": "same answer above.", "author": "curcur", "createdAt": "2020-05-12T06:02:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDA0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDkwOA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421330908", "bodyText": "Perform under checkpoint lock?", "author": "AHeise", "createdAt": "2020-05-07T08:30:18Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4Mjg4NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423482885", "bodyText": "This is a rewrite of {@link AbstractFetcher#emitRecordWithTimestamp}\n     if (record != null) {\n\t\tif (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {\n\t\t\t// fast path logic, in case there are no watermarks generated in the fetcher\n\n\t\t\t// emit the record, using the checkpoint lock to guarantee\n\t\t\t// atomicity of record emission and offset state update\n\t\t\tsynchronized (checkpointLock) {\n\t\t\t\tsourceContext.collectWithTimestamp(record, timestamp);\n\t\t\t\tpartitionState.setOffset(offset);\n\t\t\t}\n\t\t} else if (timestampWatermarkMode == PERIODIC_WATERMARKS) {\n\t\t\temitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);\n\t\t} else {\n\t\t\temitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);\n\t\t}\n\t} else {\n\t\t// if the record is null, simply just update the offset state for partition\n\t\tsynchronized (checkpointLock) {\n\t\t\tpartitionState.setOffset(offset);\n\t\t}\n\t}", "author": "curcur", "createdAt": "2020-05-12T06:09:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDkwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MzczOA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423483738", "bodyText": "If you go through emitRecordWithTimestampAndPunctuatedWatermark\nfor exmaple\nrecords are emitted under checkpointLock\nwatermarks are emitted through updateMinPunctuatedWatermark(newWatermark), not within the lock.\nThis is understandable in the current case, since watermark is only a indicator, not strictly part of a checkpoint. That's why.", "author": "curcur", "createdAt": "2020-05-12T06:11:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDkwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUxMTAxNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423511016", "bodyText": "Yes, you are not modifying state, so all good.", "author": "AHeise", "createdAt": "2020-05-12T07:13:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzMDkwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5NTg5OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421395898", "bodyText": "protected?", "author": "AHeise", "createdAt": "2020-05-07T10:16:16Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java", "diffHunk": "@@ -264,7 +264,7 @@ public FlinkKafkaConsumerBase(\n \t * @param properties - Kafka configuration properties to be adjusted\n \t * @param offsetCommitMode offset commit mode\n \t */\n-\tstatic void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {\n+\tpublic static void adjustAutoCommitConfig(Properties properties, OffsetCommitMode offsetCommitMode) {", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5OTUyMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421399523", "bodyText": "What happens if one partition has ended and we receive no watermarks anymore? Are the watermarks of the other partitions still propagated properly? Almost feels like using StatusWatermarkValve would be handy.", "author": "AHeise", "createdAt": "2020-05-07T10:22:54Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\tprivate abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tboolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tKafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\tKafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\tKafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)\n+\t\t\tthrows Exception {\n+\t\t\tbyte[] value = record.value();\n+\t\t\tdis.setBuffer(value);\n+\t\t\tint tag = IntSerializer.INSTANCE.deserialize(dis);\n+\n+\t\t\tif (tag == TAG_REC_WITHOUT_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_REC_WITH_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(LongSerializer.INSTANCE.deserialize(dis), serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_WATERMARK) {\n+\t\t\t\treturn new KafkaShuffleWatermark<>(\n+\t\t\t\t\tIntSerializer.INSTANCE.deserialize(dis), LongSerializer.INSTANCE.deserialize(dis));\n+\t\t\t}\n+\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported tag format\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * WatermarkHandler to generate watermarks.\n+\t */\n+\tprivate static class WatermarkHandler {\n+\t\tprivate final int producerParallelism;\n+\t\tprivate final Map<Integer, Long> subtaskWatermark;\n+\n+\t\tprivate long currentMinWatermark = Long.MIN_VALUE;\n+\n+\t\tWatermarkHandler(int numberOfSubtask) {\n+\t\t\tthis.producerParallelism = numberOfSubtask;\n+\t\t\tthis.subtaskWatermark = new HashMap<>(numberOfSubtask);\n+\t\t}\n+\n+\t\tpublic Optional<Watermark> checkAndGetNewWatermark(KafkaShuffleWatermark newWatermark) {\n+\t\t\t// watermarks is incremental for the same partition and PRODUCER subtask\n+\t\t\tLong currentSubTaskWatermark = subtaskWatermark.get(newWatermark.subtask);\n+\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t(currentSubTaskWatermark == null) || (currentSubTaskWatermark <= newWatermark.watermark),\n+\t\t\t\t\"Watermark should always increase\");\n+\n+\t\t\tsubtaskWatermark.put(newWatermark.subtask, newWatermark.watermark);\n+\n+\t\t\tif (subtaskWatermark.values().size() < producerParallelism) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}", "originalCommit": "3d864a6c1927fe22ac935f07871462ac0c38fc96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ5NDg2NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423494864", "bodyText": "Yes, they are very similar at first glance. But my feeling is StatusWatermarkValve is an overkill since it also includes StreamStatus. And the pick up of watermark depends on the stream status.\n\nIf one partition has ended and we receive no watermarks anymore, are the watermarks of the other partitions still propagated properly?\n\nAs long as all subtasks contain watermarks, the watermark progress properly. The only problem is if one of the subtask does not produce watermark. But that's a common question to find a min/max watermark amongst all channels.\nData is propogates fine.", "author": "curcur", "createdAt": "2020-05-12T06:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5OTUyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUxMjQwMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423512403", "bodyText": "I'd assume you need to propagate EndOfPartitionEvents downstream and adjust producerParallelism accordingly.", "author": "AHeise", "createdAt": "2020-05-12T07:15:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5OTUyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjQ0NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276444", "bodyText": "Clarified offline, not needed as MAX watermark is propagated. If we want to support bounded sources all the way, we'd need to handle EndOfPartitionEvents but we can do that in a later version. KafkaShuffle will probably almost never be used with bounded sources.", "author": "AHeise", "createdAt": "2020-05-17T15:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5OTUyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMDkzNQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421400935", "bodyText": "If this is API, I guess it should be public and either @PublicEvolving or @Experimental.", "author": "AHeise", "createdAt": "2020-05-07T10:25:25Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+class FlinkKafkaShuffle {", "originalCommit": "418a2bae3ca7ac45b7207bd7deab418db3419f2e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMTI2Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421401267", "bodyText": "nit: indent.", "author": "AHeise", "createdAt": "2020-05-07T10:26:02Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields key positions from inputStream\n+\t * @param <T> input type\n+\t */\n+\tstatic <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\tDataStream<T> inputStream,", "originalCommit": "418a2bae3ca7ac45b7207bd7deab418db3419f2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ5NTYwMA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423495600", "bodyText": "I will do a final pass later.", "author": "curcur", "createdAt": "2020-05-12T06:40:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMTI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMTkyMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421401923", "bodyText": "public", "author": "AHeise", "createdAt": "2020-05-07T10:27:21Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields key positions from inputStream\n+\t * @param <T> input type\n+\t */\n+\tstatic <T> KeyedStream<T, Tuple> persistentKeyBy(", "originalCommit": "418a2bae3ca7ac45b7207bd7deab418db3419f2e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMjMzMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421402333", "bodyText": "Shouldn't that be the same?", "author": "AHeise", "createdAt": "2020-05-07T10:28:07Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions", "originalCommit": "418a2bae3ca7ac45b7207bd7deab418db3419f2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ5Njg4Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423496887", "bodyText": "producerParallelism can be different from numberOfPartitions", "author": "curcur", "createdAt": "2020-05-12T06:43:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMjMzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUxMzU1MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423513551", "bodyText": "Yes, but what is the benefit? I guess it can be used to fan-in/fan-out at the source level?", "author": "AHeise", "createdAt": "2020-05-12T07:18:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwMjMzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNTU3NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421405575", "bodyText": "Instead of setting timeout to all methods, I'd go with a JUnit rule:\n\t@Rule\n\tpublic final Timeout timeout = Timeout.builder()\n\t\t\t.withTimeout(30, TimeUnit.SECONDS)\n\t\t\t.build();\n\nand then only use @Test on the tests. That's easier to maintain when we need to increase the timeout on azure.", "author": "AHeise", "createdAt": "2020-05-07T10:34:10Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTkxMjM3OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421912378", "bodyText": "that's a good one :-)", "author": "curcur", "createdAt": "2020-05-08T03:12:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNjc5Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421406792", "bodyText": "We use end2end in a different context, where we use a complete Flink distribution to execute the test.\nI'd simply call it testKafkaShuffle to avoid any misunderstanding.", "author": "AHeise", "createdAt": "2020-05-07T10:36:20Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void simpleEndToEndTest(String topic, int elementCount, TimeCharacteristic timeCharacteristic)", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwNzMwNA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421407304", "bodyText": "Extract to avoid duplicate code with simpleEndToEndTest.", "author": "AHeise", "createdAt": "2020-05-07T10:37:19Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void simpleEndToEndTest(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(timeCharacteristic);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\t\tFlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t * To test data is partitioned to the right partition\n+\t */\n+\tprivate void testAssignedToPartition(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(EventTime);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\t// ------- Write data to Kafka partition basesd on FlinkKafkaPartitioner ------\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = FlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+\n+\t\tdeleteTestTopic(topic);", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwODYzNA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421408634", "bodyText": "nit: also double-indent.", "author": "AHeise", "createdAt": "2020-05-07T10:39:55Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void simpleEndToEndTest(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(timeCharacteristic);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\t\tFlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t * To test data is partitioned to the right partition\n+\t */\n+\tprivate void testAssignedToPartition(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(EventTime);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\t// ------- Write data to Kafka partition basesd on FlinkKafkaPartitioner ------\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = FlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate static class PunctuatedExtractor implements AssignerWithPunctuatedWatermarks<Tuple3<Integer, Long, String>> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic long extractTimestamp(Tuple3<Integer, Long, String> element, long previousTimestamp) {\n+\t\t\treturn element.f1;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Watermark checkAndGetNextWatermark(Tuple3<Integer, Long, String> lastElement, long extractedTimestamp) {\n+\t\t\treturn new Watermark(extractedTimestamp);\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaSourceFunction extends RichParallelSourceFunction<Tuple3<Integer, Long, String>> {\n+\t\tprivate volatile boolean running = true;\n+\t\tprivate int elementCount;\n+\n+\t\tKafkaSourceFunction(int elementCount) {\n+\t\t\tthis.elementCount = elementCount;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void run(SourceContext<Tuple3<Integer, Long, String>> ctx) {\n+\t\t\tlong timestamp = 1584349939799L;\n+\t\t\tint instanceId = getRuntimeContext().getIndexOfThisSubtask();\n+\t\t\tfor (int i = 0; i < elementCount && running; i++) {\n+\t\t\t\tctx.collect(new Tuple3<>(i, timestamp++, \"source-instance-\" + instanceId));\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void cancel() {\n+\t\t\trunning = false;\n+\t\t}\n+\t}\n+\n+\tprivate static class ElementCountNoMoreThanValidator\n+\t\timplements MapFunction<Tuple3<Integer, Long, String>, Tuple3<Integer, Long, String>> {", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwODc3Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421408777", "bodyText": "nit: chop args", "author": "AHeise", "createdAt": "2020-05-07T10:40:15Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\tsimpleEndToEndTest(\"test_simple_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_processing_time\", 100000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_ingestion_time\", 100000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test(timeout = 30000L)\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_event_time\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void simpleEndToEndTest(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(timeCharacteristic);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\t\tFlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t * To test data is partitioned to the right partition\n+\t */\n+\tprivate void testAssignedToPartition(String topic, int elementCount, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n+\t\tenv.setStreamTimeCharacteristic(EventTime);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+\t\t\tenv.addSource(new KafkaSourceFunction(elementCount)).setParallelism(producerParallelism);\n+\n+\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+\n+\t\t// ------- Write data to Kafka partition basesd on FlinkKafkaPartitioner ------\n+\t\tProperties properties = kafkaServer.getStandardProperties();\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = FlinkKafkaShuffle\n+\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(elementCount * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(elementCount * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate static class PunctuatedExtractor implements AssignerWithPunctuatedWatermarks<Tuple3<Integer, Long, String>> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic long extractTimestamp(Tuple3<Integer, Long, String> element, long previousTimestamp) {\n+\t\t\treturn element.f1;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Watermark checkAndGetNextWatermark(Tuple3<Integer, Long, String> lastElement, long extractedTimestamp) {\n+\t\t\treturn new Watermark(extractedTimestamp);\n+\t\t}\n+\t}\n+\n+\tprivate static class KafkaSourceFunction extends RichParallelSourceFunction<Tuple3<Integer, Long, String>> {\n+\t\tprivate volatile boolean running = true;\n+\t\tprivate int elementCount;\n+\n+\t\tKafkaSourceFunction(int elementCount) {\n+\t\t\tthis.elementCount = elementCount;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void run(SourceContext<Tuple3<Integer, Long, String>> ctx) {\n+\t\t\tlong timestamp = 1584349939799L;\n+\t\t\tint instanceId = getRuntimeContext().getIndexOfThisSubtask();\n+\t\t\tfor (int i = 0; i < elementCount && running; i++) {\n+\t\t\t\tctx.collect(new Tuple3<>(i, timestamp++, \"source-instance-\" + instanceId));\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void cancel() {\n+\t\t\trunning = false;\n+\t\t}\n+\t}\n+\n+\tprivate static class ElementCountNoMoreThanValidator\n+\t\timplements MapFunction<Tuple3<Integer, Long, String>, Tuple3<Integer, Long, String>> {\n+\t\tprivate final int totalCount;\n+\t\tprivate int counter = 0;\n+\n+\t\tElementCountNoMoreThanValidator(int totalCount) {\n+\t\t\tthis.totalCount = totalCount;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Tuple3<Integer, Long, String> map(Tuple3<Integer, Long, String> element) throws Exception {\n+\t\t\tcounter++;\n+\n+\t\t\tif (counter > totalCount) {\n+\t\t\t\tthrow new Exception(\"Error: number of elements more than expected\");\n+\t\t\t}\n+\n+\t\t\treturn element;\n+\t\t}\n+\t}\n+\n+\tprivate static class ElementCountNoLessThanValidator\n+\t\timplements MapFunction<Tuple3<Integer, Long, String>, Tuple3<Integer, Long, String>> {\n+\t\tprivate final int totalCount;\n+\t\tprivate int counter = 0;\n+\n+\t\tElementCountNoLessThanValidator(int totalCount) {\n+\t\t\tthis.totalCount = totalCount;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Tuple3<Integer, Long, String> map(Tuple3<Integer, Long, String> element) throws Exception {\n+\t\t\tcounter++;\n+\n+\t\t\tif (counter == totalCount) {\n+\t\t\t\tthrow new SuccessException();\n+\t\t\t}\n+\n+\t\t\treturn element;\n+\t\t}\n+\t}\n+\n+\tprivate static class PartitionValidator\n+\t\textends KeyedProcessFunction<Tuple, Tuple3<Integer, Long, String>, Tuple3<Integer, Long, String>> {\n+\n+\t\tprivate final KeySelector<Tuple3<Integer, Long, String>, Tuple> keySelector;\n+\t\tprivate final int numberOfPartitions;\n+\t\tprivate final String topic;\n+\n+\t\tprivate int previousPartition;\n+\n+\t\tPartitionValidator(\n+\t\t\tKeySelector<Tuple3<Integer, Long, String>, Tuple> keySelector, int numberOfPartitions, String topic) {\n+\t\t\tthis.keySelector = keySelector;\n+\t\t\tthis.numberOfPartitions = numberOfPartitions;\n+\t\t\tthis.topic = topic;\n+\t\t\tthis.previousPartition = -1;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void processElement(\n+\t\t\tTuple3<Integer, Long, String> in, Context ctx, Collector<Tuple3<Integer, Long, String>> out)", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ5NzIyMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423497223", "bodyText": "I will do a pass on it.", "author": "curcur", "createdAt": "2020-05-12T06:44:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQwODc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQxMzMyMg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r421413322", "bodyText": "The implemented tests are really good. I miss two cases though:\n\nOut of order events (add randomness to source timestamp)\nAny failure and recovery tests. See https://github.com/apache/flink/blob/f239d680e9b8f3f5ace621b7806e0bb7e14d3fdd/flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointITCase.java for a possible approach.", "author": "AHeise", "createdAt": "2020-05-07T10:49:17Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {", "originalCommit": "38d798ced7a5e50c3b982c8fc221852b78d7ecb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ5ODI1Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423498252", "bodyText": "Both failover cases and watermark cases are added.\nBut I feel I still need some more tests\n\none on the coumser side of watermarks\none on the producer side failover.", "author": "curcur", "createdAt": "2020-05-12T06:46:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQxMzMyMg=="}], "type": "inlineReview"}, {"oid": "0793ea2bff3f2a281d497ccc134512379aa0e94f", "url": "https://github.com/apache/flink/commit/0793ea2bff3f2a281d497ccc134512379aa0e94f", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-12T06:47:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzMTMzNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423531337", "bodyText": "Seems like you add the timeCharacteristic always to the topic name. If so, then you can move it into the respective test implementation function to make the invocation cleaner.", "author": "AHeise", "createdAt": "2020-05-12T07:50:11Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,722 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElement;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElementDeserializer;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleRecord;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleWatermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableMap;\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Iterables;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PRODUCER_PARALLELISM;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\tprivate static final long INIT_TIMESTAMP = 1584349939799L;\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(30000L);\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.EXACTLY_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple_\" + ProcessingTime.toString(), 200000, ProcessingTime);", "originalCommit": "0793ea2bff3f2a281d497ccc134512379aa0e94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzNDYyNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423534626", "bodyText": "Test if watermarks are really monotonic.", "author": "AHeise", "createdAt": "2020-05-12T07:55:35Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,722 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElement;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElementDeserializer;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleRecord;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleWatermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableMap;\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Iterables;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PRODUCER_PARALLELISM;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaConsumerTestBase {\n+\n+\tprivate static final long INIT_TIMESTAMP = 1584349939799L;\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(30000L);\n+\n+\t@BeforeClass\n+\tpublic static void prepare() throws Exception {\n+\t\tKafkaProducerTestBase.prepare();\n+\t\t((KafkaTestEnvironmentImpl) kafkaServer).setProducerSemantic(FlinkKafkaProducer.Semantic.EXACTLY_ONCE);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple_\" + ProcessingTime.toString(), 200000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple_\" + IngestionTime.toString(), 200000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple_\" + EventTime.toString(), 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_\" + ProcessingTime.toString(), 300000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_\" + IngestionTime.toString(), 300000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition_\" + EventTime.toString(), 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value serialization and deserialization with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeProcessingTime() throws Exception {\n+\t\ttestRecordSerDe(ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeIngestionTime() throws Exception {\n+\t\ttestRecordSerDe(IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeEventTime() throws Exception {\n+\t\ttestRecordSerDe(EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * Failure Recovery after processing 2/3 data with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery_\" + ProcessingTime.toString(), 1000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * Failure Recovery after processing 2/3 data with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery_\" + IngestionTime.toString(), 1000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * Failure Recovery after processing 2/3 data with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryEventTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery_\" + EventTime.toString(), 1000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * Failure Recovery after data is repartitioned with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery_\" + ProcessingTime.toString(), 500, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * Failure Recovery after data is repartitioned with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery_\" + IngestionTime.toString(), 500, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * Failure Recovery after data is repartitioned with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryEventTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery_\" + EventTime.toString(), 500, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testWatermarkBroadcasting() throws Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int numElementsPerPartition = 1000;\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tMap<Integer, Collection<ConsumerRecord<byte[], byte[]>>> results = testKafkaShuffleProducer(\n+\t\t\t\"test_watermark_broadcast\" + EventTime.toString(),\n+\t\t\tenv,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproducerParallelism,\n+\t\t\tnumElementsPerPartition,\n+\t\t\tEventTime);\n+\t\tTypeSerializer<Tuple3<Integer, Long, String>> typeSerializer = createTypeSerializer(env);\n+\t\tKafkaShuffleElementDeserializer<Tuple3<Integer, Long, String>> deserializer = new KafkaShuffleElementDeserializer<>();\n+\n+\t\t// Records in a single partition are kept in order\n+\t\tfor (int p = 0; p < numberOfPartitions; p++) {\n+\t\t\tCollection<ConsumerRecord<byte[], byte[]>> records = results.get(p);\n+\t\t\tMap<Integer, List<KafkaShuffleWatermark>> watermarks = new HashMap<>();\n+\n+\t\t\tfor (ConsumerRecord<byte[], byte[]> consumerRecord : records) {\n+\t\t\t\tAssert.assertNull(consumerRecord.key());\n+\t\t\t\tKafkaShuffleElement<Tuple3<Integer, Long, String>> element =\n+\t\t\t\t\tdeserializer.deserialize(typeSerializer, consumerRecord);\n+\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\tKafkaShuffleRecord<Tuple3<Integer, Long, String>> record = element.asRecord();\n+\t\t\t\t\tAssert.assertEquals(record.getValue().f1.longValue(), INIT_TIMESTAMP + record.getValue().f0);\n+\t\t\t\t\tAssert.assertEquals(record.getTimestamp().longValue(), record.getValue().f1.longValue());\n+\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\tKafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\twatermarks.computeIfAbsent(watermark.getSubtask(), k -> new ArrayList<>());\n+\t\t\t\t\twatermarks.get(watermark.getSubtask()).add(watermark);\n+\t\t\t\t} else {\n+\t\t\t\t\tfail(\"KafkaShuffleElement is either record or watermark\");\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// According to the setting how watermarks are generated in this ITTest,\n+\t\t\t// every producer task emits a watermark corresponding to each record + the end-of-event-time watermark.\n+\t\t\t// Hence each producer sub task generates `numElementsPerPartition + 1` watermarks.\n+\t\t\t// Each producer sub task broadcasts these `numElementsPerPartition + 1` watermarks to all partitions.\n+\t\t\t// Thus in total, each producer sub task emits `(numElementsPerPartition + 1) * numberOfPartitions` watermarks.\n+\t\t\t// From the consumer side, each partition receives `(numElementsPerPartition + 1) * producerParallelism` watermarks,\n+\t\t\t// with each producer sub task produces `numElementsPerPartition + 1` watermarks.\n+\t\t\t// Besides, watermarks from the same producer sub task should keep in order.\n+\t\t\tfor (List<KafkaShuffleWatermark> subTaskWatermarks : watermarks.values()) {\n+\t\t\t\tint index = 0;\n+\t\t\t\tAssert.assertEquals(subTaskWatermarks.size(), numElementsPerPartition + 1);\n+\t\t\t\tfor (KafkaShuffleWatermark watermark : subTaskWatermarks) {\n+\t\t\t\t\tif (index == numElementsPerPartition) {\n+\t\t\t\t\t\t// the last element is the watermark that signifies end-of-event-time\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), Watermark.MAX_WATERMARK.getTimestamp());\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), INIT_TIMESTAMP + index++);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+//\t@Test\n+//\tpublic void testFailureRecovery2()\n+//\t\tthrows Exception {\n+//\t\tfinal int numberOfPartitions = 3;\n+//\t\tfinal int producerParallelism = 2;\n+//\n+//\t\tString topic = \"failure_recovery\";\n+//\t\tTimeCharacteristic timeCharacteristic = EventTime;\n+//\t\tint numElementsPerPartition =  1000;\n+//\n+//\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+//\n+//\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+//\t\tenv.setParallelism(producerParallelism);\n+//\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n+//\t\tenv.setBufferTimeout(0);\n+//\t\tenv.setStreamTimeCharacteristic(EventTime);\n+//\n+//\t\tDataStream<Tuple3<Integer, Long, String>> source =\n+//\t\t\tenv.addSource(new KafkaSourceFunction(numElementsPerPartition)).setParallelism(producerParallelism);\n+//\n+//\t\tDataStream<Tuple3<Integer, Long, String>> input = (timeCharacteristic == EventTime) ?\n+//\t\t\tsource.assignTimestampsAndWatermarks(new PunctuatedExtractor()).setParallelism(producerParallelism) : source;\n+//\n+//\t\t// ------- Write data to Kafka partition basesd on FlinkKafkaPartitioner ------\n+//\t\tProperties properties = kafkaServer.getStandardProperties();\n+//\n+//\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = FlinkKafkaShuffle\n+//\t\t\t.persistentKeyBy(input, topic, producerParallelism, numberOfPartitions, properties, 0);\n+//\t\tkeyedStream\n+//\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+//\t\t\t.setParallelism(numberOfPartitions)\n+//\t\t\t.map(new FailingIdentityMapper<>(numElementsPerPartition * producerParallelism * 2 / 3)).setParallelism(1)\n+//\t\t\t.map(new ElementCountNoMoreThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1)\n+//\t\t\t.map(new ElementCountNoLessThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1);\n+//\n+//\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+//\n+//\t\tdeleteTestTopic(topic);\n+//\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void testKafkaShuffle(String topic, int numElementsPerPartition, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = createEnvironment(producerParallelism, timeCharacteristic);\n+\t\tcreateKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerPartition, producerParallelism, timeCharacteristic, numberOfPartitions)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t * To test data is partitioned to the right partition\n+\t */\n+\tprivate void testAssignedToPartition(String topic, int numElementsPerPartition, TimeCharacteristic timeCharacteristic)\n+\t\tthrows Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = createEnvironment(producerParallelism, timeCharacteristic);\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = createKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerPartition, producerParallelism, timeCharacteristic, numberOfPartitions\n+\t\t);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test failure recovery after processing 2/3 data\n+\t */\n+\tprivate void testKafkaShuffleFailureRecovery(\n+\t\t\tString topic, int numElementsPerPartition, TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\t\tfinal int failAfterElements = numElementsPerPartition * numberOfPartitions * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env =\n+\t\t\tcreateEnvironment(producerParallelism, timeCharacteristic).enableCheckpointing(500);\n+\n+\t\tcreateKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerPartition, producerParallelism, timeCharacteristic, numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate void testAssignedToPartitionFailureRecovery(\n+\t\tString topic, int numElementsPerPartition, TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int failAfterElements = numElementsPerPartition * numberOfPartitions * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env =\n+\t\t\tcreateEnvironment(producerParallelism, timeCharacteristic).enableCheckpointing(500);\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, String>, Tuple> keyedStream = createKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerPartition, producerParallelism, timeCharacteristic, numberOfPartitions\n+\t\t);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoMoreThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1)\n+\t\t\t.map(new ElementCountNoLessThanValidator(numElementsPerPartition * producerParallelism)).setParallelism(1);\n+\n+\t\ttryExecute(env, \"KafkaShuffle partition assignment test\");\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate void testRecordSerDe(TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tfinal int numElementsPerPartition = 2000;\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+\t\t// Records in a single partition are kept in order\n+\t\tCollection<ConsumerRecord<byte[], byte[]>> records = Iterables.getOnlyElement(\n+\t\t\ttestKafkaShuffleProducer(\n+\t\t\t\t\"test_serde_\" + timeCharacteristic.toString(),\n+\t\t\t\tenv,\n+\t\t\t\t1,\n+\t\t\t\t1,\n+\t\t\t\tnumElementsPerPartition,\n+\t\t\t\ttimeCharacteristic).values());\n+\n+\t\tswitch (timeCharacteristic) {\n+\t\t\tcase ProcessingTime:\n+\t\t\t\t// NonTimestampContext, no watermark\n+\t\t\t\tAssert.assertEquals(records.size(), numElementsPerPartition);\n+\t\t\t\tbreak;\n+\t\t\tcase IngestionTime:\n+\t\t\t\t// IngestionTime uses AutomaticWatermarkContext and it emits a watermark after every `watermarkInterval`\n+\t\t\t\t// with default interval 200, hence difficult to control the number of watermarks\n+\t\t\t\tbreak;\n+\t\t\tcase EventTime:\n+\t\t\t\t// ManualWatermarkContext\n+\t\t\t\t// `numElementsPerPartition` records, `numElementsPerPartition` watermarks, and one end-of-event-time watermark\n+\t\t\t\tAssert.assertEquals(records.size(), numElementsPerPartition * 2 + 1);\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tfail(\"unknown TimeCharacteristic type\");\n+\t\t}\n+\n+\t\tTypeSerializer<Tuple3<Integer, Long, String>> typeSerializer = createTypeSerializer(env);\n+\n+\t\tKafkaShuffleElementDeserializer<Tuple3<Integer, Long, String>> deserializer = new KafkaShuffleElementDeserializer<>();\n+\n+\t\tint recordIndex = 0;\n+\t\tint watermarkIndex = 0;\n+\t\tfor (ConsumerRecord<byte[], byte[]> consumerRecord : records) {\n+\t\t\tAssert.assertNull(consumerRecord.key());\n+\t\t\tKafkaShuffleElement<Tuple3<Integer, Long, String>> element =\n+\t\t\t\tdeserializer.deserialize(typeSerializer, consumerRecord);\n+\t\t\tif (element.isRecord()) {\n+\t\t\t\tKafkaShuffleRecord<Tuple3<Integer, Long, String>> record = element.asRecord();\n+\t\t\t\tswitch (timeCharacteristic) {\n+\t\t\t\t\tcase ProcessingTime:\n+\t\t\t\t\t\tAssert.assertNull(record.getTimestamp());\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase IngestionTime:\n+\t\t\t\t\t\tAssert.assertNotNull(record.getTimestamp());\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase EventTime:\n+\t\t\t\t\t\tAssert.assertEquals(record.getTimestamp().longValue(), record.getValue().f1.longValue());\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tdefault:\n+\t\t\t\t\t\tfail(\"unknown TimeCharacteristic type\");\n+\t\t\t\t}\n+\t\t\t\tAssert.assertEquals(record.getValue().f0.intValue(), recordIndex);\n+\t\t\t\tAssert.assertEquals(record.getValue().f1.longValue(), INIT_TIMESTAMP + recordIndex);\n+\t\t\t\tAssert.assertEquals(record.getValue().f2, \"source-instance-0\");\n+\t\t\t\trecordIndex++;\n+\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\tswitch (timeCharacteristic) {\n+\t\t\t\t\tcase ProcessingTime:\n+\t\t\t\t\t\tfail(\"Watermarks should not be generated in the case of ProcessingTime\");\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase IngestionTime:\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase EventTime:\n+\t\t\t\t\t\tKafkaShuffleWatermark watermark = element.asWatermark();", "originalCommit": "0793ea2bff3f2a281d497ccc134512379aa0e94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU4ODE4OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r423588189", "bodyText": "long[]?", "author": "AHeise", "createdAt": "2020-05-12T09:19:00Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\tProcessingTimeService processingTimeProvider,\n+\t\tlong autoWatermarkInterval,\n+\t\tClassLoader userCodeClassLoader,\n+\t\tString taskNameWithSubtasks,\n+\t\tTypeSerializer<T> serializer,\n+\t\tProperties kafkaProperties,\n+\t\tlong pollTimeout,\n+\t\tMetricGroup subtaskMetricGroup,\n+\t\tMetricGroup consumerMetricGroup,\n+\t\tboolean useMetrics,\n+\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\n+\t\tpublic int getSubtask() {\n+\t\t\treturn subtask;\n+\t\t}\n+\n+\t\tpublic long getWatermark() {\n+\t\t\treturn watermark;\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\n+\t\tpublic T getValue() {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\tpublic Long getTimestamp() {\n+\t\t\treturn timestamp;\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)\n+\t\t\tthrows Exception {\n+\t\t\tbyte[] value = record.value();\n+\t\t\tdis.setBuffer(value);\n+\t\t\tint tag = IntSerializer.INSTANCE.deserialize(dis);\n+\n+\t\t\tif (tag == TAG_REC_WITHOUT_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_REC_WITH_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(LongSerializer.INSTANCE.deserialize(dis), serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_WATERMARK) {\n+\t\t\t\treturn new KafkaShuffleWatermark<>(\n+\t\t\t\t\tIntSerializer.INSTANCE.deserialize(dis), LongSerializer.INSTANCE.deserialize(dis));\n+\t\t\t}\n+\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported tag format\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * WatermarkHandler to generate watermarks.\n+\t */\n+\tprivate static class WatermarkHandler {\n+\t\tprivate final int producerParallelism;\n+\t\tprivate final Map<Integer, Long> subtaskWatermark;", "originalCommit": "e8ad11d58e19cde8d6357ec540d94f23308bf071", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "17969b130ddb6c2c9f99c338b1bad013412cf248", "url": "https://github.com/apache/flink/commit/17969b130ddb6c2c9f99c338b1bad013412cf248", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-14T02:58:42Z", "type": "forcePushed"}, {"oid": "ba529bb050fb3fa07c15fc620b5c29e3fa73ae7f", "url": "https://github.com/apache/flink/commit/ba529bb050fb3fa07c15fc620b5c29e3fa73ae7f", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-14T10:19:29Z", "type": "forcePushed"}, {"oid": "7f23666b38502f8f8e93cb876d4dccdde7df97fd", "url": "https://github.com/apache/flink/commit/7f23666b38502f8f8e93cb876d4dccdde7df97fd", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file", "committedDate": "2020-05-14T15:31:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5NDg3Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425294873", "bodyText": "its not a proper link, but I guess it's also not possible because of the redirection. Maybe just use {@code KafkaProducer#propsToMap}", "author": "AHeise", "createdAt": "2020-05-14T17:04:25Z", "path": "flink-core/src/main/java/org/apache/flink/util/PropertiesUtil.java", "diffHunk": "@@ -108,6 +109,27 @@ public static boolean getBoolean(Properties config, String key, boolean defaultV\n \t\t}\n \t}\n \n+\t/**\n+\t * Flatten a recursive {@link Properties} to a first level property map.\n+\t * In some cases, {KafkaProducer#propsToMap} for example, Properties is used purely as a HashTable", "originalCommit": "95095d671e08a12e95a7b303cbe0db5542e3f573", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzOTk3Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425539972", "bodyText": "Yep, that's why I removed the link :-)\nThanks!", "author": "curcur", "createdAt": "2020-05-15T03:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5NDg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5NzMxMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425297311", "bodyText": "Commit message should include a bit more detail, so that someone who doesn't know FLINK-15670 by heart knows what's going on.\nFor example\n[FLINK-15670][connector] Adds the producer for KafkaShuffle.\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of one shuffle step is redirected.", "author": "AHeise", "createdAt": "2020-05-14T17:08:06Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java", "diffHunk": "@@ -210,7 +210,7 @@\n \t/**", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU0MDY1MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425540650", "bodyText": "I like \"by heart\", hahaha", "author": "curcur", "createdAt": "2020-05-15T03:05:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5NzMxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5ODIxNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425298217", "bodyText": "Would package-private suffice? (Same for all similar changes in this and next commit).", "author": "AHeise", "createdAt": "2020-05-14T17:09:05Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java", "diffHunk": "@@ -210,7 +210,7 @@\n \t/**\n \t * The name of the default topic this producer is writing data to.\n \t */\n-\tprivate final String defaultTopicId;\n+\tprotected final String defaultTopicId;", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU1Mjg2OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425552869", "bodyText": "It was fine to use \"package-private\" previously, but I have moved all the KafkaShuffle related classes to a separate package. So, it is not sufficient any more.", "author": "curcur", "createdAt": "2020-05-15T03:57:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI5ODIxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMDkzMw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425300933", "bodyText": "If method parameters do not fit on the line of invocation all of them need to be chopped (including the first one).", "author": "AHeise", "createdAt": "2020-05-14T17:13:04Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\t\tString defaultTopicId,\n+\t\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\t\tProperties props,\n+\t\t\tKeySelector<IN, KEY> keySelector,\n+\t\t\tSemantic semantic,\n+\t\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\tdefaultTopicId, partitionIndex, timestamp, null, kafkaSerializer.serializeRecord(next, timestamp));", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU1NDcyMg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425554722", "bodyText": "OK, I will do another pass.", "author": "curcur", "createdAt": "2020-05-15T04:05:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMDkzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMTMzNA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425301334", "bodyText": "Please use exception chaining:\nthrow new RuntimeException(\"Fail to assign a partition number to record\", e)", "author": "AHeise", "createdAt": "2020-05-14T17:13:46Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\t\tString defaultTopicId,\n+\t\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\t\tProperties props,\n+\t\t\tKeySelector<IN, KEY> keySelector,\n+\t\t\tSemantic semantic,\n+\t\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMTUxNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425301516", "bodyText": "chop", "author": "AHeise", "createdAt": "2020-05-14T17:14:06Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\t\tString defaultTopicId,\n+\t\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\t\tProperties props,\n+\t\t\tKeySelector<IN, KEY> keySelector,\n+\t\t\tSemantic semantic,\n+\t\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\tdefaultTopicId, partitionIndex, timestamp, null, kafkaSerializer.serializeRecord(next, timestamp));\n+\t\tpendingRecords.incrementAndGet();\n+\t\ttransaction.getProducer().send(record, callback);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each watermark.\n+\t * @param watermark watermark to handle\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\tpublic void invoke(Watermark watermark) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\t\tKafkaTransactionState transaction = currentTransaction();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint subtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t// broadcast watermark\n+\t\tlong timestamp = watermark.getTimestamp();\n+\t\tfor (int partition : partitions) {\n+\t\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\t\tdefaultTopicId, partition, timestamp, null, kafkaSerializer.serializeWatermark(watermark, subtask));", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMjE5NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425302195", "bodyText": "Would writeByte suffice?", "author": "AHeise", "createdAt": "2020-05-14T17:15:13Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\t\tString defaultTopicId,\n+\t\t\tTypeInformationSerializationSchema<IN> schema,\n+\t\t\tProperties props,\n+\t\t\tKeySelector<IN, KEY> keySelector,\n+\t\t\tSemantic semantic,\n+\t\t\tint kafkaProducersPoolSize) {\n+\t\tsuper(defaultTopicId, (element, timestamp) -> null, props, semantic, kafkaProducersPoolSize);\n+\n+\t\tthis.kafkaSerializer = new KafkaSerializer<>(schema.getSerializer());\n+\t\tthis.keySelector = keySelector;\n+\n+\t\tPreconditions.checkArgument(\n+\t\t\tprops.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tnumberOfPartitions = PropertiesUtil.getInt(props, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each element.\n+\t * @param transaction transaction state;\n+\t *                    elements are written to Kafka in transactions to guarantee different level of data consistency\n+\t * @param next element to handle\n+\t * @param context context needed to handle the element\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\t@Override\n+\tpublic void invoke(KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\n+\t\t// write timestamp to Kafka if timestamp is available\n+\t\tLong timestamp = context.timestamp();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint partitionIndex;\n+\t\ttry {\n+\t\t\tpartitionIndex = KeyGroupRangeAssignment\n+\t\t\t\t.assignKeyToParallelOperator(keySelector.getKey(next), partitions.length, partitions.length);\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new RuntimeException(\"Fail to assign a partition number to record\");\n+\t\t}\n+\n+\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\tdefaultTopicId, partitionIndex, timestamp, null, kafkaSerializer.serializeRecord(next, timestamp));\n+\t\tpendingRecords.incrementAndGet();\n+\t\ttransaction.getProducer().send(record, callback);\n+\t}\n+\n+\t/**\n+\t * This is the function invoked to handle each watermark.\n+\t * @param watermark watermark to handle\n+\t * @throws FlinkKafkaException for kafka error\n+\t */\n+\tpublic void invoke(Watermark watermark) throws FlinkKafkaException {\n+\t\tcheckErroneous();\n+\t\tKafkaTransactionState transaction = currentTransaction();\n+\n+\t\tint[] partitions = getPartitions(transaction);\n+\t\tint subtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t// broadcast watermark\n+\t\tlong timestamp = watermark.getTimestamp();\n+\t\tfor (int partition : partitions) {\n+\t\t\tProducerRecord<byte[], byte[]> record = new ProducerRecord<>(\n+\t\t\t\tdefaultTopicId, partition, timestamp, null, kafkaSerializer.serializeWatermark(watermark, subtask));\n+\t\t\tpendingRecords.incrementAndGet();\n+\t\t\ttransaction.getProducer().send(record, callback);\n+\t\t}\n+\t}\n+\n+\tprivate int[] getPartitions(KafkaTransactionState transaction) {\n+\t\tint[] partitions = topicPartitionsMap.get(defaultTopicId);\n+\t\tif (partitions == null) {\n+\t\t\tpartitions = getPartitionsByTopic(defaultTopicId, transaction.getProducer());\n+\t\t\ttopicPartitionsMap.put(defaultTopicId, partitions);\n+\t\t}\n+\n+\t\tPreconditions.checkArgument(partitions.length == numberOfPartitions);\n+\n+\t\treturn partitions;\n+\t}\n+\n+\t/**\n+\t * Flink Kafka Shuffle Serializer.\n+\t */\n+\tpublic static final class KafkaSerializer<IN> implements Serializable {\n+\t\tpublic static final int TAG_REC_WITH_TIMESTAMP = 0;\n+\t\tpublic static final int TAG_REC_WITHOUT_TIMESTAMP = 1;\n+\t\tpublic static final int TAG_WATERMARK = 2;\n+\n+\t\tprivate final TypeSerializer<IN> serializer;\n+\n+\t\tprivate transient DataOutputSerializer dos;\n+\n+\t\tKafkaSerializer(TypeSerializer<IN> serializer) {\n+\t\t\tthis.serializer = serializer;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Format: TAG, (timestamp), record.\n+\t\t */\n+\t\tbyte[] serializeRecord(IN record, Long timestamp) {\n+\t\t\tif (dos == null) {\n+\t\t\t\tdos = new DataOutputSerializer(16);\n+\t\t\t}\n+\n+\t\t\ttry {\n+\t\t\t\tif (timestamp == null) {\n+\t\t\t\t\tdos.writeInt(TAG_REC_WITHOUT_TIMESTAMP);", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU1NTMwOQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425555309", "bodyText": "It should be, but again, this is similar to the tag handling of\nStreamElementSerializer\nWould you mind keeping as it is for now?", "author": "curcur", "createdAt": "2020-05-15T04:08:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMjE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMTgyMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425631821", "bodyText": "StreamElementSerializer is using 1 byte only.\n\nfor now\n\nOnce this is out, we cannot change the format easily without breaking setups.\nThis actually reminds me that it might be good to prepend a version tag as well, so that we actually have a way to change it later.\n|version (byte)|tag (byte)|payload|", "author": "AHeise", "createdAt": "2020-05-15T08:00:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMjE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc2NzQ1Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425767456", "bodyText": "I will do this.", "author": "curcur", "createdAt": "2020-05-15T12:29:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwMjE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwNTcwNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425305707", "bodyText": "Pass serializer directly.", "author": "AHeise", "createdAt": "2020-05-14T17:21:10Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleProducer.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaException;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+\n+/**\n+ * Flink Kafka Shuffle Producer Function.\n+ * It is different from {@link FlinkKafkaProducer} in the way handling elements and watermarks\n+ */\n+@Internal\n+public class FlinkKafkaShuffleProducer<IN, KEY> extends FlinkKafkaProducer<IN> {\n+\tprivate final KafkaSerializer<IN> kafkaSerializer;\n+\tprivate final KeySelector<IN, KEY> keySelector;\n+\tprivate final int numberOfPartitions;\n+\n+\tFlinkKafkaShuffleProducer(\n+\t\t\tString defaultTopicId,\n+\t\t\tTypeInformationSerializationSchema<IN> schema,", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwNjA0Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425306043", "bodyText": "Unneeded API change (see KafkaShuffleProducer).", "author": "AHeise", "createdAt": "2020-05-14T17:21:48Z", "path": "flink-core/src/main/java/org/apache/flink/api/common/serialization/TypeInformationSerializationSchema.java", "diffHunk": "@@ -126,4 +126,8 @@ public boolean isEndOfStream(T nextElement) {\n \tpublic TypeInformation<T> getProducedType() {\n \t\treturn typeInfo;\n \t}\n+\n+\tpublic TypeSerializer<T> getSerializer() {", "originalCommit": "3e64ae63ff2d366fb33cfe09f59f59f174fb1e4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwNjQwMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425306401", "bodyText": "package-private?", "author": "AHeise", "createdAt": "2020-05-14T17:22:25Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcher.java", "diffHunk": "@@ -73,7 +73,7 @@\n \n \t/** The lock that guarantees that record emission and state updates are atomic,\n \t * from the view of taking a checkpoint. */\n-\tprivate final Object checkpointLock;\n+\tprotected final Object checkpointLock;", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwNjg2OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425306869", "bodyText": "this line does not make any sense.", "author": "AHeise", "createdAt": "2020-05-14T17:23:11Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwNzA4Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425307086", "bodyText": "comment? just to be symmetric with the other fields (not strictly needed).", "author": "AHeise", "createdAt": "2020-05-14T17:23:34Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwODg4Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425308887", "bodyText": "A see huge duplicates to KafkaFetcher. I'm curios why you didn't choose to subclass in the same way as on producer/consumer.", "author": "AHeise", "createdAt": "2020-05-14T17:26:31Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU1OTA0MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425559040", "bodyText": "That's a good question.\nIt is because what I really override is fetcher's method (how fetcher fetch records, deserialize records and emit watermarks). That's why I have to subclass Fetcher anyway.\nI tried different ways, there is one way to avoid duplicated code, but needs to add a default constructor in one of consumer or fetcher base classes.\nI think it is unsafe to do it, and that's why it is ended up like this.", "author": "curcur", "createdAt": "2020-05-15T04:26:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwODg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzNjI1MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425636250", "bodyText": "I'd probably extract an AbstractKafkaFetcher from KafkaFetcher with an abstract handleRecord.", "author": "AHeise", "createdAt": "2020-05-15T08:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwODg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc3MTU1Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425771552", "bodyText": "me answer why.", "author": "curcur", "createdAt": "2020-05-15T12:36:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwODg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MjQ5NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426142494", "bodyText": "removed the duplicated code.", "author": "curcur", "createdAt": "2020-05-16T10:48:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwODg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwOTU5NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425309595", "bodyText": "This is the only difference to KafkaFetcher right? That could be overridden in an extracted handleRecord or so.", "author": "AHeise", "createdAt": "2020-05-14T17:27:34Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc3MTMzNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425771337", "bodyText": "Me Answer why.", "author": "curcur", "createdAt": "2020-05-15T12:36:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMwOTU5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMDM0OA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425310348", "bodyText": "dis is not initialized when KafkaShuffleElementDeserializer gets deserialized.", "author": "AHeise", "createdAt": "2020-05-14T17:28:38Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t/**\n+\t * An element in a KafkaShuffle. Can be a record or a Watermark.\n+\t */\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A watermark element in a KafkaShuffle. It includes\n+\t * - subtask index where the watermark is coming from\n+\t * - watermark timestamp\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\n+\t\tpublic int getSubtask() {\n+\t\t\treturn subtask;\n+\t\t}\n+\n+\t\tpublic long getWatermark() {\n+\t\t\treturn watermark;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * One value with Type T in a KafkaShuffle. This stores the value and an optional associated timestamp.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\n+\t\tpublic T getValue() {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\tpublic Long getTimestamp() {\n+\t\t\treturn timestamp;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Deserializer for KafkaShuffleElement.\n+\t * @param <T>\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)\n+\t\t\tthrows Exception {\n+\t\t\tbyte[] value = record.value();\n+\t\t\tdis.setBuffer(value);", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYwODYxNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425608617", "bodyText": "That's actually my question, why?\nI have the same problem for dos, but it seems working here?", "author": "curcur", "createdAt": "2020-05-15T07:11:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMDM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzNzA5MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425637091", "bodyText": "My guess is that in your test code it's actually never deserialized. Just provide readObject.", "author": "AHeise", "createdAt": "2020-05-15T08:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMDM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMTI0Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425311243", "bodyText": "param name is very confusing. Should also be producerParallelism.", "author": "AHeise", "createdAt": "2020-05-14T17:30:04Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\tprivate final WatermarkHandler watermarkHandler;\n+\t// ------------------------------------------------------------------------\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t/**\n+\t * An element in a KafkaShuffle. Can be a record or a Watermark.\n+\t */\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A watermark element in a KafkaShuffle. It includes\n+\t * - subtask index where the watermark is coming from\n+\t * - watermark timestamp\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\n+\t\tpublic int getSubtask() {\n+\t\t\treturn subtask;\n+\t\t}\n+\n+\t\tpublic long getWatermark() {\n+\t\t\treturn watermark;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * One value with Type T in a KafkaShuffle. This stores the value and an optional associated timestamp.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\n+\t\tpublic T getValue() {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\tpublic Long getTimestamp() {\n+\t\t\treturn timestamp;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Deserializer for KafkaShuffleElement.\n+\t * @param <T>\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)\n+\t\t\tthrows Exception {\n+\t\t\tbyte[] value = record.value();\n+\t\t\tdis.setBuffer(value);\n+\t\t\tint tag = IntSerializer.INSTANCE.deserialize(dis);\n+\n+\t\t\tif (tag == TAG_REC_WITHOUT_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_REC_WITH_TIMESTAMP) {\n+\t\t\t\treturn new KafkaShuffleRecord<>(LongSerializer.INSTANCE.deserialize(dis), serializer.deserialize(dis));\n+\t\t\t} else if (tag == TAG_WATERMARK) {\n+\t\t\t\treturn new KafkaShuffleWatermark<>(\n+\t\t\t\t\tIntSerializer.INSTANCE.deserialize(dis), LongSerializer.INSTANCE.deserialize(dis));\n+\t\t\t}\n+\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported tag format\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * WatermarkHandler to generate watermarks.\n+\t */\n+\tprivate static class WatermarkHandler {\n+\t\tprivate final int producerParallelism;\n+\t\tprivate final Map<Integer, Long> subtaskWatermark;\n+\n+\t\tprivate long currentMinWatermark = Long.MIN_VALUE;\n+\n+\t\tWatermarkHandler(int numberOfSubtask) {", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMjI1Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425312252", "bodyText": "Same as in producer. Pass serializer directly.", "author": "AHeise", "createdAt": "2020-05-14T17:31:45Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffleConsumer.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.flink.streaming.connectors.kafka.config.OffsetCommitMode;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+import org.apache.flink.util.SerializedValue;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PRODUCER_PARALLELISM;\n+\n+/**\n+ * Flink Kafka Shuffle Consumer Function.\n+ */\n+@Internal\n+public class FlinkKafkaShuffleConsumer<T> extends FlinkKafkaConsumer<T> {\n+\tprivate final TypeSerializer<T> serializer;\n+\tprivate final int producerParallelism;\n+\n+\tFlinkKafkaShuffleConsumer(String topic, TypeInformationSerializationSchema<T> schema, Properties props) {", "originalCommit": "579af1de69c7acbdd9a189d6b444b90cecc8cb11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU2MDg4OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425560889", "bodyText": "That's not easy, because I have to call the super constructor of FlinkKafkaConsumer.\nSchema is needed for the constructor of FlinkKafkaConsumer, and it can not be null even if it is not needed (not sure)\nsuper(topic, schema, props);", "author": "curcur", "createdAt": "2020-05-15T04:34:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzODMxNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425638317", "bodyText": "If KafkaShuffleElementDeserializer would implement KafkaDeserializationSchema you could directly pass it. That would use more of the abstractions that are already there.", "author": "AHeise", "createdAt": "2020-05-15T08:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MzI5NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426143294", "bodyText": "I guess the main point here is to avoid exposing TypeInformationSerializationSchema to users. FlinkKafkaShuffleConsumer is an internal function, and is wrapped under KafkaShuffle. So I guess it is fine to keep TypeInformationSerializationSchema it here.\nThe schema is needed to call the right FlinkKafkaConsumer constructor.\nThe shema is never used, can be null, but null confuses the compiler.", "author": "curcur", "createdAt": "2020-05-16T10:59:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MzM1Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426143357", "bodyText": "I will mark this resolved for now.", "author": "curcur", "createdAt": "2020-05-16T11:00:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxMjI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1OTA4OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425359089", "bodyText": "chop", "author": "AHeise", "createdAt": "2020-05-14T18:50:38Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields key positions from inputStream\n+\t * @param <T> input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream, topic, producerParallelism, numberOfPartitions, properties, keySelector(inputStream, fields));", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1OTgyNA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425359824", "bodyText": "Method javadocs usually start in the third form Writes to ... see also DataStream.", "author": "AHeise", "createdAt": "2020-05-14T18:52:05Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2MDQzNA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425360434", "bodyText": "Usually, after first sentence (or summary if it's more than one sentence), we have linebreak and start next paragraph with <p>.", "author": "AHeise", "createdAt": "2020-05-14T18:53:13Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2MjA2Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425362066", "bodyText": "Description of parameters is usually capitalized to make it easier read (also in code).", "author": "AHeise", "createdAt": "2020-05-14T18:56:04Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2MzAxMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425363011", "bodyText": "Add description when producerParallelism should be != numberOfPartitions as it's none-trivial to decide.\nAlso is there any support for just going with default degree of parallelism, such that it can be changed through configs?", "author": "AHeise", "createdAt": "2020-05-14T18:57:42Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU3MDIyMg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425570222", "bodyText": "Do you mean specifically saying \"producerParallelism != numberOfPartitions?\"\nIf think this is keyBy through a persistent channel, the producer parallelism does not matter with the max key group size? I kind of feeling confusing if adding such a comment.\nFor config changes, yes, probably in the next step; Overall, this is only a very first version, allowing users to set parallelism when writing pipelines might be enough.", "author": "curcur", "createdAt": "2020-05-15T05:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2MzAxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2NDc3MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425664771", "bodyText": "the producer parallelism does not matter with the max key group size?\n\nIf it doesn't matter, why can it be set by the user? Please give users some guidance on how to use your function. How should they know how it's implemented internally? You could also add it as a class comment to outline the approach.\nIn general, there are quite a bit of comments and variable names referring to producer, but there is no explanation what the producer is in KafkaShuffle. I'd probably harmonize source/consumer and sink/producer and only use either word.", "author": "AHeise", "createdAt": "2020-05-15T09:01:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2MzAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NTA0OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425365049", "bodyText": "I wouldn't mind as this only changes internals.", "author": "AHeise", "createdAt": "2020-05-14T19:01:06Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields key positions from inputStream\n+\t * @param <T> input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream, topic, producerParallelism, numberOfPartitions, properties, keySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Write to and read from a kafka shuffle with the partition decided by keys.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param producerParallelism parallelism of producer\n+\t * @param numberOfPartitions number of partitions\n+\t * @param properties Kafka properties\n+\t * @param keySelector key(K) based on inputStream(T)\n+\t * @param <T> input type\n+\t * @param <K> key type\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, schema, kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Write to a kafka shuffle with the partition decided by keys.\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param kafkaProperties kafka properties\n+\t * @param fields key positions from inputStream\n+\t * @param <T> input type\n+\t */\n+\tpublic static <T> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tint... fields) {\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Write to a kafka shuffle with the partition decided by keys.\n+\t * @param inputStream input stream to the kafka\n+\t * @param topic kafka topic\n+\t * @param kafkaProperties kafka properties\n+\t * @param keySelector key(K) based on input(T)\n+\t * @param <T> input type\n+\t * @param <K> key type\n+\t */\n+\tpublic static <T, K> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\t// write data to Kafka\n+\t\tFlinkKafkaShuffleProducer<T, K> kafkaProducer = new FlinkKafkaShuffleProducer<>(\n+\t\t\ttopic,\n+\t\t\tschema,\n+\t\t\tkafkaProperties,\n+\t\t\tenv.clean(keySelector),\n+\t\t\tFlinkKafkaProducer.Semantic.EXACTLY_ONCE,\n+\t\t\tFlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE);\n+\n+\t\t// make sure the sink parallelism is set to producerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PRODUCER_PARALLELISM) != null,\n+\t\t\t\"Missing producer parallelism for Kafka Shuffle\");\n+\t\tint producerParallelism = PropertiesUtil.getInt(kafkaProperties, PRODUCER_PARALLELISM, Integer.MIN_VALUE);\n+\n+\t\taddKafkaShuffle(inputStream, kafkaProducer, producerParallelism);\n+\t}\n+\n+\t/**\n+\t * Read data from a Kafka Shuffle.\n+\t * Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism\n+\t *\n+\t * @param topic kafka topic\n+\t * @param env streaming execution environment. readKeyBy environment can be different from writeKeyBy\n+\t * @param schema the record schema to read\n+\t * @param kafkaProperties kafka properties\n+\t * @param keySelector key(K) based on schema(T)\n+\t * @param <T> schema type\n+\t * @param <K> key type\n+\t * @return keyed data stream\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> readKeyBy(\n+\t\t\tString topic,\n+\t\t\tStreamExecutionEnvironment env,\n+\t\t\tTypeInformationSerializationSchema<T> schema,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\tSourceFunction<T> kafkaConsumer = new FlinkKafkaShuffleConsumer<>(topic, schema, kafkaProperties);\n+\n+\t\t// TODO: consider situations where numberOfPartitions != consumerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PARTITION_NUMBER) != null,\n+\t\t\t\"Missing partition number for Kafka Shuffle\");\n+\t\tint numberOfPartitions = PropertiesUtil.getInt(kafkaProperties, PARTITION_NUMBER, Integer.MIN_VALUE);\n+\t\tDataStream<T> outputDataStream = env.addSource(kafkaConsumer).setParallelism(numberOfPartitions);\n+\n+\t\treturn DataStreamUtils.reinterpretAsKeyedStream(outputDataStream, keySelector);\n+\t}\n+\n+\t/**\n+\t * Add a {@link StreamKafkaShuffleSink} to {@link DataStream}.\n+\t * {@link StreamKafkaShuffleSink} is associated a {@link FlinkKafkaShuffleProducer}.\n+\t *\n+\t * @param inputStream the input data stream connected to the shuffle\n+\t * @param kafkaShuffleProducer kafka shuffle sink function that can handle both records and watermark\n+\t * @param producerParallelism the number of tasks writing to the kafka shuffle\n+\t */\n+\tprivate static <T, K> void addKafkaShuffle(\n+\t\t\tDataStream<T> inputStream, FlinkKafkaShuffleProducer<T, K> kafkaShuffleProducer, int producerParallelism) {\n+\n+\t\t// read the output type of the input Transform to coax out errors about MissingTypeInfo\n+\t\tinputStream.getTransformation().getOutputType();\n+\n+\t\tStreamKafkaShuffleSink<T> shuffleSinkOperator = new StreamKafkaShuffleSink<>(kafkaShuffleProducer);\n+\t\tSinkTransformation<T> transformation = new SinkTransformation<>(\n+\t\t\tinputStream.getTransformation(),\n+\t\t\t\"kafka_shuffle\",\n+\t\t\tshuffleSinkOperator,\n+\t\t\tinputStream.getExecutionEnvironment().getParallelism());\n+\t\tinputStream.getExecutionEnvironment().addOperator(transformation);\n+\t\ttransformation.setParallelism(producerParallelism);\n+\t}\n+\n+\t// A better place to put this function is DataStream; but put it here for now to avoid changing DataStream", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NTk4NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425365984", "bodyText": "this is already done by super. Afaik you then also don't need the change in StreamSink.", "author": "AHeise", "createdAt": "2020-05-14T19:02:45Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/StreamKafkaShuffleSink.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamSink;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+\n+/**\n+ * A customized {@link StreamOperator} for executing {@link FlinkKafkaShuffleProducer} that handle\n+ * both elements and watermarks. If the shuffle sink is determined to be useful to other sinks in the future,\n+ * we should abstract this operator to data stream api. For now, we keep the operator this way to avoid\n+ * public interface change.\n+ */\n+@Internal\n+class StreamKafkaShuffleSink<IN> extends StreamSink<IN> {\n+\n+\tpublic StreamKafkaShuffleSink(FlinkKafkaShuffleProducer flinkKafkaShuffleProducer) {\n+\t\tsuper(flinkKafkaShuffleProducer);\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU3MTc5Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425571792", "bodyText": "That's a good catch! Thanks!!", "author": "curcur", "createdAt": "2020-05-15T05:20:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NTk4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NjI1MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425366250", "bodyText": "Unrelated changes, please revert or pull into a separate hotfix if you deem it necessary (I don't).", "author": "AHeise", "createdAt": "2020-05-14T19:03:15Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSink.java", "diffHunk": "@@ -35,9 +35,9 @@\n \n \tprivate final SinkTransformation<T> transformation;\n \n-\t@SuppressWarnings(\"unchecked\")\n \tprotected DataStreamSink(DataStream<T> inputStream, StreamSink<T> operator) {\n-\t\tthis.transformation = new SinkTransformation<T>(inputStream.getTransformation(), \"Unnamed\", operator, inputStream.getExecutionEnvironment().getParallelism());", "originalCommit": "ac697d5c1c9f2f3eb562041ae47ff3f620c8ce58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU3MjQ1Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425572456", "bodyText": "I will revert it.", "author": "curcur", "createdAt": "2020-05-15T05:23:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NjI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NzA3MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425367071", "bodyText": "chop", "author": "AHeise", "createdAt": "2020-05-14T19:04:49Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElement;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElementDeserializer;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleRecord;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleWatermark;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableMap;\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Iterables;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PRODUCER_PARALLELISM;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaShuffleBase {\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(600000L);\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\n+\t@Test\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 200000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 200000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 300000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 300000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value serialization and deserialization with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeProcessingTime() throws Exception {\n+\t\ttestRecordSerDe(ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeIngestionTime() throws Exception {\n+\t\ttestRecordSerDe(IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeEventTime() throws Exception {\n+\t\ttestRecordSerDe(EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testWatermarkBroadcasting() throws Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int numElementsPerProducer = 1000;\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tMap<Integer, Collection<ConsumerRecord<byte[], byte[]>>> results = testKafkaShuffleProducer(\n+\t\t\ttopic(\"test_watermark_broadcast\", EventTime),\n+\t\t\tenv,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproducerParallelism,\n+\t\t\tnumElementsPerProducer,\n+\t\t\tEventTime);\n+\t\tTypeSerializer<Tuple3<Integer, Long, Integer>> typeSerializer = createTypeSerializer(env);\n+\t\tKafkaShuffleElementDeserializer<Tuple3<Integer, Long, Integer>> deserializer = new KafkaShuffleElementDeserializer<>();\n+\n+\t\t// Records in a single partition are kept in order\n+\t\tfor (int p = 0; p < numberOfPartitions; p++) {\n+\t\t\tCollection<ConsumerRecord<byte[], byte[]>> records = results.get(p);\n+\t\t\tMap<Integer, List<KafkaShuffleWatermark>> watermarks = new HashMap<>();\n+\n+\t\t\tfor (ConsumerRecord<byte[], byte[]> consumerRecord : records) {\n+\t\t\t\tAssert.assertNull(consumerRecord.key());\n+\t\t\t\tKafkaShuffleElement<Tuple3<Integer, Long, Integer>> element =\n+\t\t\t\t\tdeserializer.deserialize(typeSerializer, consumerRecord);\n+\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\tKafkaShuffleRecord<Tuple3<Integer, Long, Integer>> record = element.asRecord();\n+\t\t\t\t\tAssert.assertEquals(record.getValue().f1.longValue(), INIT_TIMESTAMP + record.getValue().f0);\n+\t\t\t\t\tAssert.assertEquals(record.getTimestamp().longValue(), record.getValue().f1.longValue());\n+\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\tKafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\twatermarks.computeIfAbsent(watermark.getSubtask(), k -> new ArrayList<>());\n+\t\t\t\t\twatermarks.get(watermark.getSubtask()).add(watermark);\n+\t\t\t\t} else {\n+\t\t\t\t\tfail(\"KafkaShuffleElement is either record or watermark\");\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// According to the setting how watermarks are generated in this ITTest,\n+\t\t\t// every producer task emits a watermark corresponding to each record + the end-of-event-time watermark.\n+\t\t\t// Hence each producer sub task generates `numElementsPerProducer + 1` watermarks.\n+\t\t\t// Each producer sub task broadcasts these `numElementsPerProducer + 1` watermarks to all partitions.\n+\t\t\t// Thus in total, each producer sub task emits `(numElementsPerProducer + 1) * numberOfPartitions` watermarks.\n+\t\t\t// From the consumer side, each partition receives `(numElementsPerProducer + 1) * producerParallelism` watermarks,\n+\t\t\t// with each producer sub task produces `numElementsPerProducer + 1` watermarks.\n+\t\t\t// Besides, watermarks from the same producer sub task should keep in order.\n+\t\t\tfor (List<KafkaShuffleWatermark> subTaskWatermarks : watermarks.values()) {\n+\t\t\t\tint index = 0;\n+\t\t\t\tAssert.assertEquals(numElementsPerProducer + 1, subTaskWatermarks.size());\n+\t\t\t\tfor (KafkaShuffleWatermark watermark : subTaskWatermarks) {\n+\t\t\t\t\tif (index == numElementsPerProducer) {\n+\t\t\t\t\t\t// the last element is the watermark that signifies end-of-event-time\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), Watermark.MAX_WATERMARK.getTimestamp());\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), INIT_TIMESTAMP + index++);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void testKafkaShuffle(\n+\t\t\tString prefix, int numElementsPerProducer, TimeCharacteristic timeCharacteristic) throws Exception {", "originalCommit": "7f23666b38502f8f8e93cb876d4dccdde7df97fd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2NzE1MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425367151", "bodyText": "chop", "author": "AHeise", "createdAt": "2020-05-14T19:04:58Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleITCase.java", "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElement;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleElementDeserializer;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleRecord;\n+import org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.KafkaShuffleWatermark;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableMap;\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Iterables;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PARTITION_NUMBER;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.PRODUCER_PARALLELISM;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Simple End to End Test for Kafka.\n+ */\n+public class KafkaShuffleITCase extends KafkaShuffleBase {\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(600000L);\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with the default time characteristic: ProcessingTime\n+\t */\n+\n+\t@Test\n+\tpublic void testSimpleProcessingTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 200000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleIngestionTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 200000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test no data is lost or duplicated end-2-end with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSimpleEventTime() throws Exception {\n+\t\ttestKafkaShuffle(\"test_simple\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionProcessingTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 300000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionIngestionTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 300000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t * To test data is partitioned to the right partition with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionEventTime() throws Exception {\n+\t\ttestAssignedToPartition(\"test_assigned_to_partition\", 100000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value serialization and deserialization with time characteristic: ProcessingTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeProcessingTime() throws Exception {\n+\t\ttestRecordSerDe(ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: IngestionTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeIngestionTime() throws Exception {\n+\t\ttestRecordSerDe(IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testSerDeEventTime() throws Exception {\n+\t\ttestRecordSerDe(EventTime);\n+\t}\n+\n+\t/**\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t * To test value and watermark serialization and deserialization with time characteristic: EventTime\n+\t */\n+\t@Test\n+\tpublic void testWatermarkBroadcasting() throws Exception {\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int numElementsPerProducer = 1000;\n+\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tMap<Integer, Collection<ConsumerRecord<byte[], byte[]>>> results = testKafkaShuffleProducer(\n+\t\t\ttopic(\"test_watermark_broadcast\", EventTime),\n+\t\t\tenv,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproducerParallelism,\n+\t\t\tnumElementsPerProducer,\n+\t\t\tEventTime);\n+\t\tTypeSerializer<Tuple3<Integer, Long, Integer>> typeSerializer = createTypeSerializer(env);\n+\t\tKafkaShuffleElementDeserializer<Tuple3<Integer, Long, Integer>> deserializer = new KafkaShuffleElementDeserializer<>();\n+\n+\t\t// Records in a single partition are kept in order\n+\t\tfor (int p = 0; p < numberOfPartitions; p++) {\n+\t\t\tCollection<ConsumerRecord<byte[], byte[]>> records = results.get(p);\n+\t\t\tMap<Integer, List<KafkaShuffleWatermark>> watermarks = new HashMap<>();\n+\n+\t\t\tfor (ConsumerRecord<byte[], byte[]> consumerRecord : records) {\n+\t\t\t\tAssert.assertNull(consumerRecord.key());\n+\t\t\t\tKafkaShuffleElement<Tuple3<Integer, Long, Integer>> element =\n+\t\t\t\t\tdeserializer.deserialize(typeSerializer, consumerRecord);\n+\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\tKafkaShuffleRecord<Tuple3<Integer, Long, Integer>> record = element.asRecord();\n+\t\t\t\t\tAssert.assertEquals(record.getValue().f1.longValue(), INIT_TIMESTAMP + record.getValue().f0);\n+\t\t\t\t\tAssert.assertEquals(record.getTimestamp().longValue(), record.getValue().f1.longValue());\n+\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\tKafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\twatermarks.computeIfAbsent(watermark.getSubtask(), k -> new ArrayList<>());\n+\t\t\t\t\twatermarks.get(watermark.getSubtask()).add(watermark);\n+\t\t\t\t} else {\n+\t\t\t\t\tfail(\"KafkaShuffleElement is either record or watermark\");\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// According to the setting how watermarks are generated in this ITTest,\n+\t\t\t// every producer task emits a watermark corresponding to each record + the end-of-event-time watermark.\n+\t\t\t// Hence each producer sub task generates `numElementsPerProducer + 1` watermarks.\n+\t\t\t// Each producer sub task broadcasts these `numElementsPerProducer + 1` watermarks to all partitions.\n+\t\t\t// Thus in total, each producer sub task emits `(numElementsPerProducer + 1) * numberOfPartitions` watermarks.\n+\t\t\t// From the consumer side, each partition receives `(numElementsPerProducer + 1) * producerParallelism` watermarks,\n+\t\t\t// with each producer sub task produces `numElementsPerProducer + 1` watermarks.\n+\t\t\t// Besides, watermarks from the same producer sub task should keep in order.\n+\t\t\tfor (List<KafkaShuffleWatermark> subTaskWatermarks : watermarks.values()) {\n+\t\t\t\tint index = 0;\n+\t\t\t\tAssert.assertEquals(numElementsPerProducer + 1, subTaskWatermarks.size());\n+\t\t\t\tfor (KafkaShuffleWatermark watermark : subTaskWatermarks) {\n+\t\t\t\t\tif (index == numElementsPerProducer) {\n+\t\t\t\t\t\t// the last element is the watermark that signifies end-of-event-time\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), Watermark.MAX_WATERMARK.getTimestamp());\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tAssert.assertEquals(watermark.getWatermark(), INIT_TIMESTAMP + index++);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t * To test no data is lost or duplicated end-2-end\n+\t */\n+\tprivate void testKafkaShuffle(\n+\t\t\tString prefix, int numElementsPerProducer, TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tString topic = topic(prefix, timeCharacteristic);\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = createEnvironment(producerParallelism, timeCharacteristic);\n+\t\tcreateKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerProducer, producerParallelism, timeCharacteristic, numberOfPartitions)", "originalCommit": "7f23666b38502f8f8e93cb876d4dccdde7df97fd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8c1f25f63a823dfeca4ad590321a79f63362b1ca", "url": "https://github.com/apache/flink/commit/8c1f25f63a823dfeca4ad590321a79f63362b1ca", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-15T07:18:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMzE3NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425633175", "bodyText": "Please revise first line of commit message.", "author": "AHeise", "createdAt": "2020-05-15T08:02:45Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java", "diffHunk": "@@ -264,7 +264,7 @@ public FlinkKafkaConsumerBase(\n \t * @param properties - Kafka configuration properties to be adjusted\n \t * @param offsetCommitMode offset commit mode", "originalCommit": "2367700887bb42084af193d3ac271c8f201720ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEzOTY4Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426139686", "bodyText": "Do you mean remove \"-\" ?\nI did not change this code?", "author": "curcur", "createdAt": "2020-05-16T10:09:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMzE3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NTU3Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426275573", "bodyText": "I was referring to \"Kafka Shuffle Consumer Part \" in commit message. Could by symmetric to producer part.", "author": "AHeise", "createdAt": "2020-05-17T15:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMzE3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzNzQ1Mg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425637452", "bodyText": "Would be good to provide a serialVersionUID.", "author": "AHeise", "createdAt": "2020-05-15T08:11:19Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\t/** The handler to check and generate watermarks from fetched records. **/\n+\tprivate final WatermarkHandler watermarkHandler;\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t/**\n+\t * An element in a KafkaShuffle. Can be a record or a Watermark.\n+\t */\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A watermark element in a KafkaShuffle. It includes\n+\t * - subtask index where the watermark is coming from\n+\t * - watermark timestamp\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\n+\t\tpublic int getSubtask() {\n+\t\t\treturn subtask;\n+\t\t}\n+\n+\t\tpublic long getWatermark() {\n+\t\t\treturn watermark;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * One value with Type T in a KafkaShuffle. This stores the value and an optional associated timestamp.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\n+\t\tpublic T getValue() {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\tpublic Long getTimestamp() {\n+\t\t\treturn timestamp;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Deserializer for KafkaShuffleElement.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleElementDeserializer<T> implements Serializable {", "originalCommit": "2367700887bb42084af193d3ac271c8f201720ff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2MzU3NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425663575", "bodyText": "Pass serializer as ctor parameter.", "author": "AHeise", "createdAt": "2020-05-15T08:59:29Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\t/** The handler to check and generate watermarks from fetched records. **/\n+\tprivate final WatermarkHandler watermarkHandler;\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t/**\n+\t * An element in a KafkaShuffle. Can be a record or a Watermark.\n+\t */\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A watermark element in a KafkaShuffle. It includes\n+\t * - subtask index where the watermark is coming from\n+\t * - watermark timestamp\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {\n+\t\tfinal int subtask;\n+\t\tfinal long watermark;\n+\n+\t\tKafkaShuffleWatermark(int subtask, long watermark) {\n+\t\t\tthis.subtask = subtask;\n+\t\t\tthis.watermark = watermark;\n+\t\t}\n+\n+\t\tpublic int getSubtask() {\n+\t\t\treturn subtask;\n+\t\t}\n+\n+\t\tpublic long getWatermark() {\n+\t\t\treturn watermark;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * One value with Type T in a KafkaShuffle. This stores the value and an optional associated timestamp.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleRecord<T> extends KafkaShuffleElement<T> {\n+\t\tfinal T value;\n+\t\tfinal Long timestamp;\n+\n+\t\tKafkaShuffleRecord(T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = null;\n+\t\t}\n+\n+\t\tKafkaShuffleRecord(long timestamp, T value) {\n+\t\t\tthis.value = value;\n+\t\t\tthis.timestamp = timestamp;\n+\t\t}\n+\n+\t\tpublic T getValue() {\n+\t\t\treturn value;\n+\t\t}\n+\n+\t\tpublic Long getTimestamp() {\n+\t\t\treturn timestamp;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Deserializer for KafkaShuffleElement.\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleElementDeserializer<T> implements Serializable {\n+\t\tprivate transient DataInputDeserializer dis;\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElementDeserializer() {\n+\t\t\tthis.dis = new DataInputDeserializer();\n+\t\t}\n+\n+\t\t@VisibleForTesting\n+\t\tpublic KafkaShuffleElement<T> deserialize(TypeSerializer<T> serializer, ConsumerRecord<byte[], byte[]> record)", "originalCommit": "2367700887bb42084af193d3ac271c8f201720ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNzg4NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426127884", "bodyText": "Doesn't seem resolved.", "author": "AHeise", "createdAt": "2020-05-16T07:19:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2MzU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2NzA1MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425667051", "bodyText": "What does \"Writes to a kafka shuffle\" mean? I think you need to add more explanation.\nThis function also is only meaningful with readKeyBy so add a @see.", "author": "AHeise", "createdAt": "2020-05-15T09:05:45Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param keySelector key(K) based on inputStream(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, schema, kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.", "originalCommit": "6647de6a7d44d642f2584d77d46102a01a28d265", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2NzYzNQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425667635", "bodyText": "This is a good comment. Something like that should exist also for the other public API methods.", "author": "AHeise", "createdAt": "2020-05-15T09:06:47Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param keySelector key(K) based on inputStream(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, schema, kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tint... fields) {\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param keySelector Key(K) based on input(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\t// write data to Kafka\n+\t\tFlinkKafkaShuffleProducer<T, K> kafkaProducer = new FlinkKafkaShuffleProducer<>(\n+\t\t\ttopic,\n+\t\t\tschema.getSerializer(),\n+\t\t\tkafkaProperties,\n+\t\t\tenv.clean(keySelector),\n+\t\t\tFlinkKafkaProducer.Semantic.EXACTLY_ONCE,\n+\t\t\tFlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE);\n+\n+\t\t// make sure the sink parallelism is set to producerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PRODUCER_PARALLELISM) != null,\n+\t\t\t\"Missing producer parallelism for Kafka Shuffle\");\n+\t\tint producerParallelism = PropertiesUtil.getInt(kafkaProperties, PRODUCER_PARALLELISM, Integer.MIN_VALUE);\n+\n+\t\taddKafkaShuffle(inputStream, kafkaProducer, producerParallelism);\n+\t}\n+\n+\t/**\n+\t * Reads data from a Kafka Shuffle.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is equivalent to the key group sizes. Each consumer task reads from\n+\t * one or multiple partitions. Any two consumer tasks can not read from the same partition.\n+\t * Hence, the maximum parallelism of the receiving operator is the number of partitions.\n+\t * This version only supports numberOfPartitions = consumerParallelism", "originalCommit": "6647de6a7d44d642f2584d77d46102a01a28d265", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2ODA5Mw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425668093", "bodyText": "Reads data from a Kafka Shuffle previously written by writeKeyBy.", "author": "AHeise", "createdAt": "2020-05-15T09:07:35Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param keySelector key(K) based on inputStream(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, schema, kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tint... fields) {\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param keySelector Key(K) based on input(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\t// write data to Kafka\n+\t\tFlinkKafkaShuffleProducer<T, K> kafkaProducer = new FlinkKafkaShuffleProducer<>(\n+\t\t\ttopic,\n+\t\t\tschema.getSerializer(),\n+\t\t\tkafkaProperties,\n+\t\t\tenv.clean(keySelector),\n+\t\t\tFlinkKafkaProducer.Semantic.EXACTLY_ONCE,\n+\t\t\tFlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE);\n+\n+\t\t// make sure the sink parallelism is set to producerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PRODUCER_PARALLELISM) != null,\n+\t\t\t\"Missing producer parallelism for Kafka Shuffle\");\n+\t\tint producerParallelism = PropertiesUtil.getInt(kafkaProperties, PRODUCER_PARALLELISM, Integer.MIN_VALUE);\n+\n+\t\taddKafkaShuffle(inputStream, kafkaProducer, producerParallelism);\n+\t}\n+\n+\t/**\n+\t * Reads data from a Kafka Shuffle.", "originalCommit": "6647de6a7d44d642f2584d77d46102a01a28d265", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2ODY5Nw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425668697", "bodyText": "I'd go with TypeInformation here.  TypeInformationSerializationSchema is rather technical and can be easily derived from TypeInformation.", "author": "AHeise", "createdAt": "2020-05-15T09:08:36Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Use Kafka as a persistent shuffle by wrapping a Kafka Source/Sink pair together.\n+ */\n+@Experimental\n+class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tinputStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to and reads from a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is the maximum parallelism of the receiving operator.\n+\t * This version only supports numberOfPartitions = consumerParallelism.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param producerParallelism Parallelism of producer\n+\t * @param numberOfPartitions Number of partitions\n+\t * @param properties Kafka properties\n+\t * @param keySelector key(K) based on inputStream(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, schema, kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param fields Key positions from inputStream\n+\t * @param <T> Input type\n+\t */\n+\tpublic static <T> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tint... fields) {\n+\t\twriteKeyBy(inputStream, topic, kafkaProperties, keySelector(inputStream, fields));\n+\t}\n+\n+\t/**\n+\t * Writes to a kafka shuffle with the partition decided by keys.\n+\t *\n+\t * @param inputStream Input stream to the kafka\n+\t * @param topic Kafka topic\n+\t * @param kafkaProperties Kafka properties\n+\t * @param keySelector Key(K) based on input(T)\n+\t * @param <T> Input type\n+\t * @param <K> Key type\n+\t */\n+\tpublic static <T, K> void writeKeyBy(\n+\t\t\tDataStream<T> inputStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\tStreamExecutionEnvironment env = inputStream.getExecutionEnvironment();\n+\t\tTypeInformationSerializationSchema<T> schema =\n+\t\t\tnew TypeInformationSerializationSchema<>(inputStream.getType(), env.getConfig());\n+\n+\t\t// write data to Kafka\n+\t\tFlinkKafkaShuffleProducer<T, K> kafkaProducer = new FlinkKafkaShuffleProducer<>(\n+\t\t\ttopic,\n+\t\t\tschema.getSerializer(),\n+\t\t\tkafkaProperties,\n+\t\t\tenv.clean(keySelector),\n+\t\t\tFlinkKafkaProducer.Semantic.EXACTLY_ONCE,\n+\t\t\tFlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE);\n+\n+\t\t// make sure the sink parallelism is set to producerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PRODUCER_PARALLELISM) != null,\n+\t\t\t\"Missing producer parallelism for Kafka Shuffle\");\n+\t\tint producerParallelism = PropertiesUtil.getInt(kafkaProperties, PRODUCER_PARALLELISM, Integer.MIN_VALUE);\n+\n+\t\taddKafkaShuffle(inputStream, kafkaProducer, producerParallelism);\n+\t}\n+\n+\t/**\n+\t * Reads data from a Kafka Shuffle.\n+\t *\n+\t * <p>Consumers should read partitions equal to the key group indices they are assigned.\n+\t * The number of partitions is equivalent to the key group sizes. Each consumer task reads from\n+\t * one or multiple partitions. Any two consumer tasks can not read from the same partition.\n+\t * Hence, the maximum parallelism of the receiving operator is the number of partitions.\n+\t * This version only supports numberOfPartitions = consumerParallelism\n+\t *\n+\t * @param topic Kafka topic\n+\t * @param env Streaming execution environment. readKeyBy's environment can be different from writeKeyBy's\n+\t * @param schema The record schema to read\n+\t * @param kafkaProperties Kafka properties\n+\t * @param keySelector Key(K) based on schema(T)\n+\t * @param <T> Schema type\n+\t * @param <K> Key type\n+\t * @return Keyed data stream\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> readKeyBy(\n+\t\t\tString topic,\n+\t\t\tStreamExecutionEnvironment env,\n+\t\t\tTypeInformationSerializationSchema<T> schema,", "originalCommit": "6647de6a7d44d642f2584d77d46102a01a28d265", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2OTEyMQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425669121", "bodyText": "chop", "author": "AHeise", "createdAt": "2020-05-15T09:09:24Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleExactlyOnceITCase.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper;\n+import org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Failure Recovery IT Test for KafkaShuffle.\n+ */\n+public class KafkaShuffleExactlyOnceITCase extends KafkaShuffleTestBase {\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(600000L);\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: ProcessingTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: IngestionTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: EventTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryEventTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: ProcessingTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: IngestionTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: EventTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryEventTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, EventTime);\n+\t}\n+\n+\t/**\n+\t * To test failure recovery after processing 2/3 data.\n+\t *\n+\t * <p>Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t */\n+\tprivate void testKafkaShuffleFailureRecovery(\n+\t\t\tString prefix, int numElementsPerProducer, TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tString topic = topic(prefix, timeCharacteristic);\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\t\tfinal int failAfterElements = numElementsPerProducer * numberOfPartitions * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env =\n+\t\t\tcreateEnvironment(producerParallelism, timeCharacteristic).enableCheckpointing(500);\n+\n+\t\tcreateKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerProducer, producerParallelism, timeCharacteristic, numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.map(new ToInteger(producerParallelism)).setParallelism(1)\n+\t\t\t.addSink(new ValidatingExactlyOnceSink(numElementsPerProducer * producerParallelism)).setParallelism(1);\n+\n+\t\tFailingIdentityMapper.failedBefore = false;\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * To test failure recovery with partition assignment after processing 2/3 data.\n+\t *\n+\t * <p>Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t */\n+\tprivate void testAssignedToPartitionFailureRecovery(\n+\t\t\tString prefix,\n+\t\t\tint numElementsPerProducer,\n+\t\t\tTimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tString topic = topic(prefix, timeCharacteristic);\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int failAfterElements = numElementsPerProducer * producerParallelism * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = createEnvironment(producerParallelism, timeCharacteristic);\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, Integer>, Tuple> keyedStream = createKafkaShuffle(\n+\t\t\tenv,\n+\t\t\ttopic,\n+\t\t\tnumElementsPerProducer,\n+\t\t\tproducerParallelism,\n+\t\t\ttimeCharacteristic,\n+\t\t\tnumberOfPartitions);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ToInteger(producerParallelism)).setParallelism(numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.addSink(new ValidatingExactlyOnceSink(numElementsPerProducer * producerParallelism)).setParallelism(1);\n+\n+\t\tFailingIdentityMapper.failedBefore = false;\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate StreamExecutionEnvironment createEnvironment(\n+\t\t\tint producerParallelism, TimeCharacteristic timeCharacteristic) {", "originalCommit": "8c1f25f63a823dfeca4ad590321a79f63362b1ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4ODAzNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425688036", "bodyText": "KafkaShuffleWatermark is always used as a raw type, so please get rid of the type parameter.\nI also think that KafkaShuffleElement<T> should be without type parameter and only asRecord has it.\n\tpublic <T> KafkaShuffleRecord<T> asRecord() {\n\t\t\treturn (KafkaShuffleRecord<T>) this;\n\t}", "author": "AHeise", "createdAt": "2020-05-15T09:43:24Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+import static org.apache.flink.util.Preconditions.checkState;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends AbstractFetcher<T, TopicPartition> {\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(KafkaShuffleFetcher.class);\n+\n+\t/** The handler to check and generate watermarks from fetched records. **/\n+\tprivate final WatermarkHandler watermarkHandler;\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> deserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> serializer;\n+\n+\t/** The handover of data and exceptions between the consumer thread and the task thread. */\n+\tprivate final Handover handover;\n+\n+\t/** The thread that runs the actual KafkaConsumer and hand the record batches to this fetcher. */\n+\tprivate final KafkaConsumerThread consumerThread;\n+\n+\t/** Flag to mark the main work loop as alive. */\n+\tprivate volatile boolean running = true;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tuseMetrics);\n+\t\tthis.deserializer = new KafkaShuffleElementDeserializer<>();\n+\t\tthis.serializer = serializer;\n+\t\tthis.handover = new Handover();\n+\t\tthis.consumerThread = new KafkaConsumerThread(\n+\t\t\tLOG,\n+\t\t\thandover,\n+\t\t\tkafkaProperties,\n+\t\t\tunassignedPartitionsQueue,\n+\t\t\tgetFetcherName() + \" for \" + taskNameWithSubtasks,\n+\t\t\tpollTimeout,\n+\t\t\tuseMetrics,\n+\t\t\tconsumerMetricGroup,\n+\t\t\tsubtaskMetricGroup);\n+\t\tthis.watermarkHandler = new WatermarkHandler(producerParallelism);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Fetcher work methods\n+\t// ------------------------------------------------------------------------\n+\n+\t@Override\n+\tpublic void runFetchLoop() throws Exception {\n+\t\ttry {\n+\t\t\tfinal Handover handover = this.handover;\n+\n+\t\t\t// kick off the actual Kafka consumer\n+\t\t\tconsumerThread.start();\n+\n+\t\t\twhile (running) {\n+\t\t\t\t// this blocks until we get the next records\n+\t\t\t\t// it automatically re-throws exceptions encountered in the consumer thread\n+\t\t\t\tfinal ConsumerRecords<byte[], byte[]> records = handover.pollNext();\n+\n+\t\t\t\t// get the records for each topic partition\n+\t\t\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : subscribedPartitionStates()) {\n+\t\t\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords =\n+\t\t\t\t\t\trecords.records(partition.getKafkaPartitionHandle());\n+\n+\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\t\t\t\tfinal KafkaShuffleElement<T> element = deserializer.deserialize(serializer, record);\n+\n+\t\t\t\t\t\t// TODO: do we need to check the end of stream if reaching the end watermark?\n+\n+\t\t\t\t\t\tif (element.isRecord()) {\n+\t\t\t\t\t\t\t// timestamp is inherent from upstream\n+\t\t\t\t\t\t\t// If using ProcessTime, timestamp is going to be ignored (upstream does not include timestamp as well)\n+\t\t\t\t\t\t\t// If using IngestionTime, timestamp is going to be overwritten\n+\t\t\t\t\t\t\t// If using EventTime, timestamp is going to be used\n+\t\t\t\t\t\t\tsynchronized (checkpointLock) {\n+\t\t\t\t\t\t\t\tKafkaShuffleRecord<T> elementAsRecord = element.asRecord();\n+\t\t\t\t\t\t\t\tsourceContext.collectWithTimestamp(\n+\t\t\t\t\t\t\t\t\telementAsRecord.value,\n+\t\t\t\t\t\t\t\t\telementAsRecord.timestamp == null ? record.timestamp() : elementAsRecord.timestamp);\n+\t\t\t\t\t\t\t\tpartition.setOffset(record.offset());\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else if (element.isWatermark()) {\n+\t\t\t\t\t\t\tfinal KafkaShuffleWatermark watermark = element.asWatermark();\n+\t\t\t\t\t\t\tOptional<Watermark> newWatermark = watermarkHandler.checkAndGetNewWatermark(watermark);\n+\t\t\t\t\t\t\tnewWatermark.ifPresent(sourceContext::emitWatermark);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\t// this signals the consumer thread that no more work is to be done\n+\t\t\tconsumerThread.shutdown();\n+\t\t}\n+\n+\t\t// on a clean exit, wait for the runner thread\n+\t\ttry {\n+\t\t\tconsumerThread.join();\n+\t\t}\n+\t\tcatch (InterruptedException e) {\n+\t\t\t// may be the result of a wake-up interruption after an exception.\n+\t\t\t// we ignore this here and only restore the interruption state\n+\t\t\tThread.currentThread().interrupt();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\t// flag the main thread to exit. A thread interrupt will come anyways.\n+\t\trunning = false;\n+\t\thandover.close();\n+\t\tconsumerThread.shutdown();\n+\t}\n+\n+\t@Override\n+\tprotected TopicPartition createKafkaPartitionHandle(KafkaTopicPartition partition) {\n+\t\treturn new TopicPartition(partition.getTopic(), partition.getPartition());\n+\t}\n+\n+\t@Override\n+\tprotected void doCommitInternalOffsetsToKafka(\n+\t\t\tMap<KafkaTopicPartition, Long> offsets,\n+\t\t\t@Nonnull KafkaCommitCallback commitCallback) throws Exception {\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tList<KafkaTopicPartitionState<TopicPartition>> partitions = subscribedPartitionStates();\n+\n+\t\tMap<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>(partitions.size());\n+\n+\t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n+\t\t\tLong lastProcessedOffset = offsets.get(partition.getKafkaTopicPartition());\n+\t\t\tif (lastProcessedOffset != null) {\n+\t\t\t\tcheckState(lastProcessedOffset >= 0, \"Illegal offset value to commit\");\n+\n+\t\t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n+\t\t\t\t// This does not affect Flink's checkpoints/saved state.\n+\t\t\t\tlong offsetToCommit = lastProcessedOffset + 1;\n+\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n+\t\t\t}\n+\t\t}\n+\n+\t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n+\t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n+\t}\n+\n+\tprivate String getFetcherName() {\n+\t\treturn \"Kafka Shuffle Fetcher\";\n+\t}\n+\n+\t/**\n+\t * An element in a KafkaShuffle. Can be a record or a Watermark.\n+\t */\n+\t@VisibleForTesting\n+\tpublic abstract static class KafkaShuffleElement<T> {\n+\n+\t\tpublic boolean isRecord() {\n+\t\t\treturn getClass() == KafkaShuffleRecord.class;\n+\t\t}\n+\n+\t\tpublic boolean isWatermark() {\n+\t\t\treturn getClass() == KafkaShuffleWatermark.class;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleRecord<T> asRecord() {\n+\t\t\treturn (KafkaShuffleRecord<T>) this;\n+\t\t}\n+\n+\t\tpublic KafkaShuffleWatermark asWatermark() {\n+\t\t\treturn (KafkaShuffleWatermark) this;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A watermark element in a KafkaShuffle. It includes\n+\t * - subtask index where the watermark is coming from\n+\t * - watermark timestamp\n+\t */\n+\t@VisibleForTesting\n+\tpublic static class KafkaShuffleWatermark<T> extends KafkaShuffleElement<T> {", "originalCommit": "2367700887bb42084af193d3ac271c8f201720ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MDMwNg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426140306", "bodyText": "Yes, good catch.", "author": "curcur", "createdAt": "2020-05-16T10:17:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4ODAzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4ODYwMg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425688602", "bodyText": "Remove or translate into LOG.debug", "author": "AHeise", "createdAt": "2020-05-15T09:44:30Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleExactlyOnceITCase.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper;\n+import org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.IngestionTime;\n+import static org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime;\n+import static org.apache.flink.test.util.TestUtils.tryExecute;\n+\n+/**\n+ * Failure Recovery IT Test for KafkaShuffle.\n+ */\n+public class KafkaShuffleExactlyOnceITCase extends KafkaShuffleTestBase {\n+\n+\t@Rule\n+\tpublic final Timeout timeout = Timeout.millis(600000L);\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: ProcessingTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: IngestionTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after processing 2/3 data with time characteristic: EventTime.\n+\t *\n+\t * <p>Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1.\n+\t */\n+\t@Test\n+\tpublic void testFailureRecoveryEventTime() throws Exception {\n+\t\ttestKafkaShuffleFailureRecovery(\"failure_recovery\", 1000, EventTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: ProcessingTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryProcessingTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, ProcessingTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: IngestionTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryIngestionTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, IngestionTime);\n+\t}\n+\n+\t/**\n+\t * Failure Recovery after data is repartitioned with time characteristic: EventTime.\n+\t *\n+\t * <p>Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3.\n+\t */\n+\t@Test\n+\tpublic void testAssignedToPartitionFailureRecoveryEventTime() throws Exception {\n+\t\ttestAssignedToPartitionFailureRecovery(\"partition_failure_recovery\", 500, EventTime);\n+\t}\n+\n+\t/**\n+\t * To test failure recovery after processing 2/3 data.\n+\t *\n+\t * <p>Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 1; Kafka Partition # = 1; Consumer Parallelism = 1\n+\t */\n+\tprivate void testKafkaShuffleFailureRecovery(\n+\t\t\tString prefix, int numElementsPerProducer, TimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tString topic = topic(prefix, timeCharacteristic);\n+\t\tfinal int numberOfPartitions = 1;\n+\t\tfinal int producerParallelism = 1;\n+\t\tfinal int failAfterElements = numElementsPerProducer * numberOfPartitions * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env =\n+\t\t\tcreateEnvironment(producerParallelism, timeCharacteristic).enableCheckpointing(500);\n+\n+\t\tcreateKafkaShuffle(\n+\t\t\tenv, topic, numElementsPerProducer, producerParallelism, timeCharacteristic, numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.map(new ToInteger(producerParallelism)).setParallelism(1)\n+\t\t\t.addSink(new ValidatingExactlyOnceSink(numElementsPerProducer * producerParallelism)).setParallelism(1);\n+\n+\t\tFailingIdentityMapper.failedBefore = false;\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\t/**\n+\t * To test failure recovery with partition assignment after processing 2/3 data.\n+\t *\n+\t * <p>Schema: (key, timestamp, source instance Id).\n+\t * Producer Parallelism = 2; Kafka Partition # = 3; Consumer Parallelism = 3\n+\t */\n+\tprivate void testAssignedToPartitionFailureRecovery(\n+\t\t\tString prefix,\n+\t\t\tint numElementsPerProducer,\n+\t\t\tTimeCharacteristic timeCharacteristic) throws Exception {\n+\t\tString topic = topic(prefix, timeCharacteristic);\n+\t\tfinal int numberOfPartitions = 3;\n+\t\tfinal int producerParallelism = 2;\n+\t\tfinal int failAfterElements = numElementsPerProducer * producerParallelism * 2 / 3;\n+\n+\t\tcreateTestTopic(topic, numberOfPartitions, 1);\n+\n+\t\tfinal StreamExecutionEnvironment env = createEnvironment(producerParallelism, timeCharacteristic);\n+\n+\t\tKeyedStream<Tuple3<Integer, Long, Integer>, Tuple> keyedStream = createKafkaShuffle(\n+\t\t\tenv,\n+\t\t\ttopic,\n+\t\t\tnumElementsPerProducer,\n+\t\t\tproducerParallelism,\n+\t\t\ttimeCharacteristic,\n+\t\t\tnumberOfPartitions);\n+\t\tkeyedStream\n+\t\t\t.process(new PartitionValidator(keyedStream.getKeySelector(), numberOfPartitions, topic))\n+\t\t\t.setParallelism(numberOfPartitions)\n+\t\t\t.map(new ToInteger(producerParallelism)).setParallelism(numberOfPartitions)\n+\t\t\t.map(new FailingIdentityMapper<>(failAfterElements)).setParallelism(1)\n+\t\t\t.addSink(new ValidatingExactlyOnceSink(numElementsPerProducer * producerParallelism)).setParallelism(1);\n+\n+\t\tFailingIdentityMapper.failedBefore = false;\n+\n+\t\ttryExecute(env, topic);\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\tprivate StreamExecutionEnvironment createEnvironment(\n+\t\t\tint producerParallelism, TimeCharacteristic timeCharacteristic) {\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.setParallelism(producerParallelism);\n+\t\tenv.setStreamTimeCharacteristic(timeCharacteristic);\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n+\t\tenv.setBufferTimeout(0);\n+\t\tenv.enableCheckpointing(500);\n+\n+\t\treturn env;\n+\t}\n+\n+\tprivate static class ToInteger implements MapFunction<Tuple3<Integer, Long, Integer>, Integer> {\n+\t\tprivate final int producerParallelism;\n+\n+\t\tToInteger(int producerParallelism) {\n+\t\t\tthis.producerParallelism = producerParallelism;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Integer map(Tuple3<Integer, Long, Integer> element) throws Exception {\n+\t\t\tint addedInteger = element.f0 * producerParallelism + element.f2;\n+\t\t\tSystem.out.println(\"<\" + element.f0 + \",\" + element.f2 + \"> \" + addedInteger);", "originalCommit": "8c1f25f63a823dfeca4ad590321a79f63362b1ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4OTE1MQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425689151", "bodyText": "Can't we just use a random static number here? I was thinking that tests results might be easier to compare if it's not a real time stamp.", "author": "AHeise", "createdAt": "2020-05-15T09:45:29Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/shuffle/KafkaShuffleTestBase.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase;\n+import org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner;\n+import org.apache.flink.test.util.SuccessException;\n+import org.apache.flink.util.Collector;\n+\n+import org.junit.BeforeClass;\n+\n+import static org.apache.flink.streaming.api.TimeCharacteristic.EventTime;\n+\n+/**\n+ * Base Test Class for KafkaShuffle.\n+ */\n+public class KafkaShuffleTestBase extends KafkaConsumerTestBase {\n+\tstatic final long INIT_TIMESTAMP = System.currentTimeMillis();", "originalCommit": "8c1f25f63a823dfeca4ad590321a79f63362b1ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTczMTI0Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r425731246", "bodyText": "why it is easier if not a real timestamp? I can change it back to a static number, but feeling this might be more realistic.\nThis change is originally motivated by debugging the Kafka Server. I thought some data is trimmed because of the timestamp is too old. But it is not related.", "author": "curcur", "createdAt": "2020-05-15T11:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4OTE1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NzE0Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426277146", "bodyText": "As I have said, if I want to compare two shuffle topics created by two commits to track down some bug, the changed timestamp will actually be a big PITA.\nTests don't need to be realistic in the sense of data, but should be realistic in terms of workload (or queries).", "author": "AHeise", "createdAt": "2020-05-17T16:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY4OTE1MQ=="}], "type": "inlineReview"}, {"oid": "58443956ea7010e6c29da123d606b584056b7b16", "url": "https://github.com/apache/flink/commit/58443956ea7010e6c29da123d606b584056b7b16", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-15T10:58:18Z", "type": "forcePushed"}, {"oid": "8ecf08d5549084a4cd65d22bc7c29b2431c932d2", "url": "https://github.com/apache/flink/commit/8ecf08d5549084a4cd65d22bc7c29b2431c932d2", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-15T16:46:47Z", "type": "forcePushed"}, {"oid": "6d7ff00d684de2f19cce74d766824613fb86dc86", "url": "https://github.com/apache/flink/commit/6d7ff00d684de2f19cce74d766824613fb86dc86", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-16T02:54:06Z", "type": "forcePushed"}, {"oid": "a38a1f905273c5ba12a60b5397bc8eb8cd1a35f6", "url": "https://github.com/apache/flink/commit/a38a1f905273c5ba12a60b5397bc8eb8cd1a35f6", "message": "[FLINK-15670][core] Provide a utility function to flatten a recursive {@link Properties} to a first level property HashTable\n\nIn some cases, {@code KafkaProducer#propsToMap} for example, Properties is used purely as a HashTable without considering its default properties.", "committedDate": "2020-05-16T03:12:52Z", "type": "commit"}, {"oid": "6c24e114e622b0130e921d6994aad9a48aa2318b", "url": "https://github.com/apache/flink/commit/6c24e114e622b0130e921d6994aad9a48aa2318b", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-16T03:12:52Z", "type": "forcePushed"}, {"oid": "98f223d3937fd0d2611c2878b7c405e28030eda7", "url": "https://github.com/apache/flink/commit/98f223d3937fd0d2611c2878b7c405e28030eda7", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-16T06:10:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNzY2NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426127664", "bodyText": "Much better now, but I would have just extracted this inside of the for loop, since the loop itself is also the same.", "author": "AHeise", "createdAt": "2020-05-16T07:15:42Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaFetcher.java", "diffHunk": "@@ -192,6 +176,29 @@ protected String getFetcherName() {\n \t\treturn \"Kafka Fetcher\";\n \t}\n \n+\tprotected void partitionConsumerRecordsHandler(\n+\t\t\tList<ConsumerRecord<byte[], byte[]>> partitionRecords,\n+\t\t\tKafkaTopicPartitionState<TopicPartition> partition) throws Exception {\n+\n+\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n+\t\t\tdeserializer.deserialize(record, kafkaCollector);\n+\n+\t\t\t// emit the actual records. this also updates offset state atomically and emits\n+\t\t\t// watermarks\n+\t\t\temitRecordsWithTimestamps(\n+\t\t\t\tkafkaCollector.getRecords(),\n+\t\t\t\tpartition,\n+\t\t\t\trecord.offset(),\n+\t\t\t\trecord.timestamp());\n+\n+\t\t\tif (kafkaCollector.isEndOfStreamSignalled()) {\n+\t\t\t\t// end of stream signaled\n+\t\t\t\trunning = false;\n+\t\t\t\tbreak;\n+\t\t\t}", "originalCommit": "d9aaab249630fde304ae8f361bbb45e4664e6192", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyOTY4NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426129684", "bodyText": "Much better now, but I would have just extracted this inside of the for loop, since the loop itself is also the same.\n\nIt would be a bit tricky because it has a for-loop break; I have to change the logic inside. At this time, I would prefer to make more safe changes...", "author": "curcur", "createdAt": "2020-05-16T07:48:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNzY2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjE4NQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276185", "bodyText": "I don't see the issue, automatic intellij extraction already helps.\nwhile (running) {\n...\n\t\t\t\t\tfor (ConsumerRecord<byte[], byte[]> record : partitionRecords) {\n\t\t\t\t\t\tif (handleRecord(partition, record)) {\n\t\t\t\t\t\t     running = false;\n\t\t\t\t\t\t     break;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n}\n\nand\n\tprotected boolean handleRecord(KafkaTopicPartitionState<TopicPartition> partition, ConsumerRecord<byte[], byte[]> record) throws Exception {\n\t\t\n\t\tdeserializer.deserialize(record, kafkaCollector);\n\n\t\t// emit the actual records. this also updates offset state atomically and emits\n\t\t// watermarks\n\t\temitRecordsWithTimestamps(\n\t\t\t\tkafkaCollector.getRecords(),\n\t\t\t\tpartition,\n\t\t\t\trecord.offset(),\n\t\t\t\trecord.timestamp());\n\n\t\treturn kafkaCollector.isEndOfStreamSignalled();\n\t}", "author": "AHeise", "createdAt": "2020-05-17T15:53:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNzY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNzgzMg==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426127832", "bodyText": "passing null on a parameter not annotated with Nullable is quite hacky and can fail once someone adds a non-null check (which should be there). Maybe use the same trick of anonymous function as you did in KafkaProducer? If not, really extract the AbstractKafkaFetcher.", "author": "AHeise", "createdAt": "2020-05-16T07:18:45Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaShuffleFetcher.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.internal;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n+import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;\n+import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.SerializedValue;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITHOUT_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_REC_WITH_TIMESTAMP;\n+import static org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.KafkaSerializer.TAG_WATERMARK;\n+\n+/**\n+ * Fetch data from Kafka for Kafka Shuffle.\n+ */\n+@Internal\n+public class KafkaShuffleFetcher<T> extends KafkaFetcher<T> {\n+\t/** The handler to check and generate watermarks from fetched records. **/\n+\tprivate final WatermarkHandler watermarkHandler;\n+\n+\t/** The schema to convert between Kafka's byte messages, and Flink's objects. */\n+\tprivate final KafkaShuffleElementDeserializer<T> kafkaShuffleDeserializer;\n+\n+\t/** Serializer to serialize record. */\n+\tprivate final TypeSerializer<T> typeSerializer;\n+\n+\tpublic KafkaShuffleFetcher(\n+\t\t\tSourceFunction.SourceContext<T> sourceContext,\n+\t\t\tMap<KafkaTopicPartition, Long> assignedPartitionsWithInitialOffsets,\n+\t\t\tSerializedValue<AssignerWithPeriodicWatermarks<T>> watermarksPeriodic,\n+\t\t\tSerializedValue<AssignerWithPunctuatedWatermarks<T>> watermarksPunctuated,\n+\t\t\tProcessingTimeService processingTimeProvider,\n+\t\t\tlong autoWatermarkInterval,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tString taskNameWithSubtasks,\n+\t\t\tTypeSerializer<T> serializer,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tlong pollTimeout,\n+\t\t\tMetricGroup subtaskMetricGroup,\n+\t\t\tMetricGroup consumerMetricGroup,\n+\t\t\tboolean useMetrics,\n+\t\t\tint producerParallelism) throws Exception {\n+\t\tsuper(\n+\t\t\tsourceContext,\n+\t\t\tassignedPartitionsWithInitialOffsets,\n+\t\t\twatermarksPeriodic,\n+\t\t\twatermarksPunctuated,\n+\t\t\tprocessingTimeProvider,\n+\t\t\tautoWatermarkInterval,\n+\t\t\tuserCodeClassLoader,\n+\t\t\ttaskNameWithSubtasks,\n+\t\t\tnull,", "originalCommit": "d9aaab249630fde304ae8f361bbb45e4664e6192", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b085940abfb6d01e90067f822bba56e4a8c9e5ee", "url": "https://github.com/apache/flink/commit/b085940abfb6d01e90067f822bba56e4a8c9e5ee", "message": "[FLINK-15670][connector] Adds the producer for KafkaShuffle.\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-16T13:45:21Z", "type": "commit"}, {"oid": "9af69eb96e9a0ddaff4937e9d926feff92439f32", "url": "https://github.com/apache/flink/commit/9af69eb96e9a0ddaff4937e9d926feff92439f32", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-16T13:45:21Z", "type": "forcePushed"}, {"oid": "7db5cbb3e345463413344ce787e446975a413dd4", "url": "https://github.com/apache/flink/commit/7db5cbb3e345463413344ce787e446975a413dd4", "message": "fixup! [FLINK-15670][connector] Kafka Shuffle Consumer Part", "committedDate": "2020-05-17T03:18:50Z", "type": "forcePushed"}, {"oid": "726ec81f5a6f0940f785bd66ec9130ae29fdf285", "url": "https://github.com/apache/flink/commit/726ec81f5a6f0940f785bd66ec9130ae29fdf285", "message": "[FLINK-15670][connector] Kafka Shuffle Consumer Part\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-17T03:22:57Z", "type": "commit"}, {"oid": "aa6298086f01efe5d6ddd1356d7e289804f57a9b", "url": "https://github.com/apache/flink/commit/aa6298086f01efe5d6ddd1356d7e289804f57a9b", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-17T03:22:57Z", "type": "forcePushed"}, {"oid": "0093ac997f09093bf481de24c4b2ad987a9211be", "url": "https://github.com/apache/flink/commit/0093ac997f09093bf481de24c4b2ad987a9211be", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-17T10:36:18Z", "type": "forcePushed"}, {"oid": "8d5fec6e99701ad133a84b69fbf3cc77453cf8fc", "url": "https://github.com/apache/flink/commit/8d5fec6e99701ad133a84b69fbf3cc77453cf8fc", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-17T10:42:43Z", "type": "forcePushed"}, {"oid": "4536aff7587de64758b09c03e9fa295926d8d9cb", "url": "https://github.com/apache/flink/commit/4536aff7587de64758b09c03e9fa295926d8d9cb", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-17T10:44:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NTQwNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426275407", "bodyText": "I don't think adding javadoc style formatting to commit message makes much sense. Plerase remove. Sorry for seeing it so late.", "author": "AHeise", "createdAt": "2020-05-17T15:45:11Z", "path": "flink-core/src/main/java/org/apache/flink/util/PropertiesUtil.java", "diffHunk": "@@ -19,6 +19,7 @@\n \n import org.slf4j.Logger;", "originalCommit": "a38a1f905273c5ba12a60b5397bc8eb8cd1a35f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjYwOA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276608", "bodyText": "Really good to have an example, but i'd replace the type tokens by some real types.", "author": "AHeise", "createdAt": "2020-05-17T15:58:02Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjY1MA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276650", "bodyText": "Really good.", "author": "AHeise", "createdAt": "2020-05-17T15:58:27Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.\n+ *\n+ * <p>3). Job execution is decoupled by the persistent Kafka message bus. In the example, the job execution graph is\n+ * \t\t\tdecoupled to three regions: `KafkaShuffleProducer', `KafkaShuffleConsumer' and `KafkaShuffleConsumerReuse'\n+ * \t\t\tthrough `PERSISTENT DATA` as shown below. If any region fails the execution, the other two keep progressing.\n+ *\n+ * <p><pre>\n+ *     source -> ... KafkaShuffleProducer -> PERSISTENT DATA -> KafkaShuffleConsumer -> ...\n+ *                                                |\n+ *                                                | ----------> KafkaShuffleConsumerReuse -> ...", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjcwNw==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276707", "bodyText": "can be reused in an independent job\n(Flink does support streaming the datastream into multiple downstream operators already)", "author": "AHeise", "createdAt": "2020-05-17T15:59:13Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0MzI4OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426343289", "bodyText": "I mean shuffle can be reused/re-read. I guess streaming data is not reserved?", "author": "curcur", "createdAt": "2020-05-18T02:38:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjcwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0MzM2NA==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426343364", "bodyText": "I can change reuse -> read again?", "author": "curcur", "createdAt": "2020-05-18T02:39:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjgzNQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276835", "bodyText": "copy long explanation from above.", "author": "AHeise", "createdAt": "2020-05-17T16:00:08Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.\n+ *\n+ * <p>3). Job execution is decoupled by the persistent Kafka message bus. In the example, the job execution graph is\n+ * \t\t\tdecoupled to three regions: `KafkaShuffleProducer', `KafkaShuffleConsumer' and `KafkaShuffleConsumerReuse'\n+ * \t\t\tthrough `PERSISTENT DATA` as shown below. If any region fails the execution, the other two keep progressing.\n+ *\n+ * <p><pre>\n+ *     source -> ... KafkaShuffleProducer -> PERSISTENT DATA -> KafkaShuffleConsumer -> ...\n+ *                                                |\n+ *                                                | ----------> KafkaShuffleConsumerReuse -> ...\n+ * </pre>\n+ */\n+@Experimental\n+public class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * <p>Persisting keyBy shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+\t * {@link FlinkKafkaShuffleConsumer} together.\n+\t *\n+\t * <p>On the producer side, {@link FlinkKafkaShuffleProducer}\n+\t * is similar to {@link DataStream#keyBy(KeySelector)}. They use the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on where the key goes.\n+\t * Here, `numberOfPartitions` equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts its watermark\n+\t * to ALL of the Kafka partitions to make sure watermark information is propagated correctly.\n+\t *\n+\t * <p>On the consumer side, each consumer task should read partitions equal to the key group indices\n+\t * it is assigned. `numberOfPartitions` is the maximum parallelism of the consumer. This version only\n+\t * supports numberOfPartitions = consumerParallelism.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, a consumer task is responsible to emit\n+\t * watermarks. Watermarks are read from the corresponding Kafka partitions. Notice that a consumer task only starts\n+\t * to emit a watermark after reading at least one watermark from each producer task to make sure watermarks\n+\t * are monotonically increasing. Hence a consumer task needs to know `producerParallelism` as well.\n+\t *\n+\t * @see FlinkKafkaShuffle#writeKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param keySelector \t\t\tKey selector to retrieve key from `dataStream'\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t * @param <K> \t\t\t\t\tType of key\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = dataStream.getExecutionEnvironment();\n+\n+\t\twriteKeyBy(dataStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, dataStream.getType(), kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3Njg1OQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276859", "bodyText": "very good", "author": "AHeise", "createdAt": "2020-05-17T16:00:26Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.\n+ *\n+ * <p>3). Job execution is decoupled by the persistent Kafka message bus. In the example, the job execution graph is\n+ * \t\t\tdecoupled to three regions: `KafkaShuffleProducer', `KafkaShuffleConsumer' and `KafkaShuffleConsumerReuse'\n+ * \t\t\tthrough `PERSISTENT DATA` as shown below. If any region fails the execution, the other two keep progressing.\n+ *\n+ * <p><pre>\n+ *     source -> ... KafkaShuffleProducer -> PERSISTENT DATA -> KafkaShuffleConsumer -> ...\n+ *                                                |\n+ *                                                | ----------> KafkaShuffleConsumerReuse -> ...\n+ * </pre>\n+ */\n+@Experimental\n+public class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * <p>Persisting keyBy shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+\t * {@link FlinkKafkaShuffleConsumer} together.\n+\t *\n+\t * <p>On the producer side, {@link FlinkKafkaShuffleProducer}\n+\t * is similar to {@link DataStream#keyBy(KeySelector)}. They use the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on where the key goes.\n+\t * Here, `numberOfPartitions` equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts its watermark\n+\t * to ALL of the Kafka partitions to make sure watermark information is propagated correctly.\n+\t *\n+\t * <p>On the consumer side, each consumer task should read partitions equal to the key group indices\n+\t * it is assigned. `numberOfPartitions` is the maximum parallelism of the consumer. This version only\n+\t * supports numberOfPartitions = consumerParallelism.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, a consumer task is responsible to emit\n+\t * watermarks. Watermarks are read from the corresponding Kafka partitions. Notice that a consumer task only starts\n+\t * to emit a watermark after reading at least one watermark from each producer task to make sure watermarks\n+\t * are monotonically increasing. Hence a consumer task needs to know `producerParallelism` as well.\n+\t *\n+\t * @see FlinkKafkaShuffle#writeKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param keySelector \t\t\tKey selector to retrieve key from `dataStream'\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t * @param <K> \t\t\t\t\tType of key\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = dataStream.getExecutionEnvironment();\n+\n+\t\twriteKeyBy(dataStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, dataStream.getType(), kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param fields \t\t\t\tKey positions from the input data stream\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tdataStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(dataStream, fields));\n+\t}\n+\n+\t/**\n+\t * The write side of {@link FlinkKafkaShuffle#persistentKeyBy}.\n+\t *\n+\t * <p>This function contains a {@link FlinkKafkaShuffleProducer} to shuffle and persist data in Kafka.\n+\t * {@link FlinkKafkaShuffleProducer} uses the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on the key.\n+\t * Here, the number of partitions equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts each watermark\n+\t * to all of the Kafka partitions to make sure watermark information is propagated properly.\n+\t *\n+\t * <p>Attention: make sure kafkaProperties include\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} and {@link FlinkKafkaShuffle#PARTITION_NUMBER} explicitly.\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} is the parallelism of the producer.\n+\t * {@link FlinkKafkaShuffle#PARTITION_NUMBER} is the number of partitions.\n+\t * They are not necessarily the same and allowed to be set independently.", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3Njg5Ng==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276896", "bodyText": "usually added at the very top of a javadoc", "author": "AHeise", "createdAt": "2020-05-17T16:00:43Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.\n+ *\n+ * <p>3). Job execution is decoupled by the persistent Kafka message bus. In the example, the job execution graph is\n+ * \t\t\tdecoupled to three regions: `KafkaShuffleProducer', `KafkaShuffleConsumer' and `KafkaShuffleConsumerReuse'\n+ * \t\t\tthrough `PERSISTENT DATA` as shown below. If any region fails the execution, the other two keep progressing.\n+ *\n+ * <p><pre>\n+ *     source -> ... KafkaShuffleProducer -> PERSISTENT DATA -> KafkaShuffleConsumer -> ...\n+ *                                                |\n+ *                                                | ----------> KafkaShuffleConsumerReuse -> ...\n+ * </pre>\n+ */\n+@Experimental\n+public class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * <p>Persisting keyBy shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+\t * {@link FlinkKafkaShuffleConsumer} together.\n+\t *\n+\t * <p>On the producer side, {@link FlinkKafkaShuffleProducer}\n+\t * is similar to {@link DataStream#keyBy(KeySelector)}. They use the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on where the key goes.\n+\t * Here, `numberOfPartitions` equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts its watermark\n+\t * to ALL of the Kafka partitions to make sure watermark information is propagated correctly.\n+\t *\n+\t * <p>On the consumer side, each consumer task should read partitions equal to the key group indices\n+\t * it is assigned. `numberOfPartitions` is the maximum parallelism of the consumer. This version only\n+\t * supports numberOfPartitions = consumerParallelism.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, a consumer task is responsible to emit\n+\t * watermarks. Watermarks are read from the corresponding Kafka partitions. Notice that a consumer task only starts\n+\t * to emit a watermark after reading at least one watermark from each producer task to make sure watermarks\n+\t * are monotonically increasing. Hence a consumer task needs to know `producerParallelism` as well.\n+\t *\n+\t * @see FlinkKafkaShuffle#writeKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param keySelector \t\t\tKey selector to retrieve key from `dataStream'\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t * @param <K> \t\t\t\t\tType of key\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = dataStream.getExecutionEnvironment();\n+\n+\t\twriteKeyBy(dataStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, dataStream.getType(), kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param fields \t\t\t\tKey positions from the input data stream\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tdataStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(dataStream, fields));\n+\t}\n+\n+\t/**\n+\t * The write side of {@link FlinkKafkaShuffle#persistentKeyBy}.\n+\t *\n+\t * <p>This function contains a {@link FlinkKafkaShuffleProducer} to shuffle and persist data in Kafka.\n+\t * {@link FlinkKafkaShuffleProducer} uses the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on the key.\n+\t * Here, the number of partitions equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts each watermark\n+\t * to all of the Kafka partitions to make sure watermark information is propagated properly.\n+\t *\n+\t * <p>Attention: make sure kafkaProperties include\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} and {@link FlinkKafkaShuffle#PARTITION_NUMBER} explicitly.\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} is the parallelism of the producer.\n+\t * {@link FlinkKafkaShuffle#PARTITION_NUMBER} is the number of partitions.\n+\t * They are not necessarily the same and allowed to be set independently.\n+\t *\n+\t * @see FlinkKafkaShuffle#persistentKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI3NjkxOQ==", "url": "https://github.com/apache/flink/pull/11725#discussion_r426276919", "bodyText": "copy javadoc", "author": "AHeise", "createdAt": "2020-05-17T16:01:01Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/shuffle/FlinkKafkaShuffle.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.connectors.kafka.shuffle;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.operators.Keys;\n+import org.apache.flink.api.common.serialization.TypeInformationSerializationSchema;\n+import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.tuple.Tuple;\n+import org.apache.flink.runtime.state.KeyGroupRangeAssignment;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.streaming.api.datastream.KeyedStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\n+import org.apache.flink.streaming.util.keys.KeySelectorUtil;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.PropertiesUtil;\n+\n+import java.util.Properties;\n+\n+/**\n+ * {@link FlinkKafkaShuffle} uses Kafka as a message bus to shuffle and persist data at the same time.\n+ *\n+ * <p>Persisting shuffle data is useful when\n+ *     - you would like to reuse the shuffle data and/or,\n+ *     - you would like to avoid a full restart of a pipeline during failure recovery\n+ *\n+ * <p>Persisting shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+ * a {@link FlinkKafkaShuffleConsumer} together into a {@link FlinkKafkaShuffle}.\n+ * Here is an example how to use a {@link FlinkKafkaShuffle}.\n+ *\n+ * <p><pre>{@code\n+ *\tStreamExecutionEnvironment env = ... \t\t\t\t\t// create execution environment\n+ * \tDataStream<X> source = env.addSource(...)\t\t\t\t// add data stream source\n+ * \tDataStream<Y> dataStream = ...\t\t\t\t\t\t\t// some transformation(s) based on source\n+ *\n+ *\tKeyedStream<Y, KEY> keyedStream = FlinkKafkaShuffle\n+ *\t\t.persistentKeyBy(\t\t\t\t\t\t\t\t\t// keyBy shuffle through kafka\n+ * \t\t\tdataStream,\t\t\t\t\t\t\t\t\t\t// data stream to be shuffled\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// Kafka topic written to\n+ * \t\t\tproducerParallelism,\t\t\t\t\t\t\t// the number of tasks of a Kafka Producer\n+ * \t\t\tnumberOfPartitions,\t\t\t\t\t\t\t\t// the number of partitions of the Kafka topic written to\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Producer and Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key from `dataStream'\n+ *\n+ *\tkeyedStream.transform...\t\t\t\t\t\t\t\t// some other transformation(s)\n+ *\n+ * \tKeyedStream<Y, KEY> keyedStreamReuse = FlinkKafkaShuffle\n+ * \t\t.readKeyBy(\t\t\t\t\t\t\t\t\t\t\t// Read the Kafka shuffle data again for other usages\n+ * \t\t\ttopic,\t\t\t\t\t\t\t\t\t\t\t// the topic of Kafka where data is persisted\n+ * \t\t\tenv,\t\t\t\t\t\t\t\t\t\t\t// execution environment, and it can be a new environment\n+ * \t\t\ttypeInformation<Y>,\t\t\t\t\t\t\t\t// type information of the data persisted in Kafka\n+ * \t\t\tkafkaProperties,\t\t\t\t\t\t\t\t// kafka properties for Kafka Consumer\n+ * \t\t\tkeySelector<Y, KEY>);\t\t\t\t\t\t\t// key selector to retrieve key\n+ *\n+ * \tkeyedStreamReuse.transform...\t\t\t\t\t\t\t// some other transformation(s)\n+ * }</pre>\n+ *\n+ * <p>Usage of {@link FlinkKafkaShuffle#persistentKeyBy} is similar to {@link DataStream#keyBy(KeySelector)}.\n+ * The differences are:\n+ *\n+ * <p>1). Partitioning is done through {@link FlinkKafkaShuffleProducer}. {@link FlinkKafkaShuffleProducer} decides\n+ * \t\t\twhich partition a key goes when writing to Kafka\n+ *\n+ * <p>2). Shuffle data can be reused through {@link FlinkKafkaShuffle#readKeyBy}, as shown in the example above.\n+ *\n+ * <p>3). Job execution is decoupled by the persistent Kafka message bus. In the example, the job execution graph is\n+ * \t\t\tdecoupled to three regions: `KafkaShuffleProducer', `KafkaShuffleConsumer' and `KafkaShuffleConsumerReuse'\n+ * \t\t\tthrough `PERSISTENT DATA` as shown below. If any region fails the execution, the other two keep progressing.\n+ *\n+ * <p><pre>\n+ *     source -> ... KafkaShuffleProducer -> PERSISTENT DATA -> KafkaShuffleConsumer -> ...\n+ *                                                |\n+ *                                                | ----------> KafkaShuffleConsumerReuse -> ...\n+ * </pre>\n+ */\n+@Experimental\n+public class FlinkKafkaShuffle {\n+\tstatic final String PRODUCER_PARALLELISM = \"producer parallelism\";\n+\tstatic final String PARTITION_NUMBER = \"partition number\";\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * <p>Persisting keyBy shuffle is achieved by wrapping a {@link FlinkKafkaShuffleProducer} and\n+\t * {@link FlinkKafkaShuffleConsumer} together.\n+\t *\n+\t * <p>On the producer side, {@link FlinkKafkaShuffleProducer}\n+\t * is similar to {@link DataStream#keyBy(KeySelector)}. They use the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on where the key goes.\n+\t * Here, `numberOfPartitions` equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts its watermark\n+\t * to ALL of the Kafka partitions to make sure watermark information is propagated correctly.\n+\t *\n+\t * <p>On the consumer side, each consumer task should read partitions equal to the key group indices\n+\t * it is assigned. `numberOfPartitions` is the maximum parallelism of the consumer. This version only\n+\t * supports numberOfPartitions = consumerParallelism.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, a consumer task is responsible to emit\n+\t * watermarks. Watermarks are read from the corresponding Kafka partitions. Notice that a consumer task only starts\n+\t * to emit a watermark after reading at least one watermark from each producer task to make sure watermarks\n+\t * are monotonically increasing. Hence a consumer task needs to know `producerParallelism` as well.\n+\t *\n+\t * @see FlinkKafkaShuffle#writeKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param keySelector \t\t\tKey selector to retrieve key from `dataStream'\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t * @param <K> \t\t\t\t\tType of key\n+\t */\n+\tpublic static <T, K> KeyedStream<T, K> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\t\t// KafkaProducer#propsToMap uses Properties purely as a HashMap without considering the default properties\n+\t\t// So we have to flatten the default property to first level elements.\n+\t\tProperties kafkaProperties = PropertiesUtil.flatten(properties);\n+\t\tkafkaProperties.setProperty(PRODUCER_PARALLELISM, String.valueOf(producerParallelism));\n+\t\tkafkaProperties.setProperty(PARTITION_NUMBER, String.valueOf(numberOfPartitions));\n+\n+\t\tStreamExecutionEnvironment env = dataStream.getExecutionEnvironment();\n+\n+\t\twriteKeyBy(dataStream, topic, kafkaProperties, keySelector);\n+\t\treturn readKeyBy(topic, env, dataStream.getType(), kafkaProperties, keySelector);\n+\t}\n+\n+\t/**\n+\t * Uses Kafka as a message bus to persist keyBy shuffle.\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param producerParallelism \tParallelism of producer\n+\t * @param numberOfPartitions \tNumber of partitions\n+\t * @param properties \t\t\tKafka properties\n+\t * @param fields \t\t\t\tKey positions from the input data stream\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t */\n+\tpublic static <T> KeyedStream<T, Tuple> persistentKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tint producerParallelism,\n+\t\t\tint numberOfPartitions,\n+\t\t\tProperties properties,\n+\t\t\tint... fields) {\n+\t\treturn persistentKeyBy(\n+\t\t\tdataStream,\n+\t\t\ttopic,\n+\t\t\tproducerParallelism,\n+\t\t\tnumberOfPartitions,\n+\t\t\tproperties,\n+\t\t\tkeySelector(dataStream, fields));\n+\t}\n+\n+\t/**\n+\t * The write side of {@link FlinkKafkaShuffle#persistentKeyBy}.\n+\t *\n+\t * <p>This function contains a {@link FlinkKafkaShuffleProducer} to shuffle and persist data in Kafka.\n+\t * {@link FlinkKafkaShuffleProducer} uses the same key group assignment function\n+\t * {@link KeyGroupRangeAssignment#assignKeyToParallelOperator} to decide which partition a key goes.\n+\t * Hence, each producer task can potentially write to each Kafka partition based on the key.\n+\t * Here, the number of partitions equals to the key group size.\n+\t * In the case of using {@link TimeCharacteristic#EventTime}, each producer task broadcasts each watermark\n+\t * to all of the Kafka partitions to make sure watermark information is propagated properly.\n+\t *\n+\t * <p>Attention: make sure kafkaProperties include\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} and {@link FlinkKafkaShuffle#PARTITION_NUMBER} explicitly.\n+\t * {@link FlinkKafkaShuffle#PRODUCER_PARALLELISM} is the parallelism of the producer.\n+\t * {@link FlinkKafkaShuffle#PARTITION_NUMBER} is the number of partitions.\n+\t * They are not necessarily the same and allowed to be set independently.\n+\t *\n+\t * @see FlinkKafkaShuffle#persistentKeyBy\n+\t * @see FlinkKafkaShuffle#readKeyBy\n+\t *\n+\t * @param dataStream \t\t\tData stream to be shuffled\n+\t * @param topic \t\t\t\tKafka topic written to\n+\t * @param kafkaProperties \t\tKafka properties for Kafka Producer\n+\t * @param keySelector \t\t\tKey selector to retrieve key from `dataStream'\n+\t * @param <T> \t\t\t\t\tType of the input data stream\n+\t * @param <K> \t\t\t\t\tType of key\n+\t */\n+\tpublic static <T, K> void writeKeyBy(\n+\t\t\tDataStream<T> dataStream,\n+\t\t\tString topic,\n+\t\t\tProperties kafkaProperties,\n+\t\t\tKeySelector<T, K> keySelector) {\n+\n+\t\tStreamExecutionEnvironment env = dataStream.getExecutionEnvironment();\n+\t\tTypeSerializer<T> typeSerializer = dataStream.getType().createSerializer(env.getConfig());\n+\n+\t\t// write data to Kafka\n+\t\tFlinkKafkaShuffleProducer<T, K> kafkaProducer = new FlinkKafkaShuffleProducer<>(\n+\t\t\ttopic,\n+\t\t\ttypeSerializer,\n+\t\t\tkafkaProperties,\n+\t\t\tenv.clean(keySelector),\n+\t\t\tFlinkKafkaProducer.Semantic.EXACTLY_ONCE,\n+\t\t\tFlinkKafkaProducer.DEFAULT_KAFKA_PRODUCERS_POOL_SIZE);\n+\n+\t\t// make sure the sink parallelism is set to producerParallelism\n+\t\tPreconditions.checkArgument(\n+\t\t\tkafkaProperties.getProperty(PRODUCER_PARALLELISM) != null,\n+\t\t\t\"Missing producer parallelism for Kafka Shuffle\");\n+\t\tint producerParallelism = PropertiesUtil.getInt(kafkaProperties, PRODUCER_PARALLELISM, Integer.MIN_VALUE);\n+\n+\t\taddKafkaShuffle(dataStream, kafkaProducer, producerParallelism);\n+\t}\n+\n+\t/**\n+\t * The write side of {@link FlinkKafkaShuffle#persistentKeyBy}.", "originalCommit": "9dc55dc7f69126bec44af5a0ac3920e0dafea1f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2e25c9ccd9c3ce6726d40f3166fe94e087176752", "url": "https://github.com/apache/flink/commit/2e25c9ccd9c3ce6726d40f3166fe94e087176752", "message": "[FLINK-15670][connector] Kafka Shuffle API Part\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-18T02:47:23Z", "type": "commit"}, {"oid": "413cfbda4f0320915bdfeb958b480b565c920a36", "url": "https://github.com/apache/flink/commit/413cfbda4f0320915bdfeb958b480b565c920a36", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-18T02:47:23Z", "type": "commit"}, {"oid": "413cfbda4f0320915bdfeb958b480b565c920a36", "url": "https://github.com/apache/flink/commit/413cfbda4f0320915bdfeb958b480b565c920a36", "message": "[FLINK-15670] Kafka Shuffle Test Case + add log4j2 file\n\nKafkaShuffle provides a transparent Kafka source and sink pair, through which the network traffic of a shuffle step is persisted and redirected.", "committedDate": "2020-05-18T02:47:23Z", "type": "forcePushed"}]}