{"pr_number": 10922, "pr_title": "[FLINK-11899][parquet] Introduce parquet ColumnarRow split reader", "pr_createdAt": "2020-01-22T07:08:45Z", "pr_url": "https://github.com/apache/flink/pull/10922", "timeline": [{"oid": "ce1b4b24d97cc6963785f1a359a67729f9c091e0", "url": "https://github.com/apache/flink/commit/ce1b4b24d97cc6963785f1a359a67729f9c091e0", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader", "committedDate": "2020-01-24T10:41:23Z", "type": "forcePushed"}, {"oid": "f975ebd36b4b7358b265af749a71ec606413a80d", "url": "https://github.com/apache/flink/commit/f975ebd36b4b7358b265af749a71ec606413a80d", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader", "committedDate": "2020-01-24T12:04:50Z", "type": "forcePushed"}, {"oid": "6d44e21dd1e60f5093263408a9030231eab26fa5", "url": "https://github.com/apache/flink/commit/6d44e21dd1e60f5093263408a9030231eab26fa5", "message": "Integrate with hive", "committedDate": "2020-02-14T09:45:36Z", "type": "forcePushed"}, {"oid": "b302c52c6aa9f9408c3083466f2d62a6a6f33dd6", "url": "https://github.com/apache/flink/commit/b302c52c6aa9f9408c3083466f2d62a6a6f33dd6", "message": "Update", "committedDate": "2020-02-20T08:39:04Z", "type": "forcePushed"}, {"oid": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "url": "https://github.com/apache/flink/commit/6123376d9cdab1092c992dd297ee0972cde7fbf4", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader", "committedDate": "2020-02-23T07:44:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0MjQwNw==", "url": "https://github.com/apache/flink/pull/10922#discussion_r385542407", "bodyText": "Why are these considered complex types?", "author": "lirui-apache", "createdAt": "2020-02-28T07:24:31Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -113,16 +114,74 @@ public void configure(org.apache.flink.configuration.Configuration parameters) {\n \n \t@Override\n \tpublic void open(HiveTableInputSplit split) throws IOException {\n-\t\tif (!useMapRedReader && useOrcVectorizedRead(split.getHiveTablePartition())) {\n+\t\tHiveTablePartition partition = split.getHiveTablePartition();\n+\t\tif (!useMapRedReader && useOrcVectorizedRead(partition)) {\n \t\t\tthis.reader = new HiveVectorizedOrcSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n+\t\t} else if (!useMapRedReader && useParquetVectorizedRead(partition)) {\n+\t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n+\t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n \t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\tprivate boolean isComplexType(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:", "originalCommit": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njk3ODgxMQ==", "url": "https://github.com/apache/flink/pull/10922#discussion_r386978811", "bodyText": "I'll rename to isVectorizationSupport", "author": "JingsongLi", "createdAt": "2020-03-03T12:11:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0MjQwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzU4MQ==", "url": "https://github.com/apache/flink/pull/10922#discussion_r385563581", "bodyText": "Just use Unsafe::ARRAY_BYTE_BASE_OFFSET?", "author": "lirui-apache", "createdAt": "2020-02-28T08:28:08Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/vector/heap/AbstractHeapVector.java", "diffHunk": "@@ -18,14 +18,25 @@\n \n package org.apache.flink.table.dataformat.vector.heap;\n \n-import org.apache.flink.table.dataformat.vector.AbstractColumnVector;\n+import org.apache.flink.core.memory.MemoryUtils;\n+import org.apache.flink.table.dataformat.vector.writable.AbstractWritableVector;\n \n+import java.nio.ByteOrder;\n import java.util.Arrays;\n \n /**\n  * Heap vector that nullable shared structure.\n  */\n-public abstract class AbstractHeapVector extends AbstractColumnVector {\n+public abstract class AbstractHeapVector extends AbstractWritableVector {\n+\n+\tpublic static final boolean LITTLE_ENDIAN = ByteOrder.nativeOrder() == ByteOrder.LITTLE_ENDIAN;\n+\n+\tpublic static final sun.misc.Unsafe UNSAFE = MemoryUtils.UNSAFE;\n+\tpublic static final int BYTE_ARRAY_OFFSET = UNSAFE.arrayBaseOffset(byte[].class);", "originalCommit": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjIwNDMyMw==", "url": "https://github.com/apache/flink/pull/10922#discussion_r386204323", "bodyText": "Use arrayBaseOffset is more safe for cross platform.", "author": "JingsongLi", "createdAt": "2020-03-02T05:35:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzY5Mg==", "url": "https://github.com/apache/flink/pull/10922#discussion_r385563692", "bodyText": "call setNullAt ?", "author": "lirui-apache", "createdAt": "2020-02-28T08:28:24Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/vector/heap/AbstractHeapVector.java", "diffHunk": "@@ -56,11 +67,31 @@ public void reset() {\n \t\tnoNulls = true;\n \t}\n \n+\t@Override\n \tpublic void setNullAt(int i) {\n+\t\tif (i >= isNull.length) {\n+\t\t\tthrow new RuntimeException();\n+\t\t}\n \t\tisNull[i] = true;\n \t\tnoNulls = false;\n \t}\n \n+\t@Override\n+\tpublic void setNulls(int i, int count) {\n+\t\tfor (int j = 0; j < count; j++) {\n+\t\t\tisNull[i + j] = true;", "originalCommit": "6123376d9cdab1092c992dd297ee0972cde7fbf4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjIwMzk2Nw==", "url": "https://github.com/apache/flink/pull/10922#discussion_r386203967", "bodyText": "No need to update noNulls.", "author": "JingsongLi", "createdAt": "2020-03-02T05:33:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU2MzY5Mg=="}], "type": "inlineReview"}, {"oid": "1177cdde9727901d1be7938a1c2d609c02181b3b", "url": "https://github.com/apache/flink/commit/1177cdde9727901d1be7938a1c2d609c02181b3b", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader", "committedDate": "2020-03-03T12:10:09Z", "type": "commit"}, {"oid": "1177cdde9727901d1be7938a1c2d609c02181b3b", "url": "https://github.com/apache/flink/commit/1177cdde9727901d1be7938a1c2d609c02181b3b", "message": "[FLINK-11899][parquet][table-runtime] Introduce ParquetColumnarRowSplitReader", "committedDate": "2020-03-03T12:10:09Z", "type": "forcePushed"}, {"oid": "37cb28b6b3238516df476318cb28d30114689442", "url": "https://github.com/apache/flink/commit/37cb28b6b3238516df476318cb28d30114689442", "message": "Simplify ParquetColumnarRowSplitReader", "committedDate": "2020-03-04T05:34:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI0NjEyNg==", "url": "https://github.com/apache/flink/pull/10922#discussion_r388246126", "bodyText": "maybe rename to readNextRowGroup?", "author": "lirui-apache", "createdAt": "2020-03-05T11:52:23Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetColumnarRowSplitReader.java", "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.vector;\n+\n+import org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.table.dataformat.ColumnarRow;\n+import org.apache.flink.table.dataformat.vector.ColumnVector;\n+import org.apache.flink.table.dataformat.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.dataformat.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader;\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector;\n+import static org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups;\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;\n+import static org.apache.parquet.hadoop.ParquetFileReader.readFooter;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n+\n+/**\n+ * This reader is used to read a {@link VectorizedColumnBatch} from input split.\n+ */\n+public class ParquetColumnarRowSplitReader implements Closeable {\n+\n+\tprivate final boolean utcTimestamp;\n+\n+\tprivate final MessageType fileSchema;\n+\n+\tprivate final MessageType requestedSchema;\n+\n+\t/**\n+\t * The total number of rows this RecordReader will eventually read. The sum of the rows of all\n+\t * the row groups.\n+\t */\n+\tprivate final long totalRowCount;\n+\n+\tprivate final WritableColumnVector[] writableVectors;\n+\n+\tprivate final VectorizedColumnBatch columnarBatch;\n+\n+\tprivate final ColumnarRow row;\n+\n+\tprivate final LogicalType[] selectedTypes;\n+\n+\tprivate final int batchSize;\n+\n+\tprivate ParquetFileReader reader;\n+\n+\t/**\n+\t * For each request column, the reader to read this column. This is NULL if this column is\n+\t * missing from the file, in which case we populate the attribute with NULL.\n+\t */\n+\tprivate ColumnReader[] columnReaders;\n+\n+\t/**\n+\t * The number of rows that have been returned.\n+\t */\n+\tprivate long rowsReturned;\n+\n+\t/**\n+\t * The number of rows that have been reading, including the current in flight row group.\n+\t */\n+\tprivate long totalCountLoadedSoFar;\n+\n+\t// the index of the next row to return\n+\tprivate int nextRow;\n+\n+\t// the number of rows in the current batch\n+\tprivate int rowsInBatch;\n+\n+\tpublic ParquetColumnarRowSplitReader(\n+\t\t\tboolean utcTimestamp,\n+\t\t\tConfiguration conf,\n+\t\t\tLogicalType[] selectedTypes,\n+\t\t\tString[] selectedFieldNames,\n+\t\t\tColumnBatchGenerator generator,\n+\t\t\tint batchSize,\n+\t\t\tPath path,\n+\t\t\tlong splitStart,\n+\t\t\tlong splitLength) throws IOException {\n+\t\tthis.utcTimestamp = utcTimestamp;\n+\t\tthis.selectedTypes = selectedTypes;\n+\t\tthis.batchSize = batchSize;\n+\t\t// then we need to apply the predicate push down filter\n+\t\tParquetMetadata footer = readFooter(conf, path, range(splitStart, splitLength));\n+\t\tMessageType fileSchema = footer.getFileMetaData().getSchema();\n+\t\tFilterCompat.Filter filter = getFilter(conf);\n+\t\tList<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);\n+\n+\t\tthis.fileSchema = footer.getFileMetaData().getSchema();\n+\t\tthis.requestedSchema = clipParquetSchema(fileSchema, selectedFieldNames);\n+\t\tthis.reader = new ParquetFileReader(\n+\t\t\t\tconf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());\n+\n+\t\tlong totalRowCount = 0;\n+\t\tfor (BlockMetaData block : blocks) {\n+\t\t\ttotalRowCount += block.getRowCount();\n+\t\t}\n+\t\tthis.totalRowCount = totalRowCount;\n+\t\tthis.nextRow = 0;\n+\t\tthis.rowsInBatch = 0;\n+\t\tthis.rowsReturned = 0;\n+\n+\t\tcheckSchema();\n+\n+\t\tthis.writableVectors = createWritableVectors();\n+\t\tthis.columnarBatch = generator.generate(createReadableVectors());\n+\t\tthis.row = new ColumnarRow(columnarBatch);\n+\t}\n+\n+\t/**\n+\t * Clips `parquetSchema` according to `fieldNames`.\n+\t */\n+\tprivate static MessageType clipParquetSchema(GroupType parquetSchema, String[] fieldNames) {\n+\t\tType[] types = new Type[fieldNames.length];\n+\t\tfor (int i = 0; i < fieldNames.length; ++i) {\n+\t\t\tString fieldName = fieldNames[i];\n+\t\t\tif (parquetSchema.getFieldIndex(fieldName) < 0) {\n+\t\t\t\tthrow new IllegalArgumentException(fieldName + \" does not exist\");\n+\t\t\t}\n+\t\t\ttypes[i] = parquetSchema.getType(fieldName);\n+\t\t}\n+\t\treturn Types.buildMessage().addFields(types).named(\"flink-parquet\");\n+\t}\n+\n+\tprivate WritableColumnVector[] createWritableVectors() {\n+\t\tWritableColumnVector[] columns = new WritableColumnVector[selectedTypes.length];\n+\t\tfor (int i = 0; i < selectedTypes.length; i++) {\n+\t\t\tcolumns[i] = createWritableColumnVector(\n+\t\t\t\t\tbatchSize,\n+\t\t\t\t\tselectedTypes[i],\n+\t\t\t\t\trequestedSchema.getColumns().get(i).getPrimitiveType());\n+\t\t}\n+\t\treturn columns;\n+\t}\n+\n+\t/**\n+\t * Create readable vectors from writable vectors.\n+\t * Especially for decimal, see {@link ParquetDecimalVector}.\n+\t */\n+\tprivate ColumnVector[] createReadableVectors() {\n+\t\tColumnVector[] vectors = new ColumnVector[writableVectors.length];\n+\t\tfor (int i = 0; i < writableVectors.length; i++) {\n+\t\t\tvectors[i] = selectedTypes[i].getTypeRoot() == LogicalTypeRoot.DECIMAL ?\n+\t\t\t\t\tnew ParquetDecimalVector(writableVectors[i]) :\n+\t\t\t\t\twritableVectors[i];\n+\t\t}\n+\t\treturn vectors;\n+\t}\n+\n+\tprivate void checkSchema() throws IOException, UnsupportedOperationException {\n+\t\tif (selectedTypes.length != requestedSchema.getFieldCount()) {\n+\t\t\tthrow new RuntimeException(\"The quality of field type is incompatible with the request schema!\");\n+\t\t}\n+\n+\t\t/*\n+\t\t * Check that the requested schema is supported.\n+\t\t */\n+\t\tfor (int i = 0; i < requestedSchema.getFieldCount(); ++i) {\n+\t\t\tType t = requestedSchema.getFields().get(i);\n+\t\t\tif (!t.isPrimitive() || t.isRepetition(Type.Repetition.REPEATED)) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Complex types not supported.\");\n+\t\t\t}\n+\n+\t\t\tString[] colPath = requestedSchema.getPaths().get(i);\n+\t\t\tif (fileSchema.containsPath(colPath)) {\n+\t\t\t\tColumnDescriptor fd = fileSchema.getColumnDescription(colPath);\n+\t\t\t\tif (!fd.equals(requestedSchema.getColumns().get(i))) {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"Schema evolution not supported.\");\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif (requestedSchema.getColumns().get(i).getMaxDefinitionLevel() == 0) {\n+\t\t\t\t\t// Column is missing in data but the required data is non-nullable. This file is invalid.\n+\t\t\t\t\tthrow new IOException(\"Required column is missing in data file. Col: \" + Arrays.toString(colPath));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Method used to check if the end of the input is reached.\n+\t *\n+\t * @return True if the end is reached, otherwise false.\n+\t * @throws IOException Thrown, if an I/O error occurred.\n+\t */\n+\tpublic boolean reachedEnd() throws IOException {\n+\t\treturn !ensureBatch();\n+\t}\n+\n+\tpublic ColumnarRow nextRecord() {\n+\t\t// return the next row\n+\t\trow.setRowId(this.nextRow++);\n+\t\treturn row;\n+\t}\n+\n+\t/**\n+\t * Checks if there is at least one row left in the batch to return. If no more row are\n+\t * available, it reads another batch of rows.\n+\t *\n+\t * @return Returns true if there is one more row to return, false otherwise.\n+\t * @throws IOException throw if an exception happens while reading a batch.\n+\t */\n+\tprivate boolean ensureBatch() throws IOException {\n+\t\tif (nextRow >= rowsInBatch) {\n+\t\t\t// No more rows available in the Rows array.\n+\t\t\tnextRow = 0;\n+\t\t\t// Try to read the next batch if rows from the file.\n+\t\t\treturn nextBatch();\n+\t\t}\n+\t\t// there is at least one Row left in the Rows array.\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * Advances to the next batch of rows. Returns false if there are no more.\n+\t */\n+\tprivate boolean nextBatch() throws IOException {\n+\t\tfor (WritableColumnVector v : writableVectors) {\n+\t\t\tv.reset();\n+\t\t}\n+\t\tcolumnarBatch.setNumRows(0);\n+\t\tif (rowsReturned >= totalRowCount) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tcheckEndOfRowGroup();\n+\n+\t\tint num = (int) Math.min(batchSize, totalCountLoadedSoFar - rowsReturned);\n+\t\tfor (int i = 0; i < columnReaders.length; ++i) {\n+\t\t\t//noinspection unchecked\n+\t\t\tcolumnReaders[i].readToVector(num, writableVectors[i]);\n+\t\t}\n+\t\trowsReturned += num;\n+\t\tcolumnarBatch.setNumRows(num);\n+\t\trowsInBatch = num;\n+\t\treturn true;\n+\t}\n+\n+\tprivate void checkEndOfRowGroup() throws IOException {", "originalCommit": "37cb28b6b3238516df476318cb28d30114689442", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI0OTI1NQ==", "url": "https://github.com/apache/flink/pull/10922#discussion_r388249255", "bodyText": "selParquetFields?", "author": "lirui-apache", "createdAt": "2020-03-05T11:59:20Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/ParquetSplitReaderUtil.java", "diffHunk": "@@ -0,0 +1,384 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.vector;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.vector.reader.BooleanColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ByteColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.BytesColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.DoubleColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.FixedLenBytesColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.FloatColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.IntColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.LongColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ShortColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader;\n+import org.apache.flink.table.dataformat.Decimal;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.vector.ColumnVector;\n+import org.apache.flink.table.dataformat.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.dataformat.vector.heap.HeapBooleanVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapByteVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapBytesVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapDoubleVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapFloatVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapIntVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapLongVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapShortVector;\n+import org.apache.flink.table.dataformat.vector.heap.HeapTimestampVector;\n+import org.apache.flink.table.dataformat.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.VarBinaryType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.functions.SqlDateTimeUtils.dateToInternal;\n+import static org.apache.parquet.Preconditions.checkArgument;\n+\n+/**\n+ * Util for generating {@link ParquetColumnarRowSplitReader}.\n+ */\n+public class ParquetSplitReaderUtil {\n+\n+\t/**\n+\t * Util for generating partitioned {@link ParquetColumnarRowSplitReader}.\n+\t */\n+\tpublic static ParquetColumnarRowSplitReader genPartColumnarRowReader(\n+\t\t\tboolean utcTimestamp,\n+\t\t\tConfiguration conf,\n+\t\t\tString[] fullFieldNames,\n+\t\t\tDataType[] fullFieldTypes,\n+\t\t\tMap<String, Object> partitionSpec,\n+\t\t\tint[] selectedFields,\n+\t\t\tint batchSize,\n+\t\t\tPath path,\n+\t\t\tlong splitStart,\n+\t\t\tlong splitLength) throws IOException {\n+\t\tList<String> nonPartNames = Arrays.stream(fullFieldNames)\n+\t\t\t\t.filter(n -> !partitionSpec.containsKey(n))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> selNonPartNames = Arrays.stream(selectedFields)\n+\t\t\t\t.mapToObj(i -> fullFieldNames[i])\n+\t\t\t\t.filter(nonPartNames::contains).collect(Collectors.toList());\n+\n+\t\tint[] selOrcFields = selNonPartNames.stream()", "originalCommit": "37cb28b6b3238516df476318cb28d30114689442", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI3NzgyMw==", "url": "https://github.com/apache/flink/pull/10922#discussion_r388277823", "bodyText": "(\u2044 \u2044\u2022\u2044\u03c9\u2044\u2022\u2044 \u2044) Good catch, I found ParquetTableSource also has orc... I will modify all.", "author": "JingsongLi", "createdAt": "2020-03-05T13:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODI0OTI1NQ=="}], "type": "inlineReview"}, {"oid": "908bfd539ad84be428f8dd3c908680d87dfb4ec8", "url": "https://github.com/apache/flink/commit/908bfd539ad84be428f8dd3c908680d87dfb4ec8", "message": "Fix comments", "committedDate": "2020-03-05T13:03:07Z", "type": "commit"}]}