{"pr_number": 13852, "pr_title": "[FLINK-19875][table] Integrate file compaction to filesystem connector", "pr_createdAt": "2020-10-30T06:04:18Z", "pr_url": "https://github.com/apache/flink/pull/13852", "timeline": [{"oid": "64b74e0fea0674cad4b91a1fa1302a5eafa30578", "url": "https://github.com/apache/flink/commit/64b74e0fea0674cad4b91a1fa1302a5eafa30578", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector", "committedDate": "2020-11-02T02:31:22Z", "type": "forcePushed"}, {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68", "url": "https://github.com/apache/flink/commit/949d1689d348b587d0ecb7637f519af311ad1d68", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector", "committedDate": "2020-11-02T06:30:40Z", "type": "commit"}, {"oid": "949d1689d348b587d0ecb7637f519af311ad1d68", "url": "https://github.com/apache/flink/commit/949d1689d348b587d0ecb7637f519af311ad1d68", "message": "[FLINK-19875][table] Integrate file compaction to filesystem connector", "committedDate": "2020-11-02T06:30:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2MzYyMg==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515763622", "bodyText": "Why do we need to call clear here?", "author": "lirui-apache", "createdAt": "2020-11-02T06:34:35Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5OTMwMg==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515799302", "bodyText": "we don't clear here.", "author": "JingsongLi", "createdAt": "2020-11-02T08:13:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2MzYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2ODgzNw==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515768837", "bodyText": "Can we reuse the COMPACTED_PREFIX defined in CompactOperator?", "author": "lirui-apache", "createdAt": "2020-11-02T06:53:02Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTIyOQ==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769229", "bodyText": "Seems this is redundant because we list files with a filter that only returns files with this prefix", "author": "lirui-apache", "createdAt": "2020-11-02T06:54:28Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);\n+\n+\t\tString fileName = files[0].getName();\n+\t\tassertTrue(fileName, fileName.startsWith(\"compacted-part-\"));", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515769604", "bodyText": "Can we also verify there's no un-compacted files left?", "author": "lirui-apache", "createdAt": "2020-11-02T06:55:44Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FileCompactionITCaseBase.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.runtime.stream.sql;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.scala.DataStream;\n+import org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource;\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Streaming sink File Compaction ITCase base, test checkpoint.\n+ */\n+public abstract class FileCompactionITCaseBase extends StreamingTestBase {\n+\n+\t@Rule\n+\tpublic Timeout timeoutPerTest = Timeout.seconds(60);\n+\n+\tprivate String resultPath;\n+\n+\tprivate List<Row> rows;\n+\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\tresultPath = tempFolder().newFolder().toURI().toString();\n+\t\tclear();\n+\n+\t\tenv().setParallelism(3);\n+\t\tenv().enableCheckpointing(100);\n+\n+\t\trows = new ArrayList<>();\n+\t\tfor (int i = 0; i < 100; i++) {\n+\t\t\trows.add(Row.of(i, String.valueOf(i % 10), String.valueOf(i)));\n+\t\t}\n+\n+\t\tDataStream<Row> stream = new DataStream<>(env().getJavaEnv().addSource(\n+\t\t\t\tnew ParallelFiniteTestSource<>(rows),\n+\t\t\t\tnew RowTypeInfo(\n+\t\t\t\t\t\tnew TypeInformation[] {Types.INT, Types.STRING, Types.STRING},\n+\t\t\t\t\t\tnew String[] {\"a\", \"b\", \"c\"})));\n+\n+\t\ttEnv().createTemporaryView(\"my_table\",  stream);\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws IOException {\n+\t\tFileUtils.deleteDirectory(new File(URI.create(resultPath)));\n+\t}\n+\n+\tprotected abstract String format();\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\ttEnv().executeSql(\"CREATE TABLE sink_table (a int, b string, c string) with (\" + options() + \")\");\n+\t\ttEnv().executeSql(\"insert into sink_table select * from my_table\").await();\n+\n+\t\tList<Row> results = toListAndClose(tEnv().executeSql(\"select * from sink_table\").collect());\n+\t\tresults.sort(Comparator.comparingInt(o -> (Integer) o.getField(0)));\n+\t\tassertEquals(rows, results);\n+\n+\t\tFile[] files = new File(URI.create(resultPath)).listFiles(\n+\t\t\t\t(dir, name) -> name.startsWith(\"compacted-part-\"));\n+\t\tassertEquals(Arrays.toString(files), 1, files.length);", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg0ODU2Ng==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515848566", "bodyText": "I  think we can", "author": "JingsongLi", "createdAt": "2020-11-02T09:43:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg0OTA4OA==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515849088", "bodyText": "We can not assert just one file, This is because ParallelFiniteTestSource may spread data across multiple checkpoints.", "author": "JingsongLi", "createdAt": "2020-11-02T09:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg1MzAwNg==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515853006", "bodyText": "Find a bug in endInput", "author": "JingsongLi", "createdAt": "2020-11-02T09:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc2OTYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3MjY3OA==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515772678", "bodyText": "Let's have some more high-level explanations here, e.g. what will be compacted, when the compaction happens, whether files are usable before compaction, etc.", "author": "lirui-apache", "createdAt": "2020-11-02T07:06:10Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -185,4 +185,17 @@\n \t\t\t\t\t.defaultValue(\"_SUCCESS\")\n \t\t\t\t\t.withDescription(\"The file name for success-file partition commit policy,\" +\n \t\t\t\t\t\t\t\" default is '_SUCCESS'.\");\n+\n+\tpublic static final ConfigOption<Boolean> AUTO_COMPACTION =\n+\t\t\tkey(\"auto-compaction\")\n+\t\t\t\t\t.booleanType()\n+\t\t\t\t\t.defaultValue(false)\n+\t\t\t\t\t.withDescription(\"Whether to enable automatic compaction in streaming sink or not.\" +", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3NDY3OQ==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515774679", "bodyText": "FileInputFormatCompactReader?", "author": "lirui-apache", "createdAt": "2020-11-02T07:12:44Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/FileInputFormatReader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.core.fs.FileInputSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * The {@link CompactReader} to delegate {@link FileInputFormat}.\n+ */\n+public class FileInputFormatReader<T> implements CompactReader<T> {", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3ODc4MQ==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515778781", "bodyText": "Can we have a method to create builder from an OutputFileConfig instance? To make sure we won't lose anything here.", "author": "lirui-apache", "createdAt": "2020-11-02T07:24:27Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -138,15 +154,28 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n \t\t\t\t\t.setParallelism(dataStream.getParallelism());\n \t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n \t\t\tObject writer = createWriter(sinkContext);\n+\t\t\tboolean isEncoder = writer instanceof Encoder;\n \t\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n \t\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n-\t\t\t\t\t!(writer instanceof Encoder),\n+\t\t\t\t\t!isEncoder || autoCompaction,\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n \t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n \n+\t\t\tif (autoCompaction) {\n+\t\t\t\toutputFileConfig = OutputFileConfig.builder()", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwNDQ1MA==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515804450", "bodyText": "I think I can keep builder as a local field", "author": "JingsongLi", "createdAt": "2020-11-02T08:24:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc3ODc4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5MTcwMw==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515791703", "bodyText": "createCompactReaderFactory?", "author": "lirui-apache", "createdAt": "2020-11-02T07:56:26Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -161,20 +190,120 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t\t\t\t\t\t.withOutputFileConfig(outputFileConfig)\n \t\t\t\t\t\t.withRollingPolicy(rollingPolicy);\n \t\t\t}\n-\t\t\treturn createStreamingSink(\n-\t\t\t\t\ttableOptions,\n+\n+\t\t\tlong bucketCheckInterval = tableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis();\n+\n+\t\t\tDataStream<PartitionCommitInfo> writerStream;\n+\t\t\tif (autoCompaction) {\n+\t\t\t\tlong compactionSize = tableOptions\n+\t\t\t\t\t\t.getOptional(FileSystemOptions.COMPACTION_FILE_SIZE)\n+\t\t\t\t\t\t.orElse(tableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE))\n+\t\t\t\t\t\t.getBytes();\n+\n+\t\t\t\tCompactReader.Factory<RowData> reader = createCompactReader(sinkContext).orElseThrow(\n+\t\t\t\t\t\t() -> new TableException(\"Please implement available reader for compaction:\" +\n+\t\t\t\t\t\t\t\t\" BulkFormat, FileInputFormat.\"));\n+\n+\t\t\t\twriterStream = StreamingSink.compactionWriter(\n+\t\t\t\t\t\tdataStream,\n+\t\t\t\t\t\tbucketCheckInterval,\n+\t\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\t\tfsFactory,\n+\t\t\t\t\t\tpath,\n+\t\t\t\t\t\treader,\n+\t\t\t\t\t\tcompactionSize);\n+\t\t\t} else {\n+\t\t\t\twriterStream = StreamingSink.writer(\n+\t\t\t\t\t\tdataStream, bucketCheckInterval, bucketsBuilder);\n+\t\t\t}\n+\n+\t\t\treturn StreamingSink.sink(\n+\t\t\t\t\twriterStream,\n \t\t\t\t\tpath,\n-\t\t\t\t\tpartitionKeys,\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\toverwrite,\n-\t\t\t\t\tdataStream,\n-\t\t\t\t\tbucketsBuilder,\n+\t\t\t\t\tpartitionKeys,\n \t\t\t\t\tmetaStoreFactory,\n \t\t\t\t\tfsFactory,\n-\t\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_CHECK_INTERVAL).toMillis());\n+\t\t\t\t\ttableOptions);\n+\t\t}\n+\t}\n+\n+\tprivate Optional<CompactReader.Factory<RowData>> createCompactReader(Context context) {", "originalCommit": "949d1689d348b587d0ecb7637f519af311ad1d68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13", "url": "https://github.com/apache/flink/commit/b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13", "message": "Address comments and fix bug", "committedDate": "2020-11-02T09:44:35Z", "type": "commit"}, {"oid": "3fc1fd9bea063dca0c8f7c0ae5e3ff9aaef4fe17", "url": "https://github.com/apache/flink/commit/3fc1fd9bea063dca0c8f7c0ae5e3ff9aaef4fe17", "message": "Add test to end input", "committedDate": "2020-11-02T09:50:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjQyOA==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515916428", "bodyText": "Why do we call withPartPrefix twice?", "author": "lirui-apache", "createdAt": "2020-11-02T11:43:08Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -127,108 +127,118 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context sinkContext) {\n \t}\n \n \tprivate DataStreamSink<?> consume(DataStream<RowData> dataStream, Context sinkContext) {\n-\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\tif (sinkContext.isBounded()) {\n+\t\t\treturn createBatchSink(dataStream, sinkContext);\n+\t\t} else {\n+\t\t\tif (overwrite) {\n+\t\t\t\tthrow new IllegalStateException(\"Streaming mode not support overwrite.\");\n+\t\t\t}\n+\n+\t\t\treturn createStreamingSink(dataStream, sinkContext);\n+\t\t}\n+\t}\n+\n+\tprivate RowDataPartitionComputer partitionComputer() {\n+\t\treturn new RowDataPartitionComputer(\n \t\t\t\tdefaultPartName,\n \t\t\t\tschema.getFieldNames(),\n \t\t\t\tschema.getFieldDataTypes(),\n \t\t\t\tpartitionKeys.toArray(new String[0]));\n+\t}\n \n-\t\tEmptyMetaStoreFactory metaStoreFactory = new EmptyMetaStoreFactory(path);\n-\t\tOutputFileConfig outputFileConfig = OutputFileConfig.builder()\n+\tprivate DataStreamSink<RowData> createBatchSink(\n+\t\t\tDataStream<RowData> inputStream, Context sinkContext) {\n+\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(partitionComputer());\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n+\t\tbuilder.setMetaStoreFactory(new EmptyMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\tbuilder.setOutputFileConfig(OutputFileConfig.builder()\n \t\t\t\t.withPartPrefix(\"part-\" + UUID.randomUUID().toString())\n-\t\t\t\t.build();\n+\t\t\t\t.build());\n+\t\treturn inputStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(inputStream.getParallelism());\n+\t}\n+\n+\tprivate DataStreamSink<?> createStreamingSink(\n+\t\t\tDataStream<RowData> dataStream, Context sinkContext) {\n \t\tFileSystemFactory fsFactory = FileSystem::get;\n+\t\tRowDataPartitionComputer computer = partitionComputer();\n \n-\t\tif (sinkContext.isBounded()) {\n-\t\t\tFileSystemOutputFormat.Builder<RowData> builder = new FileSystemOutputFormat.Builder<>();\n-\t\t\tbuilder.setPartitionComputer(computer);\n-\t\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n-\t\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n-\t\t\tbuilder.setFormatFactory(createOutputFormatFactory(sinkContext));\n-\t\t\tbuilder.setMetaStoreFactory(metaStoreFactory);\n-\t\t\tbuilder.setFileSystemFactory(fsFactory);\n-\t\t\tbuilder.setOverwrite(overwrite);\n-\t\t\tbuilder.setStaticPartitions(staticPartitions);\n-\t\t\tbuilder.setTempPath(toStagingPath());\n-\t\t\tbuilder.setOutputFileConfig(outputFileConfig);\n-\t\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n-\t\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t\tboolean autoCompaction = tableOptions.getBoolean(FileSystemOptions.AUTO_COMPACTION);\n+\t\tObject writer = createWriter(sinkContext);\n+\t\tboolean isEncoder = writer instanceof Encoder;\n+\t\tTableBucketAssigner assigner = new TableBucketAssigner(computer);\n+\t\tTableRollingPolicy rollingPolicy = new TableRollingPolicy(\n+\t\t\t\t!isEncoder || autoCompaction,\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_FILE_SIZE).getBytes(),\n+\t\t\t\ttableOptions.get(SINK_ROLLING_POLICY_ROLLOVER_INTERVAL).toMillis());\n+\n+\t\tString randomPrefix = \"part-\" + UUID.randomUUID().toString();\n+\t\tOutputFileConfig.OutputFileConfigBuilder fileNamingBuilder = OutputFileConfig.builder();\n+\t\tfileNamingBuilder = autoCompaction ?\n+\t\t\t\tfileNamingBuilder.withPartPrefix(convertToUncompacted(randomPrefix)) :\n+\t\t\t\tfileNamingBuilder.withPartPrefix(randomPrefix);", "originalCommit": "b1bffe4fa9f3adbe03d77dd7bbf8502d39619e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMjQwNw==", "url": "https://github.com/apache/flink/pull/13852#discussion_r515922407", "bodyText": "Never mind... Just noted it's for different cases", "author": "lirui-apache", "createdAt": "2020-11-02T11:54:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjQyOA=="}], "type": "inlineReview"}]}