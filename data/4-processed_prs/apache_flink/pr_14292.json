{"pr_number": 14292, "pr_title": "[FLINK-20456][docs] Make streaming SQL concepts more approachable", "pr_createdAt": "2020-12-03T03:26:31Z", "pr_url": "https://github.com/apache/flink/pull/14292", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk4NzM1OQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535987359", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            *Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n          \n          \n            \n            - *Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:13:53Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk5MDkwOQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535990909", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n          \n          \n            \n            As long as a time attribute is not modified, and is simply forwarded from one part of a query to another, it remains a valid time attribute.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:19:30Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk5MTYwMA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535991600", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Time attributes behave like regular timestamps and accessible for calculations.\n          \n          \n            \n            Time attributes behave like regular timestamps, and are accessible for calculations.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:20:38Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n+Time attributes behave like regular timestamps and accessible for calculations.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk5MjQ2OA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535992468", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When used in computation, time attributes are materialized and act as standard timestamps. \n          \n          \n            \n            When used in calculations, time attributes are materialized and act as standard timestamps.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:21:54Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n+Time attributes behave like regular timestamps and accessible for calculations.\n+When used in computation, time attributes are materialized and act as standard timestamps. ", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk5NTkxMg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535995912", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            However, the opposite is not true; ordinary timestamp columns cannot be arbitrarily converted to time attributes in the middle of a query.\n          \n          \n            \n            However, ordinary timestamps cannot be used in place of, or be converted to, time attributes.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:27:25Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n+Time attributes behave like regular timestamps and accessible for calculations.\n+When used in computation, time attributes are materialized and act as standard timestamps. \n+However, the opposite is not true; ordinary timestamp columns cannot be arbitrarily converted to time attributes in the middle of a query.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk5NzQ4NA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r535997484", "bodyText": "Can we really argue that results are consistent with late events?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Event time allows a table program to produce results based on timestamps in every record, allowing for consistent results even in case of out-of-order events or late events. It also ensures the replayability of the results of the table program when reading records from persistent storage.\n          \n          \n            \n            Event time allows a table program to produce results based on timestamps in every record, allowing for consistent results despite out-of-order or late events. It also ensures the replayability of the results of the table program when reading records from persistent storage.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:30:03Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n+Time attributes behave like regular timestamps and accessible for calculations.\n+When used in computation, time attributes are materialized and act as standard timestamps. \n+However, the opposite is not true; ordinary timestamp columns cannot be arbitrarily converted to time attributes in the middle of a query.\n \n-Time attributes can be part of every table schema. They are defined when creating a table from a CREATE TABLE DDL or a `DataStream` or are pre-defined when using a `TableSource`. Once a time attribute has been defined at the beginning, it can be referenced as a field and can be used in time-based operations.\n-\n-As long as a time attribute is not modified and is simply forwarded from one part of the query to another, it remains a valid time attribute. Time attributes behave like regular timestamps and can be accessed for calculations. If a time attribute is used in a calculation, it will be materialized and becomes a regular timestamp. Regular timestamps do not cooperate with Flink's time and watermarking system and thus can not be used for time-based operations anymore.\n-\n-Table programs require that the corresponding time characteristic has been specified for the streaming environment:\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\n-env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // default\n-\n-// alternatively:\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-val env = StreamExecutionEnvironment.getExecutionEnvironment\n-\n-env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) // default\n-\n-// alternatively:\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"python\" markdown=\"1\">\n-{% highlight python %}\n-env = StreamExecutionEnvironment.get_execution_environment()\n-\n-env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)  # default\n-\n-# alternatively:\n-# env.set_stream_time_characteristic(TimeCharacteristic.IngestionTime)\n-# env.set_stream_time_characteristic(TimeCharacteristic.EventTime)\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-Processing time\n----------------\n-\n-Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time but does not provide determinism. It neither requires timestamp extraction nor watermark generation.\n-\n-There are three ways to define a processing time attribute.\n-\n-### Defining in create table DDL\n-\n-The processing time attribute is defined as a computed column in create table DDL using the system `PROCTIME()` function. Please see [CREATE TABLE DDL]({% link dev/table/sql/create.md %}#create-table) for more information about computed column.\n-\n-{% highlight sql %}\n-\n-CREATE TABLE user_actions (\n-  user_name STRING,\n-  data STRING,\n-  user_action_time AS PROCTIME() -- declare an additional field as a processing time attribute\n-) WITH (\n-  ...\n-);\n-\n-SELECT TUMBLE_START(user_action_time, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)\n-FROM user_actions\n-GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);\n-\n-{% endhighlight %}\n-\n-\n-### During DataStream-to-Table Conversion\n-\n-The processing time attribute is defined with the `.proctime` property during schema definition. The time attribute must only extend the physical schema by an additional logical field. Thus, it can only be defined at the end of the schema definition.\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-DataStream<Tuple2<String, String>> stream = ...;\n-\n-// declare an additional logical field as a processing time attribute\n-Table table = tEnv.fromDataStream(stream, $(\"user_name\"), $(\"data\"), $(\"user_action_time\").proctime());\n-\n-WindowedTable windowedTable = table.window(\n-        Tumble.over(lit(10).minutes())\n-            .on($(\"user_action_time\"))\n-            .as(\"userActionWindow\"));\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-val stream: DataStream[(String, String)] = ...\n-\n-// declare an additional logical field as a processing time attribute\n-val table = tEnv.fromDataStream(stream, $\"UserActionTimestamp\", $\"user_name\", $\"data\", $\"user_action_time\".proctime)\n-\n-val windowedTable = table.window(Tumble over 10.minutes on $\"user_action_time\" as \"userActionWindow\")\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-### Using a TableSource\n-\n-The processing time attribute is defined by a `TableSource` that implements the `DefinedProctimeAttribute` interface. The logical time attribute is appended to the physical schema defined by the return type of the `TableSource`.\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-// define a table source with a processing attribute\n-public class UserActionSource implements StreamTableSource<Row>, DefinedProctimeAttribute {\n-\n-\t@Override\n-\tpublic TypeInformation<Row> getReturnType() {\n-\t\tString[] names = new String[] {\"user_name\" , \"data\"};\n-\t\tTypeInformation[] types = new TypeInformation[] {Types.STRING(), Types.STRING()};\n-\t\treturn Types.ROW(names, types);\n-\t}\n-\n-\t@Override\n-\tpublic DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {\n-\t\t// create stream\n-\t\tDataStream<Row> stream = ...;\n-\t\treturn stream;\n-\t}\n-\n-\t@Override\n-\tpublic String getProctimeAttribute() {\n-\t\t// field with this name will be appended as a third field\n-\t\treturn \"user_action_time\";\n-\t}\n-}\n-\n-// register table source\n-tEnv.registerTableSource(\"user_actions\", new UserActionSource());\n-\n-WindowedTable windowedTable = tEnv\n-\t.from(\"user_actions\")\n-\t.window(Tumble\n-\t    .over(lit(10).minutes())\n-\t    .on($(\"user_action_time\"))\n-\t    .as(\"userActionWindow\"));\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-// define a table source with a processing attribute\n-class UserActionSource extends StreamTableSource[Row] with DefinedProctimeAttribute {\n-\n-\toverride def getReturnType = {\n-\t\tval names = Array[String](\"user_name\" , \"data\")\n-\t\tval types = Array[TypeInformation[_]](Types.STRING, Types.STRING)\n-\t\tTypes.ROW(names, types)\n-\t}\n-\n-\toverride def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n-\t\t// create stream\n-\t\tval stream = ...\n-\t\tstream\n-\t}\n-\n-\toverride def getProctimeAttribute = {\n-\t\t// field with this name will be appended as a third field\n-\t\t\"user_action_time\"\n-\t}\n-}\n-\n-// register table source\n-tEnv.registerTableSource(\"user_actions\", new UserActionSource)\n-\n-val windowedTable = tEnv\n-\t.from(\"user_actions\")\n-\t.window(Tumble over 10.minutes on $\"user_action_time\" as \"userActionWindow\")\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-Event time\n+Event Time\n ----------\n \n-Event time allows a table program to produce results based on the time that is contained in every record. This allows for consistent results even in case of out-of-order events or late events. It also ensures replayable results of the table program when reading records from persistent storage.\n+Event time allows a table program to produce results based on timestamps in every record, allowing for consistent results even in case of out-of-order events or late events. It also ensures the replayability of the results of the table program when reading records from persistent storage.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjAwODc1Ng==", "url": "https://github.com/apache/flink/pull/14292#discussion_r536008756", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To handle out-of-order events and distinguish between on-time and late events in streaming, Flink needs to extract timestamps from events and make some progress in time (so-called [watermarks]({% link dev/event_time.md %})).\n          \n          \n            \n            To handle out-of-order events and to distinguish between on-time and late events in streaming, Flink needs to know the timestamp for each row, and it also needs regular indications of how far along in event time the processing has progressed so far (via so-called [watermarks]({% link dev/event_time.md %})).", "author": "alpinegizmo", "createdAt": "2020-12-04T10:48:08Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -22,213 +22,42 @@ specific language governing permissions and limitations\n under the License.\n -->\n \n-Flink is able to process streaming data based on different notions of *time*. \n+Flink can process data based on different notions of time. \n \n-- *Processing time* refers to the system time of the machine (also known as \"wall-clock time\") that is executing the respective operation.\n-- *Event time* refers to the processing of streaming data based on timestamps which are attached to each row. The timestamps can encode when an event happened.\n-- *Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.\n+*Processing time* refers to the machine's system time (also known as \"wall-clock time\") that is executing the respective operation.\n+- *Event time* refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened.\n \n-For more information about time handling in Flink, see the introduction about [Event Time and Watermarks]({% link dev/event_time.md %}).\n+For more information about time handling in Flink, see the introduction about [event time and watermarks]({% link dev/event_time.md %}).\n \n-This page explains how time attributes can be defined for time-based operations in Flink's Table API & SQL.\n \n * This will be replaced by the TOC\n {:toc}\n \n Introduction to Time Attributes\n -------------------------------\n \n-Time-based operations such as windows in both the [Table API]({% link dev/table/tableApi.md %}#group-windows) and [SQL]({% link dev/table/sql/queries.md %}#group-windows) require information about the notion of time and its origin. Therefore, tables can offer *logical time attributes* for indicating time and accessing corresponding timestamps in table programs.\n+Time attributes can be part of every table schema.\n+They are defined when creating a table from a `CREATE TABLE DDL` or a `DataStream`. \n+Once a time attribute is defined, it can be referenced as a field and used in time-based operations.\n+As long as a time attribute is not modified and forwarded from one part of the query to another, it remains valid.\n+Time attributes behave like regular timestamps and accessible for calculations.\n+When used in computation, time attributes are materialized and act as standard timestamps. \n+However, the opposite is not true; ordinary timestamp columns cannot be arbitrarily converted to time attributes in the middle of a query.\n \n-Time attributes can be part of every table schema. They are defined when creating a table from a CREATE TABLE DDL or a `DataStream` or are pre-defined when using a `TableSource`. Once a time attribute has been defined at the beginning, it can be referenced as a field and can be used in time-based operations.\n-\n-As long as a time attribute is not modified and is simply forwarded from one part of the query to another, it remains a valid time attribute. Time attributes behave like regular timestamps and can be accessed for calculations. If a time attribute is used in a calculation, it will be materialized and becomes a regular timestamp. Regular timestamps do not cooperate with Flink's time and watermarking system and thus can not be used for time-based operations anymore.\n-\n-Table programs require that the corresponding time characteristic has been specified for the streaming environment:\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\n-env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // default\n-\n-// alternatively:\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-val env = StreamExecutionEnvironment.getExecutionEnvironment\n-\n-env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) // default\n-\n-// alternatively:\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)\n-// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"python\" markdown=\"1\">\n-{% highlight python %}\n-env = StreamExecutionEnvironment.get_execution_environment()\n-\n-env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)  # default\n-\n-# alternatively:\n-# env.set_stream_time_characteristic(TimeCharacteristic.IngestionTime)\n-# env.set_stream_time_characteristic(TimeCharacteristic.EventTime)\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-Processing time\n----------------\n-\n-Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time but does not provide determinism. It neither requires timestamp extraction nor watermark generation.\n-\n-There are three ways to define a processing time attribute.\n-\n-### Defining in create table DDL\n-\n-The processing time attribute is defined as a computed column in create table DDL using the system `PROCTIME()` function. Please see [CREATE TABLE DDL]({% link dev/table/sql/create.md %}#create-table) for more information about computed column.\n-\n-{% highlight sql %}\n-\n-CREATE TABLE user_actions (\n-  user_name STRING,\n-  data STRING,\n-  user_action_time AS PROCTIME() -- declare an additional field as a processing time attribute\n-) WITH (\n-  ...\n-);\n-\n-SELECT TUMBLE_START(user_action_time, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)\n-FROM user_actions\n-GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);\n-\n-{% endhighlight %}\n-\n-\n-### During DataStream-to-Table Conversion\n-\n-The processing time attribute is defined with the `.proctime` property during schema definition. The time attribute must only extend the physical schema by an additional logical field. Thus, it can only be defined at the end of the schema definition.\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-DataStream<Tuple2<String, String>> stream = ...;\n-\n-// declare an additional logical field as a processing time attribute\n-Table table = tEnv.fromDataStream(stream, $(\"user_name\"), $(\"data\"), $(\"user_action_time\").proctime());\n-\n-WindowedTable windowedTable = table.window(\n-        Tumble.over(lit(10).minutes())\n-            .on($(\"user_action_time\"))\n-            .as(\"userActionWindow\"));\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-val stream: DataStream[(String, String)] = ...\n-\n-// declare an additional logical field as a processing time attribute\n-val table = tEnv.fromDataStream(stream, $\"UserActionTimestamp\", $\"user_name\", $\"data\", $\"user_action_time\".proctime)\n-\n-val windowedTable = table.window(Tumble over 10.minutes on $\"user_action_time\" as \"userActionWindow\")\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-### Using a TableSource\n-\n-The processing time attribute is defined by a `TableSource` that implements the `DefinedProctimeAttribute` interface. The logical time attribute is appended to the physical schema defined by the return type of the `TableSource`.\n-\n-<div class=\"codetabs\" markdown=\"1\">\n-<div data-lang=\"java\" markdown=\"1\">\n-{% highlight java %}\n-// define a table source with a processing attribute\n-public class UserActionSource implements StreamTableSource<Row>, DefinedProctimeAttribute {\n-\n-\t@Override\n-\tpublic TypeInformation<Row> getReturnType() {\n-\t\tString[] names = new String[] {\"user_name\" , \"data\"};\n-\t\tTypeInformation[] types = new TypeInformation[] {Types.STRING(), Types.STRING()};\n-\t\treturn Types.ROW(names, types);\n-\t}\n-\n-\t@Override\n-\tpublic DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {\n-\t\t// create stream\n-\t\tDataStream<Row> stream = ...;\n-\t\treturn stream;\n-\t}\n-\n-\t@Override\n-\tpublic String getProctimeAttribute() {\n-\t\t// field with this name will be appended as a third field\n-\t\treturn \"user_action_time\";\n-\t}\n-}\n-\n-// register table source\n-tEnv.registerTableSource(\"user_actions\", new UserActionSource());\n-\n-WindowedTable windowedTable = tEnv\n-\t.from(\"user_actions\")\n-\t.window(Tumble\n-\t    .over(lit(10).minutes())\n-\t    .on($(\"user_action_time\"))\n-\t    .as(\"userActionWindow\"));\n-{% endhighlight %}\n-</div>\n-<div data-lang=\"scala\" markdown=\"1\">\n-{% highlight scala %}\n-// define a table source with a processing attribute\n-class UserActionSource extends StreamTableSource[Row] with DefinedProctimeAttribute {\n-\n-\toverride def getReturnType = {\n-\t\tval names = Array[String](\"user_name\" , \"data\")\n-\t\tval types = Array[TypeInformation[_]](Types.STRING, Types.STRING)\n-\t\tTypes.ROW(names, types)\n-\t}\n-\n-\toverride def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n-\t\t// create stream\n-\t\tval stream = ...\n-\t\tstream\n-\t}\n-\n-\toverride def getProctimeAttribute = {\n-\t\t// field with this name will be appended as a third field\n-\t\t\"user_action_time\"\n-\t}\n-}\n-\n-// register table source\n-tEnv.registerTableSource(\"user_actions\", new UserActionSource)\n-\n-val windowedTable = tEnv\n-\t.from(\"user_actions\")\n-\t.window(Tumble over 10.minutes on $\"user_action_time\" as \"userActionWindow\")\n-{% endhighlight %}\n-</div>\n-</div>\n-\n-Event time\n+Event Time\n ----------\n \n-Event time allows a table program to produce results based on the time that is contained in every record. This allows for consistent results even in case of out-of-order events or late events. It also ensures replayable results of the table program when reading records from persistent storage.\n+Event time allows a table program to produce results based on timestamps in every record, allowing for consistent results even in case of out-of-order events or late events. It also ensures the replayability of the results of the table program when reading records from persistent storage.\n \n-Additionally, event time allows for unified syntax for table programs in both batch and streaming environments. A time attribute in a streaming environment can be a regular field of a record in a batch environment.\n+Additionally, event time allows for unified syntax for table programs in both batch and streaming environments. A time attribute in a streaming environment can be a regular column of a row in a batch environment.\n \n-In order to handle out-of-order events and distinguish between on-time and late events in streaming, Flink needs to extract timestamps from events and make some kind of progress in time (so-called [watermarks]({% link dev/event_time.md %})).\n+To handle out-of-order events and distinguish between on-time and late events in streaming, Flink needs to extract timestamps from events and make some progress in time (so-called [watermarks]({% link dev/event_time.md %})).", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjAwOTQwNg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r536009406", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When converting a `DataStream` to a table, an event time attribute can be defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have been already assigned in the `DataStream` being converted.\n          \n          \n            \n            When converting a `DataStream` to a table, an event time attribute can be defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have already been assigned in the `DataStream` being converted.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:49:13Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -251,14 +80,12 @@ GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);\n \n ### During DataStream-to-Table Conversion\n \n-The event time attribute is defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have been assigned in the `DataStream` that is converted.\n+When converting a `DataStream` to a table, an event time attribute can be defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have been already assigned in the `DataStream` being converted.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjAxMDAwMg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r536010002", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are two ways of defining the time attribute when converting a `DataStream` into a `Table`. Depending on whether the specified `.rowtime` field name exists in the schema of the `DataStream`, the timestamp is either (1) appended as a new column to the schema or\n          \n          \n            \n            (2) replaces an existing column.\n          \n          \n            \n            There are two ways of defining the time attribute when converting a `DataStream` into a `Table`. Depending on whether the specified `.rowtime` field name exists in the schema of the `DataStream`, the timestamp is either (1) appended as a new column, or it\n          \n          \n            \n            (2) replaces an existing column.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:50:09Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -251,14 +80,12 @@ GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);\n \n ### During DataStream-to-Table Conversion\n \n-The event time attribute is defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have been assigned in the `DataStream` that is converted.\n+When converting a `DataStream` to a table, an event time attribute can be defined with the `.rowtime` property during schema definition. [Timestamps and watermarks]({% link dev/event_time.md %}) must have been already assigned in the `DataStream` being converted.\n \n-There are two ways of defining the time attribute when converting a `DataStream` into a `Table`. Depending on whether the specified `.rowtime` field name exists in the schema of the `DataStream` or not, the timestamp field is either\n+There are two ways of defining the time attribute when converting a `DataStream` into a `Table`. Depending on whether the specified `.rowtime` field name exists in the schema of the `DataStream`, the timestamp is either (1) appended as a new column to the schema or\n+(2) replaces an existing column.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjAxMDQwMg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r536010402", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time but does not provide determinism. It neither requires timestamp extraction nor watermark generation.\n          \n          \n            \n            Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time, but it will generate non-deterministic results. Processing time does not require timestamp extraction or watermark generation.", "author": "alpinegizmo", "createdAt": "2020-12-04T10:50:47Z", "path": "docs/dev/table/streaming/time_attributes.md", "diffHunk": "@@ -318,98 +145,61 @@ val windowedTable = table.window(Tumble over 10.minutes on $\"user_action_time\" a\n </div>\n </div>\n \n-### Using a TableSource\n \n-The event time attribute is defined by a `TableSource` that implements the `DefinedRowtimeAttributes` interface. The `getRowtimeAttributeDescriptors()` method returns a list of `RowtimeAttributeDescriptor` for describing the final name of a time attribute, a timestamp extractor to derive the values of the attribute, and the watermark strategy associated with the attribute.\n+Processing Time\n+---------------\n+\n+Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time but does not provide determinism. It neither requires timestamp extraction nor watermark generation.", "originalCommit": "7d1723792f216e5c11cad7cfa8d8f1d58e5c842a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyMzM2MQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538223361", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Suppose a table that tracks the price of different products in a store. \n          \n          \n            \n            Suppose a table tracks the prices of different products in a store.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:33:42Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -22,30 +22,29 @@ specific language governing permissions and limitations\n under the License.\n --> \n \n-A Temporal table is a table that evolves over time -  otherwise known in Flink as a [dynamic table]({% link dev/table/streaming/dynamic_tables.md %}). Rows in a temporal table are associated with one or more temporal periods and all Flink tables are temporal(dynamic).\n+Flink SQL operates over dynamic tables that evolve, which may either be append-only or updating. \n+Versioned tables represent a special type of updating table that remembers the past values for each key.\n \n-A temporal table contains one or more versioned table snapshots, it can be a changing history table which tracks the changes(e.g. database changelog, contains all snapshots) or a changing dimensioned table which materializes the changes(e.g. database table, contains the latest snapshot). \n+<span class=\"label label-danger\">Attention</span> The Legacy planner does not support versioned tables.\n \n-**Version**: A temporal table can split into a set of versioned table snapshots, the version in table snapshots represents the valid life circle of rows, the start time and the end time of the valid period can be assigned by users. \n-Temporal table can split to `versioned table` and `regular table` according to the table can tracks its history version or not.\n+* This will be replaced by the TOC\n+{:toc}\n \n-**Versioned table**: If the rows a in temporal table can track its history changes and visit its history versions, we call this a versioned table. Tables that comes from a database changelog can be defined as a versioned table.\n+## Concept\n \n-**Regular table**: If the row in temporal table can only track and visit its latest version\uff0cwe call this kind of temporal table as regular table. Tables that comes from a database or HBase can be defined as a regular table.\n+Dynamic tables define relations over time. \n+Often, particularly when working with metadata, a key's old value does not become irrelevant when it changes. \n \n-* This will be replaced by the TOC\n-{:toc}\n+Flink SQL can define versioned tables over any dynamic table with a `PRIMARY KEY` constraint and time attribute. \n \n-Motivation\n-----------\n+A primary key constraint in Flink means that a column or set of columns of a table or view are unique and non-null.\n+The primary key semantic on a upserting table means the materialized changes for a particular key (`INSERT`/`UPDATE`/`DELETE`) represent the changes to a single row over time. The time attribute on a upserting table defines when each change occurred.\n \n-### Correlate with a versioned table\n-Given a scenario the order stream correlates the dimension table product, the table `orders` comes from kafka which contains the real time orders, the table `product_changelog` comes from the changelog of the database table `products`,\n- the product price in table `products` is changing over time. \n+Taken together, Flink can track the changes to a row over time and maintain the period for which each value was valid for that key.\n \n-{% highlight sql %}\n-SELECT * FROM product_changelog;\n+Suppose a table that tracks the price of different products in a store. ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyNTg4OA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538225888", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            While at `13:00:00,` we found find another. \n          \n          \n            \n            While at `13:00:00,` we would find another set of prices.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:36:33Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyNzAzNw==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538227037", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            12:00:00     p_001      scooter      12.99\n          \n          \n            \n            12:00:00     p_002      basketball   19.99\n          \n          \n            \n            00:01:00     p_001      scooter      11.11\n          \n          \n            \n            00:02:00     p_002      basketball   23.11", "author": "alpinegizmo", "createdAt": "2020-12-08T10:37:44Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyODYxNg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538228616", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. \n          \n          \n            \n            Versioned tables are defined implicitly for any tables whose underlying sources or formats directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and an event-time attribute.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:39:27Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. \n \n-Let's assume that `LatestRates` is a table (e.g. stored in HBase) which is materialized with the latest rates. The `LatestRates` always represents the latest content of hbase table `rates`.\n- \n-Then the content of `LatestRates` table when we query at time `10:15:00` is:\n {% highlight sql %}\n-10:15:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      114\n-Yen       1\n-{% endhighlight %}\n-\n-Then the content of `LatestRates` table when we query at time `11:00:00` is:\n-{% highlight sql %}\n-11:00:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      116\n-Yen       1\n+update_time  product_id product_name price\n+===========  ========== ============ ===== \n+12:00:00     p_001      scooter      12.99\n+12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In Flink, this is represented by a [*regular Table*](#defining-regular-table).\n \n-Temporal Table\n---------------\n-<span class=\"label label-danger\">Attention</span> This is only supported in Blink planner.\n+## Versioned Table Sources\n \n-Flink uses primary key constraint and event time to define both versioned table and versioned view.\n+Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyOTMwMA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538229300", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Image an append-only table of currency rates. \n          \n          \n            \n            Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Imagine an append-only table of currency rates.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:40:06Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. \n \n-Let's assume that `LatestRates` is a table (e.g. stored in HBase) which is materialized with the latest rates. The `LatestRates` always represents the latest content of hbase table `rates`.\n- \n-Then the content of `LatestRates` table when we query at time `10:15:00` is:\n {% highlight sql %}\n-10:15:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      114\n-Yen       1\n-{% endhighlight %}\n-\n-Then the content of `LatestRates` table when we query at time `11:00:00` is:\n-{% highlight sql %}\n-11:00:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      116\n-Yen       1\n+update_time  product_id product_name price\n+===========  ========== ============ ===== \n+12:00:00     p_001      scooter      12.99\n+12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In Flink, this is represented by a [*regular Table*](#defining-regular-table).\n \n-Temporal Table\n---------------\n-<span class=\"label label-danger\">Attention</span> This is only supported in Blink planner.\n+## Versioned Table Sources\n \n-Flink uses primary key constraint and event time to define both versioned table and versioned view.\n+Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. \n \n-### Defining Versioned Table\n-The table is a versioned table in Flink only is the table contains primary key constraint and event time.\n {% highlight sql %}\n--- Define a versioned table\n-CREATE TABLE product_changelog (\n-  product_id STRING,\n-  product_name STRING,\n-  product_price DECIMAL(10, 4),\n-  update_time TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n-  PRIMARY KEY(product_id) NOT ENFORCED,      -- (1) defines the primary key constraint\n-  WATERMARK FOR update_time AS update_time   -- (2) defines the event time by watermark                               \n-) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'products',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'value.format' = 'debezium-json'\n-);\n+CREATE TABLE products (\n+\tproduct_id    STRING,\n+\tproduct_name  STRING,\n+\tprice         DECIMAL(32, 2),\n+\tupdate_time   TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n+\tPRIMARY KEY (product_id) NOT ENFORCED\n+\tWATERMARK FOR update_time AS update_time\n+) WITH (...);\n {% endhighlight %}\n \n-Line `(1)` defines the primary key constraint for table `product_changelog`, Line `(2)` defines the `update_time` as event time for table `product_changelog`,\n-thus table `product_changelog` is a versioned table.\n+## Versioned Table Views\n \n-**Note**: The grammar `METADATA FROM 'value.source.timestamp' VIRTUAL` means extract the database\n-operation execution time for every changelog, it's strongly recommended defines the database operation execution time \n-as event time rather than ingestion-time or time in the record, otherwise the version extract from the changelog may\n-mismatch with the version in database.\n- \n-### Defining Versioned View\n+Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Image an append-only table of currency rates. ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIyOTgwNg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538229806", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The table `currency_rates` contains a row for each currency - with respect to USD - and receives a new row each time the rate changes.\n          \n          \n            \n            The table `currency_rates` contains a row for each currency &mdash; with respect to USD &mdash; and receives a new row each time the rate changes.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:40:45Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. \n \n-Let's assume that `LatestRates` is a table (e.g. stored in HBase) which is materialized with the latest rates. The `LatestRates` always represents the latest content of hbase table `rates`.\n- \n-Then the content of `LatestRates` table when we query at time `10:15:00` is:\n {% highlight sql %}\n-10:15:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      114\n-Yen       1\n-{% endhighlight %}\n-\n-Then the content of `LatestRates` table when we query at time `11:00:00` is:\n-{% highlight sql %}\n-11:00:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      116\n-Yen       1\n+update_time  product_id product_name price\n+===========  ========== ============ ===== \n+12:00:00     p_001      scooter      12.99\n+12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In Flink, this is represented by a [*regular Table*](#defining-regular-table).\n \n-Temporal Table\n---------------\n-<span class=\"label label-danger\">Attention</span> This is only supported in Blink planner.\n+## Versioned Table Sources\n \n-Flink uses primary key constraint and event time to define both versioned table and versioned view.\n+Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. \n \n-### Defining Versioned Table\n-The table is a versioned table in Flink only is the table contains primary key constraint and event time.\n {% highlight sql %}\n--- Define a versioned table\n-CREATE TABLE product_changelog (\n-  product_id STRING,\n-  product_name STRING,\n-  product_price DECIMAL(10, 4),\n-  update_time TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n-  PRIMARY KEY(product_id) NOT ENFORCED,      -- (1) defines the primary key constraint\n-  WATERMARK FOR update_time AS update_time   -- (2) defines the event time by watermark                               \n-) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'products',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'value.format' = 'debezium-json'\n-);\n+CREATE TABLE products (\n+\tproduct_id    STRING,\n+\tproduct_name  STRING,\n+\tprice         DECIMAL(32, 2),\n+\tupdate_time   TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n+\tPRIMARY KEY (product_id) NOT ENFORCED\n+\tWATERMARK FOR update_time AS update_time\n+) WITH (...);\n {% endhighlight %}\n \n-Line `(1)` defines the primary key constraint for table `product_changelog`, Line `(2)` defines the `update_time` as event time for table `product_changelog`,\n-thus table `product_changelog` is a versioned table.\n+## Versioned Table Views\n \n-**Note**: The grammar `METADATA FROM 'value.source.timestamp' VIRTUAL` means extract the database\n-operation execution time for every changelog, it's strongly recommended defines the database operation execution time \n-as event time rather than ingestion-time or time in the record, otherwise the version extract from the changelog may\n-mismatch with the version in database.\n- \n-### Defining Versioned View\n+Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Image an append-only table of currency rates. \n \n-Flink also supports defining versioned view only if the view contains unique key constraint and event time.\n- \n-Let\u2019s assume that we have the following table `RatesHistory`:\n {% highlight sql %}\n--- Define an append-only table\n-CREATE TABLE RatesHistory (\n-    currency_time TIMESTAMP(3),\n-    currency STRING,\n-    rate DECIMAL(38, 10),\n-    WATERMARK FOR currency_time AS currency_time   -- defines the event time\n+CREATE TABLE currency_rates (\n+\tcurrency      STRING,\n+\trate          DECIMAL(32, 10)\n+\tupdate_time   TIMESTAMP(3),\n+\tWATERMARK FOR update_time AS update_time\n ) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'rates',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'format' = 'json'                                -- this is an append only source\n-)\n+\t'connector' = 'kafka',\n+\t'topic'\t    = 'rates',\n+\t'properties.bootstrap.servers' = 'localhost:9092',\n+\t'format'    = 'json'\n+);\n {% endhighlight %}\n \n-Table `RatesHistory` represents an ever growing append-only table of currency exchange rates with respect to \n-Yen (which has a rate of 1). For example, the exchange rate for the period from 09:00 to 10:45 of Euro to Yen was 114.\n-From 10:45 to 11:15 it was 116.\n-\n-{% highlight sql %}\n-SELECT * FROM RatesHistory;\n-\n-currency_time currency  rate\n-============= ========= ====\n-09:00:00      US Dollar 102\n-09:00:00      Euro      114\n-09:00:00      Yen       1\n-10:45:00      Euro      116\n-11:15:00      Euro      119\n-11:49:00      Pounds    108\n-{% endhighlight %}\n+The table `currency_rates` contains a row for each currency - with respect to USD - and receives a new row each time the rate changes.", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIzMDc4OA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538230788", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `JSON` format does not support native changelog semantics, and so Flink can only read this table as append-only.\n          \n          \n            \n            The `JSON` format does not support native changelog semantics, so Flink can only read this table as append-only.", "author": "alpinegizmo", "createdAt": "2020-12-08T10:41:44Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. \n \n-Let's assume that `LatestRates` is a table (e.g. stored in HBase) which is materialized with the latest rates. The `LatestRates` always represents the latest content of hbase table `rates`.\n- \n-Then the content of `LatestRates` table when we query at time `10:15:00` is:\n {% highlight sql %}\n-10:15:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      114\n-Yen       1\n-{% endhighlight %}\n-\n-Then the content of `LatestRates` table when we query at time `11:00:00` is:\n-{% highlight sql %}\n-11:00:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      116\n-Yen       1\n+update_time  product_id product_name price\n+===========  ========== ============ ===== \n+12:00:00     p_001      scooter      12.99\n+12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In Flink, this is represented by a [*regular Table*](#defining-regular-table).\n \n-Temporal Table\n---------------\n-<span class=\"label label-danger\">Attention</span> This is only supported in Blink planner.\n+## Versioned Table Sources\n \n-Flink uses primary key constraint and event time to define both versioned table and versioned view.\n+Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. \n \n-### Defining Versioned Table\n-The table is a versioned table in Flink only is the table contains primary key constraint and event time.\n {% highlight sql %}\n--- Define a versioned table\n-CREATE TABLE product_changelog (\n-  product_id STRING,\n-  product_name STRING,\n-  product_price DECIMAL(10, 4),\n-  update_time TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n-  PRIMARY KEY(product_id) NOT ENFORCED,      -- (1) defines the primary key constraint\n-  WATERMARK FOR update_time AS update_time   -- (2) defines the event time by watermark                               \n-) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'products',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'value.format' = 'debezium-json'\n-);\n+CREATE TABLE products (\n+\tproduct_id    STRING,\n+\tproduct_name  STRING,\n+\tprice         DECIMAL(32, 2),\n+\tupdate_time   TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n+\tPRIMARY KEY (product_id) NOT ENFORCED\n+\tWATERMARK FOR update_time AS update_time\n+) WITH (...);\n {% endhighlight %}\n \n-Line `(1)` defines the primary key constraint for table `product_changelog`, Line `(2)` defines the `update_time` as event time for table `product_changelog`,\n-thus table `product_changelog` is a versioned table.\n+## Versioned Table Views\n \n-**Note**: The grammar `METADATA FROM 'value.source.timestamp' VIRTUAL` means extract the database\n-operation execution time for every changelog, it's strongly recommended defines the database operation execution time \n-as event time rather than ingestion-time or time in the record, otherwise the version extract from the changelog may\n-mismatch with the version in database.\n- \n-### Defining Versioned View\n+Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Image an append-only table of currency rates. \n \n-Flink also supports defining versioned view only if the view contains unique key constraint and event time.\n- \n-Let\u2019s assume that we have the following table `RatesHistory`:\n {% highlight sql %}\n--- Define an append-only table\n-CREATE TABLE RatesHistory (\n-    currency_time TIMESTAMP(3),\n-    currency STRING,\n-    rate DECIMAL(38, 10),\n-    WATERMARK FOR currency_time AS currency_time   -- defines the event time\n+CREATE TABLE currency_rates (\n+\tcurrency      STRING,\n+\trate          DECIMAL(32, 10)\n+\tupdate_time   TIMESTAMP(3),\n+\tWATERMARK FOR update_time AS update_time\n ) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'rates',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'format' = 'json'                                -- this is an append only source\n-)\n+\t'connector' = 'kafka',\n+\t'topic'\t    = 'rates',\n+\t'properties.bootstrap.servers' = 'localhost:9092',\n+\t'format'    = 'json'\n+);\n {% endhighlight %}\n \n-Table `RatesHistory` represents an ever growing append-only table of currency exchange rates with respect to \n-Yen (which has a rate of 1). For example, the exchange rate for the period from 09:00 to 10:45 of Euro to Yen was 114.\n-From 10:45 to 11:15 it was 116.\n-\n-{% highlight sql %}\n-SELECT * FROM RatesHistory;\n-\n-currency_time currency  rate\n-============= ========= ====\n-09:00:00      US Dollar 102\n-09:00:00      Euro      114\n-09:00:00      Yen       1\n-10:45:00      Euro      116\n-11:15:00      Euro      119\n-11:49:00      Pounds    108\n-{% endhighlight %}\n+The table `currency_rates` contains a row for each currency - with respect to USD - and receives a new row each time the rate changes.\n+The `JSON` format does not support native changelog semantics, and so Flink can only read this table as append-only.", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIzNTQ2NA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538235464", "bodyText": "As mentioned above, currency_time should become update time throughout this view definition.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     ORDER BY currency_time DESC) AS rowNum \n          \n          \n            \n                     ORDER BY update_time DESC) AS rowNum", "author": "alpinegizmo", "createdAt": "2020-12-08T10:48:24Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -194,10 +127,10 @@ SELECT currency, rate, currency_time            -- (1) `currency_time` keeps the\n       SELECT *,\n       ROW_NUMBER() OVER (PARTITION BY currency  -- (2) the inferred unique key `currency` can be a primary key\n          ORDER BY currency_time DESC) AS rowNum ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIzNTczOQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538235739", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            (changelog kind) currency_time currency   rate\n          \n          \n            \n            (changelog kind) update_time currency   rate", "author": "alpinegizmo", "createdAt": "2020-12-08T10:48:48Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -194,10 +127,10 @@ SELECT currency, rate, currency_time            -- (1) `currency_time` keeps the\n       SELECT *,\n       ROW_NUMBER() OVER (PARTITION BY currency  -- (2) the inferred unique key `currency` can be a primary key\n          ORDER BY currency_time DESC) AS rowNum \n-      FROM RatesHistory)\n+      FROM currency_rates)\n WHERE rowNum = 1; \n \n--- the view `versioned_rates` will produce changelog as the following.\n+-- the view `versioned_rates` will produce a changelog as the following.\n (changelog kind) currency_time currency   rate", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIzODI4OQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538238289", "bodyText": "In the view defined below, currency_time must become update_time in both the SELECT and in the comment. github won't allow me to suggest changes here, unfortunately.\nAlso, I'm confused by one thing. I see how currency can be the primary key that a versioned table needs, but how does this actually work without our explicitly declaring that currency is the PRIMARY KEY for this view? Why do we specify a primary key when using CREATE TABLE, but not when using CREATE VIEW?", "author": "alpinegizmo", "createdAt": "2020-12-08T10:52:28Z", "path": "docs/dev/table/streaming/temporal_tables.md", "diffHunk": "@@ -57,134 +56,68 @@ SELECT * FROM product_changelog;\n -(DELETE)         18:00:00     p_001      scooter      12.99 \n {% endhighlight %}\n \n-The table `product_changelog` represents an ever growing changelog of database table `products`,  for example, the initial price of product `scooter` is `11.11` at `00:01:00`, and the price increases to `12.99` at `12:00:00`,\n- the product item is deleted from the table `products` at `18:00:00`.\n+Given this set of changes, we track how the price of a scooter changes over time.\n+It is initially $11.11 at `00:01:00` when added to the catalog.\n+The price then rises to $12.99 at `12:00:00` before being deleted from the catalog at `18:00:00`.\n \n-Given that we would like to output the version of `product_changelog` table of the time `10:00:00`, the following table shows the result. \n-{% highlight sql %}\n-update_time  product_id product_name price\n-===========  ========== ============ ===== \n-00:01:00     p_001      scooter      11.11\n-00:02:00     p_002      basketball   23.11\n-{% endhighlight %}\n+If we queried the table for various products' prices at different times, we would retrieve different results. At `10:00:00` the table would show one set of prices.\n \n-Given that we would like to output the version of `product_changelog` table of the time `13:00:00`, the following table shows the result. \n {% highlight sql %}\n update_time  product_id product_name price\n ===========  ========== ============ ===== \n 12:00:00     p_001      scooter      12.99\n 12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In above example, the specific version of the table is tracked by `update_time` and `product_id`,  the `product_id` would be a primary key for `product_changelog` table and `update_time` would be the event time.\n-\n-In Flink, this is represented by a [*versioned table*](#defining-versioned-table).\n-\n-### Correlate with a regular table\n \n-On the other hand, some use cases require to join a regular table which is an external database table.\n+While at `13:00:00,` we found find another. \n \n-Let's assume that `LatestRates` is a table (e.g. stored in HBase) which is materialized with the latest rates. The `LatestRates` always represents the latest content of hbase table `rates`.\n- \n-Then the content of `LatestRates` table when we query at time `10:15:00` is:\n {% highlight sql %}\n-10:15:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      114\n-Yen       1\n-{% endhighlight %}\n-\n-Then the content of `LatestRates` table when we query at time `11:00:00` is:\n-{% highlight sql %}\n-11:00:00 > SELECT * FROM LatestRates;\n-\n-currency  rate\n-========= ====\n-US Dollar 102\n-Euro      116\n-Yen       1\n+update_time  product_id product_name price\n+===========  ========== ============ ===== \n+12:00:00     p_001      scooter      12.99\n+12:00:00     p_002      basketball   19.99\n {% endhighlight %}\n \n-In Flink, this is represented by a [*regular Table*](#defining-regular-table).\n \n-Temporal Table\n---------------\n-<span class=\"label label-danger\">Attention</span> This is only supported in Blink planner.\n+## Versioned Table Sources\n \n-Flink uses primary key constraint and event time to define both versioned table and versioned view.\n+Versioned tables are defined implicity for any table whose underlying source or format directly define changelogs. Examples include the [upsert Kafka]({% link dev/table/connectors/upsert-kafka.md %}) source as well as database changelog formats such as [debezium]({% link dev/table/connectors/formats/debezium.md %}) and [canal]({% link dev/table/connectors/formats/canal.md %}). As discussed above, the only additional requirement is the `CREATE` table statement must contain a primary key and event-time attribute. \n \n-### Defining Versioned Table\n-The table is a versioned table in Flink only is the table contains primary key constraint and event time.\n {% highlight sql %}\n--- Define a versioned table\n-CREATE TABLE product_changelog (\n-  product_id STRING,\n-  product_name STRING,\n-  product_price DECIMAL(10, 4),\n-  update_time TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n-  PRIMARY KEY(product_id) NOT ENFORCED,      -- (1) defines the primary key constraint\n-  WATERMARK FOR update_time AS update_time   -- (2) defines the event time by watermark                               \n-) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'products',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'value.format' = 'debezium-json'\n-);\n+CREATE TABLE products (\n+\tproduct_id    STRING,\n+\tproduct_name  STRING,\n+\tprice         DECIMAL(32, 2),\n+\tupdate_time   TIMESTAMP(3) METADATA FROM 'value.source.timestamp' VIRTUAL,\n+\tPRIMARY KEY (product_id) NOT ENFORCED\n+\tWATERMARK FOR update_time AS update_time\n+) WITH (...);\n {% endhighlight %}\n \n-Line `(1)` defines the primary key constraint for table `product_changelog`, Line `(2)` defines the `update_time` as event time for table `product_changelog`,\n-thus table `product_changelog` is a versioned table.\n+## Versioned Table Views\n \n-**Note**: The grammar `METADATA FROM 'value.source.timestamp' VIRTUAL` means extract the database\n-operation execution time for every changelog, it's strongly recommended defines the database operation execution time \n-as event time rather than ingestion-time or time in the record, otherwise the version extract from the changelog may\n-mismatch with the version in database.\n- \n-### Defining Versioned View\n+Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Image an append-only table of currency rates. \n \n-Flink also supports defining versioned view only if the view contains unique key constraint and event time.\n- \n-Let\u2019s assume that we have the following table `RatesHistory`:\n {% highlight sql %}\n--- Define an append-only table\n-CREATE TABLE RatesHistory (\n-    currency_time TIMESTAMP(3),\n-    currency STRING,\n-    rate DECIMAL(38, 10),\n-    WATERMARK FOR currency_time AS currency_time   -- defines the event time\n+CREATE TABLE currency_rates (\n+\tcurrency      STRING,\n+\trate          DECIMAL(32, 10)\n+\tupdate_time   TIMESTAMP(3),\n+\tWATERMARK FOR update_time AS update_time\n ) WITH (\n-  'connector' = 'kafka',\n-  'topic' = 'rates',\n-  'scan.startup.mode' = 'earliest-offset',\n-  'properties.bootstrap.servers' = 'localhost:9092',\n-  'format' = 'json'                                -- this is an append only source\n-)\n+\t'connector' = 'kafka',\n+\t'topic'\t    = 'rates',\n+\t'properties.bootstrap.servers' = 'localhost:9092',\n+\t'format'    = 'json'\n+);\n {% endhighlight %}\n \n-Table `RatesHistory` represents an ever growing append-only table of currency exchange rates with respect to \n-Yen (which has a rate of 1). For example, the exchange rate for the period from 09:00 to 10:45 of Euro to Yen was 114.\n-From 10:45 to 11:15 it was 116.\n-\n-{% highlight sql %}\n-SELECT * FROM RatesHistory;\n-\n-currency_time currency  rate\n-============= ========= ====\n-09:00:00      US Dollar 102\n-09:00:00      Euro      114\n-09:00:00      Yen       1\n-10:45:00      Euro      116\n-11:15:00      Euro      119\n-11:49:00      Pounds    108\n-{% endhighlight %}\n+The table `currency_rates` contains a row for each currency - with respect to USD - and receives a new row each time the rate changes.\n+The `JSON` format does not support native changelog semantics, and so Flink can only read this table as append-only.\n+However, it is clear to us (the query developer) that this table has all the necessary information to define a versioned table. \n \n-To define a versioned table on `RatesHistory`, Flink supports defining a versioned view \n-by [deduplication query]({% link dev/table/sql/queries.md %}#deduplication) which produces an ordered changelog\n-stream with an inferred primary key(`currency`) and event time(`currency_time`).\n+Flink can reinterpret this table as a versioned table by defining a [deduplication query]({% link dev/table/sql/queries.md %}#deduplication) which produces an ordered changelog stream with an inferred primary key (currency) and event time (update_time). ", "originalCommit": "6ca7d5619e4d14b3da3cca362da015785ba199eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4Mjk1OA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r538482958", "bodyText": "The real answer is flink has a special optimization step that turns any OVER WINDOW with exactly this format into a versioned table keyed by the fields in the PARTITION BY clause.\nSELECT *,\n      ROW_NUMBER() OVER (PARTITION BY [key,] ORDER BY [time attribute] DESC) AS row_num\nWHERE \n     row_num = 1\nIn a batch query, this would return the most recent value for each currency. The streaming semantic is it provides the new value for each partition each time a more recent value is made available.", "author": "sjwiesman", "createdAt": "2020-12-08T15:13:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODIzODI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQxNzgwMg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r539417802", "bodyText": "Temporal tables are no longer \"above\".\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Please define temporal joins using [versioned table]({% link dev/table/streaming/temporal_tables.md %}) as described above in new queries.\n          \n          \n            \n            Please define temporal joins using [versioned tables]({% link dev/table/streaming/temporal_tables.md %}) in new queries.", "author": "alpinegizmo", "createdAt": "2020-12-09T15:45:38Z", "path": "docs/dev/table/streaming/legacy.md", "diffHunk": "@@ -0,0 +1,135 @@\n+---\n+title: \"Legacy Features\"\n+nav-parent_id: streaming_tableapi\n+nav-pos: 1001\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+As Flink SQL has matured there are some features that have been replaced with more modern and better functioning substitutes.\n+These legacy features remain documented here for those users that have not yet or are unable to, upgrade to the more modern variant.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+# Temporal Table Function\n+\n+The temporal table function is the legacy way of defining something akin to a [versioned table]({% link dev/table/streaming/temporal_tables.md %})\n+that can be used in a temporal table join.\n+Please define temporal joins using [versioned table]({% link dev/table/streaming/temporal_tables.md %}) as described above in new queries.", "originalCommit": "023089909048230b211d7cb9e5f9cdee432ed411", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQxODk2OA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r539418968", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Temporal table function can be defined on top of append-only streams using the [Table API]({% link dev/table/tableApi.md %}).\n          \n          \n            \n            Temporal table functions can be defined on top of append-only streams using the [Table API]({% link dev/table/tableApi.md %}).", "author": "alpinegizmo", "createdAt": "2020-12-09T15:46:55Z", "path": "docs/dev/table/streaming/legacy.md", "diffHunk": "@@ -0,0 +1,135 @@\n+---\n+title: \"Legacy Features\"\n+nav-parent_id: streaming_tableapi\n+nav-pos: 1001\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+As Flink SQL has matured there are some features that have been replaced with more modern and better functioning substitutes.\n+These legacy features remain documented here for those users that have not yet or are unable to, upgrade to the more modern variant.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+# Temporal Table Function\n+\n+The temporal table function is the legacy way of defining something akin to a [versioned table]({% link dev/table/streaming/temporal_tables.md %})\n+that can be used in a temporal table join.\n+Please define temporal joins using [versioned table]({% link dev/table/streaming/temporal_tables.md %}) as described above in new queries.\n+\n+Unlike a versioned table, temporal table functions can only be defined on top of append-only streams \n+&mdash; it does not support changelog inputs.\n+Additionally, a temporal table function cannot be defined in pure SQL DDL. \n+\n+#### Defining a Temporal Table Function\n+\n+Temporal table function can be defined on top of append-only streams using the [Table API]({% link dev/table/tableApi.md %}).", "originalCommit": "023089909048230b211d7cb9e5f9cdee432ed411", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQyMDA5Mg==", "url": "https://github.com/apache/flink/pull/14292#discussion_r539420092", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Consider an append-only table `orders` that tracks customers orders in different currencies.\n          \n          \n            \n            Consider an append-only table `orders` that tracks customers' orders in different currencies.", "author": "alpinegizmo", "createdAt": "2020-12-09T15:48:05Z", "path": "docs/dev/table/streaming/legacy.md", "diffHunk": "@@ -0,0 +1,135 @@\n+---\n+title: \"Legacy Features\"\n+nav-parent_id: streaming_tableapi\n+nav-pos: 1001\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+As Flink SQL has matured there are some features that have been replaced with more modern and better functioning substitutes.\n+These legacy features remain documented here for those users that have not yet or are unable to, upgrade to the more modern variant.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+# Temporal Table Function\n+\n+The temporal table function is the legacy way of defining something akin to a [versioned table]({% link dev/table/streaming/temporal_tables.md %})\n+that can be used in a temporal table join.\n+Please define temporal joins using [versioned table]({% link dev/table/streaming/temporal_tables.md %}) as described above in new queries.\n+\n+Unlike a versioned table, temporal table functions can only be defined on top of append-only streams \n+&mdash; it does not support changelog inputs.\n+Additionally, a temporal table function cannot be defined in pure SQL DDL. \n+\n+#### Defining a Temporal Table Function\n+\n+Temporal table function can be defined on top of append-only streams using the [Table API]({% link dev/table/tableApi.md %}).\n+The table is registered with one or more key columns, and a time attribute used for versioning.\n+\n+Suppose we have an append-only table of currency rates that we would like to \n+register as a temporal table function.\n+\n+{% highlight sql %}\n+SELECT * FROM currency_rates;\n+\n+update_time   currency   rate\n+============= =========  ====\n+09:00:00      Yen        102\n+09:00:00      Euro       114\n+09:00:00      USD        1\n+11:15:00      Euro       119\n+11:49:00      Pounds     108\n+{% endhighlight %}\n+\n+Using the Table API, we can register this stream using `currency` for the key and `update_time` as \n+the versioning time attribute.\n+\n+<div class=\"codetabs\" markdown=\"1\">\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+TemporalTableFunction rates = tEnv\n+    .from(\"currency_rates\").\n+    .createTemporalTableFunction(\"update_time\", \"currency\");\n+ \n+tEnv.registerFunction(\"rates\", rates);                                                        \n+{% endhighlight %}\n+</div>\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+rates = tEnv\n+    .from(\"currency_rates\").\n+    .createTemporalTableFunction(\"update_time\", \"currency\")\n+ \n+tEnv.registerFunction(\"rates\", rates)\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+#### Temporal Table Function Join\n+\n+Once defined, a temporal table function is used as a standard [table function]({% link dev/table/functions/udfs.md %}#table-functions).\n+Append-only tables (left input/probe side) can join with a temporal table (right input/build side),\n+i.e., a table that changes over time and tracks its changes, to retrieve the value for a key as it was at a particular point in time.\n+\n+Consider an append-only table `orders` that tracks customers orders in different currencies.", "originalCommit": "023089909048230b211d7cb9e5f9cdee432ed411", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b30eac64df6b9b84617c270344b83f01e0664856", "url": "https://github.com/apache/flink/commit/b30eac64df6b9b84617c270344b83f01e0664856", "message": "[FLINK-20456][docs] Simplify streaming joins", "committedDate": "2020-12-09T18:35:09Z", "type": "forcePushed"}, {"oid": "a30b451e0c5559047ab676a1a7726d6d9311a736", "url": "https://github.com/apache/flink/commit/a30b451e0c5559047ab676a1a7726d6d9311a736", "message": "[FLINK-20456][docs] Remove negative language from index", "committedDate": "2020-12-09T18:36:22Z", "type": "commit"}, {"oid": "f4836b4e1db7889c1312a992c7ab115d6f1c969d", "url": "https://github.com/apache/flink/commit/f4836b4e1db7889c1312a992c7ab115d6f1c969d", "message": "[FLINK-20456][docs] Make dynamic tables sound less 'academic'", "committedDate": "2020-12-09T18:36:23Z", "type": "commit"}, {"oid": "21e3b44967c660f375c020c6a05bb17753664222", "url": "https://github.com/apache/flink/commit/21e3b44967c660f375c020c6a05bb17753664222", "message": "[FLINK-20456][docs] Remove deprecated APIs from time attributes page and put event time first", "committedDate": "2020-12-09T18:36:23Z", "type": "commit"}, {"oid": "8914fa4dbb3cc090ac540d7411e27b87460e9661", "url": "https://github.com/apache/flink/commit/8914fa4dbb3cc090ac540d7411e27b87460e9661", "message": "[FLINK-20456][docs] Reorder temporal table and join pages", "committedDate": "2020-12-09T18:36:23Z", "type": "commit"}, {"oid": "c8569c2a86c276de222d5da7a740ce7424389b0d", "url": "https://github.com/apache/flink/commit/c8569c2a86c276de222d5da7a740ce7424389b0d", "message": "[FLINK-20456][docs] Simplify temporal table definition", "committedDate": "2020-12-09T18:36:24Z", "type": "commit"}, {"oid": "c6ce81f794da8c8249aa4d0a9f52aa8730b6c45c", "url": "https://github.com/apache/flink/commit/c6ce81f794da8c8249aa4d0a9f52aa8730b6c45c", "message": "[FLINK-20456][docs] Simplify streaming joins", "committedDate": "2020-12-09T18:36:51Z", "type": "commit"}, {"oid": "c6ce81f794da8c8249aa4d0a9f52aa8730b6c45c", "url": "https://github.com/apache/flink/commit/c6ce81f794da8c8249aa4d0a9f52aa8730b6c45c", "message": "[FLINK-20456][docs] Simplify streaming joins", "committedDate": "2020-12-09T18:36:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY0MzU0NA==", "url": "https://github.com/apache/flink/pull/14292#discussion_r539643544", "bodyText": "The new file is versioned_tables (plural, with an 's'). But is this where the redirect should go, or should it go to legacy.md? Maybe it would make sense to leave it this way, redirecting to the page describing the new feature, but with a brief note on versioned_tables.md that mentions and links to the content in legacy.md.\nAnd in general, should we also create redirects for the zh versions as well?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            redirect: /dev/table/streaming/versioned_table.html\n          \n          \n            \n            redirect: /dev/table/streaming/versioned_tables.html", "author": "alpinegizmo", "createdAt": "2020-12-09T20:59:55Z", "path": "docs/redirects/temporal_table.md", "diffHunk": "@@ -0,0 +1,24 @@\n+---\n+title: Temporal Tables\n+layout: redirect\n+redirect: /dev/table/streaming/versioned_table.html", "originalCommit": "c6ce81f794da8c8249aa4d0a9f52aa8730b6c45c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY2MDQ3NQ==", "url": "https://github.com/apache/flink/pull/14292#discussion_r539660475", "bodyText": "Now that you say it, I think legacy makes more sense. That's the content people are expecting to find. And that page already cross-links to versioned_tables.\nThe zh redirect happens automatically", "author": "sjwiesman", "createdAt": "2020-12-09T21:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY0MzU0NA=="}], "type": "inlineReview"}]}