{"pr_number": 13915, "pr_title": "[FLINK-19365][hive] Migrate Hive source to FLIP-27 source interface f\u2026", "pr_createdAt": "2020-11-04T03:47:35Z", "pr_url": "https://github.com/apache/flink/pull/13915", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzA0Nw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083047", "bodyText": "@nullable Long limit", "author": "JingsongLi", "createdAt": "2020-11-04T03:50:46Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzYxMQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083611", "bodyText": "A RowType is enough", "author": "JingsongLi", "createdAt": "2020-11-04T03:53:36Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzY4MQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517083681", "bodyText": "InternalTypeInfo.of(rowType)\nBTW, why override this?", "author": "JingsongLi", "createdAt": "2020-11-04T03:53:59Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTc3OA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517149778", "bodyText": "Indeed it's not needed", "author": "lirui-apache", "createdAt": "2020-11-04T07:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4MzY4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDA3Mw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084073", "bodyText": "Why in here? Should in HiveSourceFileEnumerator", "author": "JingsongLi", "createdAt": "2020-11-04T03:55:33Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate static BulkFormat<RowData, HiveSourceSplit> createBulkFormat(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\t@Nullable int[] projectedFields,\n+\t\t\tString hiveVersion,\n+\t\t\tDataType producedDataType,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tlong limit) {\n+\t\tcheckNotNull(catalogTable, \"catalogTable can not be null.\");\n+\t\treturn new HiveBulkFormatAdapter(\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\tprojectedFields != null ? projectedFields : IntStream.range(0, catalogTable.getSchema().getFieldCount()).toArray(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedDataType,\n+\t\t\t\tuseMapRedReader,\n+\t\t\t\tlimit);\n+\t}\n+\n+\tpublic static List<HiveSourceSplit> createInputSplits(", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDI4Mg==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084282", "bodyText": "Already has producedType, I think projectedFields can be removed", "author": "JingsongLi", "createdAt": "2020-11-04T03:56:34Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1OTk4OA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517159988", "bodyText": "HiveMapredSplitReader still requires it. We can remove it when we refactor HiveMapredSplitReader", "author": "lirui-apache", "createdAt": "2020-11-04T08:09:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDI4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzNDc5Mg==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517234792", "bodyText": "I think you can create projectedFields from producedType easily.", "author": "JingsongLi", "createdAt": "2020-11-04T10:14:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDI4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDQzMw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084433", "bodyText": "Better in HiveSourceFileEnumerator", "author": "JingsongLi", "createdAt": "2020-11-04T03:57:17Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// DataType of the records to be returned, with projection applied (if any)\n+\tprivate final DataType producedDataType;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tint[] projectedFields,\n+\t\t\tlong limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tDataType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, projectedFields, hiveVersion, producedDataType, useMapRedReader, limit),\n+\t\t\t\tnull);\n+\t\tPreconditions.checkArgument(!isStreamingSource, \"HiveSource currently only supports bounded mode\");\n+\t\tthis.producedDataType = producedDataType;\n+\t}\n+\n+\t@Override\n+\tpublic SimpleVersionedSerializer<HiveSourceSplit> getSplitSerializer() {\n+\t\treturn HiveSourceSplitSerializer.INSTANCE;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate static BulkFormat<RowData, HiveSourceSplit> createBulkFormat(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\t@Nullable int[] projectedFields,\n+\t\t\tString hiveVersion,\n+\t\t\tDataType producedDataType,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tlong limit) {\n+\t\tcheckNotNull(catalogTable, \"catalogTable can not be null.\");\n+\t\treturn new HiveBulkFormatAdapter(\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\tprojectedFields != null ? projectedFields : IntStream.range(0, catalogTable.getSchema().getFieldCount()).toArray(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedDataType,\n+\t\t\t\tuseMapRedReader,\n+\t\t\t\tlimit);\n+\t}\n+\n+\tpublic static List<HiveSourceSplit> createInputSplits(\n+\t\t\tint minNumSplits,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\tJobConf jobConf) throws IOException {\n+\t\tList<HiveSourceSplit> hiveSplits = new ArrayList<>();\n+\t\tFileSystem fs = null;\n+\t\tfor (HiveTablePartition partition : partitions) {\n+\t\t\tStorageDescriptor sd = partition.getStorageDescriptor();\n+\t\t\tPath inputPath = new Path(sd.getLocation());\n+\t\t\tif (fs == null) {\n+\t\t\t\tfs = inputPath.getFileSystem(jobConf);\n+\t\t\t}\n+\t\t\t// it's possible a partition exists in metastore but the data has been removed\n+\t\t\tif (!fs.exists(inputPath)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tInputFormat format;\n+\t\t\ttry {\n+\t\t\t\tformat = (InputFormat)\n+\t\t\t\t\t\tClass.forName(sd.getInputFormat(), true, Thread.currentThread().getContextClassLoader()).newInstance();\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tthrow new FlinkHiveException(\"Unable to instantiate the hadoop input format\", e);\n+\t\t\t}\n+\t\t\tReflectionUtils.setConf(format, jobConf);\n+\t\t\tjobConf.set(INPUT_DIR, sd.getLocation());\n+\t\t\t//TODO: we should consider how to calculate the splits according to minNumSplits in the future.\n+\t\t\torg.apache.hadoop.mapred.InputSplit[] splitArray = format.getSplits(jobConf, minNumSplits);\n+\t\t\tfor (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) {\n+\t\t\t\tPreconditions.checkState(inputSplit instanceof FileSplit,\n+\t\t\t\t\t\t\"Unsupported InputSplit type: \" + inputSplit.getClass().getName());\n+\t\t\t\thiveSplits.add(new HiveSourceSplit((FileSplit) inputSplit, partition, null));\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn hiveSplits;\n+\t}\n+\n+\tpublic static int getNumFiles(List<HiveTablePartition> partitions, JobConf jobConf) throws IOException {", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDg2Ng==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517084866", "bodyText": "Why do you want to create a new FileSourceSplit? Why not just use HiveSourceSplit", "author": "JingsongLi", "createdAt": "2020-11-04T03:59:15Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceSplitSerializer.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.FileSourceSplitSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n+/**\n+ * SerDe for {@link HiveSourceSplit}.\n+ */\n+public class HiveSourceSplitSerializer implements SimpleVersionedSerializer<HiveSourceSplit> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tpublic static final HiveSourceSplitSerializer INSTANCE = new HiveSourceSplitSerializer();\n+\n+\tprivate HiveSourceSplitSerializer() {\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(HiveSourceSplit split) throws IOException {\n+\t\tcheckArgument(split.getClass() == HiveSourceSplit.class, \"Cannot serialize subclasses of HiveSourceSplit\");\n+\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {\n+\t\t\tserialize(outputStream, split);\n+\t\t}\n+\t\treturn byteArrayOutputStream.toByteArray();\n+\t}\n+\n+\t@Override\n+\tpublic HiveSourceSplit deserialize(int version, byte[] serialized) throws IOException {\n+\t\tif (version == 1) {\n+\t\t\ttry (ObjectInputStream inputStream = new ObjectInputStream(new ByteArrayInputStream(serialized))) {\n+\t\t\t\treturn deserializeV1(inputStream);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tthrow new IOException(\"Unknown version: \" + version);\n+\t\t}\n+\t}\n+\n+\tprivate void serialize(ObjectOutputStream outputStream, HiveSourceSplit split) throws IOException {\n+\t\tbyte[] superBytes = FileSourceSplitSerializer.INSTANCE.serialize(new FileSourceSplit(", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2MzM4Nw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517163387", "bodyText": "I think it's better to reuse FileSourceSplitSerializer to serialize the super class, but FileSourceSplitSerializer doesn't accept sub-classes.\nOr we can just call InstantiationUtil.serializeObject to serialize the whole object. WDYT?", "author": "lirui-apache", "createdAt": "2020-11-04T08:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzNTMyMw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517235323", "bodyText": "but FileSourceSplitSerializer doesn't accept sub-classes.\n\nOh, that is.", "author": "JingsongLi", "createdAt": "2020-11-04T10:15:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI0MTM1MA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517241350", "bodyText": "I am OK to keep this as it is.", "author": "JingsongLi", "createdAt": "2020-11-04T10:25:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NDg2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NTUxMw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517085513", "bodyText": "Maybe use InstantiationUtil.serializeObject directly?", "author": "JingsongLi", "createdAt": "2020-11-04T04:01:56Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceSplitSerializer.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.FileSourceSplitSerializer;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n+/**\n+ * SerDe for {@link HiveSourceSplit}.\n+ */\n+public class HiveSourceSplitSerializer implements SimpleVersionedSerializer<HiveSourceSplit> {\n+\n+\tprivate static final int VERSION = 1;\n+\n+\tpublic static final HiveSourceSplitSerializer INSTANCE = new HiveSourceSplitSerializer();\n+\n+\tprivate HiveSourceSplitSerializer() {\n+\t}\n+\n+\t@Override\n+\tpublic int getVersion() {\n+\t\treturn VERSION;\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(HiveSourceSplit split) throws IOException {\n+\t\tcheckArgument(split.getClass() == HiveSourceSplit.class, \"Cannot serialize subclasses of HiveSourceSplit\");\n+\n+\t\tByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\t\ttry (ObjectOutputStream outputStream = new ObjectOutputStream(byteArrayOutputStream)) {", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4ODAwMA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517088000", "bodyText": "Ditto", "author": "JingsongLi", "createdAt": "2020-11-04T04:13:50Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4OTY2Mg==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517089662", "bodyText": "Why to toMapRedSplit, call another constructor?", "author": "JingsongLi", "createdAt": "2020-11-04T04:21:45Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveSourceSplit.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.CheckpointedPosition;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.Path;\n+\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A wrapper class that wraps info needed for a file input split.\n+ * Right now, it contains info about the partition of the split.\n+ */\n+public class HiveSourceSplit extends FileSourceSplit {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprotected final HiveTablePartition hiveTablePartition;\n+\n+\tpublic HiveSourceSplit(\n+\t\t\tFileSplit fileSplit,\n+\t\t\tHiveTablePartition hiveTablePartition,\n+\t\t\t@Nullable CheckpointedPosition readerPosition) throws IOException {\n+\t\tthis(\n+\t\t\t\tfileSplit.toString(),\n+\t\t\t\tnew Path(fileSplit.getPath().toString()),\n+\t\t\t\tfileSplit.getStart(),\n+\t\t\t\tfileSplit.getLength(),\n+\t\t\t\tfileSplit.getLocations(),\n+\t\t\t\treaderPosition,\n+\t\t\t\thiveTablePartition\n+\t\t);\n+\t}\n+\n+\tpublic HiveSourceSplit(\n+\t\t\tString id,\n+\t\t\tPath filePath,\n+\t\t\tlong offset,\n+\t\t\tlong length,\n+\t\t\tString[] hostnames,\n+\t\t\t@Nullable CheckpointedPosition readerPosition,\n+\t\t\tHiveTablePartition hiveTablePartition) {\n+\t\tsuper(id, filePath, offset, length, hostnames, readerPosition);\n+\t\tthis.hiveTablePartition = checkNotNull(hiveTablePartition, \"hiveTablePartition can not be null\");\n+\t}\n+\n+\tpublic HiveTablePartition getHiveTablePartition() {\n+\t\treturn hiveTablePartition;\n+\t}\n+\n+\tpublic FileSplit toMapRedSplit() {\n+\t\treturn new FileSplit(new org.apache.hadoop.fs.Path(path().toString()), offset(), length(), hostnames());\n+\t}\n+\n+\t@Override\n+\tpublic FileSourceSplit updateWithCheckpointedPosition(@Nullable CheckpointedPosition position) {\n+\t\ttry {\n+\t\t\treturn new HiveSourceSplit(toMapRedSplit(), hiveTablePartition, position);", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzExMDM1MA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517110350", "bodyText": "This will fail when partition values can not be found in path.\nI create #13919 to refactor this.", "author": "JingsongLi", "createdAt": "2020-11-04T05:50:03Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\t// TODO: need a way to support limit push down\n+\t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t(RowType) producedDataType.getLogicalType(),\n+\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\tjobConfWrapper.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal),\n+\t\t\t\t\t(PartitionValueConverter) (colName, valStr, type) -> split.getHiveTablePartition().getPartitionSpec().get(colName),", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzExMjIwMQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517112201", "bodyText": "If you don't want migrate HiveMapredSplitReader now, can you create JIRA for this?", "author": "JingsongLi", "createdAt": "2020-11-04T05:57:07Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\t// indices of fields to be returned, with projection applied (if any)\n+\tprivate final int[] selectedFields;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final DataType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, -1 represents no limit.\n+\tprivate final long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tint[] selectedFields, String hiveVersion, DataType producedDataType, boolean useMapRedReader, long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.selectedFields = selectedFields;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\t// TODO: need a way to support limit push down\n+\t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t(RowType) producedDataType.getLogicalType(),\n+\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\tjobConfWrapper.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal),\n+\t\t\t\t\t(PartitionValueConverter) (colName, valStr, type) -> split.getHiveTablePartition().getPartitionSpec().get(colName),\n+\t\t\t\t\tDEFAULT_SIZE,\n+\t\t\t\t\thiveVersion.startsWith(\"3\"),\n+\t\t\t\t\tfalse\n+\t\t\t);\n+\t\t} else {\n+\t\t\treturn new HiveMapRedBulkFormat();\n+\t\t}\n+\t}\n+\n+\tprivate boolean useParquetVectorizedRead(HiveTablePartition partition) {\n+\t\tboolean isParquet = partition.getStorageDescriptor().getSerdeInfo().getSerializationLib()\n+\t\t\t\t.toLowerCase().contains(\"parquet\");\n+\t\tif (!isParquet) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tfor (int i : selectedFields) {\n+\t\t\tif (isVectorizationUnsupported(fieldTypes[i].getLogicalType())) {\n+\t\t\t\tLOG.info(\"Fallback to hadoop mapred reader, unsupported field type: \" + fieldTypes[i]);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\tLOG.info(\"Use flink parquet ColumnarRowData reader.\");\n+\t\treturn true;\n+\t}\n+\n+\tprivate static boolean isVectorizationUnsupported(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\tcase ARRAY:\n+\t\t\tcase MULTISET:\n+\t\t\tcase MAP:\n+\t\t\tcase ROW:\n+\t\t\tcase DISTINCT_TYPE:\n+\t\t\tcase STRUCTURED_TYPE:\n+\t\t\tcase NULL:\n+\t\t\tcase RAW:\n+\t\t\tcase SYMBOL:\n+\t\t\tdefault:\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\tprivate class HiveMapRedBulkFormat implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\t\tthrows IOException {\n+\t\t\treturn new HiveReader(split);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\t\tassert split.getReaderPosition().isPresent();\n+\t\t\tHiveReader hiveReader = new HiveReader(split);\n+\t\t\thiveReader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\t\treturn hiveReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isSplittable() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic TypeInformation<RowData> getProducedType() {\n+\t\t\treturn (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(producedDataType);\n+\t\t}\n+\t}\n+\n+\tprivate class HiveReader implements BulkFormat.Reader<RowData> {\n+\n+\t\tprivate final HiveMapredSplitReader hiveMapredSplitReader;\n+\t\tprivate final RowDataSerializer serializer;\n+\t\tprivate long numRead = 0;\n+\n+\t\tprivate HiveReader(HiveSourceSplit split) throws IOException {\n+\t\t\tJobConf clonedConf = new JobConf(jobConfWrapper.conf());\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tHiveTableInputSplit oldSplit = new HiveTableInputSplit(-1, split.toMapRedSplit(), clonedConf, split.getHiveTablePartition());\n+\t\t\thiveMapredSplitReader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, oldSplit, hiveShim);", "originalCommit": "197131cd8bb420b5914d07af898afd04089af554", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxMDg2OQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517210869", "bodyText": "Just created https://issues.apache.org/jira/browse/FLINK-19965 for that", "author": "lirui-apache", "createdAt": "2020-11-04T09:36:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzExMjIwMQ=="}], "type": "inlineReview"}, {"oid": "6ed0a96f5a7cefad7260b8549cac601cc8cdd376", "url": "https://github.com/apache/flink/commit/6ed0a96f5a7cefad7260b8549cac601cc8cdd376", "message": "[FLINK-19365][hive] Migrate Hive source to FLIP-27 source interface for batch", "committedDate": "2020-11-04T11:18:14Z", "type": "commit"}, {"oid": "fe892ec28edc2ec9710fa25a8acc96c195a8a524", "url": "https://github.com/apache/flink/commit/fe892ec28edc2ec9710fa25a8acc96c195a8a524", "message": "address comments", "committedDate": "2020-11-04T11:18:15Z", "type": "commit"}, {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "url": "https://github.com/apache/flink/commit/54abdd1fb2ee2b4c478366d013c02631a559ac58", "message": "address comments", "committedDate": "2020-11-04T12:03:15Z", "type": "commit"}, {"oid": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "url": "https://github.com/apache/flink/commit/54abdd1fb2ee2b4c478366d013c02631a559ac58", "message": "address comments", "committedDate": "2020-11-04T12:03:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDc4MA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517740780", "bodyText": "I think you should use LimitableBulkFormat here.", "author": "JingsongLi", "createdAt": "2020-11-05T02:11:21Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.List;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\t@Nullable Long limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tRowType producedDataType) {\n+\t\tsuper(\n+\t\t\t\tnew org.apache.flink.core.fs.Path[1],\n+\t\t\t\tnew HiveSourceFileEnumerator.Provider(partitions, new JobConfWrapper(jobConf)),\n+\t\t\t\tDEFAULT_SPLIT_ASSIGNER,\n+\t\t\t\tcreateBulkFormat(new JobConf(jobConf), catalogTable, hiveVersion, producedDataType, useMapRedReader, limit),", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MDg4MQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517740881", "bodyText": "Can you change limit.intValue() / 1000 to (int) (limit / 1000)", "author": "JingsongLi", "createdAt": "2020-11-05T02:11:46Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveParallelismInference.java", "diffHunk": "@@ -57,9 +57,9 @@\n \t * Apply limit to calculate the parallelism.\n \t * Here limit is the limit in query <code>SELECT * FROM xxx LIMIT [limit]</code>.\n \t */\n-\tint limit(long limit) {\n-\t\tif (limit > 0) {\n-\t\t\tparallelism = Math.min(parallelism, (int) limit / 1000);\n+\tint limit(Long limit) {\n+\t\tif (limit != null) {\n+\t\t\tparallelism = Math.min(parallelism, limit.intValue() / 1000);", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTA2NQ==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741065", "bodyText": "Remove limit things in this class", "author": "JingsongLi", "createdAt": "2020-11-05T02:12:23Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, null represents no limit.\n+\tprivate final Long limit;", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTIyOA==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741228", "bodyText": "producedRowType", "author": "JingsongLi", "createdAt": "2020-11-05T02:12:46Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0MTQzMg==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517741432", "bodyText": "producedRowType", "author": "JingsongLi", "createdAt": "2020-11-05T02:13:03Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSource.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.connector.file.src.AbstractFileSource;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter;\n+import org.apache.flink.connectors.hive.read.HiveSourceSplit;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.List;\n+\n+import static org.apache.flink.connector.file.src.FileSource.DEFAULT_SPLIT_ASSIGNER;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * A unified data source that reads a hive table.\n+ */\n+public class HiveSource extends AbstractFileSource<RowData, HiveSourceSplit> implements ResultTypeQueryable<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tHiveSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tList<HiveTablePartition> partitions,\n+\t\t\t@Nullable Long limit,\n+\t\t\tString hiveVersion,\n+\t\t\tboolean useMapRedReader,\n+\t\t\tboolean isStreamingSource,\n+\t\t\tRowType producedDataType) {", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0Mjk3Mw==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517742973", "bodyText": "Please remove limit things", "author": "JingsongLi", "createdAt": "2020-11-05T02:15:07Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.LimitableBulkFormat;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.RowDataSerializer;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * A BulkFormat implementation for HiveSource. This implementation delegates reading to other BulkFormat instances,\n+ * because different hive partitions may need different BulkFormat to do the reading.\n+ */\n+public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveBulkFormatAdapter.class);\n+\n+\t// schema evolution configs are not available in older versions of IOConstants, let's define them ourselves\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS = \"schema.evolution.columns\";\n+\tprivate static final String SCHEMA_EVOLUTION_COLUMNS_TYPES = \"schema.evolution.columns.types\";\n+\n+\tprivate final JobConfWrapper jobConfWrapper;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String hiveVersion;\n+\tprivate final HiveShim hiveShim;\n+\tprivate final RowType producedDataType;\n+\tprivate final boolean useMapRedReader;\n+\t// We should limit the input read count of the splits, null represents no limit.\n+\tprivate final Long limit;\n+\n+\tpublic HiveBulkFormatAdapter(JobConfWrapper jobConfWrapper, List<String> partitionKeys, String[] fieldNames, DataType[] fieldTypes,\n+\t\t\tString hiveVersion, RowType producedDataType, boolean useMapRedReader, Long limit) {\n+\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.hiveVersion = hiveVersion;\n+\t\tthis.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\tthis.producedDataType = producedDataType;\n+\t\tthis.useMapRedReader = useMapRedReader;\n+\t\tthis.limit = limit;\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\tthrows IOException {\n+\t\treturn createBulkFormatForSplit(split).createReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\treturn createBulkFormatForSplit(split).restoreReader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(producedDataType);\n+\t}\n+\n+\tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n+\t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\tPartitionFieldExtractor<HiveSourceSplit> extractor = (PartitionFieldExtractor<HiveSourceSplit>)\n+\t\t\t\t\t(split1, fieldName, fieldType) -> split1.getHiveTablePartition().getPartitionSpec().get(fieldName);\n+\t\t\treturn LimitableBulkFormat.create(\n+\t\t\t\t\tParquetColumnarRowInputFormat.createPartitionedFormat(\n+\t\t\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\t\t\tproducedDataType,\n+\t\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\t\textractor,\n+\t\t\t\t\t\t\tDEFAULT_SIZE,\n+\t\t\t\t\t\t\thiveVersion.startsWith(\"3\"),\n+\t\t\t\t\t\t\tfalse),\n+\t\t\t\t\tlimit);\n+\t\t} else {\n+\t\t\treturn new HiveMapRedBulkFormat();\n+\t\t}\n+\t}\n+\n+\tprivate boolean useParquetVectorizedRead(HiveTablePartition partition) {\n+\t\tboolean isParquet = partition.getStorageDescriptor().getSerdeInfo().getSerializationLib()\n+\t\t\t\t.toLowerCase().contains(\"parquet\");\n+\t\tif (!isParquet) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tfor (RowType.RowField field : producedDataType.getFields()) {\n+\t\t\tif (isVectorizationUnsupported(field.getType())) {\n+\t\t\t\tLOG.info(\"Fallback to hadoop mapred reader, unsupported field type: \" + field.getType());\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\tLOG.info(\"Use flink parquet ColumnarRowData reader.\");\n+\t\treturn true;\n+\t}\n+\n+\tprivate static boolean isVectorizationUnsupported(LogicalType t) {\n+\t\tswitch (t.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\tcase DECIMAL:\n+\t\t\tcase TINYINT:\n+\t\t\tcase SMALLINT:\n+\t\t\tcase INTEGER:\n+\t\t\tcase BIGINT:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase DATE:\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\treturn false;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\tcase ARRAY:\n+\t\t\tcase MULTISET:\n+\t\t\tcase MAP:\n+\t\t\tcase ROW:\n+\t\t\tcase DISTINCT_TYPE:\n+\t\t\tcase STRUCTURED_TYPE:\n+\t\t\tcase NULL:\n+\t\t\tcase RAW:\n+\t\t\tcase SYMBOL:\n+\t\t\tdefault:\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\tprivate class HiveMapRedBulkFormat implements BulkFormat<RowData, HiveSourceSplit> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> createReader(Configuration config, HiveSourceSplit split)\n+\t\t\t\tthrows IOException {\n+\t\t\treturn new HiveReader(split);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Reader<RowData> restoreReader(Configuration config, HiveSourceSplit split) throws IOException {\n+\t\t\tassert split.getReaderPosition().isPresent();\n+\t\t\tHiveReader hiveReader = new HiveReader(split);\n+\t\t\thiveReader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\t\treturn hiveReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isSplittable() {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic TypeInformation<RowData> getProducedType() {\n+\t\t\treturn InternalTypeInfo.of(producedDataType);\n+\t\t}\n+\t}\n+\n+\tprivate class HiveReader implements BulkFormat.Reader<RowData> {\n+\n+\t\tprivate final HiveMapredSplitReader hiveMapredSplitReader;\n+\t\tprivate final RowDataSerializer serializer;\n+\t\tprivate final ArrayResultIterator<RowData> iterator = new ArrayResultIterator<>();\n+\t\tprivate final int[] selectedFields;\n+\t\tprivate long numRead = 0;\n+\n+\t\tprivate HiveReader(HiveSourceSplit split) throws IOException {\n+\t\t\tselectedFields = computeSelectedFields();\n+\t\t\tJobConf clonedConf = new JobConf(jobConfWrapper.conf());\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tHiveTableInputSplit oldSplit = new HiveTableInputSplit(-1, split.toMapRedSplit(), clonedConf, split.getHiveTablePartition());\n+\t\t\thiveMapredSplitReader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, oldSplit, hiveShim);\n+\t\t\tserializer = new RowDataSerializer(producedDataType);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> readBatch() throws IOException {\n+\t\t\tRowData[] records = new RowData[DEFAULT_SIZE];\n+\t\t\tfinal long skipCount = numRead;\n+\t\t\tint num = 0;\n+\t\t\twhile (!hiveMapredSplitReader.reachedEnd() && num < DEFAULT_SIZE && !reachLimit()) {\n+\t\t\t\trecords[num++] = serializer.copy(nextRecord());\n+\t\t\t}\n+\t\t\tif (num == 0) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\titerator.set(records, num, NO_OFFSET, skipCount);\n+\t\t\treturn iterator;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\thiveMapredSplitReader.close();\n+\t\t}\n+\n+\t\tprivate RowData nextRecord() throws IOException {\n+\t\t\tRowData res = hiveMapredSplitReader.nextRecord(null);\n+\t\t\tnumRead++;\n+\t\t\treturn res;\n+\t\t}\n+\n+\t\tprivate boolean reachLimit() {\n+\t\t\treturn limit != null && numRead >= limit;", "originalCommit": "54abdd1fb2ee2b4c478366d013c02631a559ac58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a0ef5815192defc2fc93df716e73d8a8410367fd", "url": "https://github.com/apache/flink/commit/a0ef5815192defc2fc93df716e73d8a8410367fd", "message": "address comments", "committedDate": "2020-11-05T02:45:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc2NzIyNg==", "url": "https://github.com/apache/flink/pull/13915#discussion_r517767226", "bodyText": "We should use SourceProvider instead of DataStream", "author": "JingsongLi", "createdAt": "2020-11-05T03:10:36Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -180,15 +182,26 @@ private boolean isStreamingSource() {\n \t}\n \n \tprivate DataStream<RowData> createBatchSource(StreamExecutionEnvironment execEnv,\n-\t\t\tTypeInformation<RowData> typeInfo, HiveTableInputFormat inputFormat) {\n-\t\tDataStreamSource<RowData> source = execEnv.createInput(inputFormat, typeInfo);\n+\t\t\tList<HiveTablePartition> allHivePartitions) {\n+\t\tHiveSource hiveSource = new HiveSource(", "originalCommit": "a0ef5815192defc2fc93df716e73d8a8410367fd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}