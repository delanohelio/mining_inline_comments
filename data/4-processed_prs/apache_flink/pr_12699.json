{"pr_number": 12699, "pr_title": "[FLINK-18349][docs] Add release notes for Flink 1.11", "pr_createdAt": "2020-06-17T17:22:13Z", "pr_url": "https://github.com/apache/flink/pull/12699", "timeline": [{"oid": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "url": "https://github.com/apache/flink/commit/9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-17T17:25:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMTY0Mw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442011643", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n          \n          \n            \n            ####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))", "author": "aljoscha", "createdAt": "2020-06-18T07:06:17Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMjEyOA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442012128", "bodyText": "Yes? \ud83d\ude05", "author": "aljoscha", "createdAt": "2020-06-18T07:07:24Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNjU3NQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442106575", "bodyText": "Of course I forgot to delete my template line :)", "author": "pnowojski", "createdAt": "2020-06-18T09:50:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMjEyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMzUxNA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442013514", "bodyText": "I think you cannot use Kafka connectors compiled against an older Flink version. This is mentioned in the release notes of https://issues.apache.org/jira/browse/FLINK-17376, which is not yet added here.", "author": "aljoscha", "createdAt": "2020-06-18T07:10:32Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. \n+We will also remove it from UI in a follow-up separate ticket future.\n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. ", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwODUyMA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442108520", "bodyText": "@zentol can you clarify this? This comes directly from https://issues.apache.org/jira/browse/FLINK-15115", "author": "pnowojski", "createdAt": "2020-06-18T09:53:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMzUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MDQzNw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442970437", "bodyText": "I assumed that the connectors work against non-deprecated @public APIs, which according to our contracts are supposed to work until 2.0.0 . If that is not the case, then we have to adjust the release notes naturally.", "author": "zentol", "createdAt": "2020-06-19T17:45:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMzUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIxNTA3Nw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r444215077", "bodyText": "I remove the second half of the sentence from https://issues.apache.org/jira/browse/FLINK-15115. We should update also the text here.\nSide note, I remember two or three occurrences of this by now, where a @Public class uses internally APIs that are not @Public. We're currently not verifying this. We would have to transitively check if any \"public\" code uses purely public code. I can start a ML discussion about this, at least to bring some awareness.", "author": "aljoscha", "createdAt": "2020-06-23T13:17:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMzUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3MzEyNg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445373126", "bodyText": "Should we maybe add that they have been removed from the repository?", "author": "tillrohrmann", "createdAt": "2020-06-25T07:53:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxMzUxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxNDA1Nw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442014057", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            #### Removal of eprecated TimestampExtractor ([FLINK-17655](https://issues.apache.org/jira/browse/FLINK-17655))\n          \n          \n            \n            #### Removal of deprecated TimestampExtractor ([FLINK-17655](https://issues.apache.org/jira/browse/FLINK-17655))", "author": "aljoscha", "createdAt": "2020-06-18T07:11:39Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. \n+We will also remove it from UI in a follow-up separate ticket future.\n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+Prior version of these connectors will continue to work with Flink.\n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `{MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). \n+`MailboxExecutor#yield` or `MailboxExecutor#tryYield` methods can be used for actions that should give control to other actions temporarily (equivalent of `StreamTask#getCheckpointLock().wait()`), if the current operator is blocked. \n+`MailboxExecutor` can be accessed by using `YieldingOperatorFactory`. Example usage can be found in the `AsyncWaitOperator`.\n+\n+Note, `SourceFunction.SourceContext.getCheckpointLock` is still available for custom implementations of `SourceFunction` interface. \n+\n+#### Reversed dependency from flink-streaming-java to flink-client ([FLINK-15090](https://issues.apache.org/jira/browse/FLINK-15090))\n+Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients anymore`. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.\n+\n+#### AsyncWaitOperator is chainable again ([FLINK-16219](https://issues.apache.org/jira/browse/FLINK-16219))\n+`AsyncWaitOperator` will be allowed to be chained by default with all operators, except of tasks with `SourceFunction`.\n+This mostly revert limitation introduced as a bug fix for https://issues.apache.org/jira/browse/FLINK-13063.\n+\n+#### Changed argument types of ShuffleEnvironment#createInputGates and #createResultPartitionWriters methods ([FLINK-16586](https://issues.apache.org/jira/browse/FLINK-16586))\n+The argument type of methods `ShuffleEnvironment#createInputGates` and `#createResultPartitionWriters` are adjusted from `Collection` to `List` for satisfying the order guarantee requirement in unaligned checkpoint.\n+It will break the compatibility if users already implemented a custom `ShuffleService` based on `ShuffleServiceFactory` interface.\n+\n+#### Deprecation of CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible ([FLINK-17520](https://issues.apache.org/jira/browse/FLINK-17520))\n+The `boolean isOuterSnapshotCompatible(TypeSerializer)` on the `CompositeTypeSerializerSnapshot` class has been deprecated, in favor of a new `OuterSchemaCompatibility resolveOuterSchemaCompatibility(TypeSerializer)` method.\n+Please implement that instead.\n+Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.\n+\n+#### Removal of eprecated TimestampExtractor ([FLINK-17655](https://issues.apache.org/jira/browse/FLINK-17655))", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAxNjg3Ng==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442016876", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            #### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n          \n          \n            \n            FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n          \n          \n            \n            #### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))", "author": "aljoscha", "createdAt": "2020-06-18T07:17:29Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzMjQyNg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442032426", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The existing `StreamTableSink` implementations should remove emitDataStream method.\n          \n          \n            \n            The existing `StreamTableSink` implementations should remove `emitDataStream` method.", "author": "tillrohrmann", "createdAt": "2020-06-18T07:46:36Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNDAxNA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442034014", "bodyText": "Should this sentence say: If the provided precision is invalid/not supported, then a default value will be used?", "author": "tillrohrmann", "createdAt": "2020-06-18T07:49:32Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. ", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMDY3Nw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442110677", "bodyText": "I think not really. I rephrased it to:\n\nHowever in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.", "author": "pnowojski", "createdAt": "2020-06-18T09:57:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNDAxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNjU0Mw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442036543", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n          \n          \n            \n            Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side.", "author": "tillrohrmann", "createdAt": "2020-06-18T07:53:58Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. ", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNzE0OA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442037148", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            But this info in web UI still exists and always shows 0 now. \n          \n          \n            \n            The web UI still displays this value but it is always `0` now.", "author": "tillrohrmann", "createdAt": "2020-06-18T07:55:03Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. ", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNzMwNg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442037306", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We will also remove it from UI in a follow-up separate ticket future.", "author": "tillrohrmann", "createdAt": "2020-06-18T07:55:20Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. \n+We will also remove it from UI in a follow-up separate ticket future.", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzOTg0Mg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442039842", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients anymore`. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.\n          \n          \n            \n            Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients` anymore. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.", "author": "tillrohrmann", "createdAt": "2020-06-18T07:59:36Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. \n+We will also remove it from UI in a follow-up separate ticket future.\n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+Prior version of these connectors will continue to work with Flink.\n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `{MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). \n+`MailboxExecutor#yield` or `MailboxExecutor#tryYield` methods can be used for actions that should give control to other actions temporarily (equivalent of `StreamTask#getCheckpointLock().wait()`), if the current operator is blocked. \n+`MailboxExecutor` can be accessed by using `YieldingOperatorFactory`. Example usage can be found in the `AsyncWaitOperator`.\n+\n+Note, `SourceFunction.SourceContext.getCheckpointLock` is still available for custom implementations of `SourceFunction` interface. \n+\n+#### Reversed dependency from flink-streaming-java to flink-client ([FLINK-15090](https://issues.apache.org/jira/browse/FLINK-15090))\n+Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients anymore`. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MDE5NA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442040194", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This mostly revert limitation introduced as a bug fix for https://issues.apache.org/jira/browse/FLINK-13063.\n          \n          \n            \n            This mostly revert limitation introduced as a bug fix for [FLINK-13063](https://issues.apache.org/jira/browse/FLINK-13063).", "author": "tillrohrmann", "createdAt": "2020-06-18T08:00:11Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However, the precision provided by users takes no effect but a default value will always be used. \n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Bla ([FLINK-](https://issues.apache.org/jira/browse/FLINK-))\n+\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric of `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any following data after barrier until alignment on downstream side. \n+But this info in web UI still exists and always shows 0 now. \n+We will also remove it from UI in a follow-up separate ticket future.\n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+Prior version of these connectors will continue to work with Flink.\n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `{MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). \n+`MailboxExecutor#yield` or `MailboxExecutor#tryYield` methods can be used for actions that should give control to other actions temporarily (equivalent of `StreamTask#getCheckpointLock().wait()`), if the current operator is blocked. \n+`MailboxExecutor` can be accessed by using `YieldingOperatorFactory`. Example usage can be found in the `AsyncWaitOperator`.\n+\n+Note, `SourceFunction.SourceContext.getCheckpointLock` is still available for custom implementations of `SourceFunction` interface. \n+\n+#### Reversed dependency from flink-streaming-java to flink-client ([FLINK-15090](https://issues.apache.org/jira/browse/FLINK-15090))\n+Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients anymore`. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.\n+\n+#### AsyncWaitOperator is chainable again ([FLINK-16219](https://issues.apache.org/jira/browse/FLINK-16219))\n+`AsyncWaitOperator` will be allowed to be chained by default with all operators, except of tasks with `SourceFunction`.\n+This mostly revert limitation introduced as a bug fix for https://issues.apache.org/jira/browse/FLINK-13063.", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA4NzA0MQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442087041", "bodyText": "We will need to add the below lines into release note if RC3 is produced:\nRemoval of deprecated OptionsFactory and ConfigurableOptionsFactory classes (FLINK-18242)\nThe deprecated OptionsFactory and ConfigurableOptionsFactory classes have been removed, please use RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory instead. Please also recompile your application codes if any class extending DefaultConfigurableOptionsFactory.", "author": "carp84", "createdAt": "2020-06-18T09:16:29Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,220 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Changed packages of `TableEnvironment` ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+FLINK-15947\tFinish moving scala expression DSL to flink-table-api-scala\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove emitDataStream method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal if deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`. \n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface. \n+Implementors of custom `StateBackend` should adjust their implementations.\n+", "originalCommit": "9e0ef3ed99a12bef53da6132c0dcd8e545af7351", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMzY2NQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442113665", "bodyText": "I've added those lines to be in sync with the code.", "author": "pnowojski", "createdAt": "2020-06-18T10:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA4NzA0MQ=="}], "type": "inlineReview"}, {"oid": "385b140fa6a22f8c9b74d99b7897d77e0e7ef0cd", "url": "https://github.com/apache/flink/commit/385b140fa6a22f8c9b74d99b7897d77e0e7ef0cd", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-18T10:02:15Z", "type": "forcePushed"}, {"oid": "1270ea15515812cdbff794b2a1345e425b3aa30d", "url": "https://github.com/apache/flink/commit/1270ea15515812cdbff794b2a1345e425b3aa30d", "message": "Added missing release notes", "committedDate": "2020-06-18T10:43:11Z", "type": "forcePushed"}, {"oid": "2a876970e69c0f14f31125441e92432aab3ac817", "url": "https://github.com/apache/flink/commit/2a876970e69c0f14f31125441e92432aab3ac817", "message": "Added missing release notes", "committedDate": "2020-06-18T10:45:32Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxNjUzMA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442616530", "bodyText": "anounced -> announced", "author": "zhijiangW", "createdAt": "2020-06-19T03:50:39Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxODg0Mw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442618843", "bodyText": "it's ->its", "author": "zhijiangW", "createdAt": "2020-06-19T04:01:34Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. \n+The web UI still displays this value but it is always `0` now. \n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+Prior version of these connectors will continue to work with Flink.\n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters.", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTMwMw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442619303", "bodyText": "remove {", "author": "zhijiangW", "createdAt": "2020-06-19T04:03:52Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. \n+The web UI still displays this value but it is always `0` now. \n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+Prior version of these connectors will continue to work with Flink.\n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail it's Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `{MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). ", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4NzA4Mg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442687082", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            \n          \n          \n            \n            #### Flink Docker Integration Improvements\n          \n          \n            \n            \n          \n          \n            \n            The examples of `Dockerfiles` and docker image `build.sh` scripts have been removed from [the Flink Github repository](https://github.com/apache/flink). The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:\n          \n          \n            \n            - `flink-contrib/docker-flink`\n          \n          \n            \n            - `flink-container/docker`\n          \n          \n            \n            - `flink-container/kubernetes`\n          \n          \n            \n            \n          \n          \n            \n            Check the updated user documentation for [Flink Docker integration](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) instead. It now describes in detail how to [use](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-a-flink-image) and [customize](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#customize-flink-image) [the Flink official docker image](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#docker-hub-flink-images): configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:\n          \n          \n            \n            - [docker run](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-flink-image)\n          \n          \n            \n            - [docker compose](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-compose)\n          \n          \n            \n            - [docker swarm](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-swarm)\n          \n          \n            \n            - [standalone Kubernetes](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html)", "author": "azagrebin", "createdAt": "2020-06-19T07:47:56Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MDM0OA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442690348", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Memory Management\n          \n          \n            \n            ### Memory Management\n          \n          \n            \n            \n          \n          \n            \n            #### New Flink Master Memory Model\n          \n          \n            \n            \n          \n          \n            \n            ##### Overview\n          \n          \n            \n            \n          \n          \n            \n            With [FLIP-116](https://cwiki.apache.org/confluence/display/FLINK/FLIP-116%3A+Unified+Memory+Configuration+for+Job+Managers), a new memory model has been introduced for the Flink Master. New configuration options have been introduced to control the memory consumption of the Flink Master process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.\n          \n          \n            \n            \n          \n          \n            \n            Please, check the user documentation for [more details](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html).\n          \n          \n            \n            \n          \n          \n            \n            If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. See also [the migration guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#migrate-job-manager-memory-configuration).\n          \n          \n            \n            \n          \n          \n            \n            ##### Deprecation and breaking changes\n          \n          \n            \n            \n          \n          \n            \n            The following options are deprecated:\n          \n          \n            \n             * `jobmanager.heap.size`\n          \n          \n            \n             * `jobmanager.heap.mb`\n          \n          \n            \n            \n          \n          \n            \n            If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:\n          \n          \n            \n             * [JVM Heap](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-jvm-heap) ([`jobmanager.memory.heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-heap-size)) for standalone and Mesos deployments\n          \n          \n            \n             * [Total Process Memory](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-total-memory) ([`jobmanager.memory.process.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-process-size)) for containerized deployments (Kubernetes and Yarn)\n          \n          \n            \n            \n          \n          \n            \n            The following options have been removed and have no effect anymore:\n          \n          \n            \n             * `containerized.heap-cutoff-ratio`\n          \n          \n            \n             * `containerized.heap-cutoff-min`\n          \n          \n            \n            \n          \n          \n            \n            There is [no container cut-off](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#container-cut-off-memory) anymore.\n          \n          \n            \n            \n          \n          \n            \n            ##### JVM arguments\n          \n          \n            \n            \n          \n          \n            \n            The `direct` and `metaspace` memory of the Flink Master's JVM process are now limited by configurable values:\n          \n          \n            \n             * [`jobmanager.memory.off-heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-off-heap-size)\n          \n          \n            \n             * [`jobmanager.memory.jvm-metaspace.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-jvm-metaspace-size)\n          \n          \n            \n            \n          \n          \n            \n            See also [JVM Parameters](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html#jvm-parameters).\n          \n          \n            \n            \n          \n          \n            \n            <span class=\"label label-warning\">Attention</span> These new limits can produce the respective `OutOfMemoryError` exceptions if they are not configured properly or there is a respective memory leak. See also [the troubleshooting guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_trouble.html#outofmemoryerror-direct-buffer-memory).", "author": "azagrebin", "createdAt": "2020-06-19T07:54:47Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MDk2Mg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442970962", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n          \n          \n            \n            Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts need to rename that file.\n          \n      \n    \n    \n  \n\nIn accordance with the documentation guide.", "author": "zentol", "createdAt": "2020-06-19T17:46:28Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MTE1MA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442971150", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And you do not convert to/from DataStream switch to:\n          \n          \n            \n            And you do not convert to/from DataStream, switch to:", "author": "zentol", "createdAt": "2020-06-19T17:46:57Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MTIwMg==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442971202", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If you do convert to/from DataStream/DataSet change your imports to one of:\n          \n          \n            \n            If you do convert to/from DataStream/DataSet, change your imports to one of:", "author": "zentol", "createdAt": "2020-06-19T17:47:03Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MTI3Mw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442971273", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n          \n          \n            \n            Additionally, if you use Scala's implicit conversions to/from DataStream/DataSet, import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`", "author": "zentol", "createdAt": "2020-06-19T17:47:13Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3MTU2NA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r442971564", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.\n          \n          \n            \n            The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` have been renamed to `log4j-session.properties` and `logback-session.xml`.", "author": "zentol", "createdAt": "2020-06-19T17:47:53Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,246 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts simply need to rename that file.\n+\n+### Memory Management\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+\t\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as anounced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally if you use Scala's implicit conversions to/from DataStream/DataSet import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` haven been renamed into `log4j-session.properties` and `logback-session.xml`.", "originalCommit": "2a876970e69c0f14f31125441e92432aab3ac817", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3f05203e0a15d143fc9a60c3faf176b5c1ecfde4", "url": "https://github.com/apache/flink/commit/3f05203e0a15d143fc9a60c3faf176b5c1ecfde4", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-24T14:00:14Z", "type": "forcePushed"}, {"oid": "a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "url": "https://github.com/apache/flink/commit/a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-24T14:03:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3MTgxMQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445371811", "bodyText": "What exactly are the changes here? Was the change that TimeType only supported a precision of 0 and now it also supports other values? Or does it mean that the only valid precision value for TimeType is 0 and if one configures a different value, then it will fail with an exception? If it is the latter I would suggest to rephrase it into: The precision of TimeType can only be 0. Otherwise it reads as if it was in the past. The same applies to every other item in this list.", "author": "tillrohrmann", "createdAt": "2020-06-25T07:51:16Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,291 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Support for Hadoop 3.0.0 and higher ([FLINK-11086](https://issues.apache.org/jira/browse/FLINK-11086))\n+Flink project does not provide any updated \"flink-shaded-hadoop-*\" jars.\n+Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via `lib/` folder.\n+Also, the `include-hadoop` Maven profile has been removed.\n+\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts need to rename that file.\n+\n+#### Flink Docker Integration Improvements\n+The examples of `Dockerfiles` and docker image `build.sh` scripts have been removed from [the Flink Github repository](https://github.com/apache/flink). The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:\n+- `flink-contrib/docker-flink`\n+- `flink-container/docker`\n+- `flink-container/kubernetes`\n+\n+Check the updated user documentation for [Flink Docker integration](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) instead. It now describes in detail how to [use](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-a-flink-image) and [customize](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#customize-flink-image) [the Flink official docker image](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#docker-hub-flink-images): configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:\n+- [docker run](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-flink-image)\n+- [docker compose](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-compose)\n+- [docker swarm](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-swarm)\n+- [standalone Kubernetes](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html)\n+\n+### Memory Management\n+#### New Flink Master Memory Model\n+##### Overview\n+With [FLIP-116](https://cwiki.apache.org/confluence/display/FLINK/FLIP-116%3A+Unified+Memory+Configuration+for+Job+Managers), a new memory model has been introduced for the Flink Master. New configuration options have been introduced to control the memory consumption of the Flink Master process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.\n+\n+Please, check the user documentation for [more details](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html).\n+\n+If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. See also [the migration guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#migrate-job-manager-memory-configuration).\n+\n+##### Deprecation and breaking changes\n+The following options are deprecated:\n+ * `jobmanager.heap.size`\n+ * `jobmanager.heap.mb`\n+\n+If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:\n+ * [JVM Heap](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-jvm-heap) ([`jobmanager.memory.heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-heap-size)) for standalone and Mesos deployments\n+ * [Total Process Memory](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-total-memory) ([`jobmanager.memory.process.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-process-size)) for containerized deployments (Kubernetes and Yarn)\n+\n+The following options have been removed and have no effect anymore:\n+ * `containerized.heap-cutoff-ratio`\n+ * `containerized.heap-cutoff-min`\n+\n+There is [no container cut-off](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#container-cut-off-memory) anymore.\n+\n+##### JVM arguments\n+The `direct` and `metaspace` memory of the Flink Master's JVM process are now limited by configurable values:\n+ * [`jobmanager.memory.off-heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-off-heap-size)\n+ * [`jobmanager.memory.jvm-metaspace.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-jvm-metaspace-size)\n+\n+See also [JVM Parameters](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html#jvm-parameters).\n+\n+<span class=\"label label-warning\">Attention</span> These new limits can produce the respective `OutOfMemoryError` exceptions if they are not configured properly or there is a respective memory leak. See also [the troubleshooting guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_trouble.html#outofmemoryerror-direct-buffer-memory).\n+\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as announced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream, switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet, change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally, if you use Scala's implicit conversions to/from DataStream/DataSet, import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` have been renamed to `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported", "originalCommit": "a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5NjczMw==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445496733", "bodyText": "I believe it's this:\n\nOr does it mean that the only valid precision value for TimeType is 0 and if one configures a different value, then it will fail with an exception?\n\nYes, of course it should be can instead of could. I will rephrase it.", "author": "pnowojski", "createdAt": "2020-06-25T11:43:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3MTgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3MzM0OQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445373349", "bodyText": "Same here that we removed the code from the repository.", "author": "tillrohrmann", "createdAt": "2020-06-25T07:53:51Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,291 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Support for Hadoop 3.0.0 and higher ([FLINK-11086](https://issues.apache.org/jira/browse/FLINK-11086))\n+Flink project does not provide any updated \"flink-shaded-hadoop-*\" jars.\n+Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via `lib/` folder.\n+Also, the `include-hadoop` Maven profile has been removed.\n+\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts need to rename that file.\n+\n+#### Flink Docker Integration Improvements\n+The examples of `Dockerfiles` and docker image `build.sh` scripts have been removed from [the Flink Github repository](https://github.com/apache/flink). The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:\n+- `flink-contrib/docker-flink`\n+- `flink-container/docker`\n+- `flink-container/kubernetes`\n+\n+Check the updated user documentation for [Flink Docker integration](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) instead. It now describes in detail how to [use](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-a-flink-image) and [customize](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#customize-flink-image) [the Flink official docker image](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#docker-hub-flink-images): configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:\n+- [docker run](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-flink-image)\n+- [docker compose](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-compose)\n+- [docker swarm](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-swarm)\n+- [standalone Kubernetes](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html)\n+\n+### Memory Management\n+#### New Flink Master Memory Model\n+##### Overview\n+With [FLIP-116](https://cwiki.apache.org/confluence/display/FLINK/FLIP-116%3A+Unified+Memory+Configuration+for+Job+Managers), a new memory model has been introduced for the Flink Master. New configuration options have been introduced to control the memory consumption of the Flink Master process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.\n+\n+Please, check the user documentation for [more details](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html).\n+\n+If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. See also [the migration guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#migrate-job-manager-memory-configuration).\n+\n+##### Deprecation and breaking changes\n+The following options are deprecated:\n+ * `jobmanager.heap.size`\n+ * `jobmanager.heap.mb`\n+\n+If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:\n+ * [JVM Heap](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-jvm-heap) ([`jobmanager.memory.heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-heap-size)) for standalone and Mesos deployments\n+ * [Total Process Memory](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-total-memory) ([`jobmanager.memory.process.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-process-size)) for containerized deployments (Kubernetes and Yarn)\n+\n+The following options have been removed and have no effect anymore:\n+ * `containerized.heap-cutoff-ratio`\n+ * `containerized.heap-cutoff-min`\n+\n+There is [no container cut-off](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#container-cut-off-memory) anymore.\n+\n+##### JVM arguments\n+The `direct` and `metaspace` memory of the Flink Master's JVM process are now limited by configurable values:\n+ * [`jobmanager.memory.off-heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-off-heap-size)\n+ * [`jobmanager.memory.jvm-metaspace.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-jvm-metaspace-size)\n+\n+See also [JVM Parameters](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html#jvm-parameters).\n+\n+<span class=\"label label-warning\">Attention</span> These new limits can produce the respective `OutOfMemoryError` exceptions if they are not configured properly or there is a respective memory leak. See also [the troubleshooting guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_trouble.html#outofmemoryerror-direct-buffer-memory).\n+\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as announced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream, switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet, change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally, if you use Scala's implicit conversions to/from DataStream/DataSet, import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` have been renamed to `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. \n+The web UI still displays this value but it is always `0` now. \n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. ", "originalCommit": "a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3NDc3MQ==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445374771", "bodyText": "I think the interface has been updated to contain a default implementation.", "author": "tillrohrmann", "createdAt": "2020-06-25T07:56:10Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,291 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Support for Hadoop 3.0.0 and higher ([FLINK-11086](https://issues.apache.org/jira/browse/FLINK-11086))\n+Flink project does not provide any updated \"flink-shaded-hadoop-*\" jars.\n+Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via `lib/` folder.\n+Also, the `include-hadoop` Maven profile has been removed.\n+\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts need to rename that file.\n+\n+#### Flink Docker Integration Improvements\n+The examples of `Dockerfiles` and docker image `build.sh` scripts have been removed from [the Flink Github repository](https://github.com/apache/flink). The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:\n+- `flink-contrib/docker-flink`\n+- `flink-container/docker`\n+- `flink-container/kubernetes`\n+\n+Check the updated user documentation for [Flink Docker integration](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) instead. It now describes in detail how to [use](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-a-flink-image) and [customize](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#customize-flink-image) [the Flink official docker image](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#docker-hub-flink-images): configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:\n+- [docker run](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-flink-image)\n+- [docker compose](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-compose)\n+- [docker swarm](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-swarm)\n+- [standalone Kubernetes](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html)\n+\n+### Memory Management\n+#### New Flink Master Memory Model\n+##### Overview\n+With [FLIP-116](https://cwiki.apache.org/confluence/display/FLINK/FLIP-116%3A+Unified+Memory+Configuration+for+Job+Managers), a new memory model has been introduced for the Flink Master. New configuration options have been introduced to control the memory consumption of the Flink Master process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.\n+\n+Please, check the user documentation for [more details](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html).\n+\n+If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. See also [the migration guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#migrate-job-manager-memory-configuration).\n+\n+##### Deprecation and breaking changes\n+The following options are deprecated:\n+ * `jobmanager.heap.size`\n+ * `jobmanager.heap.mb`\n+\n+If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:\n+ * [JVM Heap](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-jvm-heap) ([`jobmanager.memory.heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-heap-size)) for standalone and Mesos deployments\n+ * [Total Process Memory](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-total-memory) ([`jobmanager.memory.process.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-process-size)) for containerized deployments (Kubernetes and Yarn)\n+\n+The following options have been removed and have no effect anymore:\n+ * `containerized.heap-cutoff-ratio`\n+ * `containerized.heap-cutoff-min`\n+\n+There is [no container cut-off](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#container-cut-off-memory) anymore.\n+\n+##### JVM arguments\n+The `direct` and `metaspace` memory of the Flink Master's JVM process are now limited by configurable values:\n+ * [`jobmanager.memory.off-heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-off-heap-size)\n+ * [`jobmanager.memory.jvm-metaspace.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-jvm-metaspace-size)\n+\n+See also [JVM Parameters](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html#jvm-parameters).\n+\n+<span class=\"label label-warning\">Attention</span> These new limits can produce the respective `OutOfMemoryError` exceptions if they are not configured properly or there is a respective memory leak. See also [the troubleshooting guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_trouble.html#outofmemoryerror-direct-buffer-memory).\n+\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as announced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream, switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet, change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally, if you use Scala's implicit conversions to/from DataStream/DataSet, import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` have been renamed to `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. \n+The web UI still displays this value but it is always `0` now. \n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail its Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). \n+`MailboxExecutor#yield` or `MailboxExecutor#tryYield` methods can be used for actions that should give control to other actions temporarily (equivalent of `StreamTask#getCheckpointLock().wait()`), if the current operator is blocked. \n+`MailboxExecutor` can be accessed by using `YieldingOperatorFactory`. Example usage can be found in the `AsyncWaitOperator`.\n+\n+Note, `SourceFunction.SourceContext.getCheckpointLock` is still available for custom implementations of `SourceFunction` interface. \n+\n+#### Reversed dependency from flink-streaming-java to flink-client ([FLINK-15090](https://issues.apache.org/jira/browse/FLINK-15090))\n+Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients` anymore. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.\n+\n+#### AsyncWaitOperator is chainable again ([FLINK-16219](https://issues.apache.org/jira/browse/FLINK-16219))\n+`AsyncWaitOperator` will be allowed to be chained by default with all operators, except of tasks with `SourceFunction`.\n+This mostly revert limitation introduced as a bug fix for [FLINK-13063](https://issues.apache.org/jira/browse/FLINK-13063).\n+\n+#### Changed argument types of ShuffleEnvironment#createInputGates and #createResultPartitionWriters methods ([FLINK-16586](https://issues.apache.org/jira/browse/FLINK-16586))\n+The argument type of methods `ShuffleEnvironment#createInputGates` and `#createResultPartitionWriters` are adjusted from `Collection` to `List` for satisfying the order guarantee requirement in unaligned checkpoint.\n+It will break the compatibility if users already implemented a custom `ShuffleService` based on `ShuffleServiceFactory` interface.\n+\n+#### Deprecation of CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible ([FLINK-17520](https://issues.apache.org/jira/browse/FLINK-17520))\n+The `boolean isOuterSnapshotCompatible(TypeSerializer)` on the `CompositeTypeSerializerSnapshot` class has been deprecated, in favor of a new `OuterSchemaCompatibility resolveOuterSchemaCompatibility(TypeSerializer)` method.\n+Please implement that instead.\n+Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.\n+\n+#### Removal of deprecated TimestampExtractor ([FLINK-17655](https://issues.apache.org/jira/browse/FLINK-17655))\n+The long-deprecated `TimestampExtractor` was removed along with API methods in the DataStream API.\n+Please use the new `TimestampAssigner` and `WatermarkStrategies` for working with timestamps and watermarks in the DataStream API.\n+\n+#### Deprecation of ListCheckpointed interface ([FLINK-6258](https://issues.apache.org/jira/browse/FLINK-6258))\n+The `ListCheckpointed` interface has been deprecated because it uses Java Serialization for checkpointing state which is problematic for savepoint compatibility.\n+Use the `CheckpointedFunction` interface instead, which gives more control over state serialization.\n+\n+#### Extended CheckpointListener interface ([FLINK-8871](https://issues.apache.org/jira/browse/FLINK-8871))\n+Added a new method `#notifyCheckpointAborted(long checkpointId)` to the `CheckpointListener` interface.\n+Existing users which do not want to act on checkpoint aborted notifications should provide empty method to implement it.", "originalCommit": "a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3NTI5NA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445375294", "bodyText": "Are the alternatives for the state access now @aljoscha?", "author": "tillrohrmann", "createdAt": "2020-06-25T07:57:01Z", "path": "docs/release-notes/flink-1.11.md", "diffHunk": "@@ -0,0 +1,291 @@\n+---\n+title: \"Release Notes - Flink 1.11\"\n+---\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+\n+\n+These release notes discuss important aspects, such as configuration, behavior,\n+or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read\n+these notes carefully if you are planning to upgrade your Flink version to 1.11.\n+\n+* This will be replaced by the TOC\n+{:toc}\n+\n+### Clusters & Deployment\n+#### Support for Hadoop 3.0.0 and higher ([FLINK-11086](https://issues.apache.org/jira/browse/FLINK-11086))\n+Flink project does not provide any updated \"flink-shaded-hadoop-*\" jars.\n+Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via `lib/` folder.\n+Also, the `include-hadoop` Maven profile has been removed.\n+\n+#### Removal of `LegacyScheduler` ([FLINK-15629](https://issues.apache.org/jira/browse/FLINK-15629))\n+Flink no longer supports the legacy scheduler. \n+Hence, setting `jobmanager.scheduler: legacy` will no longer work and fail with an `IllegalArgumentException`. \n+The only valid option for `jobmanager.scheduler` is the default value `ng`.\n+\n+#### Bind user code class loader to lifetime of a slot ([FLINK-16408](https://issues.apache.org/jira/browse/FLINK-16408))\n+The user code class loader is being reused by the `TaskExecutor` as long as there is at least a single slot allocated for the respective job. \n+This changes Flink's recovery behaviour slightly so that it will not reload static fields.\n+The benefit is that this change drastically reduces pressure on the JVM's metaspace.\n+\n+#### Replaced `slave` file name with `workers` ([FLINK-18307](https://issues.apache.org/jira/browse/FLINK-18307))\n+For Standalone Setups, the file with the worker nodes is no longer called `slaves` but `workers`.\n+Previous setups that use the `start-cluster.sh` and `stop-cluster.sh` scripts need to rename that file.\n+\n+#### Flink Docker Integration Improvements\n+The examples of `Dockerfiles` and docker image `build.sh` scripts have been removed from [the Flink Github repository](https://github.com/apache/flink). The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:\n+- `flink-contrib/docker-flink`\n+- `flink-container/docker`\n+- `flink-container/kubernetes`\n+\n+Check the updated user documentation for [Flink Docker integration](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) instead. It now describes in detail how to [use](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-a-flink-image) and [customize](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#customize-flink-image) [the Flink official docker image](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#docker-hub-flink-images): configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:\n+- [docker run](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#how-to-run-flink-image)\n+- [docker compose](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-compose)\n+- [docker swarm](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html#flink-with-docker-swarm)\n+- [standalone Kubernetes](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html)\n+\n+### Memory Management\n+#### New Flink Master Memory Model\n+##### Overview\n+With [FLIP-116](https://cwiki.apache.org/confluence/display/FLINK/FLIP-116%3A+Unified+Memory+Configuration+for+Job+Managers), a new memory model has been introduced for the Flink Master. New configuration options have been introduced to control the memory consumption of the Flink Master process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.\n+\n+Please, check the user documentation for [more details](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html).\n+\n+If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. See also [the migration guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#migrate-job-manager-memory-configuration).\n+\n+##### Deprecation and breaking changes\n+The following options are deprecated:\n+ * `jobmanager.heap.size`\n+ * `jobmanager.heap.mb`\n+\n+If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:\n+ * [JVM Heap](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-jvm-heap) ([`jobmanager.memory.heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-heap-size)) for standalone and Mesos deployments\n+ * [Total Process Memory](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup_master.html#configure-total-memory) ([`jobmanager.memory.process.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-process-size)) for containerized deployments (Kubernetes and Yarn)\n+\n+The following options have been removed and have no effect anymore:\n+ * `containerized.heap-cutoff-ratio`\n+ * `containerized.heap-cutoff-min`\n+\n+There is [no container cut-off](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html#container-cut-off-memory) anymore.\n+\n+##### JVM arguments\n+The `direct` and `metaspace` memory of the Flink Master's JVM process are now limited by configurable values:\n+ * [`jobmanager.memory.off-heap.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-off-heap-size)\n+ * [`jobmanager.memory.jvm-metaspace.size`](https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html#jobmanager-memory-jvm-metaspace-size)\n+\n+See also [JVM Parameters](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html#jvm-parameters).\n+\n+<span class=\"label label-warning\">Attention</span> These new limits can produce the respective `OutOfMemoryError` exceptions if they are not configured properly or there is a respective memory leak. See also [the troubleshooting guide](https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_trouble.html#outofmemoryerror-direct-buffer-memory).\n+\n+#### Removal of deprecated mesos.resourcemanager.tasks.mem ([FLINK-15198](https://issues.apache.org/jira/browse/FLINK-15198))\n+The `mesos.resourcemanager.tasks.mem` option, deprecated in 1.10 in favour of `taskmanager.memory.process.size`, has been completely removed and will have no effect anymore in 1.11+.\n+\n+### Table API & SQL\n+#### Blink is now the default planner ([FLINK-16934](https://issues.apache.org/jira/browse/FLINK-16934))\n+The default table planner has been changed to blink.\n+\n+#### Changed package structure for Table API ([FLINK-15947](https://issues.apache.org/jira/browse/FLINK-15947))\n+Due to various issues with packages `org.apache.flink.table.api.scala/java` all classes from those packages were relocated. \n+Moreover the scala expressions were moved to `org.apache.flink.table.api` as announced in Flink 1.9.\n+\n+If you used one of:\n+* `org.apache.flink.table.api.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.scala.BatchTableEnvironment` \n+\n+And you do not convert to/from DataStream, switch to:\n+* `org.apache.flink.table.api.TableEnvironment` \n+\n+If you do convert to/from DataStream/DataSet, change your imports to one of:\n+* `org.apache.flink.table.api.bridge.java.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.StreamTableEnvironment`\n+* `org.apache.flink.table.api.bridge.java.BatchTableEnvironment`\n+* `org.apache.flink.table.api.bridge.scala.BatchTableEnvironment` \n+\n+For the Scala expressions use the import:\n+* `org.apache.flink.table.api._` instead of `org.apache.flink.table.api.bridge.scala._` \n+\n+Additionally, if you use Scala's implicit conversions to/from DataStream/DataSet, import `org.apache.flink.table.api.bridge.scala._` instead of `org.apache.flink.table.api.scala._`\n+\n+#### Removal of deprecated `StreamTableSink` ([FLINK-16362](https://issues.apache.org/jira/browse/FLINK-16362))\n+The existing `StreamTableSink` implementations should remove `emitDataStream` method.\n+\n+#### Removal of `BatchTableSink#emitDataSet` ([FLINK-16535](https://issues.apache.org/jira/browse/FLINK-16535))\n+The existing `BatchTableSink` implementations should rename `emitDataSet` to `consumeDataSet` and return `DataSink`.\n+  \n+#### Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() ([FLINK-16363](https://issues.apache.org/jira/browse/FLINK-16363))\n+In previous versions, `TableEnvironment.execute()` and `StreamExecutionEnvironment.execute()` can both trigger table and DataStream programs.\n+Since Flink 1.11.0, table programs can only be triggered by `TableEnvironment.execute()`. \n+Once table program is converted into DataStream program (through `toAppendStream()` or `toRetractStream()` method), it can only be triggered by `StreamExecutionEnvironment.execute()`.\n+\n+#### Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() ([FLINK-17126](https://issues.apache.org/jira/browse/FLINK-17126))\n+In previous versions, `BatchTableEnvironment.execute()` and `ExecutionEnvironment.execute()` can both trigger table and DataSet programs for legacy batch planner.\n+Since Flink 1.11.0, batch table programs can only be triggered by `BatchEnvironment.execute()`.\n+Once table program is converted into DataSet program (through `toDataSet()` method), it can only be triggered by `ExecutionEnvironment.execute()`.\n+\n+#### Added a changeflag to Row type ([FLINK-16998](https://issues.apache.org/jira/browse/FLINK-16998))\n+An additional change flag called `RowKind` was added to the `Row` type.\n+This changed the serialization format and will trigger a state migration.\n+\n+### Configuration\n+#### Renamed log4j-yarn-session.properties and logback-yarn.xml properties files ([FLINK-17527](https://issues.apache.org/jira/browse/FLINK-17527))\n+The logging properties files `log4j-yarn-session.properties` and `logback-yarn.xml` have been renamed to `log4j-session.properties` and `logback-session.xml`.\n+Moreover, `yarn-session.sh` and `kubernetes-session.sh` use these logging properties files.\n+\n+### State\n+#### Removal of deprecated background cleanup toggle (State TTL) ([FLINK-15620](https://issues.apache.org/jira/browse/FLINK-15620))\n+The `StateTtlConfig#cleanupInBackground` has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.\n+\n+####  Removal of deprecated option to disable TTL compaction filter ([FLINK-15621](https://issues.apache.org/jira/browse/FLINK-15621))\n+The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+.\n+Because of that the following option and methods have been removed in 1.11: \n+- `state.backend.rocksdb.ttl.compaction.filter.enabled`\n+- `StateTtlConfig#cleanupInRocksdbCompactFilter()`\n+- `RocksDBStateBackend#isTtlCompactionFilterEnabled`\n+- `RocksDBStateBackend#enableTtlCompactionFilter`\n+- `RocksDBStateBackend#disableTtlCompactionFilter`\n+- (state_backend.py) `is_ttl_compaction_filter_enabled`\n+- (state_backend.py) `enable_ttl_compaction_filter`\n+- (state_backend.py) `disable_ttl_compaction_filter`\n+\n+#### Changed argument type of StateBackendFactory#createFromConfig ([FLINK-16913](https://issues.apache.org/jira/browse/FLINK-16913))\n+Starting from Flink 1.11 the `StateBackendFactory#createFromConfig` interface now takes `ReadableConfig` instead of `Configuration`.\n+A `Configuration` class is still a valid argument to that method, as it implements the ReadableConfig interface.\n+Implementors of custom `StateBackend` should adjust their implementations.\n+\n+#### Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes ([FLINK-18242](https://issues.apache.org/jira/browse/FLINK-18242))\n+The deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes have been removed.\n+Please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead.\n+Please also recompile your application codes if any class extends `DefaultConfigurableOptionsFactory`.\n+\n+#### Enabled by default setTotalOrderSeek ([FLINK-17800](https://issues.apache.org/jira/browse/FLINK-17800))\n+Since Flink-1.11 the option `setTotalOrderSeek` will be enabled by default for RocksDB's `ReadOptions`.\n+This is in order to prevent user from miss using `optimizeForPointLookup`.\n+For backward compatibility we support customizing `ReadOptions` through `RocksDBOptionsFactory`.\n+Please set `setTotalOrderSeek` back to false if any performance regression observed (it shouldn't happen according to our testing).\n+\n+#### Increased default size of `state.backend.fs.memory-threshold` ([FLINK-17865](https://issues.apache.org/jira/browse/FLINK-17865))\n+The default value of `state.backend.fs.memory-threshold` has been increased from 1K to 20K to prevent too many small files created on remote FS for small states.\n+Jobs with large parallelism on source or stateful operators may have \"JM OOM\" or \"RPC message exceeding maximum frame size\" problem with this change.\n+If you encounter such issues please manually set the configuration back to 1K.\n+\n+### PyFlink\n+#### Throw exceptions for the unsupported data types ([FLINK-16606](https://issues.apache.org/jira/browse/FLINK-16606))\n+DataTypes can be configured with some parameters, e.g., precision.\n+However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used.\n+To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. \n+Changes include:\n+- the precision for `TimeType` could only be `0`\n+- the length for `VarBinaryType`/`VarCharType` could only be `0x7fffffff`\n+- the precision/scale for `DecimalType` could only be `38`/`18`\n+- the precision for `TimestampType`/`LocalZonedTimestampType` could only be `3`\n+- the resolution for `DayTimeIntervalType` could only be `SECOND` and the `fractionalPrecision` could only be `3`\n+- the resolution for `YearMonthIntervalType` could only be `MONTH` and the `yearPrecision` could only be `2`\n+- the `CharType`/`BinaryType`/`ZonedTimestampType` is not supported\n+\n+### Monitoring\n+#### Converted all MetricReporters to plugins ([FLINK-16963](https://issues.apache.org/jira/browse/FLINK-16963))\n+All MetricReporters that come with Flink have been converted to plugins.\n+They should no longer be placed into `/lib` directory (doing so may result in dependency conflicts!), but `/plugins/<some_directory>` instead.\n+\n+#### Changed of DataDog's metric reporter Counter metrics ([FLINK-15438](https://issues.apache.org/jira/browse/FLINK-15438))\n+The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. \n+This aligns the count semantics with the DataDog documentation.\n+\n+#### Switch to Log4j 2 by default ([FLINK-15672](https://issues.apache.org/jira/browse/FLINK-15672))\n+Flink now uses Log4j2 by default. \n+Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.\n+\n+#### Changed behaviour of JobManager API's log request ([FLINK-16303](https://issues.apache.org/jira/browse/FLINK-16303))\n+Requesting an unavailable log or stdout file from the JobManager's HTTP server returns status code 404 now. \n+In previous releases, the HTTP server would return a file with `(file unavailable)` as its content.\n+\n+#### Removal of lastCheckpointAlignmentBuffered metric ([FLINK-16404](https://issues.apache.org/jira/browse/FLINK-16404))\n+Note that the metric `lastCheckpointAlignmentBuffered` has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. \n+The web UI still displays this value but it is always `0` now. \n+\n+### Connectors\n+#### Dropped Kafka 0.8/0.9 connectors ([FLINK-15115](https://issues.apache.org/jira/browse/FLINK-15115))\n+The Kafka 0.8 and 0.9 connectors are no longer under active development. \n+\n+#### Dropped Elasticsearch 2.x connector ([FLINK-16046](https://issues.apache.org/jira/browse/FLINK-16046))\n+The Elasticsearch 2 connector is no longer under active development. \n+Prior version of these connectors will continue to work with Flink. \n+\n+#### Removal of deprecated `KafkaPartitioner` ([FLINK-15862](https://issues.apache.org/jira/browse/FLINK-15862))\n+Deprecated `KafkaPartitioner` was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.\n+\n+#### Refined fallback filesystems to only handle specific filesystems ([FLINK-16015](https://issues.apache.org/jira/browse/FLINK-16015))\n+By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. \n+Added `fs.allowed-fallback-filesystems` configuration option to override this behaviour.\n+\n+#### Deprecation of FileSystem#getKind ([FLINK-16400](https://issues.apache.org/jira/browse/FLINK-16400))\n+`org.apache.flink.core.fs.FileSystem#getKind` method has been formally deprecated, as it was not used by Flink.\n+\n+### Runtime\n+#### Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint ([FLINK-17350](https://issues.apache.org/jira/browse/FLINK-17350))\n+Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail its Task (and job) immediately, regardless of the configuration parameters.\n+Since Flink 1.5 such failures could be ignored by setting `setTolerableCheckpointFailureNumber(...)` or its deprecated `setFailTaskOnCheckpointError(...)` predecessor.\n+Now both options will only affect asynchronous failures.\n+\n+#### Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber ([FLINK-17351](https://issues.apache.org/jira/browse/FLINK-17351))\n+Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by `CheckpointConfig#setTolerableCheckpointFailureNumber(...)`.\n+\n+### Miscellaneous Interface Changes\n+#### Removal of deprecated StreamTask#getCheckpointLock() ([FLINK-12484](https://issues.apache.org/jira/browse/FLINK-12484))\n+DataStream API no longer provides `StreamTask#getCheckpointLock` method, which was deprecated in Flink 1.10. \n+Users should use `MailboxExecutor` to run actions that require synchronization with the task's thread (e.g. collecting output produced by an external thread). \n+`MailboxExecutor#yield` or `MailboxExecutor#tryYield` methods can be used for actions that should give control to other actions temporarily (equivalent of `StreamTask#getCheckpointLock().wait()`), if the current operator is blocked. \n+`MailboxExecutor` can be accessed by using `YieldingOperatorFactory`. Example usage can be found in the `AsyncWaitOperator`.\n+\n+Note, `SourceFunction.SourceContext.getCheckpointLock` is still available for custom implementations of `SourceFunction` interface. \n+\n+#### Reversed dependency from flink-streaming-java to flink-client ([FLINK-15090](https://issues.apache.org/jira/browse/FLINK-15090))\n+Starting from Flink 1.11.0, the `flink-streaming-java` module does not have a dependency on `flink-clients` anymore. If your project was depending on this transitive dependency you now have to add `flink-clients` as an explicit dependency.\n+\n+#### AsyncWaitOperator is chainable again ([FLINK-16219](https://issues.apache.org/jira/browse/FLINK-16219))\n+`AsyncWaitOperator` will be allowed to be chained by default with all operators, except of tasks with `SourceFunction`.\n+This mostly revert limitation introduced as a bug fix for [FLINK-13063](https://issues.apache.org/jira/browse/FLINK-13063).\n+\n+#### Changed argument types of ShuffleEnvironment#createInputGates and #createResultPartitionWriters methods ([FLINK-16586](https://issues.apache.org/jira/browse/FLINK-16586))\n+The argument type of methods `ShuffleEnvironment#createInputGates` and `#createResultPartitionWriters` are adjusted from `Collection` to `List` for satisfying the order guarantee requirement in unaligned checkpoint.\n+It will break the compatibility if users already implemented a custom `ShuffleService` based on `ShuffleServiceFactory` interface.\n+\n+#### Deprecation of CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible ([FLINK-17520](https://issues.apache.org/jira/browse/FLINK-17520))\n+The `boolean isOuterSnapshotCompatible(TypeSerializer)` on the `CompositeTypeSerializerSnapshot` class has been deprecated, in favor of a new `OuterSchemaCompatibility resolveOuterSchemaCompatibility(TypeSerializer)` method.\n+Please implement that instead.\n+Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.\n+\n+#### Removal of deprecated TimestampExtractor ([FLINK-17655](https://issues.apache.org/jira/browse/FLINK-17655))\n+The long-deprecated `TimestampExtractor` was removed along with API methods in the DataStream API.\n+Please use the new `TimestampAssigner` and `WatermarkStrategies` for working with timestamps and watermarks in the DataStream API.\n+\n+#### Deprecation of ListCheckpointed interface ([FLINK-6258](https://issues.apache.org/jira/browse/FLINK-6258))\n+The `ListCheckpointed` interface has been deprecated because it uses Java Serialization for checkpointing state which is problematic for savepoint compatibility.\n+Use the `CheckpointedFunction` interface instead, which gives more control over state serialization.\n+\n+#### Extended CheckpointListener interface ([FLINK-8871](https://issues.apache.org/jira/browse/FLINK-8871))\n+Added a new method `#notifyCheckpointAborted(long checkpointId)` to the `CheckpointListener` interface.\n+Existing users which do not want to act on checkpoint aborted notifications should provide empty method to implement it.\n+\n+#### Removal of deprecated state access methods ([FLINK-17376](https://issues.apache.org/jira/browse/FLINK-17376))\n+We removed deprecated state access methods `RuntimeContext#getFoldingState()`, `OperatorStateStore#getSerializableListState()` and `OperatorStateStore#getOperatorState()`.\n+This means that some code that was compiled against Flink 1.10 will not work with a Flink 1.11 cluster.\n+An example of this is our Kafka connector which internally used `OperatorStateStore.getSerializableListState`.", "originalCommit": "a5ba950dc1eba6a27cfbb521ba42666fbb44c5e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5ODU1OA==", "url": "https://github.com/apache/flink/pull/12699#discussion_r445498558", "bodyText": "If there are, @aljoscha could you propose a change in a separate PR? I'm about to merge this one.", "author": "pnowojski", "createdAt": "2020-06-25T11:47:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM3NTI5NA=="}], "type": "inlineReview"}, {"oid": "95ac5eafcaa13c20d525949682616c8b4c7b5cfa", "url": "https://github.com/apache/flink/commit/95ac5eafcaa13c20d525949682616c8b4c7b5cfa", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-25T11:49:45Z", "type": "commit"}, {"oid": "95ac5eafcaa13c20d525949682616c8b4c7b5cfa", "url": "https://github.com/apache/flink/commit/95ac5eafcaa13c20d525949682616c8b4c7b5cfa", "message": "[FLINK-18349][docs] Add release notes for Flink 1.11", "committedDate": "2020-06-25T11:49:45Z", "type": "forcePushed"}]}