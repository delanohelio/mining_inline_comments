{"pr_number": 13119, "pr_title": "[FLINK-18885][python] Add partitioning interfaces for Python DataStre\u2026", "pr_createdAt": "2020-08-11T15:11:05Z", "pr_url": "https://github.com/apache/flink/pull/13119", "timeline": [{"oid": "9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "url": "https://github.com/apache/flink/commit/9b7c269c1f59721e9a2f9accea9fd3d30e18b65a", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API.", "committedDate": "2020-08-11T16:58:51Z", "type": "forcePushed"}, {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4", "url": "https://github.com/apache/flink/commit/ec6e531e07a441ccb328f735cc40d618087798c4", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API.", "committedDate": "2020-08-12T02:15:53Z", "type": "commit"}, {"oid": "ec6e531e07a441ccb328f735cc40d618087798c4", "url": "https://github.com/apache/flink/commit/ec6e531e07a441ccb328f735cc40d618087798c4", "message": "[FLINK-18885][python] Add partitioning interfaces for Python DataStream API.", "committedDate": "2020-08-12T02:15:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjAwMQ==", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972001", "bodyText": "Cannot override partitioning for KeyedStream.", "author": "hequn8128", "createdAt": "2020-08-12T02:41:09Z", "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':\n+        return self._origin_stream.project(*field_indexes)\n+\n+    def rescale(self) -> 'DataStream':\n+        return self._origin_stream.rescale()\n+\n+    def rebalance(self) -> 'DataStream':\n+        return self._origin_stream.rebalance()\n+\n+    def forward(self) -> 'DataStream':\n+        return self._origin_stream.forward()\n+\n+    def broadcast(self) -> 'DataStream':\n+        return self._origin_stream.broadcast()", "originalCommit": "ec6e531e07a441ccb328f735cc40d618087798c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNjQ1NA==", "url": "https://github.com/apache/flink/pull/13119#discussion_r469006454", "bodyText": "Yes, I have tested them and revised the code.", "author": "shuiqiangchen", "createdAt": "2020-08-12T05:02:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MjI3Mg==", "url": "https://github.com/apache/flink/pull/13119#discussion_r468972272", "bodyText": "project on Keyedstream should preserve the hash partitioning, i.e., we can't simply project on its origin_stream.", "author": "hequn8128", "createdAt": "2020-08-12T02:42:15Z", "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)\n+\n+    def shuffle(self) -> 'DataStream':\n+        return self._origin_stream.shuffle()\n+\n+    def project(self, *field_indexes) -> 'DataStream':", "originalCommit": "ec6e531e07a441ccb328f735cc40d618087798c4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjcyMQ==", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976721", "bodyText": "I find we have bugs when handling parallelism for the keyby. Suppose we perform x -> key_by-> map, the internal java stream graph would be x -> map1 -> keyBy -> map2 -> map3. The parallelism of map1 should equal to x and map2 should equal to map3.", "author": "hequn8128", "createdAt": "2020-08-12T02:59:40Z", "path": "flink-python/pyflink/datastream/data_stream.py", "diffHunk": "@@ -563,6 +654,27 @@ def key_by(self, key_selector: Union[Callable, KeySelector],\n                key_type_info: TypeInformation = None) -> 'KeyedStream':\n         return self._origin_stream.key_by(key_selector, key_type_info)\n \n+    def union(self, *streams) -> 'DataStream':\n+        return self._values().union(*streams)", "originalCommit": "ec6e531e07a441ccb328f735cc40d618087798c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDcwNw==", "url": "https://github.com/apache/flink/pull/13119#discussion_r469004707", "bodyText": "Yes, it's a critical issue that would affect the correctness of DAG, I will fix it in this JIRA.", "author": "shuiqiangchen", "createdAt": "2020-08-12T04:55:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njk1Mg==", "url": "https://github.com/apache/flink/pull/13119#discussion_r468976952", "bodyText": "These tests are not correct. Exceptions should be thrown since we cannot override partitioning for KeyedStream.", "author": "hequn8128", "createdAt": "2020-08-12T03:00:32Z", "path": "flink-python/pyflink/datastream/tests/test_data_stream.py", "diffHunk": "@@ -242,6 +242,68 @@ def test_print_with_align_output(self):\n         self.assertEqual(3, len(plan['nodes']))\n         self.assertEqual(\"Sink: Print to Std. Out\", plan['nodes'][2]['type'])\n \n+    def test_union_stream(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_2 = self.env.from_collection([4, 5, 6])\n+        ds_3 = self.env.from_collection([7, 8, 9])\n+\n+        united_stream = ds_3.union(ds_1, ds_2)\n+\n+        united_stream.map(lambda x: x + 1).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        source_ids = []\n+        union_node_pre_ids = []\n+        for node in exec_plan['nodes']:\n+            if node['pact'] == 'Data Source':\n+                source_ids.append(node['id'])\n+            if node['pact'] == 'Operator':\n+                for pre in node['predecessors']:\n+                    union_node_pre_ids.append(pre['id'])\n+\n+        source_ids.sort()\n+        union_node_pre_ids.sort()\n+        self.assertEqual(source_ids, union_node_pre_ids)\n+\n+    def test_project(self):\n+        ds = self.env.from_collection([[1, 2, 3, 4], [5, 6, 7, 8]],\n+                                      type_info=Types.TUPLE(\n+                                          [Types.INT(), Types.INT(), Types.INT(), Types.INT()]))\n+        ds.project(1, 3).map(lambda x: (x[0], x[1] + 1)).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        self.assertEqual(exec_plan['nodes'][1]['type'], 'Projection')\n+\n+    def test_broadcast(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.broadcast().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        broadcast_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = broadcast_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'BROADCAST')\n+\n+    def test_rebalance(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rebalance().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rebalance_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rebalance_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'REBALANCE')\n+\n+    def test_rescale(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.rescale().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        rescale_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = rescale_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'RESCALE')\n+\n+    def test_shuffle(self):\n+        ds_1 = self.env.from_collection([1, 2, 3])\n+        ds_1.shuffle().map(lambda x: x + 1).set_parallelism(3).add_sink(self.test_sink)\n+        exec_plan = eval(self.env.get_execution_plan())\n+        shuffle_node = exec_plan['nodes'][1]\n+        pre_ship_strategy = shuffle_node['predecessors'][0]['ship_strategy']\n+        self.assertEqual(pre_ship_strategy, 'SHUFFLE')", "originalCommit": "ec6e531e07a441ccb328f735cc40d618087798c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTAwNDk3OQ==", "url": "https://github.com/apache/flink/pull/13119#discussion_r469004979", "bodyText": "Thank you , I have added a specific test for overriding partitioning of KeyedStream.\n    def test_keyed_stream_partitioning(self):         \n        ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)])\n        keyed_stream = ds.key_by(lambda x: x[1])\n        with self.assertRaises(Exception):\n            keyed_stream.shuffle()\n\n        with self.assertRaises(Exception):\n            keyed_stream.rebalance()\n\n        with self.assertRaises(Exception):\n            keyed_stream.rescale()\n\n        with self.assertRaises(Exception):\n            keyed_stream.broadcast()\n\n        with self.assertRaises(Exception):\n            keyed_stream.forward()", "author": "shuiqiangchen", "createdAt": "2020-08-12T04:56:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njk1Mg=="}], "type": "inlineReview"}, {"oid": "ca9d8e144dc513a34f5bf31523f1db79757a09a0", "url": "https://github.com/apache/flink/commit/ca9d8e144dc513a34f5bf31523f1db79757a09a0", "message": "[FLINK-18885][python] Correct the partitioning of KeyedStream, add test for KeyedStream partitioning.", "committedDate": "2020-08-12T05:07:35Z", "type": "commit"}]}