{"pr_number": 13957, "pr_title": "[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema", "pr_createdAt": "2020-11-06T08:30:19Z", "pr_url": "https://github.com/apache/flink/pull/13957", "timeline": [{"oid": "1b3e6364c0b98eb4b82d396ab91ed9b928f54a85", "url": "https://github.com/apache/flink/commit/1b3e6364c0b98eb4b82d396ab91ed9b928f54a85", "message": "[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema", "committedDate": "2020-11-07T02:27:21Z", "type": "commit"}, {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "url": "https://github.com/apache/flink/commit/0c4e62b055b5cd62d95be004423262ed9ce5eb86", "message": "Add DebeziumJsonFileSystemITCase", "committedDate": "2020-11-07T03:37:48Z", "type": "commit"}, {"oid": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "url": "https://github.com/apache/flink/commit/0c4e62b055b5cd62d95be004423262ed9ce5eb86", "message": "Add DebeziumJsonFileSystemITCase", "committedDate": "2020-11-07T03:37:48Z", "type": "forcePushed"}, {"oid": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "url": "https://github.com/apache/flink/commit/f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "message": "Fix case", "committedDate": "2020-11-07T05:17:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519100605", "bodyText": "return reader?", "author": "wuchong", "createdAt": "2020-11-07T05:03:35Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/DeserializationSchemaAdapter.java", "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.DelimitedInputFormat;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.ArrayResultIterator;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.metrics.MetricGroup;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.flink.util.UserCodeClassLoader;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.connector.file.src.util.CheckpointedPosition.NO_OFFSET;\n+import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+\n+/**\n+ * Adapter to turn a {@link DeserializationSchema} into a {@link BulkFormat}.\n+ */\n+public class DeserializationSchemaAdapter implements BulkFormat<RowData, FileSourceSplit> {\n+\n+\tprivate static final int BATCH_SIZE = 100;\n+\n+\t// NOTE, deserializationSchema produce full format fields with original order\n+\tprivate final DeserializationSchema<RowData> deserializationSchema;\n+\n+\tprivate final String[] fieldNames;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final int[] projectFields;\n+\tprivate final RowType projectedRowType;\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\n+\tprivate final int[] toProjectedField;\n+\tprivate final RowData.FieldGetter[] formatFieldGetters;\n+\n+\tpublic DeserializationSchemaAdapter(\n+\t\t\tDeserializationSchema<RowData> deserializationSchema,\n+\t\t\tTableSchema schema,\n+\t\t\tint[] projectFields,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue) {\n+\t\tthis.deserializationSchema = deserializationSchema;\n+\t\tthis.fieldNames = schema.getFieldNames();\n+\t\tthis.fieldTypes = schema.getFieldDataTypes();\n+\t\tthis.projectFields = projectFields;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\n+\t\tList<String> projectedNames = Arrays.stream(projectFields)\n+\t\t\t\t.mapToObj(idx -> schema.getFieldNames()[idx])\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.projectedRowType = RowType.of(\n+\t\t\t\tArrays.stream(projectFields).mapToObj(idx ->\n+\t\t\t\t\t\tschema.getFieldDataTypes()[idx].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tprojectedNames.toArray(new String[0]));\n+\n+\t\tList<String> formatFields = Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tList<String> formatProjectedFields = projectedNames.stream()\n+\t\t\t\t.filter(field -> !partitionKeys.contains(field))\n+\t\t\t\t.collect(Collectors.toList());\n+\n+\t\tthis.toProjectedField = formatProjectedFields.stream()\n+\t\t\t\t.mapToInt(projectedNames::indexOf)\n+\t\t\t\t.toArray();\n+\n+\t\tthis.formatFieldGetters = new RowData.FieldGetter[formatProjectedFields.size()];\n+\t\tfor (int i = 0; i < formatProjectedFields.size(); i++) {\n+\t\t\tString name = formatProjectedFields.get(i);\n+\t\t\tthis.formatFieldGetters[i] = RowData.createFieldGetter(\n+\t\t\t\t\tschema.getFieldDataType(name).get().getLogicalType(),\n+\t\t\t\t\tformatFields.indexOf(name));\n+\t\t}\n+\t}\n+\n+\tprivate DeserializationSchema<RowData> createDeserialization() throws IOException {\n+\t\ttry {\n+\t\t\tDeserializationSchema<RowData> deserialization = InstantiationUtil.clone(deserializationSchema);\n+\t\t\tdeserialization.open(new DeserializationSchema.InitializationContext() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic MetricGroup getMetricGroup() {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"MetricGroup is unsupported in BulkFormat.\");\n+\t\t\t\t}\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic UserCodeClassLoader getUserCodeClassLoader() {\n+\t\t\t\t\treturn (UserCodeClassLoader) Thread.currentThread().getContextClassLoader();\n+\t\t\t\t}\n+\t\t\t});\n+\t\t\treturn deserialization;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new IOException(e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Reader createReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\treturn new Reader(config, split);\n+\t}\n+\n+\t@Override\n+\tpublic Reader restoreReader(Configuration config, FileSourceSplit split) throws IOException {\n+\t\tReader reader = new Reader(config, split);\n+\t\treader.seek(split.getReaderPosition().get().getRecordsAfterOffset());\n+\t\treturn null;", "originalCommit": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExOTM2OA==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519119368", "bodyText": "Streaming restore case: no test cover", "author": "JingsongLi", "createdAt": "2020-11-07T05:59:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEyMjcyMg==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519122722", "bodyText": "But there is no way, because the current file system does not support streaming reading", "author": "JingsongLi", "createdAt": "2020-11-07T06:11:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwMDYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519108294", "bodyText": "IntStream.range?", "author": "wuchong", "createdAt": "2020-11-07T05:29:26Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -251,6 +258,14 @@ private RowDataPartitionComputer partitionComputer() {\n \t\t\t\t//noinspection unchecked\n \t\t\t\treturn Optional.of(FileInputFormatCompactReader.factory((FileInputFormat<RowData>) format));\n \t\t\t}\n+\t\t} else if (deserializationFormat != null) {\n+\t\t\t// NOTE, we need pass full format types to deserializationFormat\n+\t\t\tDeserializationSchema<RowData> decoder = deserializationFormat.createRuntimeDecoder(\n+\t\t\t\t\tcreateSourceContext(context), getFormatDataType());\n+\t\t\tint[] projectedFields = IntStream.of(0, schema.getFieldCount()).toArray();", "originalCommit": "0c4e62b055b5cd62d95be004423262ed9ce5eb86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExOTQ2MA==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519119460", "bodyText": "Streaming sink compaction case: no test cover", "author": "JingsongLi", "createdAt": "2020-11-07T05:59:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEyMjU3NA==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519122574", "bodyText": "I'll add Json Compaction test", "author": "JingsongLi", "createdAt": "2020-11-07T06:11:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwODI5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTEwOTcyMA==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519109720", "bodyText": "Select some fields to trigger the projection push down? We can write one more column into the sink, e.g UPPER(name), and then select out the upper_name instead of the `name.", "author": "wuchong", "createdAt": "2020-11-07T05:33:50Z", "path": "flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFileSystemITCase.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.debezium;\n+\n+import org.apache.flink.table.planner.runtime.utils.StreamingTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.flink.util.CollectionUtil;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static java.lang.String.format;\n+\n+/**\n+ * Test Filesystem connector with DebeziumJson.\n+ */\n+public class DebeziumJsonFileSystemITCase extends StreamingTestBase {\n+\n+\tprivate static final List<String> EXPECTED = Arrays.asList(\n+\t\t\t\"+I(101,scooter,Small 2-wheel scooter,3.14)\",\n+\t\t\t\"+I(102,car battery,12V car battery,8.1)\",\n+\t\t\t\"+I(103,12-pack drill bits,12-pack of drill bits with sizes ranging from #40 to #3,0.8)\",\n+\t\t\t\"+I(104,hammer,12oz carpenter's hammer,0.75)\",\n+\t\t\t\"+I(105,hammer,14oz carpenter's hammer,0.875)\",\n+\t\t\t\"+I(106,hammer,16oz carpenter's hammer,1.0)\",\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.3)\",\n+\t\t\t\"+I(108,jacket,water resistent black wind breaker,0.1)\",\n+\t\t\t\"+I(109,spare tire,24 inch spare tire,22.2)\",\n+\t\t\t\"-D(106,hammer,16oz carpenter's hammer,1.0)\", // -U\n+\t\t\t\"+I(106,hammer,18oz carpenter hammer,1.0)\", // +U\n+\t\t\t\"-D(107,rocks,box of assorted rocks,5.3)\", // -U\n+\t\t\t\"+I(107,rocks,box of assorted rocks,5.1)\", // +U\n+\t\t\t\"+I(110,jacket,water resistent white wind breaker,0.2)\",\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.18)\",\n+\t\t\t\"-D(110,jacket,water resistent white wind breaker,0.2)\", // -U\n+\t\t\t\"+I(110,jacket,new water resistent white wind breaker,0.5)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.18)\", // -U\n+\t\t\t\"+I(111,scooter,Big 2-wheel scooter ,5.17)\", // +U\n+\t\t\t\"-D(111,scooter,Big 2-wheel scooter ,5.17)\"\n+\t);\n+\n+\tprivate File source;\n+\tprivate File sink;\n+\n+\tprivate void prepareTables(boolean isPartition) throws IOException {\n+\t\tbyte[] bytes = readBytes(\"debezium-data-schema-exclude.txt\");\n+\t\tsource = TEMPORARY_FOLDER.newFolder();\n+\t\tFile file;\n+\t\tif (isPartition) {\n+\t\t\tFile partition = new File(source, \"p=1\");\n+\t\t\tpartition.mkdirs();\n+\t\t\tfile = new File(partition, \"my_file\");\n+\t\t} else {\n+\t\t\tfile = new File(source, \"my_file\");\n+\t\t}\n+\t\tfile.createNewFile();\n+\t\tFiles.write(file.toPath(), bytes);\n+\n+\t\tsink = TEMPORARY_FOLDER.newFolder();\n+\n+\t\tenv().setParallelism(1);\n+\t}\n+\n+\tprivate void createTable(String name, String path, boolean isPartition) {\n+\t\ttEnv().executeSql(format(\"create table %s (\", name) +\n+\t\t\t\t\"id int, name string, description string, weight float\" +\n+\t\t\t\t(isPartition ? \", p int) partitioned by (p) \" : \")\") +\n+\t\t\t\t\" with (\" +\n+\t\t\t\t\"'connector'='filesystem',\" +\n+\t\t\t\t\"'format'='debezium-json',\" +\n+\t\t\t\tformat(\"'path'='%s'\", path) +\n+\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testNonPartition() throws Exception {\n+\t\tprepareTables(false);\n+\t\tcreateTable(\"source\", source.toURI().toString(), false);\n+\t\tcreateTable(\"sink\", sink.toURI().toString(), false);\n+\n+\t\ttEnv().executeSql(\"insert into sink select * from source\").await();\n+\t\tCloseableIterator<Row> iter = tEnv().executeSql(\"select * from sink\").collect();", "originalCommit": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTExMTIyNQ==", "url": "https://github.com/apache/flink/pull/13957#discussion_r519111225", "bodyText": "remove.", "author": "wuchong", "createdAt": "2020-11-07T05:38:08Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java", "diffHunk": "@@ -94,6 +94,10 @@\n  */\n public class BinaryRowDataTest {\n \n+\tpublic static void main(String[] args) {", "originalCommit": "f9c06b641c424a4e3f4e91ecc668b599dd45b6e1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "41b8b8583a8b8e08979515d6bcb1475fdd231664", "url": "https://github.com/apache/flink/commit/41b8b8583a8b8e08979515d6bcb1475fdd231664", "message": "Address comments", "committedDate": "2020-11-07T06:09:28Z", "type": "commit"}, {"oid": "e71978fc6c6da487d3b75da6a3fa8b86d6bd2f2e", "url": "https://github.com/apache/flink/commit/e71978fc6c6da487d3b75da6a3fa8b86d6bd2f2e", "message": "Add JsonFileCompactionITCase", "committedDate": "2020-11-07T06:13:32Z", "type": "commit"}]}