{"pr_number": 10762, "pr_title": "[FLINK-15115][kafka] Drop Kafka 0.8/0.9", "pr_createdAt": "2020-01-03T14:33:04Z", "pr_url": "https://github.com/apache/flink/pull/10762", "timeline": [{"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "url": "https://github.com/apache/flink/commit/6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-03T14:33:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2MA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318960", "bodyText": "Should we throw UnsupportedOperationException here in order to avoid NPE somewhere else in the code?", "author": "tillrohrmann", "createdAt": "2020-01-10T16:28:12Z", "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;", "originalCommit": "18dd7d344258743705da311508a1fc7d0873da45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMzgyMQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365613821", "bodyText": "Tests still passes with all methods that return something throwing an exception instead, so we'll go with that.", "author": "zentol", "createdAt": "2020-01-12T21:52:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2OQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318969", "bodyText": "Same here with the UnsupportedOperationException.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:28:13Z", "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<MetricName, ? extends Metric> metrics() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<PartitionInfo> partitionsFor(String s) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<String, List<PartitionInfo>> listTopics() {\n+\t\t\treturn null;\n+\t\t}", "originalCommit": "18dd7d344258743705da311508a1fc7d0873da45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMTEwMA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365321100", "bodyText": "docs/dev/datastream_api.md and docs/dev/datastream_api.zh.md still contain a reference to FlinkKafkaConsumer08", "author": "tillrohrmann", "createdAt": "2020-01-10T16:32:35Z", "path": "docs/dev/connectors/kafka.zh.md", "diffHunk": "@@ -161,22 +151,18 @@ Flink \u7684 Kafka consumer \u79f0\u4e3a `FlinkKafkaConsumer08`\uff08\u6216\u9002\u7528\u4e8e Kafka 0.9.\n {% highlight java %}\n Properties properties = new Properties();\n properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n-// \u4ec5 Kafka 0.8 \u9700\u8981\n-properties.setProperty(\"zookeeper.connect\", \"localhost:2181\");\n properties.setProperty(\"group.id\", \"test\");\n DataStream<String> stream = env\n-  .addSource(new FlinkKafkaConsumer08<>(\"topic\", new SimpleStringSchema(), properties));\n+  .addSource(new FlinkKafkaConsumer09<>(\"topic\", new SimpleStringSchema(), properties));\n {% endhighlight %}\n </div>\n <div data-lang=\"scala\" markdown=\"1\">\n {% highlight scala %}\n val properties = new Properties()\n properties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\n-// \u4ec5 Kafka 0.8 \u9700\u8981\n-properties.setProperty(\"zookeeper.connect\", \"localhost:2181\")\n properties.setProperty(\"group.id\", \"test\")\n stream = env\n-    .addSource(new FlinkKafkaConsumer08[String](\"topic\", new SimpleStringSchema(), properties))\n+    .addSource(new FlinkKafkaConsumer09[String](\"topic\", new SimpleStringSchema(), properties))", "originalCommit": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjAwMA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365322000", "bodyText": "flink-runtime/pom.xml contains reference to Kafka 0.8 in line 562.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:34:20Z", "path": "tools/travis/stage.sh", "diffHunk": "@@ -128,11 +128,6 @@ flink-connectors/flink-sql-connector-kafka,\"\n MODULES_TESTS=\"\\\n flink-tests\"\n \n-# we can only build the Kafka 0.8 connector when building for Scala 2.11\n-if [[ $PROFILE == *\"scala-2.11\"* ]]; then\n-    MODULES_CONNECTORS=\"$MODULES_CONNECTORS,flink-connectors/flink-connector-kafka-0.8\"\n-fi\n-", "originalCommit": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365322652", "bodyText": "ConnectorDescriptorValidator.java contains a reference to Kafka 0.8 in line 47.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:35:36Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java", "diffHunk": "@@ -38,7 +38,6 @@\n public class KafkaValidator extends ConnectorDescriptorValidator {\n \n \tpublic static final String CONNECTOR_TYPE_VALUE_KAFKA = \"kafka\";\n-\tpublic static final String CONNECTOR_VERSION_VALUE_08 = \"0.8\";", "originalCommit": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMzAwNQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365323005", "bodyText": "KafkaShortRetentionTestBase.java contains in line 253 Kafka 0.8 specific code. In line 254 the same class contains Kafka 0.9 specific code.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyNjEwOA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365326108", "bodyText": "docs/dev/event_time.md, docs/dev/event_time.zh.md, docs/dev/event_timestamps_watermarks.md and docs/dev/event_timestamps_watermarks.zh.md contains a reference to FlinkKafkaConsumer09", "author": "tillrohrmann", "createdAt": "2020-01-10T16:42:54Z", "path": "docs/dev/connectors/kafka.md", "diffHunk": "@@ -263,7 +255,7 @@ Example:\n {% highlight java %}\n final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n \n-FlinkKafkaConsumer09<String> myConsumer = new FlinkKafkaConsumer09<>(...);", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyNzc1MA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365327750", "bodyText": "In line 117, there is still a dependency to flink-connector-kafka-0.9.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:46:15Z", "path": "flink-connectors/flink-connector-kafka-0.10/pom.xml", "diffHunk": "@@ -46,7 +46,7 @@ under the License.\n \n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n-\t\t\t<artifactId>flink-connector-kafka-0.9_${scala.binary.version}</artifactId>\n+\t\t\t<artifactId>flink-connector-kafka-base_${scala.binary.version}</artifactId>", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODQ4MQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328481", "bodyText": "Can we avoid null and instead pass in an empty collection? If not, then let's add @Nullable annotation.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:47:51Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -173,7 +205,38 @@ public FlinkKafkaConsumer010(Pattern subscriptionPattern, DeserializationSchema<\n \t */\n \t@PublicEvolving\n \tpublic FlinkKafkaConsumer010(Pattern subscriptionPattern, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(subscriptionPattern, deserializer, props);\n+\t\tthis(null, subscriptionPattern, deserializer, props);", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMjg3NQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365612875", "bodyText": "The KafkTopicsDescriptor actually relies on this being null. I'll add Nullable to the private constructor for the time being.", "author": "zentol", "createdAt": "2020-01-12T21:39:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODY4OQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328689", "bodyText": "Same here with the null value for Pattern. I think it would be good to avoid it if possible.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:48:20Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -130,7 +162,7 @@ public FlinkKafkaConsumer010(List<String> topics, DeserializationSchema<T> deser\n \t *           The properties that are used to configure both the fetcher and the offset handler.\n \t */\n \tpublic FlinkKafkaConsumer010(List<String> topics, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(topics, deserializer, props);\n+\t\tthis(topics, null, deserializer, props);", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMjk4Nw==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365612987", "bodyText": "Will add Nullable with the same reasoning as for the list of topics.", "author": "zentol", "createdAt": "2020-01-12T21:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODY4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTA3MQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331071", "bodyText": "I would suggest to either fail or to return an empty map but not null.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:53:28Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTE0Mw==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331143", "bodyText": "Same here with null.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:53:37Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9db1c796618532ffa1ca3542714d56957aa1bffc", "url": "https://github.com/apache/flink/commit/9db1c796618532ffa1ca3542714d56957aa1bffc", "message": "[FLINK-15115][kafka] Reduce mocking\n\nReplace the mocked KafkaConsumer with a custom Consumer implementation. Migrate all users toward the Consumer interface instead of the concrete KafkaConsumer implementation.\nUsing an actual implementation highlights API changes when migrating to later custom versions and prevents issues due to subtle mocking gotchas.", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "url": "https://github.com/apache/flink/commit/9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "message": "[FLINK-15115][kafka] Drop Kafka 0.8", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "url": "https://github.com/apache/flink/commit/2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9 SQL jar", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "url": "https://github.com/apache/flink/commit/458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "message": "[hotfix][kafka][legal] Correct version in NOTICE", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-13T11:38:18Z", "type": "forcePushed"}]}