{"pr_number": 13990, "pr_title": "[FLINK-20053][table][doc] Add document for file compaction", "pr_createdAt": "2020-11-09T02:55:06Z", "pr_url": "https://github.com/apache/flink/pull/13990", "timeline": [{"oid": "bcabafeacefca7370b3c2f3569ff8608704a282b", "url": "https://github.com/apache/flink/commit/bcabafeacefca7370b3c2f3569ff8608704a282b", "message": "[FLINK-20053][table][doc] Add document for file compaction", "committedDate": "2020-11-09T02:52:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMTgyMQ==", "url": "https://github.com/apache/flink/pull/13990#discussion_r520211821", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If you want a smaller checkpoint interval and do not want to generate a large number of small files,\n          \n          \n            \n            it is recommended that you open file compaction:\n          \n          \n            \n            The file sink supports file compactions, which allows applications to have smaller checkpoint intervals without generating a large number of files.", "author": "sjwiesman", "createdAt": "2020-11-10T00:41:13Z", "path": "docs/dev/table/connectors/filesystem.md", "diffHunk": "@@ -150,6 +150,41 @@ become finished on the next checkpoint) control the size and number of these par\n **NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.rollover-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together\n if you don't want to wait a long period before observe the data exists in file system. For other formats (avro, orc), you can just set parameter `execution.checkpointing.interval` in flink-conf.yaml.\n \n+### File Compaction\n+\n+If you want a smaller checkpoint interval and do not want to generate a large number of small files,\n+it is recommended that you open file compaction:", "originalCommit": "bcabafeacefca7370b3c2f3569ff8608704a282b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMjQ5Mw==", "url": "https://github.com/apache/flink/pull/13990#discussion_r520212493", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            After you open file compaction, small files that are not large enough will be merged into large files,\n          \n          \n            \n            It is worth noting that:\n          \n          \n            \n            - Only files in a single checkpoint are compacted, that is, at least the same number of files as the number of checkpoints is generated.\n          \n          \n            \n            - The file before merging is invisible, so the visibility of the file may be: checkpoint interval + compaction time.\n          \n          \n            \n            If enabled, file compaction will merge multiple small files into larger files based on the target file size.\n          \n          \n            \n            When running file compaction in production, please be aware that:\n          \n          \n            \n            - Only files in a single checkpoint are compacted, that is, at least the same number of files as the number of checkpoints is generated.\n          \n          \n            \n            - The file before merging is invisible, so the visibility of the file may be: checkpoint interval + compaction time.", "author": "sjwiesman", "createdAt": "2020-11-10T00:43:22Z", "path": "docs/dev/table/connectors/filesystem.md", "diffHunk": "@@ -150,6 +150,41 @@ become finished on the next checkpoint) control the size and number of these par\n **NOTE:** For row formats (csv, json), you can set the parameter `sink.rolling-policy.file-size` or `sink.rolling-policy.rollover-interval` in the connector properties and parameter `execution.checkpointing.interval` in flink-conf.yaml together\n if you don't want to wait a long period before observe the data exists in file system. For other formats (avro, orc), you can just set parameter `execution.checkpointing.interval` in flink-conf.yaml.\n \n+### File Compaction\n+\n+If you want a smaller checkpoint interval and do not want to generate a large number of small files,\n+it is recommended that you open file compaction:\n+\n+<table class=\"table table-bordered\">\n+  <thead>\n+    <tr>\n+        <th class=\"text-left\" style=\"width: 20%\">Key</th>\n+        <th class=\"text-left\" style=\"width: 15%\">Default</th>\n+        <th class=\"text-left\" style=\"width: 10%\">Type</th>\n+        <th class=\"text-left\" style=\"width: 55%\">Description</th>\n+    </tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+        <td><h5>auto-compaction</h5></td>\n+        <td style=\"word-wrap: break-word;\">false</td>\n+        <td>Boolean</td>\n+        <td>Whether to enable automatic compaction in streaming sink or not. The data will be written to temporary files. After the checkpoint is completed, the temporary files generated by a checkpoint will be compacted. The temporary files are invisible before compaction.</td>\n+    </tr>\n+    <tr>\n+        <td><h5>compaction.file-size</h5></td>\n+        <td style=\"word-wrap: break-word;\">(none)</td>\n+        <td>MemorySize</td>\n+        <td>The compaction target file size, the default value is the rolling file size.</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+After you open file compaction, small files that are not large enough will be merged into large files,\n+It is worth noting that:\n+- Only files in a single checkpoint are compacted, that is, at least the same number of files as the number of checkpoints is generated.\n+- The file before merging is invisible, so the visibility of the file may be: checkpoint interval + compaction time.", "originalCommit": "bcabafeacefca7370b3c2f3569ff8608704a282b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5e8032593a51b2e783d5112d6ce8bdb1d296f1ad", "url": "https://github.com/apache/flink/commit/5e8032593a51b2e783d5112d6ce8bdb1d296f1ad", "message": "Update docs/dev/table/connectors/filesystem.md\n\nCo-authored-by: Seth Wiesman <sjwiesman@gmail.com>", "committedDate": "2020-11-10T05:48:23Z", "type": "commit"}, {"oid": "47b5c80ab665abae3fe3df53e1faacd91962171c", "url": "https://github.com/apache/flink/commit/47b5c80ab665abae3fe3df53e1faacd91962171c", "message": "Update docs/dev/table/connectors/filesystem.md\n\nCo-authored-by: Seth Wiesman <sjwiesman@gmail.com>", "committedDate": "2020-11-10T05:48:31Z", "type": "commit"}, {"oid": "5a9da943be11b60050d27af2c1fc3887b0a2d5e1", "url": "https://github.com/apache/flink/commit/5a9da943be11b60050d27af2c1fc3887b0a2d5e1", "message": "Copy to chinese doc", "committedDate": "2020-11-10T05:49:49Z", "type": "commit"}, {"oid": "3b8ea5e344eee55ad6df6d186b6c0740206a4726", "url": "https://github.com/apache/flink/commit/3b8ea5e344eee55ad6df6d186b6c0740206a4726", "message": "delete accidental files", "committedDate": "2020-11-11T02:32:38Z", "type": "commit"}]}