{"pr_number": 12657, "pr_title": "[FLINK-18086][tests][e2e][kafka] Migrate SQLClientKafkaITCase to use DDL and new options to create tables", "pr_createdAt": "2020-06-15T12:54:30Z", "pr_url": "https://github.com/apache/flink/pull/12657", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU2OTM0NA==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440569344", "bodyText": "most operations in executeQueryInternal  method and executeUpdateInternal method are already wrapped in user classloader, only deployer.deploy() is  needed. Otherwise, it's better we should remove those wrappers.  btw, add some tests in sql client to verify the fix ?", "author": "godfreyhe", "createdAt": "2020-06-16T03:45:13Z", "path": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java", "diffHunk": "@@ -462,7 +462,7 @@ public ResolvedExpression parseSqlExpression(String sqlExpression, TableSchema i\n \t@Override\n \tpublic ResultDescriptor executeQuery(String sessionId, String query) throws SqlExecutionException {\n \t\tfinal ExecutionContext<?> context = getExecutionContext(sessionId);\n-\t\treturn executeQueryInternal(sessionId, context, query);\n+\t\treturn context.wrapClassLoader(() -> executeQueryInternal(sessionId, context, query));", "originalCommit": "14cdb5d641588f9dfa7f8536fe92e3e57abd8e14", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "url": "https://github.com/apache/flink/commit/50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover.", "committedDate": "2020-06-16T03:51:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNDYzOQ==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440624639", "bodyText": "I don't think we need start a thread.\nThe inputs are something like commands, should print them blocking.", "author": "JingsongLi", "createdAt": "2020-06-16T06:54:35Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {\n+\t\tnew Thread(() -> {", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY0Mzc2Mw==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440643763", "bodyText": "This keeps align with processInputStream, we shouldn't block here, because the blocking/non-blocking is determined by user AutoClosableProcessBuilder#runNonBlocking or AutoClosableProcessBuilder#runBlocking().", "author": "wuchong", "createdAt": "2020-06-16T07:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNDYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY0NjU3Mg==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440646572", "bodyText": "I don't think it should be same to processInputStream.\nFor a process, maybe this thread not start, but process ended. I'd like to think of Input lines as commands.", "author": "JingsongLi", "createdAt": "2020-06-16T07:37:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNDYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY1MTM4Mg==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440651382", "bodyText": "Even if we don't start a thread here, it is still possible the process is finished when we want to print lines to the process.", "author": "wuchong", "createdAt": "2020-06-16T07:46:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNDYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNTMzOA==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440625338", "bodyText": "processOutputStream(inputs) looks confuse to me. Can we just name it printLinesToProcess?", "author": "JingsongLi", "createdAt": "2020-06-16T06:56:00Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY1NTkyNA==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440655924", "bodyText": "I just want to keep align with the previous processStream(InputStream ..., Consumer...). I think we should have a consistent naming for them. Howabout rename them to consumeOutput(InputStream, Consumer), produceInput(OutputStream, List<String>)?", "author": "wuchong", "createdAt": "2020-06-16T07:53:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNTMzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNjQ5OQ==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440626499", "bodyText": "inputs -> inputLines?", "author": "JingsongLi", "createdAt": "2020-06-16T06:58:23Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYyNjYwMQ==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440626601", "bodyText": "inputLines can be String...", "author": "JingsongLi", "createdAt": "2020-06-16T06:58:36Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -89,6 +95,11 @@ public AutoClosableProcessBuilder setStderrProcessor(final Consumer<String> stde\n \t\t\treturn this;\n \t\t}\n \n+\t\tpublic AutoClosableProcessBuilder setStdInputs(final List<String> inputLines) {", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYzMDA0Mw==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440630043", "bodyText": "Add disable-quote-character to remove \"?", "author": "JingsongLi", "createdAt": "2020-06-16T07:06:07Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test/java/org/apache/flink/tests/util/kafka/SQLClientKafkaITCase.java", "diffHunk": "@@ -179,80 +174,55 @@ public void testKafka() throws Exception {\n \t\t}\n \t}\n \n-\tprivate void insertIntoAvroTable(ClusterController clusterController) throws IOException {\n-\t\tLOG.info(\"Executing SQL: Kafka {} JSON -> Kafka {} Avro\", kafkaSQLVersion, kafkaSQLVersion);\n-\t\tString sqlStatement1 = \"INSERT INTO AvroBothTable\\n\" +\n-\t\t\t\t\"  SELECT\\n\" +\n-\t\t\t\t\"    CAST(TUMBLE_START(rowtime, INTERVAL '1' HOUR) AS VARCHAR) AS event_timestamp,\\n\" +\n-\t\t\t\t\"    user,\\n\" +\n-\t\t\t\t\"    RegReplace(event.message, ' is ', ' was ') AS message,\\n\" +\n-\t\t\t\t\"    COUNT(*) AS duplicate_count\\n\" +\n-\t\t\t\t\"  FROM JsonSourceTable\\n\" +\n-\t\t\t\t\"  WHERE user IS NOT NULL\\n\" +\n-\t\t\t\t\"  GROUP BY\\n\" +\n-\t\t\t\t\"    user,\\n\" +\n-\t\t\t\t\"    event.message,\\n\" +\n-\t\t\t\t\"    TUMBLE(rowtime, INTERVAL '1' HOUR)\";\n-\n-\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlStatement1)\n-\t\t\t\t.addJar(sqlAvroJar)\n-\t\t\t\t.addJars(apacheAvroJars)\n-\t\t\t\t.addJar(sqlJsonJar)\n-\t\t\t\t.addJar(sqlConnectorKafkaJar)\n-\t\t\t\t.addJar(sqlToolBoxJar)\n-\t\t\t\t.setSessionEnvFile(this.sqlClientSessionConf.toAbsolutePath().toString())\n-\t\t\t\t.build());\n-\t}\n-\n-\tprivate void insertIntoCsvSinkTable(ClusterController clusterController) throws IOException {\n-\t\tLOG.info(\"Executing SQL: Kafka {} Avro -> Csv sink\", kafkaSQLVersion);\n-\t\tString sqlStatement2 = \"INSERT INTO CsvSinkTable\\n\" +\n-\t\t\t\t\"   SELECT AvroBothTable.*, RegReplace('Test constant folding.', 'Test', 'Success') AS constant\\n\" +\n-\t\t\t\t\"   FROM AvroBothTable\";\n-\n-\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlStatement2)\n-\t\t\t\t.addJar(sqlAvroJar)\n-\t\t\t\t.addJars(apacheAvroJars)\n-\t\t\t\t.addJar(sqlJsonJar)\n-\t\t\t\t.addJar(sqlConnectorKafkaJar)\n-\t\t\t\t.addJar(sqlToolBoxJar)\n-\t\t\t\t.setSessionEnvFile(this.sqlClientSessionConf.toAbsolutePath().toString())\n-\t\t\t\t.build()\n-\t\t);\n+\tprivate void executeSqlStatements(ClusterController clusterController, List<String> sqlLines) throws IOException {\n+\t\tLOG.info(\"Executing Kafka {} end-to-end SQL statements.\", kafkaSQLVersion);\n+\t\tclusterController.submitSQLJob(new SQLJobSubmission.SQLJobSubmissionBuilder(sqlLines)\n+\t\t\t.addJar(sqlAvroJar)\n+\t\t\t.addJars(apacheAvroJars)\n+\t\t\t.addJar(sqlConnectorKafkaJar)\n+\t\t\t.addJar(sqlToolBoxJar)\n+\t\t\t.build());\n \t}\n \n-\tprivate String initializeSessionYaml(Map<String, String> vars) throws IOException {\n-\t\tURL url = SQLClientKafkaITCase.class.getClassLoader().getResource(KAFKA_JSON_SOURCE_SCHEMA_YAML);\n+\tprivate List<String> initializeSqlLines(Map<String, String> vars) throws IOException {\n+\t\tURL url = SQLClientKafkaITCase.class.getClassLoader().getResource(KAFKA_E2E_SQL);\n \t\tif (url == null) {\n-\t\t\tthrow new FileNotFoundException(KAFKA_JSON_SOURCE_SCHEMA_YAML);\n+\t\t\tthrow new FileNotFoundException(KAFKA_E2E_SQL);\n \t\t}\n \n-\t\tString schema = FileUtils.readFileUtf8(new File(url.getFile()));\n-\t\tfor (Map.Entry<String, String> var : vars.entrySet()) {\n-\t\t\tschema = schema.replace(var.getKey(), var.getValue());\n+\t\tList<String> lines = Files.readAllLines(new File(url.getFile()).toPath());\n+\t\tList<String> result = new ArrayList<>();\n+\t\tfor (String line : lines) {\n+\t\t\tfor (Map.Entry<String, String> var : vars.entrySet()) {\n+\t\t\t\tline = line.replace(var.getKey(), var.getValue());\n+\t\t\t}\n+\t\t\tresult.add(line);\n \t\t}\n-\t\treturn schema;\n+\n+\t\treturn result;\n \t}\n \n \tprivate void checkCsvResultFile() throws Exception {\n \t\tboolean success = false;\n \t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofSeconds(120));\n-\t\twhile (!success && deadline.hasTimeLeft()) {\n+\t\twhile (deadline.hasTimeLeft()) {\n \t\t\tif (Files.exists(result)) {\n-\t\t\t\tbyte[] bytes = Files.readAllBytes(result);\n-\t\t\t\tString[] lines = new String(bytes, Charsets.UTF_8).split(\"\\n\");\n-\t\t\t\tif (lines.length == 4) {\n+\t\t\t\tList<String> lines = readCsvResultFiles(result);\n+\t\t\t\tif (lines.size() == 4) {\n \t\t\t\t\tsuccess = true;\n \t\t\t\t\tassertThat(\n-\t\t\t\t\t\tlines,\n+\t\t\t\t\t\tlines.toArray(new String[0]),\n \t\t\t\t\t\tarrayContainingInAnyOrder(\n-\t\t\t\t\t\t\t\"2018-03-12 08:00:00.000,Alice,This was a warning.,2,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Bob,This was another warning.,1,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Steve,This was another info.,2,Success constant folding.\",\n-\t\t\t\t\t\t\t\"2018-03-12 09:00:00.000,Alice,This was a info.,1,Success constant folding.\"\n+\t\t\t\t\t\t\t\"\\\"2018-03-12 08:00:00.000\\\",Alice,\\\"This was a warning.\\\",2,\\\"Success constant folding.\\\"\",", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcxODgyMA==", "url": "https://github.com/apache/flink/pull/12657#discussion_r440718820", "bodyText": "Actually, try with resource will close this output put stream, and in this way, will close process.\nI think you should create a better method name for this, and add comments to explain.", "author": "JingsongLi", "createdAt": "2020-06-16T09:35:47Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/AutoClosableProcess.java", "diffHunk": "@@ -167,6 +186,18 @@ private static void processStream(final InputStream stream, final Consumer<Strin\n \t\t).start();\n \t}\n \n+\tprivate static void processOutputStream(final OutputStream stream, final List<String> inputs) {\n+\t\tnew Thread(() -> {\n+\t\t\ttry (PrintStream printStream = new PrintStream(stream, true, StandardCharsets.UTF_8.name())) {", "originalCommit": "50108ca39ff114f9c85ae5f2b5cc72eb3dd4357a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "url": "https://github.com/apache/flink/commit/957ed3e9d9c1d0b48d11f6573dc3a0d85de01fd9", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover.", "committedDate": "2020-06-16T15:16:21Z", "type": "forcePushed"}, {"oid": "7ce45b5c846fa276c21e2c7aa4b9ff1c29ddb1f7", "url": "https://github.com/apache/flink/commit/7ce45b5c846fa276c21e2c7aa4b9ff1c29ddb1f7", "message": "[FLINK-18086][tests] Support to set standard inputs for AutoClosableProcess", "committedDate": "2020-06-16T16:39:51Z", "type": "commit"}, {"oid": "06d96b5f671575b0334b9ee21829e8c8e178f3c6", "url": "https://github.com/apache/flink/commit/06d96b5f671575b0334b9ee21829e8c8e178f3c6", "message": "[FLINK-18086][e2e] Migrate SQLClientKafkaITCase to use DDL and new options to create tables\n\nThis closes #12657", "committedDate": "2020-06-16T16:39:51Z", "type": "commit"}, {"oid": "ca1f7642c7120800f1ac93575dcdfa0223c097a4", "url": "https://github.com/apache/flink/commit/ca1f7642c7120800f1ac93575dcdfa0223c097a4", "message": "[FLINK-18302][sql-cli] Fix SQL client uses wrong class loader when execute INSERT statements", "committedDate": "2020-06-16T16:39:51Z", "type": "commit"}, {"oid": "e2db1dcc6a5a60210a4452849048b915cf794aaf", "url": "https://github.com/apache/flink/commit/e2db1dcc6a5a60210a4452849048b915cf794aaf", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover.", "committedDate": "2020-06-16T16:39:51Z", "type": "commit"}, {"oid": "e2db1dcc6a5a60210a4452849048b915cf794aaf", "url": "https://github.com/apache/flink/commit/e2db1dcc6a5a60210a4452849048b915cf794aaf", "message": "[FLINK-18303][filesystem][hive] Fix Filesystem connector doesn't flush part files after rolling interval\n\nThis commit introduces option 'sink.rolling-policy.check-interval' (default 1min) to control the frequency to check part file rollover.", "committedDate": "2020-06-16T16:39:51Z", "type": "forcePushed"}]}