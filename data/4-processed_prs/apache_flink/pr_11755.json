{"pr_number": 11755, "pr_title": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory", "pr_createdAt": "2020-04-15T12:47:24Z", "pr_url": "https://github.com/apache/flink/pull/11755", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTU3Mg==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665572", "bodyText": "Can we abstract RowCsvInputFormatTest to reuse code?", "author": "JingsongLi", "createdAt": "2020-04-22T04:43:55Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java", "diffHunk": "@@ -0,0 +1,798 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test suites for {@link BaseRowCsvInputformat}.\n+ */\n+public class BaseRowCsvInputformatTest {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTc0NA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665744", "bodyText": "After #11796 , please add streaming test too.", "author": "JingsongLi", "createdAt": "2020-04-22T04:44:28Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * Test fot {@link BaseRowCsvFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class BaseRowCsvFilesystemITCase extends BatchFileSystemITCaseBase {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NjYzMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412666630", "bodyText": "This class is for single row test? I think we don't need test single row, should be include in CsvRowDeSerializationSchemaTest.\nWe should add multi-rows test, or just add tests in RowCsvFilesystemITCase.java", "author": "JingsongLi", "createdAt": "2020-04-22T04:47:18Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Assert;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.function.Consumer;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test for {@link BaseRowCsvInputformat} and {@link BaseRowCsvEncoder}.\n+ */\n+public class BaseRowCsvDeSerializationTest extends TestLogger {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NzEwMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412667101", "bodyText": "use RowPartitionComputer.restorePartValueFromType instead", "author": "JingsongLi", "createdAt": "2020-04-22T04:48:50Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java", "diffHunk": "@@ -0,0 +1,310 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.createFieldRuntimeConverters;\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Input format that reads csv file into {@link BaseRow}.\n+ */\n+public class BaseRowCsvInputformat extends AbstractCsvInputFormat<BaseRow> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final int[] selectFields;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final boolean ignoreParseErrors;\n+\tprivate final long limit;\n+\tprivate final List<String> csvFieldNames;\n+\tprivate final List<TypeInformation> csvFieldTypes;\n+\tprivate final List<String> csvSelectFieldNames;\n+\tprivate final List<TypeInformation> csvSelectTypes;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient CsvRowDeserializationSchema.RuntimeConverter runtimeConverter;\n+\tprivate transient List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate transient MappingIterator<JsonNode> iterator;\n+\tprivate transient boolean end;\n+\tprivate transient long emitted;\n+\t// reuse object for per record\n+\tprivate transient GenericRow rowData;\n+\n+\tprivate BaseRowCsvInputformat(\n+\t\tPath[] filePaths,\n+\t\tCsvSchema csvSchema,\n+\t\tRowTypeInfo rowType,\n+\t\tint[] selectFields,\n+\t\tList<String> partitionKeys,\n+\t\tString defaultPartValue,\n+\t\tboolean ignoreParseErrors,\n+\t\tlong limit) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\t\t\tthis.emitted = 0;\n+\t\t\t// partition field\n+\t\t\tthis.csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvFieldTypes = csvFieldNames.stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\t// project field\n+\t\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t\tthis.csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tsuper.open(split);\n+\t\tthis.end = false;\n+\t\tthis.iterator = new CsvMapper()\n+\t\t\t.readerFor(JsonNode.class)\n+\t\t\t.with(csvSchema)\n+\t\t\t.readValues(csvInputStream);\n+\t\tprepareRuntimeConverter();\n+\t\tfillPartitionValueForRecord();\n+\t}\n+\n+\tprivate void fillPartitionValueForRecord() {\n+\t\trowData = new GenericRow(selectFields.length);\n+\t\tPath path = currentSplit.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trowData.setField(i, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dff5cdb51db746c02151afd5b4624555def81fac", "url": "https://github.com/apache/flink/commit/dff5cdb51db746c02151afd5b4624555def81fac", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory.", "committedDate": "2020-05-07T08:28:41Z", "type": "forcePushed"}, {"oid": "4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "url": "https://github.com/apache/flink/commit/4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "message": "[FLINK-14267] Introduce Row Csv Encoder && [FLINK-14257] Integrate csv to file system connector", "committedDate": "2020-05-07T15:06:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIwMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441201", "bodyText": "Add empty line.", "author": "JingsongLi", "createdAt": "2020-05-09T02:08:43Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override", "originalCommit": "64d407d3530d308b4504282ae8fadb1503955f99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIxMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441210", "bodyText": "Remove empty line", "author": "JingsongLi", "createdAt": "2020-05-09T02:08:52Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+", "originalCommit": "64d407d3530d308b4504282ae8fadb1503955f99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767499", "bodyText": "return null after end = true. And remove !reachedEnd() in while.\nThis can make codes clear and correct emitted number.", "author": "JingsongLi", "createdAt": "2020-05-11T04:03:15Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;", "originalCommit": "0fd33d570a8591ef7e5a523c29366a34aa6b986e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4NDUyMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422984520", "bodyText": "I update the logic, please review again.", "author": "leonardBang", "createdAt": "2020-05-11T11:51:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzY4Ng==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767686", "bodyText": "Minor: this class can be a lambda.", "author": "JingsongLi", "createdAt": "2020-05-11T04:03:55Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;\n+\t\t\t\t} catch (Throwable t) {\n+\t\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\t\tthrow new IOException(\"Failed to deserialize CSV row.\", t);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (csvRow == null && !reachedEnd());\n+\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tif (csvRow != null) {\n+\t\t\t\treturnRecord = rowData;\n+\t\t\t\tfor (int i = 0; i < csvSelectFieldToCsvFieldMapping.length; i++) {\n+\t\t\t\t\treturnRecord.setField(csvSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\tcsvRow.getField(csvSelectFieldToCsvFieldMapping[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\temitted++;\n+\t\t\treturn returnRecord;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\tsuper.close();\n+\t\t\tif (reader != null) {\n+\t\t\t\treader.close();\n+\t\t\t\treader = null;\n+\t\t\t}\n+\t\t\tif (inputStreamReader != null) {\n+\t\t\t\tinputStreamReader.close();\n+\t\t\t\tinputStreamReader = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A {@link Encoder} writes {@link RowData} record into {@link java.io.OutputStream} with csv format.\n+\t */\n+\tpublic static class CsvRowDataEncoder implements Encoder<RowData> {", "originalCommit": "0fd33d570a8591ef7e5a523c29366a34aa6b986e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b72a27e8fa3a6b21e5505435be096eea97d7da07", "url": "https://github.com/apache/flink/commit/b72a27e8fa3a6b21e5505435be096eea97d7da07", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory", "committedDate": "2020-05-11T10:12:44Z", "type": "commit"}, {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "message": "rebase and address comment", "committedDate": "2020-05-11T11:52:11Z", "type": "commit"}, {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "message": "rebase and address comment", "committedDate": "2020-05-11T11:52:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzY2NA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423664", "bodyText": "CsvFileSystemFormatFactory", "author": "JingsongLi", "createdAt": "2020-05-12T02:18:04Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzgyNQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423825", "bodyText": "Use getOptionalChar.ifPresent getOptionalString.ifPresent", "author": "JingsongLi", "createdAt": "2020-05-12T02:18:39Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzk0MQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423941", "bodyText": "CsvFilesystemBatchITCase\nOthers too.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:07Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAxOA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424018", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:27Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAzMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424031", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:31Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {\n+\n+\t\t@Override\n+\t\tpublic String[] formatProperties() {\n+\t\t\tList<String> ret = new ArrayList<>();\n+\t\t\tret.add(\"'format'='csv'\");\n+\t\t\tret.add(\"'format.field-delimiter'=';'\");\n+\t\t\tret.add(\"'format.quote-character'='#'\");\n+\t\t\treturn ret.toArray(new String[0]);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Enriched IT cases that including testParseError and testEscapeChar for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class EnrichedCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDA1MQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424051", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:36Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in stream mode.\n+ */\n+public class CsvRowDataFilesystemStreamITCase extends FsStreamingSinkITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "message": "address comments", "committedDate": "2020-05-12T04:06:03Z", "type": "commit"}, {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "message": "address comments", "committedDate": "2020-05-12T04:06:03Z", "type": "forcePushed"}]}