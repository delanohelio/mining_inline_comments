{"pr_number": 12687, "pr_title": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "pr_createdAt": "2020-06-16T16:36:55Z", "pr_url": "https://github.com/apache/flink/pull/12687", "timeline": [{"oid": "472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "url": "https://github.com/apache/flink/commit/472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "message": "update e2e test with new connector format", "committedDate": "2020-06-17T06:49:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyNTcwOQ==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441325709", "bodyText": "Add license file for protobuf", "author": "JingsongLi", "createdAt": "2020-06-17T07:06:16Z", "path": "flink-connectors/flink-sql-connector-hbase/src/main/resources/META-INF/NOTICE", "diffHunk": "@@ -0,0 +1,27 @@\n+flink-sql-connector-hbase\n+Copyright 2014-2020 The Apache Software Foundation\n+\n+This product includes software developed at\n+The Apache Software Foundation (http://www.apache.org/).\n+\n+This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)\n+\n+- org.apache.hbase:hbase-common:1.4.3\n+- org.apache.hbase:hbase-protocol:1.4.3\n+- org.apache.hbase:hbase-procedure:1.4.3\n+- org.apache.hbase:hbase-client:1.4.3\n+- org.apache.hbase:hbase-prefix-tree:1.4.3\n+- org.apache.htrace:htrace-core:3.1.0-incubating\n+- org.apache.zookeeper:zookeeper:3.4.14\n+- commons-codec:commons-codec:1.10\n+- commons-configuration:commons-configuration:1.7\n+- commons-lang:commons-lang:2.6\n+- commons-logging:commons-logging:1.1.3\n+- com.google.guava:guava:12.0.1\n+- com.yammer.metrics:metrics-core:2.2.0\n+- io.netty:netty-all:4.1.44.Final\n+\n+This project bundles the following dependencies under the BSD license.\n+See bundled license files for details.\n+\n+- com.google.protobuf:protobuf-java:2.5.0", "originalCommit": "472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyODM0OA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441328348", "bodyText": "sort them by order", "author": "JingsongLi", "createdAt": "2020-06-17T07:11:37Z", "path": "flink-connectors/flink-sql-connector-hbase/src/main/resources/META-INF/NOTICE", "diffHunk": "@@ -0,0 +1,27 @@\n+flink-sql-connector-hbase\n+Copyright 2014-2020 The Apache Software Foundation\n+\n+This product includes software developed at\n+The Apache Software Foundation (http://www.apache.org/).\n+\n+This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)\n+\n+- org.apache.hbase:hbase-common:1.4.3", "originalCommit": "472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM2NzY0Mg==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441367642", "bodyText": "Simplify to\nAutoClosableProcess\n\t.create(hbaseDir.resolve(Paths.get(\"bin\", \"hbase\")).toString(), \"shell\")\n\t.setStdoutProcessor(stdoutProcessor)\n\t.setStdInputs(cmd)\n\t.runBlocking();", "author": "wuchong", "createdAt": "2020-06-17T08:19:41Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-hbase/src/main/java/org/apache/flink/tests/util/hbase/LocalStandaloneHBaseResource.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.tests.util.hbase;\n+\n+import org.apache.flink.tests.util.AutoClosableProcess;\n+import org.apache.flink.tests.util.CommandLineWrapper;\n+import org.apache.flink.tests.util.activation.OperatingSystemRestriction;\n+import org.apache.flink.tests.util.cache.DownloadCache;\n+import org.apache.flink.util.OperatingSystem;\n+\n+import org.junit.rules.TemporaryFolder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * {@link HBaseResource} that downloads hbase and set up a local hbase cluster.\n+ */\n+public class LocalStandaloneHBaseResource implements HBaseResource {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(LocalStandaloneHBaseResource.class);\n+\n+\tprivate final TemporaryFolder tmp = new TemporaryFolder();\n+\n+\tprivate final DownloadCache downloadCache = DownloadCache.get();\n+\tprivate Path hbaseDir;\n+\n+\tLocalStandaloneHBaseResource() {\n+\t\tOperatingSystemRestriction.forbid(\n+\t\t\tString.format(\"The %s relies on UNIX utils and shell scripts.\", getClass().getSimpleName()),\n+\t\t\tOperatingSystem.WINDOWS);\n+\t}\n+\n+\tprivate static String getHBaseDownloadUrl() {\n+\t\treturn \"https://archive.apache.org/dist/hbase/1.4.3/hbase-1.4.3-bin.tar.gz\";\n+\t}\n+\n+\t@Override\n+\tpublic void before() throws Exception {\n+\t\ttmp.create();\n+\t\tdownloadCache.before();\n+\n+\t\tthis.hbaseDir = tmp.newFolder(\"hbase\").toPath().toAbsolutePath();\n+\t\tsetupHBaseDist();\n+\t\tsetupHBaseCluster();\n+\t}\n+\n+\tprivate void setupHBaseDist() throws IOException {\n+\t\tfinal Path downloadDirectory = tmp.newFolder(\"getOrDownload\").toPath();\n+\t\tfinal Path hbaseArchive = downloadCache.getOrDownload(getHBaseDownloadUrl(), downloadDirectory);\n+\n+\t\tLOG.info(\"HBase localtion: {}\", hbaseDir.toAbsolutePath());\n+\t\tAutoClosableProcess.runBlocking(CommandLineWrapper\n+\t\t\t.tar(hbaseArchive)\n+\t\t\t.extract()\n+\t\t\t.zipped()\n+\t\t\t.strip(1)\n+\t\t\t.targetDir(hbaseDir)\n+\t\t\t.build());\n+\n+\t\tLOG.info(\"Configure {} as hbase.tmp.dir\", hbaseDir.toAbsolutePath());\n+\t\tfinal String tmpDirConfig = \"<configuration><property><name>hbase.tmp.dir</name><value>\" + hbaseDir + \"</value></property></configuration>\";\n+\t\tFiles.write(hbaseDir.resolve(Paths.get(\"conf\", \"hbase-site.xml\")), tmpDirConfig.getBytes());\n+\t}\n+\n+\tprivate void setupHBaseCluster() throws IOException {\n+\t\tLOG.info(\"Starting HBase cluster\");\n+\t\tAutoClosableProcess.runBlocking(\n+\t\t\thbaseDir.resolve(Paths.get(\"bin\", \"start-hbase.sh\")).toString());\n+\n+\t\twhile (!isHBaseRunning()) {\n+\t\t\ttry {\n+\t\t\t\tLOG.info(\"Waiting for HBase to start\");\n+\t\t\t\tThread.sleep(500L);\n+\t\t\t} catch (InterruptedException e) {\n+\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void afterTestSuccess() {\n+\t\ttry {\n+\t\t\tLOG.info(\"Stopping HBase Cluster\");\n+\t\t\tAutoClosableProcess.runBlocking(\n+\t\t\t\thbaseDir.resolve(Paths.get(\"bin\", \"hbase-daemon.sh\")).toString(),\n+\t\t\t\t\"stop\",\n+\t\t\t\t\"master\");\n+\n+\t\t\twhile (isHBaseRunning()) {\n+\t\t\t\ttry {\n+\t\t\t\t\tLOG.info(\"Waiting for HBase to stop\");\n+\t\t\t\t\tThread.sleep(500L);\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} catch (IOException ioe) {\n+\t\t\tLOG.warn(\"Error while shutting down hbase.\", ioe);\n+\t\t}\n+\t\tdownloadCache.afterTestSuccess();\n+\t\ttmp.delete();\n+\t}\n+\n+\tprivate static boolean isHBaseRunning() {\n+\t\ttry {\n+\t\t\tfinal AtomicBoolean atomicHMasterStarted = new AtomicBoolean(false);\n+\t\t\tqueryHMasterStatus(line -> atomicHMasterStarted.compareAndSet(false, line.contains(\"HMaster\")));\n+\t\t\treturn atomicHMasterStarted.get();\n+\t\t} catch (IOException ioe) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\n+\tprivate static void queryHMasterStatus(final Consumer<String> stdoutProcessor) throws IOException {\n+\t\tAutoClosableProcess\n+\t\t\t.create(\"jps\")\n+\t\t\t.setStdoutProcessor(stdoutProcessor)\n+\t\t\t.runBlocking();\n+\t}\n+\n+\t@Override\n+\tpublic void createTable(String tableName, String... columnFamilies) throws IOException {\n+\t\tfinal String createTable = String.format(\"create '%s',\", tableName) +\n+\t\t\tArrays.stream(columnFamilies)\n+\t\t\t\t.map(cf -> String.format(\"{NAME=>'%s'}\", cf))\n+\t\t\t\t.collect(Collectors.joining(\",\"));\n+\n+\t\texecuteHBaseShell(createTable);\n+\t}\n+\n+\t@Override\n+\tpublic List<String> scanTable(String tableName) throws IOException {\n+\t\tfinal List<String> result = new ArrayList<>();\n+\t\texecuteHBaseShell(String.format(\"scan '%s'\", tableName), line -> {\n+\t\t\tif (line.contains(\"value=\")) {\n+\t\t\t\tresult.add(line);\n+\t\t\t}\n+\t\t});\n+\t\treturn result;\n+\t}\n+\n+\t@Override\n+\tpublic void putData(String tableName, String rowKey, String columnFamily, String columnQualifier, String value) throws IOException {\n+\t\texecuteHBaseShell(\n+\t\t\tString.format(\"put '%s','%s','%s:%s','%s'\", tableName, rowKey, columnFamily, columnQualifier, value));\n+\t}\n+\n+\tprivate void executeHBaseShell(String cmd) throws IOException {\n+\t\texecuteHBaseShell(cmd, line -> {\n+\t\t});\n+\t}\n+\n+\tprivate void executeHBaseShell(String cmd, Consumer<String> stdoutProcessor) throws IOException {\n+\t\ttry (AutoClosableProcess autoClosableProcess = AutoClosableProcess\n+\t\t\t.create(hbaseDir.resolve(Paths.get(\"bin\", \"hbase\")).toString(), \"shell\")\n+\t\t\t.setStdoutProcessor(stdoutProcessor)\n+\t\t\t.runNonBlocking()) {\n+\n+\t\t\ttry (PrintStream printStream = new PrintStream(autoClosableProcess.getProcess().getOutputStream(), true, StandardCharsets.UTF_8.name())) {\n+\t\t\t\tLOG.info(\"Executing hbase shell: {}\", cmd);\n+\t\t\t\tprintStream.println(cmd);\n+\t\t\t}\n+\n+\t\t\ttry {\n+\t\t\t\tautoClosableProcess.getProcess().waitFor();\n+\t\t\t} catch (InterruptedException e) {\n+\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t}\n+\t\t}\n+\t}", "originalCommit": "8a8a8090a66006f8386f0737b08d5428fdc53394", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM3MDk0NA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441370944", "bodyText": "Not found yarn.classpath?", "author": "wuchong", "createdAt": "2020-06-17T08:25:11Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-hbase/src/test/java/org/apache/flink/tests/util/hbase/SQLClientHBaseITCase.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.tests.util.hbase;\n+\n+import org.apache.flink.api.common.time.Deadline;\n+import org.apache.flink.tests.util.TestUtils;\n+import org.apache.flink.tests.util.cache.DownloadCache;\n+import org.apache.flink.tests.util.categories.PreCommit;\n+import org.apache.flink.tests.util.categories.TravisGroup1;\n+import org.apache.flink.tests.util.flink.ClusterController;\n+import org.apache.flink.tests.util.flink.FlinkResource;\n+import org.apache.flink.tests.util.flink.FlinkResourceSetup;\n+import org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory;\n+import org.apache.flink.tests.util.flink.SQLJobSubmission;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.hamcrest.Matchers.arrayContainingInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * End-to-end test for the HBase connectors.\n+ */\n+@Category(value = {TravisGroup1.class, PreCommit.class})\n+public class SQLClientHBaseITCase extends TestLogger {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(SQLClientHBaseITCase.class);\n+\n+\tprivate static final String HBASE_E2E_SQL = \"hbase_e2e.sql\";\n+\n+\t@Rule\n+\tpublic final HBaseResource hbase;\n+\n+\t@Rule\n+\tpublic final FlinkResource flink = new LocalStandaloneFlinkResourceFactory()\n+\t\t.create(FlinkResourceSetup.builder().build());\n+\n+\t@Rule\n+\tpublic final TemporaryFolder tmp = new TemporaryFolder();\n+\n+\t@ClassRule\n+\tpublic static final DownloadCache DOWNLOAD_CACHE = DownloadCache.get();\n+\n+\tprivate static final Path sqlToolBoxJar = TestUtils.getResourceJar(\".*SqlToolbox.jar\");\n+\tprivate static final Path sqlConnectorHBaseJar = TestUtils.getResourceJar(\".*hbase.jar\");\n+\tprivate List<Path> hadoopClasspathJars;\n+\n+\tpublic SQLClientHBaseITCase() {\n+\t\tthis.hbase = HBaseResource.get();\n+\t}\n+\n+\t@Before\n+\tpublic void before() throws Exception {\n+\t\tDOWNLOAD_CACHE.before();\n+\t\tPath tmpPath = tmp.getRoot().toPath();\n+\t\tLOG.info(\"The current temporary path: {}\", tmpPath);\n+\n+\t\t// Prepare all hadoop jars under HADOOP_CLASSPATH, use yarn.classpath which contains all hadoop jars\n+\t\tPath currentModulePath = Paths.get(\"\").toAbsolutePath();\n+\t\tPath flinkHomePath = currentModulePath.getParent().getParent().toAbsolutePath();\n+\t\tPath hadoopClasspath = Paths.get(flinkHomePath.toString(), \"/flink-yarn-tests/target/yarn.classpath\");\n+\t\tFile hadoopClasspathFile = new File(hadoopClasspath.toString());\n+\n+\t\tif (!hadoopClasspathFile.exists()) {\n+\t\t\tthrow new FileNotFoundException(HBASE_E2E_SQL);", "originalCommit": "8a8a8090a66006f8386f0737b08d5428fdc53394", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMTQ4Mg==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441431482", "bodyText": "htrace-core bundles several dependencies but doesn't have a proper NOTICE. Maybe we should list the dependencies:\nhtrace-core\nFrom: 'FasterXML' (http://fasterxml.com/)\n  - Jackson-annotations (http://wiki.fasterxml.com/JacksonHome) com.fasterxml.jackson.core:jackson-annotations:bundle:2.4.0\n    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)\n  - Jackson-core (http://wiki.fasterxml.com/JacksonHome) com.fasterxml.jackson.core:jackson-core:bundle:2.4.0\n    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)\n  - jackson-databind (http://wiki.fasterxml.com/JacksonHome) com.fasterxml.jackson.core:jackson-databind:bundle:2.4.0\n    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)\n\nFrom: 'The Apache Software Foundation' (http://www.apache.org/)\n  - Commons Logging (http://commons.apache.org/logging) commons-logging:commons-logging:jar:1.1.1\n    License: The Apache Software License, Version 2.0  (http://www.apache.org/licenses/LICENSE-2.0.txt)\n\nThe commons-logging is a different version.", "author": "wuchong", "createdAt": "2020-06-17T10:02:15Z", "path": "flink-connectors/flink-sql-connector-hbase/src/main/resources/META-INF/NOTICE", "diffHunk": "@@ -0,0 +1,27 @@\n+flink-sql-connector-hbase\n+Copyright 2014-2020 The Apache Software Foundation\n+\n+This product includes software developed at\n+The Apache Software Foundation (http://www.apache.org/).\n+\n+This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)\n+\n+- commons-codec:commons-codec:1.10\n+- commons-configuration:commons-configuration:1.7\n+- commons-lang:commons-lang:2.6\n+- commons-logging:commons-logging:1.1.3\n+- com.google.guava:guava:12.0.1\n+- com.yammer.metrics:metrics-core:2.2.0\n+- io.netty:netty-all:4.1.44.Final\n+- org.apache.hbase:hbase-common:1.4.3\n+- org.apache.hbase:hbase-protocol:1.4.3\n+- org.apache.hbase:hbase-procedure:1.4.3\n+- org.apache.hbase:hbase-client:1.4.3\n+- org.apache.hbase:hbase-prefix-tree:1.4.3\n+- org.apache.htrace:htrace-core:3.1.0-incubating", "originalCommit": "72524f4c7ed614aa45c288e8a17229f6ada9a253", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE0Ng==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441482146", "bodyText": "I agree we should list all bundled dependencies by htrace-core, and also need to add different version of commons-logging", "author": "leonardBang", "createdAt": "2020-06-17T11:42:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMTQ4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc1MjAwNw==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441752007", "bodyText": "Maybe we should separate these dependencies in the list of dependencies, otherwise, people will be confused where the dependencies are coming from", "author": "rmetzger", "createdAt": "2020-06-17T18:41:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMTQ4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzNzI2MA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441437260", "bodyText": "Shall we list all the netty dependencies if netty-all is included?\nio.netty:netty-codec-smtp:4.1.44.Final\nio.netty:netty-transport-native-epoll:4.1.44.Final\nio.netty:netty-codec-redis:4.1.44.Final\nio.netty:netty-resolver-dns:4.1.44.Final\nio.netty:netty-codec-dns:4.1.44.Final\nio.netty:netty-resolver-dns-native-macos:4.1.44.Final\nio.netty:netty-codec-mqtt:4.1.44.Final\nio.netty:netty-codec-http2:4.1.44.Final\nio.netty:netty-transport-native-unix-common:4.1.44.Final\nio.netty:netty-transport-sctp:4.1.44.Final\nio.netty:netty-transport-native-epoll:4.1.44.Final\nio.netty:netty-codec-http:4.1.44.Final\nio.netty:netty-transport-udt:4.1.44.Final\nio.netty:netty-handler-proxy:4.1.44.Final\nio.netty:netty-resolver:4.1.44.Final\nio.netty:netty-common:4.1.44.Final\nio.netty:netty-codec-xml:4.1.44.Final\nio.netty:netty-transport:4.1.44.Final\nio.netty:netty-codec:4.1.44.Final\nio.netty:netty-codec-socks:4.1.44.Final\nio.netty:netty-codec-stomp:4.1.44.Final\nio.netty:netty-buffer:4.1.44.Final\nio.netty:netty-transport-rxtx:4.1.44.Final\nio.netty:netty-codec-haproxy:4.1.44.Final\nio.netty:netty-codec-haproxy:4.1.44.Final\nio.netty:netty-tcnative:2.0.28.Final\nio.netty:netty-transport-native-kqueue:4.1.44.Final\nio.netty:netty-handler:4.1.44.Final\nio.netty:netty-codec-memcache:4.1.44.Final\n\nI'm not sure about this, but as there are no other 3rd party dependencies in netty-all, maybe it's fine just include netty-all.", "author": "wuchong", "createdAt": "2020-06-17T10:12:17Z", "path": "flink-connectors/flink-sql-connector-hbase/src/main/resources/META-INF/NOTICE", "diffHunk": "@@ -0,0 +1,27 @@\n+flink-sql-connector-hbase\n+Copyright 2014-2020 The Apache Software Foundation\n+\n+This product includes software developed at\n+The Apache Software Foundation (http://www.apache.org/).\n+\n+This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)\n+\n+- commons-codec:commons-codec:1.10\n+- commons-configuration:commons-configuration:1.7\n+- commons-lang:commons-lang:2.6\n+- commons-logging:commons-logging:1.1.3\n+- com.google.guava:guava:12.0.1\n+- com.yammer.metrics:metrics-core:2.2.0\n+- io.netty:netty-all:4.1.44.Final", "originalCommit": "72524f4c7ed614aa45c288e8a17229f6ada9a253", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjY0Mg==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441482642", "bodyText": "cc @JingsongLi Could you help confirm this?", "author": "leonardBang", "createdAt": "2020-06-17T11:43:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzNzI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NDA2OA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441484068", "bodyText": "Maybe we need, see hadoop notice https://cwiki.apache.org/confluence/display/HADOOP/Bundled+dependencies", "author": "JingsongLi", "createdAt": "2020-06-17T11:46:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzNzI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc3MTcxOA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441771718", "bodyText": "I believe we don't need to \"expand\" the dependencies included in \"netty-all\". Everything that is included in netty-all is licensed to the netty project (as long as there's nothing shaded etc.)", "author": "rmetzger", "createdAt": "2020-06-17T19:06:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzNzI2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0ODgxMA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441748810", "bodyText": "This dependency does not seem to be included in the jar (according to my maven shade plugin output)", "author": "rmetzger", "createdAt": "2020-06-17T18:35:43Z", "path": "flink-connectors/flink-sql-connector-hbase/src/main/resources/META-INF/NOTICE", "diffHunk": "@@ -0,0 +1,60 @@\n+flink-sql-connector-hbase\n+Copyright 2014-2020 The Apache Software Foundation\n+\n+This product includes software developed at\n+The Apache Software Foundation (http://www.apache.org/).\n+\n+This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)\n+\n+- commons-codec:commons-codec:1.10\n+- commons-configuration:commons-configuration:1.7\n+- commons-lang:commons-lang:2.6\n+- commons-logging:commons-logging:1.1.1", "originalCommit": "ea01ddac48d67d2288ae1a9f9c85cc46b0b03c70", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk0MTYyMA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441941620", "bodyText": "maven shade plugin output may miss them?  These dependencies\ncom.fasterxml.jackson.core:jackson-annotations:2.4.0\ncom.fasterxml.jackson.core:jackson-core:2.4.0 \ncom.fasterxml.jackson.core:jackson-databind:2.4.0\ncommons-logging:commons-logging:1.1.1\n\nhas been shaded in htrace-core, I found the dependencies version in maven shade plugin output are different with these dependencies in META-INF/DEPENDENCIES of htrace-core-3.1.0-incubating.jar", "author": "leonardBang", "createdAt": "2020-06-18T03:02:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0ODgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk1MDM0Mg==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441950342", "bodyText": "htrace-core is an uber jar which shades jackson and commons-logging in it. I think that's why we can't find the dependencies in maven shade plugin output?", "author": "wuchong", "createdAt": "2020-06-18T03:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0ODgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk1NDczNA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441954734", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the  flink project root directory;\" +\n          \n          \n            \n            \t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the flink project root directory;\" +", "author": "wuchong", "createdAt": "2020-06-18T03:58:20Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/LocalStandaloneFlinkResourceFactory.java", "diffHunk": "@@ -80,6 +80,33 @@ public FlinkResource create(FlinkResourceSetup setup) {\n \t\treturn new LocalStandaloneFlinkResource(distributionDirectory.get(), logBackupDirectory.orElse(null), setup);\n \t}\n \n+\t/**\n+\t * Utils to find the flink project root directory.\n+\t * @param currentDirectory\n+\t * @return The flink project root directory.\n+\t */\n+\tpublic static Path getProjectRootDirectory(Path currentDirectory) {\n+\t\tPath projectRootPath;\n+\t\tOptional<Path> projectRoot = PROJECT_ROOT_DIRECTORY.get();\n+\t\tif (projectRoot.isPresent()) {\n+\t\t\t// running with maven\n+\t\t\tprojectRootPath = projectRoot.get();\n+\t\t} else {\n+\t\t\t// running in the IDE; working directory is test module\n+\t\t\tOptional<Path> projectRootDirectory = findProjectRootDirectory(currentDirectory);\n+\t\t\t// this distinction is required in case this class is used outside of Flink\n+\t\t\tif (projectRootDirectory.isPresent()) {\n+\t\t\t\tprojectRootPath = projectRootDirectory.get();\n+\t\t\t} else {\n+\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\"The 'rootDir' property was not set and the flink project root directory could not be found\" +\n+\t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the  flink project root directory;\" +", "originalCommit": "7aa36ae36a4ab5db1dc6fd03b0ed8b3d8ac9b044", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk1NDk5Ng==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441954996", "bodyText": "We can use this method to replace the code block in LocalStandaloneFlinkResourceFactory#create().", "author": "wuchong", "createdAt": "2020-06-18T03:59:26Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/LocalStandaloneFlinkResourceFactory.java", "diffHunk": "@@ -80,6 +80,33 @@ public FlinkResource create(FlinkResourceSetup setup) {\n \t\treturn new LocalStandaloneFlinkResource(distributionDirectory.get(), logBackupDirectory.orElse(null), setup);\n \t}\n \n+\t/**\n+\t * Utils to find the flink project root directory.\n+\t * @param currentDirectory\n+\t * @return The flink project root directory.\n+\t */\n+\tpublic static Path getProjectRootDirectory(Path currentDirectory) {", "originalCommit": "7aa36ae36a4ab5db1dc6fd03b0ed8b3d8ac9b044", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "769512268be9cb063af00d107cbd1382576fb6fb", "url": "https://github.com/apache/flink/commit/769512268be9cb063af00d107cbd1382576fb6fb", "message": "address Chesnay's comments", "committedDate": "2020-06-18T09:30:37Z", "type": "forcePushed"}, {"oid": "15681586a0c9fab7bfbefe943e7b6b1cd6700864", "url": "https://github.com/apache/flink/commit/15681586a0c9fab7bfbefe943e7b6b1cd6700864", "message": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "committedDate": "2020-06-22T12:16:20Z", "type": "commit"}, {"oid": "15681586a0c9fab7bfbefe943e7b6b1cd6700864", "url": "https://github.com/apache/flink/commit/15681586a0c9fab7bfbefe943e7b6b1cd6700864", "message": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "committedDate": "2020-06-22T12:16:20Z", "type": "forcePushed"}]}