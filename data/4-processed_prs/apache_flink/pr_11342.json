{"pr_number": 11342, "pr_title": "[FLINK-16483][python] Add Python building blocks to make sure the basic functionality of vectorized Python UDF could work", "pr_createdAt": "2020-03-07T13:59:08Z", "pr_url": "https://github.com/apache/flink/pull/11342", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUxMjExMw==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389512113", "bodyText": "Remove the blank between the version range? It seems it contains no blank usually.", "author": "hequn8128", "createdAt": "2020-03-09T08:17:21Z", "path": "flink-python/setup.py", "diffHunk": "@@ -224,7 +224,8 @@ def remove_if_exists(file_path):\n         author_email='dev@flink.apache.org',\n         python_requires='>=3.5',\n         install_requires=['py4j==0.10.8.1', 'python-dateutil==2.8.0', 'apache-beam==2.19.0',\n-                          'cloudpickle==1.2.2', 'avro-python3>=1.8.1,<=1.9.1', 'jsonpickle==1.2'],\n+                          'cloudpickle==1.2.2', 'avro-python3>=1.8.1,<=1.9.1', 'jsonpickle==1.2',\n+                          'pandas>=0.23.4, <=0.25.3', 'pyarrow>=0.15.1, <=0.16.0'],", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzMjg2Mw==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389532863", "bodyText": "How about adding a PythonFunctionKind class which contains GENERAL and PANDAS? It is consistent with Java and also would be easier for users to add the parameter.", "author": "hequn8128", "createdAt": "2020-03-09T09:06:27Z", "path": "flink-python/pyflink/table/udf.py", "diffHunk": "@@ -297,15 +308,17 @@ def _get_python_env():\n     return gateway.jvm.org.apache.flink.table.functions.python.PythonEnv(exec_type)\n \n \n-def _create_udf(f, input_types, result_type, deterministic, name):\n-    return UserDefinedScalarFunctionWrapper(f, input_types, result_type, deterministic, name)\n+def _create_udf(f, input_types, result_type, udf_type, deterministic, name):\n+    return UserDefinedScalarFunctionWrapper(\n+        f, input_types, result_type, udf_type, deterministic, name)\n \n \n def _create_udtf(f, input_types, result_types, deterministic, name):\n     return UserDefinedTableFunctionWrapper(f, input_types, result_types, deterministic, name)\n \n \n-def udf(f=None, input_types=None, result_type=None, deterministic=None, name=None):\n+def udf(f=None, input_types=None, result_type=None, deterministic=None, name=None,\n+        udf_type=\"general\"):", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc0NDY3NA==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389744674", "bodyText": "Maybe we should not contain this commit in this PR? as it is included in FLINK-16273\uff1fotherwise there will be two commits for addressing the same problem.", "author": "hequn8128", "createdAt": "2020-03-09T14:56:42Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/arrow/AbstractArrowPythonScalarFunctionRunner.java", "diffHunk": "@@ -46,6 +46,12 @@\n \n \tprivate static final String SCHEMA_ARROW_CODER_URN = \"flink:coder:schema:scalar_function:arrow:v1\";\n \n+\tstatic {", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1ODU1MQ==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389758551", "bodyText": "Why we only get the first batch in the batch list?", "author": "hequn8128", "createdAt": "2020-03-09T15:16:42Z", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):\n+            self._batch_writer = pa.RecordBatchStreamWriter(self._resettable_io, self._schema)\n+\n+        self._resettable_io.set_output_stream(out_stream)\n+        self._batch_writer.write_batch(self._create_batch(cols))\n+\n+    def decode_from_stream(self, in_stream, nested):\n+        if not hasattr(self, \"_batch_reader\"):\n+            def load_from_stream(stream):\n+                reader = pa.ipc.open_stream(stream)\n+                for batch in reader:\n+                    yield batch\n+\n+            self._batch_reader = load_from_stream(self._resettable_io)\n+\n+        self._resettable_io.set_input_bytes(in_stream.read_all())\n+        table = pa.Table.from_batches([next(self._batch_reader)])", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc2MzQ4NA==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389763484", "bodyText": "Why we must init the _batch_writer here instead of in __init__?", "author": "hequn8128", "createdAt": "2020-03-09T15:23:32Z", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc2OTIxNQ==", "url": "https://github.com/apache/flink/pull/11342#discussion_r389769215", "bodyText": "I'm wondering if we can avoid these hasattr. Can we use pa.ipc.open_stream read the stream directly and return the batch?", "author": "hequn8128", "createdAt": "2020-03-09T15:31:35Z", "path": "flink-python/pyflink/fn_execution/coder_impl.py", "diffHunk": "@@ -373,3 +376,45 @@ def internal_to_timestamp(self, milliseconds, nanoseconds):\n         second, microsecond = (milliseconds // 1000,\n                                milliseconds % 1000 * 1000 + nanoseconds // 1000)\n         return datetime.datetime.utcfromtimestamp(second).replace(microsecond=microsecond)\n+\n+\n+class ArrowCoderImpl(StreamCoderImpl):\n+\n+    def __init__(self, schema):\n+        self._schema = schema\n+        self._resettable_io = ResettableIO()\n+\n+    def encode_to_stream(self, cols, out_stream, nested):\n+        if not hasattr(self, \"_batch_writer\"):\n+            self._batch_writer = pa.RecordBatchStreamWriter(self._resettable_io, self._schema)\n+\n+        self._resettable_io.set_output_stream(out_stream)\n+        self._batch_writer.write_batch(self._create_batch(cols))\n+\n+    def decode_from_stream(self, in_stream, nested):\n+        if not hasattr(self, \"_batch_reader\"):", "originalCommit": "27f93cc605b980ae0ce93b1d5503031ae98f6cd9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e82d816f679e1ff790ebeae58ec6501411831b2", "url": "https://github.com/apache/flink/commit/0e82d816f679e1ff790ebeae58ec6501411831b2", "message": "address review", "committedDate": "2020-03-10T03:14:09Z", "type": "forcePushed"}, {"oid": "e431a7c17f43bba9ae2dd6e471b0191942cb7287", "url": "https://github.com/apache/flink/commit/e431a7c17f43bba9ae2dd6e471b0191942cb7287", "message": "[FLINK-16483][python] Add Python building blocks to make sure the basic functionality of vectorized Python UDF could work", "committedDate": "2020-03-10T03:18:08Z", "type": "commit"}, {"oid": "c506306f83eec4450719dfdfb2fe205a3fb69857", "url": "https://github.com/apache/flink/commit/c506306f83eec4450719dfdfb2fe205a3fb69857", "message": "address review", "committedDate": "2020-03-10T03:18:08Z", "type": "commit"}, {"oid": "c506306f83eec4450719dfdfb2fe205a3fb69857", "url": "https://github.com/apache/flink/commit/c506306f83eec4450719dfdfb2fe205a3fb69857", "message": "address review", "committedDate": "2020-03-10T03:18:08Z", "type": "forcePushed"}]}