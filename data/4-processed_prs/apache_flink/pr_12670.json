{"pr_number": 12670, "pr_title": "[FLINK-18290][checkpointing] Fail job on checkpoint future failure instead of System.exit", "pr_createdAt": "2020-06-16T07:09:28Z", "pr_url": "https://github.com/apache/flink/pull/12670", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440786956", "bodyText": "It's a serious issue if it gets here. CheckpointCoordinator might be never recover even we fail the job. The state of isTrigger might not be reset, and it couldn't be reset forever. So we should reset the isTriggering to false first and then call the failureManager.", "author": "ifndef-SleePy", "createdAt": "2020-06-16T11:44:01Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -538,51 +542,61 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint, pendingCheckpoint, timer),\n \t\t\t\t\t\t\ttimer);\n \n-\t\t\tFutureUtils.assertNoException(\n-\t\t\t\tCompletableFuture.allOf(masterStatesComplete, coordinatorCheckpointsComplete)\n-\t\t\t\t\t.handleAsync(\n-\t\t\t\t\t\t(ignored, throwable) -> {\n-\t\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n-\t\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n-\n-\t\t\t\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n-\t\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n-\n-\t\t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\t\t// the initialization might not be finished yet\n-\t\t\t\t\t\t\t\tif (checkpoint == null) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n-\t\t\t\t\t\t\t\t}\n+\t\t\tFutureUtils.waitForAll(asList(masterStatesComplete, coordinatorCheckpointsComplete))\n+\t\t\t\t.handleAsync(\n+\t\t\t\t\t(ignored, throwable) -> {\n+\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n+\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n+\n+\t\t\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n+\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n+\n+\t\t\t\t\t\tif (throwable != null) {\n+\t\t\t\t\t\t\t// the initialization might not be finished yet\n+\t\t\t\t\t\t\tif (checkpoint == null) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n \t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(\n-\t\t\t\t\t\t\t\t\t\tcheckpoint,\n-\t\t\t\t\t\t\t\t\t\tnew CheckpointException(\n-\t\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n-\t\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n-\t\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n-\t\t\t\t\t\t\t\t\tsnapshotTaskState(\n-\t\t\t\t\t\t\t\t\t\ttimestamp,\n-\t\t\t\t\t\t\t\t\t\tcheckpointId,\n-\t\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n-\t\t\t\t\t\t\t\t\t\trequest.props,\n-\t\t\t\t\t\t\t\t\t\texecutions,\n-\t\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n-\n-\t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n-\n-\t\t\t\t\t\t\t\t\tonTriggerSuccess();\n-\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n \t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(\n+\t\t\t\t\t\t\t\t\tcheckpoint,\n+\t\t\t\t\t\t\t\t\tnew CheckpointException(\n+\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n+\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n+\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n+\t\t\t\t\t\t\t\tsnapshotTaskState(\n+\t\t\t\t\t\t\t\t\ttimestamp,\n+\t\t\t\t\t\t\t\t\tcheckpointId,\n+\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n+\t\t\t\t\t\t\t\t\trequest.props,\n+\t\t\t\t\t\t\t\t\texecutions,\n+\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n+\n+\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n+\n+\t\t\t\t\t\t\t\tonTriggerSuccess();\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t\treturn null;\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t},\n+\t\t\t\t\ttimer)\n+\t\t\t\t.whenComplete((unused, error) -> {\n+\t\t\t\t\tif (error != null) {\n+\t\t\t\t\t\tif (!isShutdown()) {\n+\t\t\t\t\t\t\tfailureManager.handleJobLevelCheckpointException(new CheckpointException(EXCEPTION, error), Optional.empty());", "originalCommit": "5b7a339a0755861a904a119770fc6a26edc59fd8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NjUyNA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440796524", "bodyText": "I think if a failure occurs here, then there must a programming error or it is an artifact of a non properly shut down task. In the first case, I think we should fail hard. In the latter case (e.g. if (isShutdown()) we should ignore the exception/log it on debug).", "author": "tillrohrmann", "createdAt": "2020-06-16T12:02:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NzA0MQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440797041", "bodyText": "Consequently, I'm not convinced that we need to call the failureManager here.", "author": "tillrohrmann", "createdAt": "2020-06-16T12:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgwMjc0MA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440802740", "bodyText": "Which exceptions do you expect here to appear?", "author": "tillrohrmann", "createdAt": "2020-06-16T12:14:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkwMzIzNQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440903235", "bodyText": "Which exceptions do you expect here to appear?\n\nProgramming errors here like we had recently.\n\nI think we should fail hard.\n\nDo you mean System.exit by \"fail hard\"? I think it is not suitable because it:\n\nhides error details\nhides error itself in failover setup?\nprevents resource cleanup\nfails other jobs\n\nJust throwing an exception here also won't work because this will only fail the timer thread.", "author": "rkhachatryan", "createdAt": "2020-06-16T14:39:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkwNjE0MQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440906141", "bodyText": "If we are only talking about programming errors, then I believe we should call System.exit (== fail hard) because programming errors usually leave the system in a corrupted state.\nWhy do you think that it hides error details?\nNot doing resource clean up in a failure case is acceptable.\nFailing other jobs if the process has been corrupted is fine as well.", "author": "tillrohrmann", "createdAt": "2020-06-16T14:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkxNTYxMw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440915613", "bodyText": "Why do you think that it hides error details?\n\nBecause System.exit can leave log buffers containing error details unflushed.\nPlus\n\nhides error itself in failover setup?\n\nI.e. if programming error corrupts state then job restart can be undesirable.\nJust crashing doesn't prevent restart, but failing the job does.", "author": "rkhachatryan", "createdAt": "2020-06-16T14:55:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MDM1Nw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441280357", "bodyText": "I think doing System.exit is not nice practice in general. Flushing logs is one thing, but there might be some other exception handling code that wouldn't be execute as expected. Maybe some production code, like cleaning up temp files, maybe some debug code someone was counting on. Also it's confusing, as it's not the normal/usual way how Flink fails (with System.exit I guess there would be no message in the error log, but only on stderr?).\nIn many other places we have checkStates for programming errors.\nIf failing single job is not enough, we should fail whole JM, but via some exception. However I'm not buying the argument why should we fail other jobs as CheckpointCoordinator is bound to a single job. For example we are not failing whole TM if there was a checkState or NPE failure in the runtime code of a single job.", "author": "pnowojski", "createdAt": "2020-06-17T04:57:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0NTk4Nw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441345987", "bodyText": "Also it's confusing, as it's not the normal/usual way how Flink fails (with System.exit I guess there would be no message in the error log, but only on stderr?).\n\nI fear that your statement is not true @pnowojski. We actually try to set wherever possible FatalExitExceptionHandler as the uncaught exception handler for every thread we start (it is not perfect yet). Hence, if there is an exception which bubbles up in the runtime code (e.g. checkState), then it will shut down the JVM. I believe that this is the right thing to do as these situations constitute a programming error and it is impossible to recover from all kinds of programming errors. Moreover, in case of programming errors or other fatal errors it is in general not possible to properly clean up. Just imagine an OOM when triggering the clean up code, for example.\nMoreover, we are talking about code where the assumption is that all recoverable exceptions are properly being handled before it is called. Hence, it only makes sense to fail hard if this assumption is wrong.\n\nI.e. if programming error corrupts state then job restart can be undesirable.\nJust crashing doesn't prevent restart, but failing the job does.\n\nThis is true, but at the moment we don't really support that you can directly fail a job w/o restarts, hence, the next best thing is to fail the process so that people see the problem.\n\nBecause System.exit can leave log buffers containing error details unflushed.\n\nYes this can potentially happen. So far, however, this has never been a problem.", "author": "tillrohrmann", "createdAt": "2020-06-17T07:43:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0OTAzMg==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441349032", "bodyText": "If you don't really know what kind of exceptions can occur after handleAsync is called, then this is a different story. In this case I would advise to only log the error. However, then I would strongly suggest to get the CheckpointCoordinator in order soon because it is not healthy that random exceptions are allowed to fly around.", "author": "tillrohrmann", "createdAt": "2020-06-17T07:48:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0OTY5NA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441449694", "bodyText": "I've reverted to System.exit after an offline discussion with @pnowojski (the motivation is to have more a simpler behavior and changeset given the time pressure of 1.11)", "author": "rkhachatryan", "createdAt": "2020-06-17T10:35:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4Njk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440787003", "bodyText": "Should we check the RejectedExecutionException here? It's kind of case by case. Do you think log.warn for every exception is better? In the future we could shut down timer and CheckpointCoordinator in a more elegant way to avoid this warning log.", "author": "ifndef-SleePy", "createdAt": "2020-06-16T11:44:06Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -538,51 +542,61 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint, pendingCheckpoint, timer),\n \t\t\t\t\t\t\ttimer);\n \n-\t\t\tFutureUtils.assertNoException(\n-\t\t\t\tCompletableFuture.allOf(masterStatesComplete, coordinatorCheckpointsComplete)\n-\t\t\t\t\t.handleAsync(\n-\t\t\t\t\t\t(ignored, throwable) -> {\n-\t\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n-\t\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n-\n-\t\t\t\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n-\t\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n-\n-\t\t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\t\t// the initialization might not be finished yet\n-\t\t\t\t\t\t\t\tif (checkpoint == null) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n-\t\t\t\t\t\t\t\t}\n+\t\t\tFutureUtils.waitForAll(asList(masterStatesComplete, coordinatorCheckpointsComplete))\n+\t\t\t\t.handleAsync(\n+\t\t\t\t\t(ignored, throwable) -> {\n+\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n+\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n+\n+\t\t\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n+\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n+\n+\t\t\t\t\t\tif (throwable != null) {\n+\t\t\t\t\t\t\t// the initialization might not be finished yet\n+\t\t\t\t\t\t\tif (checkpoint == null) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n \t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(\n-\t\t\t\t\t\t\t\t\t\tcheckpoint,\n-\t\t\t\t\t\t\t\t\t\tnew CheckpointException(\n-\t\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n-\t\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n-\t\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n-\t\t\t\t\t\t\t\t\tsnapshotTaskState(\n-\t\t\t\t\t\t\t\t\t\ttimestamp,\n-\t\t\t\t\t\t\t\t\t\tcheckpointId,\n-\t\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n-\t\t\t\t\t\t\t\t\t\trequest.props,\n-\t\t\t\t\t\t\t\t\t\texecutions,\n-\t\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n-\n-\t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n-\n-\t\t\t\t\t\t\t\t\tonTriggerSuccess();\n-\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n \t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(\n+\t\t\t\t\t\t\t\t\tcheckpoint,\n+\t\t\t\t\t\t\t\t\tnew CheckpointException(\n+\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n+\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n+\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n+\t\t\t\t\t\t\t\tsnapshotTaskState(\n+\t\t\t\t\t\t\t\t\ttimestamp,\n+\t\t\t\t\t\t\t\t\tcheckpointId,\n+\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n+\t\t\t\t\t\t\t\t\trequest.props,\n+\t\t\t\t\t\t\t\t\texecutions,\n+\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n+\n+\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n+\n+\t\t\t\t\t\t\t\tonTriggerSuccess();\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t\treturn null;\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t},\n+\t\t\t\t\ttimer)\n+\t\t\t\t.whenComplete((unused, error) -> {\n+\t\t\t\t\tif (error != null) {\n+\t\t\t\t\t\tif (!isShutdown()) {\n+\t\t\t\t\t\t\tfailureManager.handleJobLevelCheckpointException(new CheckpointException(EXCEPTION, error), Optional.empty());\n+\t\t\t\t\t\t} else if (error instanceof RejectedExecutionException) {", "originalCommit": "5b7a339a0755861a904a119770fc6a26edc59fd8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NzkyNA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440797924", "bodyText": "I think we can say that we tolerate exception after the shut down because we have closed the timer and can no longer guarantee that things keep working.", "author": "tillrohrmann", "createdAt": "2020-06-16T12:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzMzAxMQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440933011", "bodyText": "Yes, I think we should check exception type, because RejectedExecutionException during shutdown is expected (though we should fix it with RejectedExecutionHandler in the future).\nOther exceptions, even during the shutdown are not expected. They also might have occurred before the shutdown.", "author": "rkhachatryan", "createdAt": "2020-06-16T15:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzNTkwNA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441335904", "bodyText": "But once the CheckpointCoordinator is shut down, these exceptions should no longer matter, at least not in terms of correctness of the job. I don't really understand why this was not a problem before and now we want to fine grained filter out exceptions, this does not seem like a straight forward solution to me.", "author": "tillrohrmann", "createdAt": "2020-06-17T07:26:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0Njc5OA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441346798", "bodyText": "But once the CheckpointCoordinator is shut down, these exceptions should no longer matter, at least not in terms of correctness of the job.\n\nSo we are talkiing about the case isShutdown() == true branch.\nI agree about job correctness and in this case exceptions are only logged.\nI think this can be helpful during troubleshooting or debugging.\n\nI don't really understand why this was not a problem before and now we want to fine grained filter out exceptions, this does not seem like a straight forward solution to me.\n\nWe didn't have this code in previous releases. CheckpointCoordinator was heavily refactored this release (than partially reverted). During the development, we did have problem recently and had to guess that there is an NPE.", "author": "rkhachatryan", "createdAt": "2020-06-17T07:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NTk2MQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441355961", "bodyText": "Yeah but the NPE occurred while the CheckpointCoordinator was not shut down. Hence it would be properly logged. Can you describe me a situation where it would matter to log an exception after the shut down?", "author": "tillrohrmann", "createdAt": "2020-06-17T08:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxNTI3MQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441415271", "bodyText": "That NPE was a programming error and it could happen before the shutdown, for example in onTriggerFailure.", "author": "rkhachatryan", "createdAt": "2020-06-17T09:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4NzAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyMzg4MA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440723880", "bodyText": "Maybe call it isCheckpointFailure as it seems that that we are ignoring a whole bunch of exception types.", "author": "tillrohrmann", "createdAt": "2020-06-16T09:44:17Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java", "diffHunk": "@@ -81,22 +78,24 @@ public void handleTaskLevelCheckpointException(\n \t\t\tCheckpointException exception,\n \t\t\tlong checkpointId,\n \t\t\tExecutionAttemptID executionAttemptID) {\n-\t\tcheckFailureCounter(exception, checkpointId);\n-\t\tif (continuousFailureCounter.get() > tolerableCpFailureNumber) {\n+\t\thandleException(exception, checkpointId, (failureCallback, e) -> failureCallback.failJobDueToTaskFailure(e, executionAttemptID));\n+\t}\n+\n+\tprivate void handleException(CheckpointException exception, long checkpointId, BiConsumer<FailJobCallback, Exception> onFailure) {\n+\t\tif (isFailure(exception) &&\n+\t\t\t\tcountedCheckpointIds.add(checkpointId) &&\n+\t\t\t\tcontinuousFailureCounter.incrementAndGet() > tolerableCpFailureNumber) {\n \t\t\tclearCount();\n-\t\t\tfailureCallback.failJobDueToTaskFailure(new FlinkRuntimeException(\"Exceeded checkpoint tolerable failure threshold.\"), executionAttemptID);\n+\t\t\tonFailure.accept(failureCallback, new FlinkRuntimeException(\"Exceeded checkpoint tolerable failure threshold.\"));\n \t\t}\n \t}\n \n-\tpublic void checkFailureCounter(\n-\t\t\tCheckpointException exception,\n-\t\t\tlong checkpointId) {\n+\tprivate boolean isFailure(CheckpointException exception) {", "originalCommit": "9e093628c7c2facb4913315997d0844a7a1b4e47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2MzY2OQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440963669", "bodyText": "Addressed in #12686", "author": "rkhachatryan", "createdAt": "2020-06-16T15:57:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyMzg4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4OTE1OA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440789158", "bodyText": "This should not be necessary since we are clearing continuousFailureCounter in clearCount.", "author": "tillrohrmann", "createdAt": "2020-06-16T11:48:34Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java", "diffHunk": "@@ -84,7 +84,7 @@ public void handleTaskLevelCheckpointException(\n \tprivate void handleException(CheckpointException exception, long checkpointId, BiConsumer<FailJobCallback, Exception> onFailure) {\n \t\tif (isFailure(exception) &&\n \t\t\t\tcountedCheckpointIds.add(checkpointId) &&\n-\t\t\t\tcontinuousFailureCounter.incrementAndGet() > tolerableCpFailureNumber) {\n+\t\t\t\tcontinuousFailureCounter.incrementAndGet() == tolerableCpFailureNumber + 1) {", "originalCommit": "ce537ac0417ebb159a4589f028a024814e03d03f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2NDY0MQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440964641", "bodyText": "I've added testOnlySucceededCheckpointIdRemoved in #12686 to demonstrate this.", "author": "rkhachatryan", "createdAt": "2020-06-16T15:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4OTE1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4OTU4NQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440789585", "bodyText": "This test fails when running ce537ac0417ebb159a4589f028a024814e03d03f.", "author": "tillrohrmann", "createdAt": "2020-06-16T11:49:27Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManagerTest.java", "diffHunk": "@@ -76,7 +76,7 @@ public void testTotalCountValue() {\n \t\t\tfailureManager.handleJobLevelCheckpointException(new CheckpointException(reason), -1);\n \t\t}\n \n-\t\tassertEquals(2, callback.getInvokeCounter());\n+\t\tassertEquals(1, callback.getInvokeCounter());", "originalCommit": "ce537ac0417ebb159a4589f028a024814e03d03f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2NTYwOA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440965608", "bodyText": "I will address it in #12686.", "author": "rkhachatryan", "createdAt": "2020-06-16T16:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4OTU4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MDEwNQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440790105", "bodyText": "Can we give this condition a bit more expressive name?", "author": "tillrohrmann", "createdAt": "2020-06-16T11:50:27Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java", "diffHunk": "@@ -83,9 +83,10 @@ public void handleTaskLevelCheckpointException(\n \n \tprivate void handleException(CheckpointException exception, long checkpointId, BiConsumer<FailJobCallback, Exception> onFailure) {\n \t\tif (isFailure(exception) &&\n+\t\t\t\t!isAlreadyFailed() && // prevent unnecessary storing checkpointId\n \t\t\t\tcountedCheckpointIds.add(checkpointId) &&\n \t\t\t\tcontinuousFailureCounter.incrementAndGet() == tolerableCpFailureNumber + 1) {", "originalCommit": "092e3c70f30ead3bcf3ba5142cc3e54bb954f124", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2MzAxNg==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440963016", "bodyText": "Addressed in #12686", "author": "rkhachatryan", "createdAt": "2020-06-16T15:56:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MDEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MjIzNg==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440792236", "bodyText": "I think we are changing an invariant of this class here. Before it seems that countedCheckpointIds.size() == continuousFailureCounter.get() which is now not longer true. I'm a bit sceptical that these changes can be justified to be put in a hotfix commit.", "author": "tillrohrmann", "createdAt": "2020-06-16T11:54:28Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java", "diffHunk": "@@ -83,9 +83,10 @@ public void handleTaskLevelCheckpointException(\n \n \tprivate void handleException(CheckpointException exception, long checkpointId, BiConsumer<FailJobCallback, Exception> onFailure) {\n \t\tif (isFailure(exception) &&\n+\t\t\t\t!isAlreadyFailed() && // prevent unnecessary storing checkpointId\n \t\t\t\tcountedCheckpointIds.add(checkpointId) &&\n \t\t\t\tcontinuousFailureCounter.incrementAndGet() == tolerableCpFailureNumber + 1) {\n-\t\t\tclearCount();", "originalCommit": "092e3c70f30ead3bcf3ba5142cc3e54bb954f124", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2NTQ0Nw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440965447", "bodyText": "I will address it in #12686.", "author": "rkhachatryan", "createdAt": "2020-06-16T15:59:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MjIzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MzAyMQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440793021", "bodyText": "I would refrain from using static imports. The problem I see is that another person needs to know that this belongs to Optional when looking at this code.", "author": "tillrohrmann", "createdAt": "2020-06-16T11:55:50Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java", "diffHunk": "@@ -78,13 +74,13 @@ public void handleTaskLevelCheckpointException(\n \t\t\tCheckpointException exception,\n \t\t\tlong checkpointId,\n \t\t\tExecutionAttemptID executionAttemptID) {\n-\t\thandleException(exception, checkpointId, (failureCallback, e) -> failureCallback.failJobDueToTaskFailure(e, executionAttemptID));\n+\t\thandleException(exception, of(checkpointId), (failureCallback, e) -> failureCallback.failJobDueToTaskFailure(e, executionAttemptID));", "originalCommit": "0e1e1ee97fef1de8a970521f5cd23399e61f8b33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDk2MzI0Mg==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440963242", "bodyText": "Addressed in #12686", "author": "rkhachatryan", "createdAt": "2020-06-16T15:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MzAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5ODUwMw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440798503", "bodyText": "I think we could add FutureUtils.assertNoException here again if we used handle or exceptionally instead of whenComplete for the final exception handler.", "author": "tillrohrmann", "createdAt": "2020-06-16T12:06:06Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -539,51 +541,62 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint, pendingCheckpoint, timer),\n \t\t\t\t\t\t\ttimer);\n \n-\t\t\tFutureUtils.assertNoException(\n-\t\t\t\tCompletableFuture.allOf(masterStatesComplete, coordinatorCheckpointsComplete)\n-\t\t\t\t\t.handleAsync(\n-\t\t\t\t\t\t(ignored, throwable) -> {\n-\t\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n-\t\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n-\n-\t\t\t\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n-\t\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n-\n-\t\t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\t\t// the initialization might not be finished yet\n-\t\t\t\t\t\t\t\tif (checkpoint == null) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n-\t\t\t\t\t\t\t\t}\n+\t\t\tCompletableFuture", "originalCommit": "3bf8cc9dac706a48383be393551fdb25069be051", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgwMDc4OQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440800789", "bodyText": "A test is missing for the original problem.", "author": "tillrohrmann", "createdAt": "2020-06-16T12:10:22Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -539,51 +541,62 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint, pendingCheckpoint, timer),\n \t\t\t\t\t\t\ttimer);\n \n-\t\t\tFutureUtils.assertNoException(\n-\t\t\t\tCompletableFuture.allOf(masterStatesComplete, coordinatorCheckpointsComplete)\n-\t\t\t\t\t.handleAsync(\n-\t\t\t\t\t\t(ignored, throwable) -> {\n-\t\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n-\t\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n-\n-\t\t\t\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n-\t\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n-\n-\t\t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\t\t// the initialization might not be finished yet\n-\t\t\t\t\t\t\t\tif (checkpoint == null) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n-\t\t\t\t\t\t\t\t}\n+\t\t\tCompletableFuture\n+\t\t\t\t.allOf(masterStatesComplete, coordinatorCheckpointsComplete)\n+\t\t\t\t.handleAsync(\n+\t\t\t\t\t(ignored, throwable) -> {\n+\t\t\t\t\t\tfinal PendingCheckpoint checkpoint =\n+\t\t\t\t\t\t\tFutureUtils.getWithoutException(pendingCheckpointCompletableFuture);\n+\n+\t\t\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\t\t\tcheckpoint != null || throwable != null,\n+\t\t\t\t\t\t\t\"Either the pending checkpoint needs to be created or an error must have been occurred.\");\n+\n+\t\t\t\t\t\tif (throwable != null) {\n+\t\t\t\t\t\t\t// the initialization might not be finished yet\n+\t\t\t\t\t\t\tif (checkpoint == null) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(request, throwable);\n \t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n-\t\t\t\t\t\t\t\t\tonTriggerFailure(\n-\t\t\t\t\t\t\t\t\t\tcheckpoint,\n-\t\t\t\t\t\t\t\t\t\tnew CheckpointException(\n-\t\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n-\t\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n-\t\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n-\t\t\t\t\t\t\t\t\tsnapshotTaskState(\n-\t\t\t\t\t\t\t\t\t\ttimestamp,\n-\t\t\t\t\t\t\t\t\t\tcheckpointId,\n-\t\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n-\t\t\t\t\t\t\t\t\t\trequest.props,\n-\t\t\t\t\t\t\t\t\t\texecutions,\n-\t\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n-\n-\t\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n-\n-\t\t\t\t\t\t\t\t\tonTriggerSuccess();\n-\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\tonTriggerFailure(checkpoint, throwable);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tif (checkpoint.isDiscarded()) {\n+\t\t\t\t\t\t\t\tonTriggerFailure(\n+\t\t\t\t\t\t\t\t\tcheckpoint,\n+\t\t\t\t\t\t\t\t\tnew CheckpointException(\n+\t\t\t\t\t\t\t\t\t\tCheckpointFailureReason.TRIGGER_CHECKPOINT_FAILURE,\n+\t\t\t\t\t\t\t\t\t\tcheckpoint.getFailureCause()));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t// no exception, no discarding, everything is OK\n+\t\t\t\t\t\t\t\tfinal long checkpointId = checkpoint.getCheckpointId();\n+\t\t\t\t\t\t\t\tsnapshotTaskState(\n+\t\t\t\t\t\t\t\t\ttimestamp,\n+\t\t\t\t\t\t\t\t\tcheckpointId,\n+\t\t\t\t\t\t\t\t\tcheckpoint.getCheckpointStorageLocation(),\n+\t\t\t\t\t\t\t\t\trequest.props,\n+\t\t\t\t\t\t\t\t\texecutions,\n+\t\t\t\t\t\t\t\t\trequest.advanceToEndOfTime);\n+\n+\t\t\t\t\t\t\t\tcoordinatorsToCheckpoint.forEach((ctx) -> ctx.afterSourceBarrierInjection(checkpointId));\n+\n+\t\t\t\t\t\t\t\tonTriggerSuccess();\n \t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t\treturn null;\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t},\n+\t\t\t\t\ttimer)\n+\t\t\t\t.whenComplete((unused, error) -> {\n+\t\t\t\t\tif (error != null) {\n+\t\t\t\t\t\tif (!isShutdown()) {\n+\t\t\t\t\t\t\tfailureManager.handleJobLevelCheckpointException(new CheckpointException(EXCEPTION, error), Optional.empty());\n+\t\t\t\t\t\t} else if (error instanceof RejectedExecutionException) {\n+\t\t\t\t\t\t\tLOG.debug(\"Execution rejected during shutdown\");\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tLOG.warn(\"Error encountered during shutdown\", error);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});", "originalCommit": "3bf8cc9dac706a48383be393551fdb25069be051", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkwNzEyOA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r440907128", "bodyText": "When publishing the PR I planned to add it later, but I've found it difficult to reproduce the exact error. Do you have any idea how / whether to test it?", "author": "rkhachatryan", "createdAt": "2020-06-16T14:44:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgwMDc4OQ=="}], "type": "inlineReview"}, {"oid": "edce7433fbf7d8f64feb9e0a5c541fb50ddb3cd0", "url": "https://github.com/apache/flink/commit/edce7433fbf7d8f64feb9e0a5c541fb50ddb3cd0", "message": "[FLINK-18290][checkpointing] Fail job on checkpoint future failure instead of System.exit", "committedDate": "2020-06-16T15:20:06Z", "type": "forcePushed"}, {"oid": "ad5d1ffa1d4a457d2f90f109b6eb9b87aa15c2b1", "url": "https://github.com/apache/flink/commit/ad5d1ffa1d4a457d2f90f109b6eb9b87aa15c2b1", "message": "[FLINK-18290][checkpointing] Fail job on checkpoint future failure instead of System.exit", "committedDate": "2020-06-16T15:32:18Z", "type": "forcePushed"}, {"oid": "8c0119088686cbbe7253404ea13be77ef0b48716", "url": "https://github.com/apache/flink/commit/8c0119088686cbbe7253404ea13be77ef0b48716", "message": "[FLINK-18290][checkpointing] Fail job on checkpoint future failure instead of System.exit", "committedDate": "2020-06-16T22:36:20Z", "type": "forcePushed"}, {"oid": "a1d89c0bec4f034697dfd75cab099958995d4281", "url": "https://github.com/apache/flink/commit/a1d89c0bec4f034697dfd75cab099958995d4281", "message": "[FLINK-18290][checkpointing] Don't System.exit on CheckpointCoordinator failure if it is shut down", "committedDate": "2020-06-17T10:32:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MDI4NA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441470284", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t\t\t\tExceptionUtils.rethrow(error);\n          \n          \n            \n            \t\t\t\t\t\t\tthrow new CompletionException(error)\n          \n      \n    \n    \n  \n\nis better because it won't extend the stack trace with another exception.", "author": "tillrohrmann", "createdAt": "2020-06-17T11:17:35Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -582,7 +583,18 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\ttimer)\n+\t\t\t\t\t.exceptionally(error -> {\n+\t\t\t\t\t\tisTriggering = false;\n+\t\t\t\t\t\tif (!isShutdown()) {\n+\t\t\t\t\t\t\tExceptionUtils.rethrow(error);", "originalCommit": "a1d89c0bec4f034697dfd75cab099958995d4281", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MDU2NQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441470565", "bodyText": "I think we don't need to reset the internal state of CheckpointCoordinator here because it either is shut down or it will fail.", "author": "tillrohrmann", "createdAt": "2020-06-17T11:18:11Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -582,7 +583,18 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\ttimer)\n+\t\t\t\t\t.exceptionally(error -> {\n+\t\t\t\t\t\tisTriggering = false;", "originalCommit": "a1d89c0bec4f034697dfd75cab099958995d4281", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MTQ0Mw==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441471443", "bodyText": "Can't we collapse these two cases into logging error on debug if isShutdown() == true?", "author": "tillrohrmann", "createdAt": "2020-06-17T11:20:10Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java", "diffHunk": "@@ -582,7 +583,18 @@ private void startTriggeringCheckpoint(CheckpointTriggerRequest request) {\n \n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t},\n-\t\t\t\t\t\ttimer));\n+\t\t\t\t\t\ttimer)\n+\t\t\t\t\t.exceptionally(error -> {\n+\t\t\t\t\t\tisTriggering = false;\n+\t\t\t\t\t\tif (!isShutdown()) {\n+\t\t\t\t\t\t\tExceptionUtils.rethrow(error);\n+\t\t\t\t\t\t} else if (error instanceof RejectedExecutionException) {\n+\t\t\t\t\t\t\tLOG.debug(\"Execution rejected during shutdown\");\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tLOG.warn(\"Error encountered during shutdown\", error);\n+\t\t\t\t\t\t}", "originalCommit": "a1d89c0bec4f034697dfd75cab099958995d4281", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3Nzg5OQ==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441477899", "bodyText": "I think we shouldn't log unexpected exceptions on debug level.", "author": "rkhachatryan", "createdAt": "2020-06-17T11:33:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MTQ0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MTYzOA==", "url": "https://github.com/apache/flink/pull/12670#discussion_r441481638", "bodyText": "In case of a shut down I would argue that exceptions can occur if the concurrent tasks are not properly stopped before. But it does not hurt to keep in on warn given that it should not really occur.", "author": "tillrohrmann", "createdAt": "2020-06-17T11:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MTQ0Mw=="}], "type": "inlineReview"}, {"oid": "37ada58c9248a7c2c659450dad9e793d20c34d98", "url": "https://github.com/apache/flink/commit/37ada58c9248a7c2c659450dad9e793d20c34d98", "message": "[FLINK-18290][checkpointing] Don't System.exit on CheckpointCoordinator failure if it is shut down", "committedDate": "2020-06-17T11:34:05Z", "type": "commit"}, {"oid": "37ada58c9248a7c2c659450dad9e793d20c34d98", "url": "https://github.com/apache/flink/commit/37ada58c9248a7c2c659450dad9e793d20c34d98", "message": "[FLINK-18290][checkpointing] Don't System.exit on CheckpointCoordinator failure if it is shut down", "committedDate": "2020-06-17T11:34:05Z", "type": "forcePushed"}]}