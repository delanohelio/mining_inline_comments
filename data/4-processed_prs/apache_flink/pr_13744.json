{"pr_number": 13744, "pr_title": "[FLINK-19766][table-runtime] Introduce File streaming compaction operators", "pr_createdAt": "2020-10-22T09:10:49Z", "pr_url": "https://github.com/apache/flink/pull/13744", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3MzM3Nw==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511773377", "bodyText": "Missing serialVersionUID", "author": "lirui-apache", "createdAt": "2020-10-26T08:01:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3MzU3NA==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511773574", "bodyText": "Missing serialVersionUID", "author": "lirui-apache", "createdAt": "2020-10-26T08:01:29Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511777653", "bodyText": "What's the purpose of this method?", "author": "lirui-apache", "createdAt": "2020-10-26T08:10:30Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.\n+\t */\n+\tpublic static class EndInputFile implements CoordinatorInput {\n+\t\tprivate final long checkpointId;\n+\t\tprivate final int taskId;\n+\t\tprivate final int numberOfTasks;\n+\n+\t\tpublic EndInputFile(long checkpointId, int taskId, int numberOfTasks) {\n+\t\t\tthis.checkpointId = checkpointId;\n+\t\t\tthis.taskId = taskId;\n+\t\t\tthis.numberOfTasks = numberOfTasks;\n+\t\t}\n+\n+\t\tpublic long getCheckpointId() {\n+\t\t\treturn checkpointId;\n+\t\t}\n+\n+\t\tpublic int getTaskId() {\n+\t\t\treturn taskId;\n+\t\t}\n+\n+\t\tpublic int getNumberOfTasks() {\n+\t\t\treturn numberOfTasks;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The output of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorOutput extends Serializable {}\n+\n+\t/**\n+\t * The unit of a single compaction.\n+\t */\n+\tpublic static class CompactionUnit implements CoordinatorOutput {\n+\n+\t\tprivate final int unitId;\n+\t\tprivate final String partition;\n+\n+\t\t// Store strings to improve serialization performance.\n+\t\tprivate final String[] pathStrings;\n+\n+\t\tpublic CompactionUnit(int unitId, String partition, List<Path> unit) {\n+\t\t\tthis.unitId = unitId;\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.pathStrings = unit.stream()\n+\t\t\t\t\t.map(Path::toUri)\n+\t\t\t\t\t.map(URI::toString)\n+\t\t\t\t\t.toArray(String[]::new);\n+\t\t}\n+\n+\t\tpublic boolean isTaskMessage(int taskId) {", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM4NDgzMg==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512384832", "bodyText": "* {@link CompactionUnit} and {@link EndCompaction} must be sent to the downstream in an orderly\n * manner, while {@link EndCompaction} is broadcast emitting, so unit and endCompaction use the\n * broadcast emitting mechanism together. Since unit is broadcast, we want it to be processed by\n * a single task, so we carry the ID in the unit and let the downstream task select its own unit.", "author": "JingsongLi", "createdAt": "2020-10-27T02:48:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM5MjU4Mg==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512392582", "bodyText": "Good catch, there is a bug here, should be:\npublic boolean isTaskMessage(int taskNumber, int taskId) {\n    return unitId % taskNumber == taskId;\n}", "author": "JingsongLi", "createdAt": "2020-10-27T03:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc3NzY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgzMTQ0NA==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511831444", "bodyText": "Throw an exception for unknown elements?", "author": "lirui-apache", "createdAt": "2020-10-26T09:44:32Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinator.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.stream.TaskTracker;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.table.runtime.util.BinPacking;\n+import org.apache.flink.util.function.SupplierWithException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.function.Function;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which coordinate input files to compaction units.\n+ * - Receives in-flight input files inside checkpoint.\n+ * - Receives all upstream end input messages after the checkpoint completes successfully,\n+ *   starts coordination.\n+ *\n+ * <p>NOTE: The coordination is a stable algorithm, which can ensure that the downstream can\n+ *          perform compaction at any time without worrying about fail over.\n+ *\n+ * <p>STATE: This operator stores input files in state, after the checkpoint completes successfully,\n+ *           input files are taken out from the state for coordination.\n+ */\n+public class CompactCoordinator extends AbstractStreamOperator<CoordinatorOutput> implements\n+\t\tOneInputStreamOperator<CoordinatorInput, CoordinatorOutput> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(CompactCoordinator.class);\n+\n+\tprivate final SupplierWithException<FileSystem, IOException> fsFactory;\n+\tprivate final long targetFileSize;\n+\n+\tprivate transient FileSystem fileSystem;\n+\n+\tprivate transient ListState<Map<Long, Map<String, List<Path>>>> inputFilesState;\n+\tprivate transient TreeMap<Long, Map<String, List<Path>>> inputFiles;\n+\tprivate transient Map<String, List<Path>> currentInputFiles;\n+\n+\tprivate transient TaskTracker inputTaskTracker;\n+\n+\tpublic CompactCoordinator(\n+\t\t\tSupplierWithException<FileSystem, IOException> fsFactory,\n+\t\t\tlong targetFileSize) {\n+\t\tthis.fsFactory = fsFactory;\n+\t\tthis.targetFileSize = targetFileSize;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\n+\t\tfileSystem = fsFactory.get();\n+\n+\t\tListStateDescriptor<Map<Long, Map<String, List<Path>>>> filesDescriptor =\n+\t\t\t\tnew ListStateDescriptor<>(\"files-state\", new MapSerializer<>(\n+\t\t\t\t\t\tLongSerializer.INSTANCE,\n+\t\t\t\t\t\tnew MapSerializer<>(\n+\t\t\t\t\t\t\t\tStringSerializer.INSTANCE,\n+\t\t\t\t\t\t\t\tnew ListSerializer<>(\n+\t\t\t\t\t\t\t\t\t\tnew KryoSerializer<>(Path.class, getExecutionConfig())))));\n+\t\tinputFilesState = context.getOperatorStateStore().getListState(filesDescriptor);\n+\t\tinputFiles = new TreeMap<>();\n+\t\tcurrentInputFiles = new HashMap<>();\n+\t\tif (context.isRestored()) {\n+\t\t\tinputFiles.putAll(inputFilesState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CoordinatorInput> element) throws Exception {\n+\t\tCoordinatorInput value = element.getValue();\n+\t\tif (value instanceof InputFile) {\n+\t\t\tInputFile file = (InputFile) value;\n+\t\t\tcurrentInputFiles.computeIfAbsent(file.getPartition(), k -> new ArrayList<>()).add(file.getFile());\n+\t\t} else if (value instanceof EndInputFile) {\n+\t\t\tEndInputFile endInputFile = (EndInputFile) value;\n+\t\t\tif (inputTaskTracker == null) {\n+\t\t\t\tinputTaskTracker = new TaskTracker(endInputFile.getNumberOfTasks());\n+\t\t\t}\n+\n+\t\t\t// ensure all files are ready to be compacted.\n+\t\t\tboolean triggerCommit = inputTaskTracker.add(\n+\t\t\t\t\tendInputFile.getCheckpointId(), endInputFile.getTaskId());\n+\t\t\tif (triggerCommit) {\n+\t\t\t\tcommitUpToCheckpoint(endInputFile.getCheckpointId());\n+\t\t\t}\n+\t\t}", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg2MzcxNg==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511863716", "bodyText": "It seems more of a flag to end checkpoint rather than file input?", "author": "lirui-apache", "createdAt": "2020-10-26T10:37:58Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactMessages.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Util class for all compaction messages.\n+ *\n+ * <p>The compaction operator graph is:\n+ * TempFileWriter|parallel ---(InputFile&EndInputFile)---> CompactCoordinator|non-parallel\n+ * ---(CompactionUnit&EndCompaction)--->CompactOperator|parallel---(PartitionCommitInfo)--->\n+ * PartitionCommitter|non-parallel\n+ *\n+ * <p>Because the end message is a kind of barrier of record messages, they can only be transmitted\n+ * in the way of full broadcast in the link from coordinator to compact operator.\n+ */\n+public class CompactMessages {\n+\tprivate CompactMessages() {}\n+\n+\t/**\n+\t * The input of compact coordinator.\n+\t */\n+\tpublic interface CoordinatorInput extends Serializable {}\n+\n+\t/**\n+\t * A partitioned input file.\n+\t */\n+\tpublic static class InputFile implements CoordinatorInput {\n+\t\tprivate final String partition;\n+\t\tprivate final Path file;\n+\n+\t\tpublic InputFile(String partition, Path file) {\n+\t\t\tthis.partition = partition;\n+\t\t\tthis.file = file;\n+\t\t}\n+\n+\t\tpublic String getPartition() {\n+\t\t\treturn partition;\n+\t\t}\n+\n+\t\tpublic Path getFile() {\n+\t\t\treturn file;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A flag to end file input.", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxNzg4Mg==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511917882", "bodyText": "If something goes wrong in this method and the job fails over, will this method be called again for the same checkpointId?", "author": "lirui-apache", "createdAt": "2020-10-26T12:20:30Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/compact/CompactOperator.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.core.fs.FSDataInputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.FileSystemKind;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.fs.RecoverableFsDataOutputStream;\n+import org.apache.flink.core.fs.RecoverableWriter;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.BucketWriter;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.stream.PartitionCommitInfo;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.util.IOUtils;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.flink.util.function.SupplierWithException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+/**\n+ * Receives compaction units to do compaction. Send partition commit information after\n+ * compaction finished.\n+ *\n+ * <p>Use {@link BulkFormat} to read and use {@link BucketWriter} to write.\n+ *\n+ * <p>STATE: This operator stores expired files in state, after the checkpoint completes successfully,\n+ *           We can ensure that these files will not be used again and they can be deleted from the\n+ *           file system.\n+ */\n+public class CompactOperator<T> extends AbstractStreamOperator<PartitionCommitInfo>\n+\t\timplements OneInputStreamOperator<CoordinatorOutput, PartitionCommitInfo>, BoundedOneInput {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final String UNCOMPACTED_PREFIX = \".uncompacted-\";\n+\n+\tprivate static final String COMPACTED_PREFIX = \"compacted-\";\n+\n+\tprivate final SupplierWithException<FileSystem, IOException> fsFactory;\n+\tprivate final CompactReader.Factory<T> readerFactory;\n+\tprivate final CompactWriter.Factory<T> writerFactory;\n+\n+\tprivate transient FileSystem fileSystem;\n+\n+\tprivate transient ListState<Map<Long, List<Path>>> expiredFilesState;\n+\tprivate transient TreeMap<Long, List<Path>> expiredFiles;\n+\tprivate transient List<Path> currentExpiredFiles;\n+\n+\tprivate transient Set<String> partitions;\n+\n+\tpublic CompactOperator(\n+\t\t\tSupplierWithException<FileSystem, IOException> fsFactory,\n+\t\t\tCompactReader.Factory<T> readerFactory,\n+\t\t\tCompactWriter.Factory<T> writerFactory) {\n+\t\tthis.fsFactory = fsFactory;\n+\t\tthis.readerFactory = readerFactory;\n+\t\tthis.writerFactory = writerFactory;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.partitions = new HashSet<>();\n+\t\tthis.fileSystem = fsFactory.get();\n+\n+\t\tListStateDescriptor<Map<Long, List<Path>>> metaDescriptor =\n+\t\t\t\tnew ListStateDescriptor<>(\"expired-files\", new MapSerializer<>(\n+\t\t\t\t\t\tLongSerializer.INSTANCE,\n+\t\t\t\t\t\tnew ListSerializer<>(new KryoSerializer<>(Path.class, getExecutionConfig()))\n+\t\t\t\t));\n+\t\tthis.expiredFilesState = context.getOperatorStateStore().getListState(metaDescriptor);\n+\t\tthis.expiredFiles = new TreeMap<>();\n+\t\tthis.currentExpiredFiles = new ArrayList<>();\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tthis.expiredFiles.putAll(this.expiredFilesState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CoordinatorOutput> element) throws Exception {\n+\t\tCoordinatorOutput value = element.getValue();\n+\t\tif (value instanceof CompactionUnit) {\n+\t\t\tCompactionUnit unit = (CompactionUnit) value;\n+\t\t\tif (unit.isTaskMessage(getRuntimeContext().getNumberOfParallelSubtasks())) {\n+\t\t\t\tString partition = unit.getPartition();\n+\t\t\t\tList<Path> paths = unit.getPaths();\n+\n+\t\t\t\tdoCompact(paths);\n+\t\t\t\tthis.partitions.add(partition);\n+\n+\t\t\t\t// Only after the current checkpoint is successfully executed can delete\n+\t\t\t\t// the expired files, so as to ensure the existence of the files.\n+\t\t\t\tthis.currentExpiredFiles.addAll(paths);\n+\t\t\t}\n+\t\t} else if (value instanceof EndCompaction) {\n+\t\t\tendCompaction(((EndCompaction) value).getCheckpointId());\n+\t\t}\n+\t}\n+\n+\tprivate void endCompaction(long checkpoint) {\n+\t\tthis.output.collect(new StreamRecord<>(new PartitionCommitInfo(\n+\t\t\t\tcheckpoint,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(this.partitions))));\n+\t\tthis.partitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\n+\t\texpiredFilesState.clear();\n+\t\texpiredFiles.put(context.getCheckpointId(), new ArrayList<>(currentExpiredFiles));\n+\t\texpiredFilesState.add(expiredFiles);\n+\t\tcurrentExpiredFiles.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3ODE4Mw==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512378183", "bodyText": "No, then will wait for next checkpoint notify.", "author": "JingsongLi", "createdAt": "2020-10-27T02:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxNzg4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng==", "url": "https://github.com/apache/flink/pull/13744#discussion_r511929276", "bodyText": "I think CompactCoordinator doesn't guarantee to generate CompactionUnit in any specific order of partitions, right?", "author": "lirui-apache", "createdAt": "2020-10-26T12:41:04Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactCoordinatorTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream.compact;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CompactionUnit;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorInput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.CoordinatorOutput;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndCompaction;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.EndInputFile;\n+import org.apache.flink.table.filesystem.stream.compact.CompactMessages.InputFile;\n+import org.apache.flink.util.function.ThrowingConsumer;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test for {@link CompactCoordinator}.\n+ */\n+public class CompactCoordinatorTest extends AbstractCompactTestBase {\n+\n+\t@Test\n+\tpublic void testCoordinatorCrossCheckpoints() throws Exception {\n+\t\tAtomicReference<OperatorSubtaskState> state = new AtomicReference<>();\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f0\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f1\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f2\", 2)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f3\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f4\", 1)), 0);\n+\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f5\", 5)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p1\", newFile(\"f6\", 4)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(1, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f7\", 3)), 0);\n+\t\t\tharness.processElement(new InputFile(\"p0\", newFile(\"f8\", 2)), 0);\n+\n+\t\t\tstate.set(harness.snapshot(2, 0));\n+\t\t});\n+\n+\t\trunCoordinator(harness -> {\n+\t\t\tharness.setup();\n+\t\t\tharness.initializeState(state.get());\n+\t\t\tharness.open();\n+\n+\t\t\tharness.processElement(new EndInputFile(2, 0, 1), 0);\n+\n+\t\t\tList<CoordinatorOutput> outputs = harness.extractOutputValues();\n+\n+\t\t\tAssert.assertEquals(7, outputs.size());\n+\n+\t\t\tassertUnit(outputs.get(0), 0, \"p0\", Arrays.asList(\"f0\", \"f1\", \"f4\"));", "originalCommit": "074ab07749ff798563e785d2c2fafeee1302ac74", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3ODQ2MQ==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512378461", "bodyText": "You mean the order of partitions? There is no relationship between partitions, so there is no need to guarantee this.", "author": "JingsongLi", "createdAt": "2020-10-27T02:26:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM4ODMyMw==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512388323", "bodyText": "Yeah... but then how could we assert the first output is for p0?", "author": "lirui-apache", "createdAt": "2020-10-27T03:01:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzNDczMg==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512434732", "bodyText": "You mean we should sort it before assert?", "author": "JingsongLi", "createdAt": "2020-10-27T06:01:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzNTk2NA==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512435964", "bodyText": "OK, I'll do it", "author": "JingsongLi", "createdAt": "2020-10-27T06:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkyOTI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUxMzg1MQ==", "url": "https://github.com/apache/flink/pull/13744#discussion_r512513851", "bodyText": "Also verify f3 and f6 are not compacted at this point.", "author": "lirui-apache", "createdAt": "2020-10-27T08:56:05Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/filesystem/stream/compact/CompactOperatorTest.java", "diffHunk": "@@ -105,8 +105,63 @@ public void testCompactOperator() throws Exception {\n \t\t});\n \t}\n \n+\t@Test\n+\tpublic void testUnitSelection() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<CoordinatorOutput, PartitionCommitInfo> harness0 = create(2, 0);\n+\t\tharness0.setup();\n+\t\tharness0.open();\n+\n+\t\tOneInputStreamOperatorTestHarness<CoordinatorOutput, PartitionCommitInfo> harness1 = create(2, 1);\n+\t\tharness1.setup();\n+\t\tharness1.open();\n+\n+\t\tPath f0 = newFile(\".uncompacted-f0\", 3);\n+\t\tPath f1 = newFile(\".uncompacted-f1\", 2);\n+\t\tPath f2 = newFile(\".uncompacted-f2\", 2);\n+\t\tPath f3 = newFile(\".uncompacted-f3\", 5);\n+\t\tPath f4 = newFile(\".uncompacted-f4\", 1);\n+\t\tPath f5 = newFile(\".uncompacted-f5\", 5);\n+\t\tPath f6 = newFile(\".uncompacted-f6\", 4);\n+\t\tFileSystem fs = f0.getFileSystem();\n+\n+\t\t// broadcast\n+\t\tharness0.processElement(new CompactionUnit(0, \"p0\", Arrays.asList(f0, f1, f4)), 0);\n+\t\tharness0.processElement(new CompactionUnit(1, \"p0\", Collections.singletonList(f3)), 0);\n+\t\tharness0.processElement(new CompactionUnit(2, \"p0\", Arrays.asList(f2, f5)), 0);\n+\t\tharness0.processElement(new CompactionUnit(3, \"p0\", Collections.singletonList(f6)), 0);\n+\n+\t\tharness1.processElement(new CompactionUnit(0, \"p0\", Arrays.asList(f0, f1, f4)), 0);\n+\t\tharness1.processElement(new CompactionUnit(1, \"p0\", Collections.singletonList(f3)), 0);\n+\t\tharness1.processElement(new CompactionUnit(2, \"p0\", Arrays.asList(f2, f5)), 0);\n+\t\tharness1.processElement(new CompactionUnit(3, \"p0\", Collections.singletonList(f6)), 0);\n+\n+\t\tharness0.processElement(new EndCompaction(1), 0);\n+\n+\t\t// check all compacted file generated\n+\t\tAssert.assertTrue(fs.exists(new Path(folder, \"compacted-f0\")));\n+\t\tAssert.assertTrue(fs.exists(new Path(folder, \"compacted-f2\")));", "originalCommit": "b30d49e889a255acb42f286e54d56f1760ff367e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "164c8b5b7750ed567813d70f160ef9f064d0a3b0", "url": "https://github.com/apache/flink/commit/164c8b5b7750ed567813d70f160ef9f064d0a3b0", "message": "move arguments to context", "committedDate": "2020-10-28T04:12:47Z", "type": "forcePushed"}, {"oid": "6ceb696c9f765f1b0e58ed1b8f2ceab696f06b5b", "url": "https://github.com/apache/flink/commit/6ceb696c9f765f1b0e58ed1b8f2ceab696f06b5b", "message": "[FLINK-19766][table-runtime] Introduce File streaming compaction operators", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "081f246e4fd2c94bed78c0b6d1f8253b363fd8b6", "url": "https://github.com/apache/flink/commit/081f246e4fd2c94bed78c0b6d1f8253b363fd8b6", "message": "checkstyle", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "fe114c1f02afed44232f55766ff52fcb41047faa", "url": "https://github.com/apache/flink/commit/fe114c1f02afed44232f55766ff52fcb41047faa", "message": "minor fixes", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "07e3fe5bca79bd9d7594fd925928c60a9b2ef817", "url": "https://github.com/apache/flink/commit/07e3fe5bca79bd9d7594fd925928c60a9b2ef817", "message": "checkstyle", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "cf39956f03064a858a8c6facd3748f0bc833dcaf", "url": "https://github.com/apache/flink/commit/cf39956f03064a858a8c6facd3748f0bc833dcaf", "message": "Address comments", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "dbfe01e9dfd35d4e814cccd542bc0383e93a0d89", "url": "https://github.com/apache/flink/commit/dbfe01e9dfd35d4e814cccd542bc0383e93a0d89", "message": "Fix testUnitSelection", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "ef03602df01dcb0f4db8966123d57bd11069b87c", "url": "https://github.com/apache/flink/commit/ef03602df01dcb0f4db8966123d57bd11069b87c", "message": "Address comment", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "34c9585c497203079d2bebf9d8fbc7b98198c851", "url": "https://github.com/apache/flink/commit/34c9585c497203079d2bebf9d8fbc7b98198c851", "message": "Also verify f3 and f6 are not compacted at this point", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "129d98d9a9a33c03b80faea0ed2a0ccbe417a425", "url": "https://github.com/apache/flink/commit/129d98d9a9a33c03b80faea0ed2a0ccbe417a425", "message": "checkstyle", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "90ec250adca937019bb843e1ee60bee172f06753", "url": "https://github.com/apache/flink/commit/90ec250adca937019bb843e1ee60bee172f06753", "message": "move arguments to context", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "12d05a126f13fb07675b52f3b00a5a1b11b26c03", "url": "https://github.com/apache/flink/commit/12d05a126f13fb07675b52f3b00a5a1b11b26c03", "message": "checkstyle", "committedDate": "2020-10-29T02:41:03Z", "type": "commit"}, {"oid": "039f49ada096d2f85fc6908618f77c8483a446fb", "url": "https://github.com/apache/flink/commit/039f49ada096d2f85fc6908618f77c8483a446fb", "message": "Fix case", "committedDate": "2020-10-29T02:41:04Z", "type": "commit"}, {"oid": "039f49ada096d2f85fc6908618f77c8483a446fb", "url": "https://github.com/apache/flink/commit/039f49ada096d2f85fc6908618f77c8483a446fb", "message": "Fix case", "committedDate": "2020-10-29T02:41:04Z", "type": "forcePushed"}]}