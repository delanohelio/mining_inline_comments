{"pr_number": 14246, "pr_title": "[FLINK-20273][table/kafka] Fix the Kafka round-robin behaviour when k\u2026", "pr_createdAt": "2020-11-27T09:27:31Z", "pr_url": "https://github.com/apache/flink/pull/14246", "timeline": [{"oid": "9d856871e79c3bbe65054c145c51e410f83d5c65", "url": "https://github.com/apache/flink/commit/9d856871e79c3bbe65054c145c51e410f83d5c65", "message": "[FLINK-20273][table/kafka] Fix the Kafka round-robin behaviour when keys are specified", "committedDate": "2020-11-27T09:09:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMxMjc3OA==", "url": "https://github.com/apache/flink/pull/14246#discussion_r537312778", "bodyText": "We should still need to check the partitioner is in the allowed enums.", "author": "wuchong", "createdAt": "2020-12-07T08:24:48Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -314,12 +317,12 @@ private static void validateScanStartupMode(ReadableConfig tableOptions) {\n \tprivate static void validateSinkPartitioner(ReadableConfig tableOptions) {\n \t\ttableOptions.getOptional(SINK_PARTITIONER)\n \t\t\t\t.ifPresent(partitioner -> {\n-\t\t\t\t\tif (!SINK_PARTITIONER_ENUMS.contains(partitioner.toLowerCase())) {\n-\t\t\t\t\t\tif (partitioner.isEmpty()) {\n-\t\t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\t\tString.format(\"Option '%s' should be a non-empty string.\",\n-\t\t\t\t\t\t\t\t\t\t\tSINK_PARTITIONER.key()));\n-\t\t\t\t\t\t}\n+\t\t\t\t\tif (partitioner.equals(SINK_PARTITIONER_VALUE_ROUND_ROBIN) && tableOptions.getOptional(KEY_FIELDS).isPresent()) {", "originalCommit": "9d856871e79c3bbe65054c145c51e410f83d5c65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzM1MjQ1Ng==", "url": "https://github.com/apache/flink/pull/14246#discussion_r537352456", "bodyText": "It's hard to check here because users may input the class name as user-defined partitioner.", "author": "fsk119", "createdAt": "2020-12-07T09:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMxMjc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyNTkzOA==", "url": "https://github.com/apache/flink/pull/14246#discussion_r537325938", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n          \n          \n            \n            It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.\n          \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records. It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            \n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same Kafka partition, which could reduce the cost of the network connections.", "author": "wuchong", "createdAt": "2020-12-07T08:46:23Z", "path": "docs/dev/table/connectors/kafka.md", "diffHunk": "@@ -525,9 +527,9 @@ See more about how to use the CDC formats in [debezium-json]({% link dev/table/c\n ### Sink Partitioning\n \n The config option `sink.partitioner` specifies output partitioning from Flink's partitions into Kafka's partitions.\n-By default, a Kafka sink writes to at most as many partitions as its own parallelism (each parallel instance of the sink writes to exactly one partition).\n-In order to distribute the writes to more partitions or control the routing of rows into partitions, a custom sink partitioner can be provided. The `round-robin` partitioner is useful to avoid an unbalanced partitioning.\n-However, it will cause a lot of network connections between all the Flink instances and all the Kafka brokers.\n+By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n+It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n+In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "originalCommit": "9d856871e79c3bbe65054c145c51e410f83d5c65", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyNjMyOQ==", "url": "https://github.com/apache/flink/pull/14246#discussion_r537326329", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n          \n          \n            \n            It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.\n          \n          \n            \n            By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records. It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n          \n          \n            \n            \n          \n          \n            \n            In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "author": "wuchong", "createdAt": "2020-12-07T08:46:58Z", "path": "docs/dev/table/connectors/kafka.zh.md", "diffHunk": "@@ -526,9 +528,9 @@ See more about how to use the CDC formats in [debezium-json]({% link dev/table/c\n ### Sink Partitioning\n \n The config option `sink.partitioner` specifies output partitioning from Flink's partitions into Kafka's partitions.\n-By default, a Kafka sink writes to at most as many partitions as its own parallelism (each parallel instance of the sink writes to exactly one partition).\n-In order to distribute the writes to more partitions or control the routing of rows into partitions, a custom sink partitioner can be provided. The `round-robin` partitioner is useful to avoid an unbalanced partitioning.\n-However, it will cause a lot of network connections between all the Flink instances and all the Kafka brokers.\n+By default, Flink uses the [Kafka default partitioner](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java) to parititon records.\n+It uses the [sticky partition strategy](https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/) for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\n+In order to control the routing of rows into partitions, a custom sink partitioner can be provided. The 'fixed' partitioner will write the records in the same Flink partition into the same partition, which could reduce the cost of the network connections.", "originalCommit": "9d856871e79c3bbe65054c145c51e410f83d5c65", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "96531a0172fef0b4943b198139269a333a6715a6", "url": "https://github.com/apache/flink/commit/96531a0172fef0b4943b198139269a333a6715a6", "message": "Apply suggestions from code review\n\nCo-authored-by: Jark Wu <imjark@gmail.com>", "committedDate": "2020-12-07T09:26:50Z", "type": "commit"}, {"oid": "129faf543cd4bb829a3951388ee894521dccd88f", "url": "https://github.com/apache/flink/commit/129faf543cd4bb829a3951388ee894521dccd88f", "message": "delete useless enum", "committedDate": "2020-12-07T10:22:29Z", "type": "commit"}]}