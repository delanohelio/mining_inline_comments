{"pr_number": 12478, "pr_title": "[FLINK-17869][task][checkpointing] Fix race condition when calling ChannelStateWriter.abort", "pr_createdAt": "2020-06-04T07:44:42Z", "pr_url": "https://github.com/apache/flink/pull/12478", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI2NjUxOQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r435266519", "bodyText": "This is just an optimization.", "author": "rkhachatryan", "createdAt": "2020-06-04T13:44:33Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -158,6 +159,16 @@ private ChannelStateWriter openChannelStateWriter() {\n \t@Override\n \tpublic void abortCheckpointOnBarrier(long checkpointId, Throwable cause, OperatorChain<?, ?> operatorChain) throws IOException {\n \t\tLOG.debug(\"Aborting checkpoint via cancel-barrier {} for task {}\", checkpointId, taskName);\n+\t\tlastCheckpointId = Math.max(lastCheckpointId, checkpointId);\n+\t\tIterator<Long> iterator = abortedCheckpointIds.iterator();\n+\t\twhile (iterator.hasNext()) {\n+\t\t\tlong next = iterator.next();\n+\t\t\tif (next < lastCheckpointId) {\n+\t\t\t\titerator.remove();\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}", "originalCommit": "be8fbcf506b8dd38e5425cf772a55f033f0962b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r435273439", "bodyText": "This is just an optimization (possible now with abort and cleanup=false).", "author": "rkhachatryan", "createdAt": "2020-06-04T13:51:08Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -281,6 +281,8 @@ public void notifyCheckpointAborted(long checkpointId, OperatorChain<?, ?> opera\n \t\t\t\t}\n \t\t\t}\n \n+\t\t\tchannelStateWriter.abort(checkpointId, new CancellationException(\"checkpoint aborted via notification\"), false);", "originalCommit": "420b7b3f645625b0b43f056440509cdd53bbd4ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwODAzMw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436508033", "bodyText": "I like this optimization.\nWhat happens if some other thread still enqueues data to this checkpoint? Or the writes dismissed?", "author": "AHeise", "createdAt": "2020-06-08T07:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU3NzM1MQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436577351", "bodyText": "Yes, write requests will be ignored when dequeued by ChannelStateWriteRequestExecutor.", "author": "rkhachatryan", "createdAt": "2020-06-08T09:46:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI3MzQzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMTI0NQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436501245", "bodyText": "nit: usually we add task name to front, also it would be nice to use %d > %d for the comparison values.", "author": "AHeise", "createdAt": "2020-06-08T07:24:00Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -102,11 +102,11 @@ public void start(long checkpointId, CheckpointOptions checkpointOptions) {\n \t\tLOG.debug(\"{} starting checkpoint {} ({})\", taskName, checkpointId, checkpointOptions);\n \t\tChannelStateWriteResult result = new ChannelStateWriteResult();\n \t\tChannelStateWriteResult put = results.computeIfAbsent(checkpointId, id -> {\n-\t\t\tPreconditions.checkState(results.size() < maxCheckpoints, \"results.size() > maxCheckpoints\", results.size(), maxCheckpoints);\n+\t\t\tPreconditions.checkState(results.size() < maxCheckpoints, String.format(\"can't start %d, results.size() > maxCheckpoints: %d, %d, %s\", checkpointId, results.size(), maxCheckpoints, taskName));", "originalCommit": "72bfc6b84e33e5abcee8d078f6d4d00904f626f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436502262", "bodyText": "I also thought about that in FLINK-17218, but ultimately couldn't find a reason why any arbitrary large value offers an advantage over the current value. It makes the failing checkState more unlikely, but didn't eliminate it.\nI also thought about removing the checkState, but it helped me to find some issues previously.\nTL;DR I'm not convinced that this is a proper fix.", "author": "AHeise", "createdAt": "2020-06-08T07:26:13Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 100; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "originalCommit": "4ee338a57d401b7ae9c79d926f47637d4e6431e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU2MzY5OQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436563699", "bodyText": "The map is already cleared during the regular checkpoint abortion by the task thread but it happens with some delay. I see only two reasons why its size could significantly exceed max-concurrent-checkpoints:\n\nbug in abort procedure\ntask thread is stuck while netty thread continues to receive new barriers fast\n\nTheoretically, 2nd case is unbounded, but in practice, I didn't observe issues with a value of 5 in UnalignedITCase after fixing other issues. I think such a scenario where task thread is not aborting writes would signify some problem and should be investigated.\nBesides that, aborting async parts of previous checkpoints upon receiving a new barrier won't work with max-concurrent-checkpoints > 1.", "author": "rkhachatryan", "createdAt": "2020-06-08T09:21:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzIzMjUxMg==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437232512", "bodyText": "Just thinking loud.\nPreviously, the issue was that sources received new RPC triggers while being stuck, which enqueued a ton of mails.\nWith a checkpointing interval of 100ms, you only need to be stuck for 10s until you enqueue 100 mails and hit the limit.\nBut I guess, the assumption is that now notifyCheckpointAborted is called reliably. And if doesn't, we need to fail probably anyways, since something is broken.\nSo I guess this solution is as good as it gets. I trade-off between quickly identifying bugs in abort and running potentially into some issues with ultra slow tasks and ultra fast checkpoint barriers.", "author": "AHeise", "createdAt": "2020-06-09T08:35:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwMjI2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNDI4NA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436504284", "bodyText": "Should that be warning? I think on concept level yes.\nHowever, in reality, in current UC, this case happens probably quite frequently (barrier overtakes by cancellation marker doesn't). I wouldn't like seeing the log polluted with WARN.", "author": "AHeise", "createdAt": "2020-06-08T07:30:34Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -196,9 +207,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {\n+\t\t\tLOG.warn(\"Out of order checkpoint barrier (aborted previously?): {} >= {}\", lastCheckpointId, metadata.getCheckpointId());", "originalCommit": "be8fbcf506b8dd38e5425cf772a55f033f0962b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNTg0MA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436505840", "bodyText": "Please rename getWriteResult. I think most developers would expect a getter to be idempotent.\nHow about removeWriteResult? That would be similar to how a Java map works. Or getAndRemoveWriteResult to be completely explicit.", "author": "AHeise", "createdAt": "2020-06-08T07:34:14Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -141,15 +141,10 @@ boolean isDone() {\n \tvoid abort(long checkpointId, Throwable cause);\n \n \t/**\n-\t * Must be called after {@link #start(long, CheckpointOptions)}.\n+\t * Must be called after {@link #start(long, CheckpointOptions)} once.\n \t */\n \tChannelStateWriteResult getWriteResult(long checkpointId);", "originalCommit": "9fc0ec3ed824c9864430ed97bd681d510c8ee18e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUzNDE2NA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436534164", "bodyText": "Good point, I'll rename it to getAndRemoveWriteResult.", "author": "rkhachatryan", "createdAt": "2020-06-08T08:30:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNTg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNzIwNQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436507205", "bodyText": "I guess the intend of cleanup is to avoid throwing an exception if getWriteResult(long) is called nonetheless, right?\nSo in which cases can that happen? Do we ensure cleanup in these cases (e.g., is abort then called twice?)?", "author": "AHeise", "createdAt": "2020-06-08T07:37:18Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java", "diffHunk": "@@ -137,8 +137,9 @@ boolean isDone() {\n \n \t/**\n \t * Aborts the checkpoint and fails pending result for this checkpoint.\n+\t * @param cleanup true if {@link #getWriteResult(long)} is not supposed to be called afterwards.\n \t */\n-\tvoid abort(long checkpointId, Throwable cause);\n+\tvoid abort(long checkpointId, Throwable cause, boolean cleanup);", "originalCommit": "dad26a041df3442f65b6592bb10611828cf80d37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU2ODg5Nw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r436568897", "bodyText": "I guess the intend of cleanup is to avoid throwing an exception if getWriteResult(long) is called nonetheless, right?\n\nYes.\n\nSo in which cases can that happen?\n\nabort with cleanup=false can be called when:\n\nreceiving abort notification via RPC\nCheckpointBarrierUnaligner.allBarriersReceivedFuture is cancelled\n\n\nDo we ensure cleanup in these cases (e.g., is abort then called twice?)?\n\nYes, for each checkpoint either \"regular\" abort or getWriteResult is eventually called.", "author": "rkhachatryan", "createdAt": "2020-06-08T09:31:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUwNzIwNQ=="}], "type": "inlineReview"}, {"oid": "29f103ced731a6939acb7d414e4545b07f13f370", "url": "https://github.com/apache/flink/commit/29f103ced731a6939acb7d414e4545b07f13f370", "message": "[FLINK-17869][task][checkpointing] Abort writing of channel state by RPC\nnotification", "committedDate": "2020-06-08T09:53:31Z", "type": "forcePushed"}, {"oid": "d587ae3d72bfca59ea734c668d43fcdeb72cff96", "url": "https://github.com/apache/flink/commit/d587ae3d72bfca59ea734c668d43fcdeb72cff96", "message": "[FLINK-17869][task][checkpointing] Abort writing of channel state by RPC\nnotification", "committedDate": "2020-06-09T09:13:00Z", "type": "forcePushed"}, {"oid": "3cf48798c9220b9c1d8d9ca5100688b45a292613", "url": "https://github.com/apache/flink/commit/3cf48798c9220b9c1d8d9ca5100688b45a292613", "message": "[FLINK-17869][hotfix] Add taskName to ChannelStateWriter log messages", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "aafa060ee611f855654e8e9653bdf11ef54e8f84", "url": "https://github.com/apache/flink/commit/aafa060ee611f855654e8e9653bdf11ef54e8f84", "message": "[FLINK-17869][hotfix] Don't pass ChannelStateWrite Future to AsyncCheckpointRunnable\n\nOperatorSnapshotFinalizer already waits and holds this future.\nChannelStateWriter.getWriteResult() can then be non-idempotent.\nChannelStateWriter.stop() can then be removed.", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "1582b47414a048efc0d9b634ffa57b020610a47f", "url": "https://github.com/apache/flink/commit/1582b47414a048efc0d9b634ffa57b020610a47f", "message": "[FLINK-17869][task][checkpointing] Revert \"[FLINK-17218][checkpointing] Ensuring that ChannelStateWriter aborts previous checkpoints before a new checkpoint is started.\"\n\nThis reverts commit 24ff415f1b76392f75dea7c3538558d24fcb7058\nwhich introduced a race condition when task thread and netty\nthread compete for ChannelStateWriteResult.\n\nInstead, next commits fix it by:\n1. Map size validation error will be prevented simply by increasing the limit\n2. When a checkpoint is subsumed, it's write result will be removed from on future completion", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6", "url": "https://github.com/apache/flink/commit/6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6", "message": "[FLINK-17869][task][checkpointing] Abort channel state write if checkpoint is subsumed\n\nMotivation: stop writing channel state ASAP if the checkpoint is subsumed\n\nChanges:\n1. complete CheckpointBarrierUnaligner.ThreadSafeUnaligner#allBarriersReceivedFuture\n2. abort channel state write on its erroneous completion\n3. add cleanup parameter to ChannelStateWriter.abort to use cleanup=false\nin the call above", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "884371e7dc8a604f5b55b8e39f27a1bffd002173", "url": "https://github.com/apache/flink/commit/884371e7dc8a604f5b55b8e39f27a1bffd002173", "message": "[FLINK-17869][task][checkpointing] Increase ChannelStateWriterImpl.DEFAULT_MAX_CHECKPOINTS\n\nChannelStateWriter map is cleaned up by the task thread,\nso the check in netty thread should take possible delay\ninto account.", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "26343fa05d8e72cf21cea7246e332c7850e05db3", "url": "https://github.com/apache/flink/commit/26343fa05d8e72cf21cea7246e332c7850e05db3", "message": "[FLINK-17869][task][checkpointing] Ignore out of order checkpoints in SubtaskCheckpointCoordinator\n\nCheck (by task thread) whether the current checkpoint was already aborted in the following scenario:\n1. on checkpoint barrier ThreadSafeUnaligner sends a mail to start checkpointing (netty thread)\n2. on cancellation marker CheckpointBarrierUnaligner aborts it (task thread)\n3. task thread processes a mail to start checkpointing", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "a3e92dd698cbc6786d5699e4f5aeab2f115587e5", "url": "https://github.com/apache/flink/commit/a3e92dd698cbc6786d5699e4f5aeab2f115587e5", "message": "[FLINK-17869][tests] Unignore UnalignedCheckpointITCase", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "22c99845ef4f863f1753d17b109fd2faecc8201e", "url": "https://github.com/apache/flink/commit/22c99845ef4f863f1753d17b109fd2faecc8201e", "message": "[FLINK-17869][task][checkpointing] Abort writing of channel state by RPC\nnotification", "committedDate": "2020-06-09T09:37:42Z", "type": "commit"}, {"oid": "22c99845ef4f863f1753d17b109fd2faecc8201e", "url": "https://github.com/apache/flink/commit/22c99845ef4f863f1753d17b109fd2faecc8201e", "message": "[FLINK-17869][task][checkpointing] Abort writing of channel state by RPC\nnotification", "committedDate": "2020-06-09T09:37:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437897063", "bodyText": "I am not quite sure whether we still have race condition here. abortCheckpointOnBarrier might be called from CheckpointBarrierUnaligner after triggering checkpoint into mailbox. After aborting, we did not remove the checkpoint action from mailbox, so the checkpoint might still happen afterwards. So how to guarantee that #getAndRemoveWriteResult would never be called after aborting?", "author": "zhijiangW", "createdAt": "2020-06-10T06:52:19Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -186,7 +186,7 @@ public void abortCheckpointOnBarrier(long checkpointId, Throwable cause, Operato\n \n \t\tcheckpointStorage.clearCacheFor(checkpointId);\n \n-\t\tchannelStateWriter.abort(checkpointId, cause);\n+\t\tchannelStateWriter.abort(checkpointId, cause, true);", "originalCommit": "6eee77db29211ce1fc8c23d7f6e3df6af9fd12c6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkxNTc0Mw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437915743", "bodyText": "I got the answer when seeing the following commit \"Ignore out of order checkpoints in SubtaskCheckpointCoordinator\". It updates the lastCheckpointId to prevent the following aborted checkpoint execution.", "author": "zhijiangW", "createdAt": "2020-06-10T07:31:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzOTAzOA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437939038", "bodyText": "Right, there are several race conditions that are addressed in different commits.", "author": "rkhachatryan", "createdAt": "2020-06-10T08:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg5NzA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437906197", "bodyText": "I guess the current way seems a temporary work-around solution, not an elegant way. The initial purpose for introducing this threshold is for logic validating and avoiding invalid checkpoints retained in writer forever. But if we consider the abort delay into this threshold, it seems somehow lose the initial meaning for the guard, and we are really not sure what is the proper value for this threshold.\nThe proper way might resolve the potential race condition in essence, but it might pay more efforts not feasible ATM. So I think we might leave another debt here in future.", "author": "zhijiangW", "createdAt": "2020-06-10T07:12:40Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriterImpl.java", "diffHunk": "@@ -56,7 +56,7 @@\n public class ChannelStateWriterImpl implements ChannelStateWriter {\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(ChannelStateWriterImpl.class);\n-\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 5; // currently, only single in-flight checkpoint is supported\n+\tprivate static final int DEFAULT_MAX_CHECKPOINTS = 1000; // includes max-concurrent-checkpoints + checkpoints to be aborted (scheduled via mailbox)", "originalCommit": "884371e7dc8a604f5b55b8e39f27a1bffd002173", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMzQxMA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437933410", "bodyText": "Yes, me and @AHeise also had these concerns too and discussed it above (I increased the limit to 1K since then).\nI'm not sure if there is a proper solution because in the general case (max-concurrent-checkpoints > 1) we simply don't know which checkpoints to abort. Another issue is thread-safety, which is probably solvable but with some extra complexity which I think doesn't pay off here.", "author": "rkhachatryan", "createdAt": "2020-06-10T07:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk5NTYwNQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437995605", "bodyText": "Ok, let's keep it as now since no better solutions right now. And further improve it if possible future.", "author": "zhijiangW", "createdAt": "2020-06-10T09:40:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkwNjE5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437930188", "bodyText": "For the below logic for checking abort request from RPC:\nif (checkAndClearAbortedStatus(metadata.getCheckpointId())) {\n\t\t\tLOG.info(\"Checkpoint {} has been notified as aborted, would not trigger any checkpoint.\", metadata.getCheckpointId());\n\t\t\treturn;\n}\n\nDo we also need to call channelStateWriter.abort for RPC abort? If so, maybe we can integrate these two cases together for better handling the actions for aborting.", "author": "zhijiangW", "createdAt": "2020-06-10T07:53:32Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java", "diffHunk": "@@ -221,9 +232,14 @@ public void checkpointState(\n \t\t// We generally try to emit the checkpoint barrier as soon as possible to not affect downstream\n \t\t// checkpoint alignments\n \n+\t\tif (lastCheckpointId >= metadata.getCheckpointId()) {", "originalCommit": "26343fa05d8e72cf21cea7246e332c7850e05db3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzODMzMA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437938330", "bodyText": "Yes, I added it as a separate commit (it is just an optimization because we'll still get a \"regular\" abort call).", "author": "rkhachatryan", "createdAt": "2020-06-10T08:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzOTgyNw==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437939827", "bodyText": "In addition, it is a bit tough to understand the semantic of different cases now. The previous lastCheckpointId only reflects the already happened checkpoint, so we can rely on lastCheckpointId >= metadata.getCheckpointId() to discard all the following smaller checkpoints based on the checkpoint id incremental guarantee.\nNow the lastCheckpointId also reflects the canceled id from CheckpointBarrierHandler, so if the handler notifies to abort checkpoint 10, should we also need to ignore checkpoint 9. If so, it is also different with the abort case from RPC. If RPC notifies to abort checkpoint 10, the checkpoint 9 can still execute afterwards since we store the aborted id from RPC in the abortedCheckpointIds set.", "author": "zhijiangW", "createdAt": "2020-06-10T08:10:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk0NzQ5MA==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437947490", "bodyText": "Yes, this is the right semantics and I agree it is more complex now.\nI described it in the comment for the aborted checkpoints field.\nHowever, I think this new complexity comes from the domain (or maybe design), and therefore can't be avoided in implementation.", "author": "rkhachatryan", "createdAt": "2020-06-10T08:22:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk5NDk2OQ==", "url": "https://github.com/apache/flink/pull/12478#discussion_r437994969", "bodyText": "Yes, the current complexity somehow comes from coupling abort from handler with abort from RPC together, since abortedCheckpointIds will compare with lastCheckpointId(aborted id from handler) in some processes.\nBut indeed I can not think of an easy solution right now and also have not thought of critical problems ATM. So let's further discuss it if encountering problems future.", "author": "zhijiangW", "createdAt": "2020-06-10T09:39:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDE4OA=="}], "type": "inlineReview"}]}