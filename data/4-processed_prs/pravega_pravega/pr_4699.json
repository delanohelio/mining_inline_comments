{"pr_number": 4699, "pr_title": "Issue 4676: (PDP-34) Part 2 of 4 - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.", "pr_createdAt": "2020-04-16T01:47:36Z", "pr_url": "https://github.com/pravega/pravega/pull/4699", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "21221c46235598cd3ea78f0255cfeb7457fad145", "url": "https://github.com/pravega/pravega/commit/21221c46235598cd3ea78f0255cfeb7457fad145", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-04T23:18:00Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "url": "https://github.com/pravega/pravega/commit/931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-12T02:40:50Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "url": "https://github.com/pravega/pravega/commit/5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-24T21:48:59Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzQ5Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447037492", "bodyText": "Remind me whether this is support for write at offset or append only at the end.", "author": "fpj", "createdAt": "2020-06-29T14:59:22Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,440 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4ODE5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453788193", "bodyText": "It should be append only at end", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:48:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzQ5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzMDcyMQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454630721", "bodyText": "It should append only at the end.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:39:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzQ5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MzI3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447043278", "bodyText": "Is spotbugs really complaining about this? We don't do it in other parts of the code and it doesn't complain, why does it complain here, do you know?", "author": "fpj", "createdAt": "2020-06-29T15:07:16Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,440 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws NullPointerException      If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IndexOutOfBoundsException Throws IndexOutOfBoundsException in case of invalid index.\n+     */\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks using storage native  functionality. (Eg. Multipart upload.)\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be appended together.\n+     *               The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            throw e; // make spotbugs happy", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MTkzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454641933", "bodyText": "I'm not sure why", "author": "sachin-j-joshi", "createdAt": "2020-07-14T21:00:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MzI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NTkxMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447045910", "bodyText": "Doesn't FileChannel have a truncation operation?", "author": "fpj", "createdAt": "2020-06-29T15:10:56Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,431 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMyMzQwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r449323403", "bodyText": "We need supportsTruncation to support truncation at front, not end.", "author": "sachin-j-joshi", "createdAt": "2020-07-03T00:37:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NTkxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0ODU2OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447048568", "bodyText": "In general, the value I see in javadocs for overridden methods is the additional information about the specifics of the implementation. Otherwise, just copying from parent class or interface doesn't add much. I'm in favor of adding comments to reflect what is relevant for the particular implementation.", "author": "fpj", "createdAt": "2020-06-29T15:14:41Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,431 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MjE1OQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454642159", "bodyText": "removed redundant comments.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T21:00:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0ODU2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDQ5OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447050498", "bodyText": "I also thought that HDFS had a truncate operation, is there a reason for us to say that it doesn't support truncate?\nhttps://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#truncate", "author": "fpj", "createdAt": "2020-06-29T15:17:17Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMyMzU5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r449323593", "bodyText": "We need supportsTruncation to support truncation at front, not end.", "author": "sachin-j-joshi", "createdAt": "2020-07-03T00:38:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDQ5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MzQ5Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455143496", "bodyText": "I think one way to disambiguate this is to name this operation HeadTruncation. We may want to rename these methods to supportsHeadTruncation. We can do it in a future issue.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:31:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDQ5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MTIxOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447051218", "bodyText": "Can we import rather than have the full reference?", "author": "fpj", "createdAt": "2020-06-29T15:18:17Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (org.apache.hadoop.fs.FileAlreadyExistsException e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MzY3Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454643673", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T21:03:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MTIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MTczNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447051736", "bodyText": "Remove new line.", "author": "fpj", "createdAt": "2020-06-29T15:19:00Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (org.apache.hadoop.fs.FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws NullPointerException      If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NTQxMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447055412", "bodyText": "This comment needs to change similarly to the comment in the previous PR, it should be 2^61 - 1 instead of 2^62. I'm actually wondering why I'm seeing this here, is it because this PR includes changes of Part 1?", "author": "fpj", "createdAt": "2020-06-29T15:24:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NzM1OQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447057359", "bodyText": "I actually confirmed that this is the case, that the changes of Part 1 are also here. What's the plan exactly? Do you expect git to automatically sort this out when Part 1 is merged?", "author": "fpj", "createdAt": "2020-06-29T15:26:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NTQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMyMzg1NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r449323855", "bodyText": "Re-targeted base branch.", "author": "sachin-j-joshi", "createdAt": "2020-07-03T00:40:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NTQxMg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "url": "https://github.com/pravega/pravega/commit/000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-02T15:06:31Z", "type": "forcePushed"}, {"oid": "2915a6f096ffbf738c471619c5495ec5b98716a8", "url": "https://github.com/pravega/pravega/commit/2915a6f096ffbf738c471619c5495ec5b98716a8", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Rename ChunkManager to ChunkedSegmentStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-08T17:05:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453553047", "bodyText": "Should we be using preconditions here instead? Also, don't we need to check the fromOffset with respect to the bufferOffset? Is fromOffset relative to buggerOffset?", "author": "fpj", "createdAt": "2020-07-13T10:29:16Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4OTA5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453789093", "bodyText": "You need to check bufferOffset and length and validate against buffer as well", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:50:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODA3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638078", "bodyText": "This check is not required as it is already checked by the caller method in base class.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:53:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODE1MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638150", "bodyText": "removed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:53:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NTE4OQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453555189", "bodyText": "Why is this throwing UnsupportedOperationException, where is this coming from?", "author": "fpj", "createdAt": "2020-07-13T10:33:20Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MjM1Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454642357", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T21:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NTE4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NjUzOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453556538", "bodyText": "It is a bit odd to throw FileNotFoundException as this is not about files, maybe have an exception that derives from IOException or ChunkStorageException. Would it be a problem to have both ObjectNotFoundException and FileNotFoundException from a catch perspective? Maybe they can both derive from a NotFoundException class?", "author": "fpj", "createdAt": "2020-07-13T10:35:40Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcyMDUzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453720533", "bodyText": "This should throw ChunkNotFoundException. Looks like missed that one. (I am not sure why compiler did not complain. ).\nI'll fix this", "author": "sachin-j-joshi", "createdAt": "2020-07-13T15:09:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NjUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODUwNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638505", "bodyText": "fixed. Was leftover from refactor.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:53:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NjUzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1OTcxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453559715", "bodyText": "We recently had some issues with MPUs in ECS, and here I see that we are always performing concatenation with MPU, could you remind me why this is the right approach?", "author": "fpj", "createdAt": "2020-07-13T10:41:50Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcyODQzMQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453728431", "bodyText": "Based on our experience with ExtendedS3, there is now a config value minSizeLimitForConcat. If size of the source object is below this size then concat is not called. Instead concat is performed using appends.\nTo avoid confusion, we removed concept of separate \"concat using native capabilities\" vs \"concat using append\" from ChunkStorage. Instead of thatChunkedSegmentStorage itself now decides whether to call concat or not and it directly translates \"concat using append\" into appropriate write() calls on ChunkStorage .", "author": "sachin-j-joshi", "createdAt": "2020-07-13T15:20:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1OTcxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MTg3NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453561874", "bodyText": "I'm also wondering if this should be a pre-condition.", "author": "fpj", "createdAt": "2020-07-13T10:45:48Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return true;\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), config.getPrefix() + handle.getChunkName()).withAcl(acl));\n+    }\n+\n+    private <T> T throwException(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                throw new ChunkNotFoundException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                throw new ChunkAlreadyExistsException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                throw new IllegalArgumentException(chunkName, e);\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                throw new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message));\n+            }\n+        }\n+\n+        if (e instanceof IndexOutOfBoundsException) {\n+            throw new ArrayIndexOutOfBoundsException(e.getMessage());\n+        }\n+\n+        throw Exceptions.sneakyThrow(e);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), config.getPrefix() + chunkName).getObjects().isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MzI2Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454643267", "bodyText": "We must throw chunk already exists exception.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T21:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MTg3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MjUwOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453562509", "bodyText": "This one is breaking the pattern of catching exception and call throwException. Could we do it differently to follow the same pattern?", "author": "fpj", "createdAt": "2020-07-13T10:46:54Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return true;\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), config.getPrefix() + handle.getChunkName()).withAcl(acl));\n+    }\n+\n+    private <T> T throwException(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                throw new ChunkNotFoundException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                throw new ChunkAlreadyExistsException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                throw new IllegalArgumentException(chunkName, e);\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                throw new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message));\n+            }\n+        }\n+\n+        if (e instanceof IndexOutOfBoundsException) {\n+            throw new ArrayIndexOutOfBoundsException(e.getMessage());\n+        }\n+\n+        throw Exceptions.sneakyThrow(e);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), config.getPrefix() + chunkName).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), config.getPrefix() + chunkName, null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+            return true;\n+        } catch (S3Exception e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDAwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794003", "bodyText": "This method is supposed to return a boolean, so I think that's why we have it that way.", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:58:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MjUwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNzQwMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454627400", "bodyText": "Yes that is correct it needs to return boolean.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MjUwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2NDg4Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453564887", "bodyText": "I see that you are trying to make this uniform across chunk storage implementations. One way maybe is to have a \"NotFoundException\" and derive one subtype for each chunk storage implementation?", "author": "fpj", "createdAt": "2020-07-13T10:51:31Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNzU4OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454627588", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2NDg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2ODAxNw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453568017", "bodyText": "Should these two be  in the inverted order? The same question holds for other before() and after() calls below.", "author": "fpj", "createdAt": "2020-07-13T10:57:51Z", "path": "bindings/src/test/java/io/pravega/storage/hdfs/HDFSSimpleStorageTest.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkedRollingStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.SimpleStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.SystemJournalTests;\n+import lombok.Getter;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.util.concurrent.Executor;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/***\n+ * Unit tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+ */\n+public class HDFSSimpleStorageTest extends SimpleStorageTests {\n+    @Rule\n+    public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+    private TestContext testContext = new TestContext();\n+\n+    @Before\n+    public void before() throws Exception {\n+        super.before();\n+        testContext.setUp();\n+    }\n+\n+    @After\n+    public void after() throws Exception {\n+        testContext.tearDown();\n+        super.after();\n+    }\n+\n+    protected ChunkStorage getChunkStorage()  throws Exception {\n+        return testContext.getChunkStorage();\n+    }\n+\n+    /**\n+     * {@link ChunkedRollingStorageTests} tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+     */\n+    public static class HDFSRollingTests extends ChunkedRollingStorageTests {\n+        @Rule\n+        public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+        private TestContext testContext = new TestContext();\n+\n+        @Before\n+        public void before() throws Exception {\n+            super.before();\n+            testContext.setUp();\n+        }\n+\n+        @After\n+        public void after() throws Exception {\n+            testContext.tearDown();\n+            super.after();\n+        }\n+\n+        protected ChunkStorage getChunkStorage(Executor executor)  throws Exception {\n+            return testContext.getChunkStorage();\n+        }\n+    }\n+\n+    /**\n+     * {@link ChunkStorageTests} tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+     */\n+    public static class HDFSChunkStorageTests extends ChunkStorageTests {\n+        @Rule\n+        public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+        private TestContext testContext = new TestContext();\n+\n+        @Before\n+        public void before() throws Exception {\n+            testContext.setUp();\n+            super.before();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU5OTE0Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454599143", "bodyText": "Test context should be initialized before base class methods are called (which call overridden methods in this class).", "author": "sachin-j-joshi", "createdAt": "2020-07-14T19:41:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2ODAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYxNzEyMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454617122", "bodyText": "Fixed after().", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:14:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2ODAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453569185", "bodyText": "I'm not sure I understand the purpose of this mock, there is only a single test case it is serving, doReadTest.", "author": "fpj", "createdAt": "2020-07-13T11:00:16Z", "path": "bindings/src/test/java/io/pravega/storage/filesystem/FileSystemChunkStorageMockTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import lombok.Getter;\n+import lombok.Setter;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.mockito.ArgumentCaptor;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.spi.AbstractInterruptibleChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.*;\n+import static org.mockito.Mockito.mock;\n+\n+/**\n+ * Unit tests for {@link FileSystemChunkStorage} that uses mocks.\n+ */\n+public class FileSystemChunkStorageMockTest {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYxOTM1MQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454619351", "bodyText": "There are some corner cases in read logic that can only be tested using Mockito mock.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:19:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwNzQ3OQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454907479", "bodyText": "Ok, this name confused me. if this is about testing the read logic in a given way, then name it accordingly. I thought you're defining a mock to be used across test cases, while it sounds like there are aspects of the read logic that require mocking, so the test is about read, not mocking.", "author": "fpj", "createdAt": "2020-07-15T09:10:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyODUxNA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455028514", "bodyText": "This is not only about the read logic, but testing FileSystemChunkStorage functionality by mocking underlying file system apis.  In this context the name correctly reflects the intent.\nMocks are useful to create hard to reproduce scenarios. In future I do intent to add more tests around handling exception.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T12:54:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDMzMQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453574331", "bodyText": "Why is this necessary?", "author": "fpj", "createdAt": "2020-07-13T11:10:47Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODc0OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638748", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:54:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDk4Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453574983", "bodyText": "This doesn't really throw ChunkStorageException, does it?", "author": "fpj", "createdAt": "2020-07-13T11:12:06Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public void throwExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            throw new ChunkNotFoundException(chunkName, message);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            throw new ChunkAlreadyExistsException(chunkName, message);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NTE1NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453795154", "bodyText": "It doesn't have to", "author": "andreipaduroiu", "createdAt": "2020-07-13T17:00:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyODE2MQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454628161", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:35:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3ODQ0Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453578446", "bodyText": "The false parameter here indicates that the file metadata change is not flushed. Given that we are changing the size of the file, I'd think that the metadata flush is necessary. What do you think?", "author": "fpj", "createdAt": "2020-07-13T11:18:55Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public void throwExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            throw new ChunkNotFoundException(chunkName, message);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            throw new ChunkAlreadyExistsException(chunkName, message);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Timer timer = new Timer();\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length != 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Timer timer = new Timer();\n+\n+        if (handle.isReadOnly()) {\n+            throw new IllegalArgumentException(\"Write called on a readonly handle of chunk \" + handle.getChunkName());\n+        }\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length != 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(false);\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {\n+                    while (length > 0) {\n+                        long bytesTransferred = targetChannel.transferFrom(sourceFile.getChannel(), offset, length);\n+                        offset += bytesTransferred;\n+                        length -= bytesTransferred;\n+                    }\n+                    targetChannel.force(false);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ1OTgzNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454459835", "bodyText": "According to the fsync documentation, if file metadata is changed as a result of file data change then metadata is also forced sync. (any metadata change that affects ability to read data correctly). It will not however sync if the only change is related to non data attributes.\nIn this case, this is effectively same as force(true).  I'll make it true to avoid confusion.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T15:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3ODQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3OTMxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453579315", "bodyText": "Why is it swallowing the exception? Maybe the file is there and the exception is due to something else, in which case the return value is incorrect.", "author": "fpj", "createdAt": "2020-07-13T11:20:31Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NzAwMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453797002", "bodyText": "This code is copied from the existing HDFSStorage.java. However the code within the try block is also very different. Can we check the contracts for that method to see if it warrants this whole try-catch block in here too?", "author": "andreipaduroiu", "createdAt": "2020-07-13T17:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3OTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODg1NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638855", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:54:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3OTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MDM1OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453580358", "bodyText": "Same comment about preconditions and checking fromOffset and bufferOffset. make sure that we are comprehensively checking everything we need to check.", "author": "fpj", "createdAt": "2020-07-13T11:22:43Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ2MDY0Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454460643", "bodyText": "Removed, this check was unnecessary. The preconditions are checked already.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T15:52:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MDM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MTE2Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453581167", "bodyText": "The {} is not needed.", "author": "fpj", "createdAt": "2020-07-13T11:24:25Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MTI5NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454641294", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:59:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MTE2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjE5Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582192", "bodyText": "it doesn't look like we need this boolean retValue declaration.", "author": "fpj", "createdAt": "2020-07-13T11:26:29Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzODk5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454638993", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:54:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjE5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjQ5Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582492", "bodyText": "This status is not used anywhere.", "author": "fpj", "createdAt": "2020-07-13T11:27:09Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNTIzOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454635239", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:47:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjQ5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjY1Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582657", "bodyText": "This status isn't used anywhere.", "author": "fpj", "createdAt": "2020-07-13T11:27:31Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NDI5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453584293", "bodyText": "Fix this comment as the last part does not read correctly.", "author": "fpj", "createdAt": "2020-07-13T11:30:42Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNTQxNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454635416", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NDI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjAzNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453586036", "bodyText": "Maybe point to documentation about this so that anyone checking this code understands the impact of this choice. it would be particularly important to document the potential impact of setting this to true. A quick search reveals comments going both ways.", "author": "fpj", "createdAt": "2020-07-13T11:34:12Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we\n+        // want to own our implementation so that when we close it, we don't interfere with others.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTEzNDUxMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455134510", "bodyText": "updated.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T15:19:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjAzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjkwMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453586900", "bodyText": "Could we check this as a precondition?", "author": "fpj", "createdAt": "2020-07-13T11:35:47Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we\n+        // want to own our implementation so that when we close it, we don't interfere with others.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\n+        if (!this.config.isReplaceDataNodesOnFailure()) {\n+            // Default is DEFAULT, so we only set this if we want it disabled.\n+            conf.set(\"dfs.client.block.write.replace-datanode-on-failure.policy\", \"NEVER\");\n+        }\n+\n+        this.fileSystem = openFileSystem(conf);\n+        log.info(\"Initialized (HDFSHost = '{}'\", this.config.getHdfsHostURL());\n+    }\n+\n+    private FileSystem openFileSystem(Configuration conf) throws IOException {\n+        return FileSystem.get(conf);\n+    }\n+    //endregion\n+\n+    //region Helpers\n+    private void ensureInitializedAndNotClosed() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem != null, \"HDFSStorage is not initialized.\");\n+    }\n+\n+    //endregion\n+\n+    //Region HDFS helper methods.\n+\n+    /**\n+     * Gets an HDFS-friendly path prefix for the given chunk name by pre-pending the HDFS root from the config.\n+     */\n+    private String getPathPrefix(String chunkName) {\n+        return this.config.getHdfsRoot() + Path.SEPARATOR + chunkName;\n+    }\n+\n+    /**\n+     * Gets the full HDFS Path to a file for the given chunk, startOffset and epoch.\n+     */\n+    private Path getFilePath(String chunkName) {\n+        Preconditions.checkState(chunkName != null && chunkName.length() > 0, \"chunkName must be non-null and non-empty\");\n+        return new Path(getPathPrefix(chunkName));\n+    }\n+\n+    /**\n+     * Determines whether the given FileStatus indicates the file is read-only.\n+     *\n+     * @param fs The FileStatus to check.\n+     * @return True or false.\n+     */\n+    private boolean isReadOnly(FileStatus fs) {\n+        return fs.getPermission().getUserAction() == FsAction.READ;\n+    }\n+\n+    private int readInternal(ChunkHandle handle, byte[] buffer, long offset, int bufferOffset, int length) throws IOException {\n+        //There is only one file per chunkName.\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(offset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", offset));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzOTc5NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454639794", "bodyText": "Precondition is checked elsewhere. But still need to handle runtime exception.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:56:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjkwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453790312", "bodyText": "This stealthy way of throwing exceptions can lead to bugs down the line. For example, the return 0 line below is unreachable, but the compiler does not know that.\nPlease rework this to include the throw keyword.", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:52:23Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDM3Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453790377", "bodyText": "Same below too.", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MzMyOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453793328", "bodyText": "I think this can be done by rewriting this as return throwException(...)", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:57:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MDAxMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454640010", "bodyText": "Exactly - that is what I did.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:56:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MTExNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453791115", "bodyText": "\"Concatenated\"", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:53:39Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453792153", "bodyText": "I don't understand where this is coming from. Why are we not seeing this in other parts of the code? What specifically is triggering it?", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:55:21Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNzAyNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454627025", "bodyText": "No idea! Couldn't get rid of it.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:32:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MTc2NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455141764", "bodyText": "So what is the Spotbugs actual message? What method in the try block above has such a weird declaration that spotbugs isn't happy?", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:29:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwODk0MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455208940", "bodyText": "Summary\nWarning Type \tNumber\nDodgy code Warnings \t1\nTotal \t1\nWarnings\nClick on a warning row to see full context information.\nDodgy code Warnings\nCode \tWarning\nREC \tException is caught when Exception is not thrown in io.pravega.storage.extendeds3.ExtendedS3ChunkStorage.doConcat(ConcatArgument[])\nDetails\nREC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\nThis method uses a try-catch block that catches Exception objects, but Exception is not thrown within the try block, and RuntimeException is not explicitly caught. It is a common bug pattern to say try { ... } catch (Exception e) { something } as a shorthand for catching a number of types of exception each of whose catch blocks is identical, but this construct also accidentally catches RuntimeException as well, masking potential bugs.\nA better approach is to either explicitly catch the specific exceptions that are thrown, or to explicitly catch RuntimeException exception, rethrow it, and then catch all non-Runtime Exceptions, as shown below:\ntry {\n    ...\n} catch (RuntimeException e) {\n    throw e;\n} catch (Exception e) {\n    ... deal with all non-runtime exceptions ...\n}", "author": "sachin-j-joshi", "createdAt": "2020-07-15T17:19:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwOTIwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455209203", "bodyText": "S3Exception is derived from RuntimeException", "author": "sachin-j-joshi", "createdAt": "2020-07-15T17:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjI5Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456112296", "bodyText": "Ok. I see now.", "author": "andreipaduroiu", "createdAt": "2020-07-16T22:24:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453792766", "bodyText": "I want to understand how we recover from a partially completed concat. What happens if the process crashes:\n\nIn the middle of the above loop (when we build the MPU).\nBetween the above loop and the below loop (cleanup).", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:56:21Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNjc0Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454626746", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:32:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MDMwMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455140300", "bodyText": "I don't understand how this is fixed. How is the garbage cleaned up? Is there garbage? If it's already handled, let's write it in a comment.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:27:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxMDY5Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455310697", "bodyText": "Moved the deletion and garbage collection to upper layers (ChunkedSegmentStorage).", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDI0Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794243", "bodyText": "Revert this and all the other files that have no-op changes.", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:58:49Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3SegmentHandle.java", "diffHunk": "@@ -17,7 +17,6 @@\n     private final String segmentName;\n     private final boolean isReadOnly;\n \n-", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNDc0Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454634746", "bodyText": "done.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:47:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDI0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDM5MQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794391", "bodyText": "Revert this file.", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:59:06Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -107,7 +107,6 @@ public ExtendedS3Storage(S3Client client, ExtendedS3StorageConfig config) {\n \n     //endregion\n \n-", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNDg0Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454634842", "bodyText": "done", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDg4OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794888", "bodyText": "Please see my comment in ExtendedS3ChunkStorage.java", "author": "andreipaduroiu", "createdAt": "2020-07-13T16:59:56Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDY0MDE3MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454640170", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDg4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NTk2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453795966", "bodyText": "Javadoc for all these fields.", "author": "andreipaduroiu", "createdAt": "2020-07-13T17:01:38Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemUtils.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.util.Set;\n+\n+/**\n+ * Utils for File System.\n+ */\n+public class FileSystemUtils {\n+    public static final Set<PosixFilePermission> READ_ONLY_PERMISSION = ImmutableSet.of(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyODM2Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454628362", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:35:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NTk2Ng=="}], "type": "inlineReview"}, {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T23:13:16Z", "type": "commit"}, {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T23:13:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4ODMwMQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454188301", "bodyText": "I see that we are concatenating very often config.getPrefix() + handle.getChunkName():\n\nit is better to have a common function to compose the final object name\nwe should validate the chunkName and check for invalid values for S3, probably sanitize it by replacing invalid characters\nwhat about adding some know separator between the prefix and the chunkName ? this way it won't be possible to mix data from two separate Pravega clusters that share the same S3 bucket. I know this is a remote possibility because \"prefix\" should be unique and checkName will be some kind of UUID, but adding a check does not cost very much and will make this safer", "author": "eolivelli", "createdAt": "2020-07-14T08:23:22Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());", "originalCommit": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNDI4MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454634280", "bodyText": "I fixed the first issue.\nNot sure about 2nd and 3rd. These names are generated by us and should not contain invalid chars.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4ODMwMQ=="}], "type": "inlineReview"}, {"oid": "dd27958f57c16d871c7bb1eb390817d5787a7c17", "url": "https://github.com/pravega/pravega/commit/dd27958f57c16d871c7bb1eb390817d5787a7c17", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-14T19:45:12Z", "type": "commit"}, {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d", "url": "https://github.com/pravega/pravega/commit/7070fa7fd2549b9bcf3577e4260be3882218524d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-14T20:31:30Z", "type": "commit"}, {"oid": "9cb3101676aff678eb0bf589ddfbdcaa933a6847", "url": "https://github.com/pravega/pravega/commit/9cb3101676aff678eb0bf589ddfbdcaa933a6847", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Update comment.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T15:12:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MzkwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455083903", "bodyText": "This can be a long process for a single operation. Irrespective of the time this operation can take, my question is: what happens if this operation fails within this loop? Do we leave chunks half concatenated? How do we complete this operation if it is interrupted in the middle of the processing (some chunks are concatenated, some others not yet)?", "author": "RaulGracia", "createdAt": "2020-07-15T14:13:33Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MDU2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455140566", "bodyText": "Extended S3 guarantees that MPU operation is automatic.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T15:27:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MzkwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzA4OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455087088", "bodyText": "Are we expecting false here if this process cannot complete? Otherwise, why to return anything at all? It can either execute normally or throw and do not return anything.", "author": "RaulGracia", "createdAt": "2020-07-15T14:17:45Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI2MDIyOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455260229", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-15T18:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzA4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4ODcyMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455088720", "bodyText": "If this \"NoSuchKey\" is something of the protocol, put it at least as a constant for readability", "author": "RaulGracia", "createdAt": "2020-07-15T14:19:55Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE2MTkwMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455161902", "bodyText": "see below", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4ODcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455089182", "bodyText": "I don't know if we would need a place to set all these hardcoded strings for better readability.", "author": "RaulGracia", "createdAt": "2020-07-15T14:20:32Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTEzNzgwMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455137800", "bodyText": "IMO, This is the best place to keep them. They are ExtendedS3 specific and nobody else should need to know about them.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:23:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE2MTc5Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455161797", "bodyText": "agree.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:01:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA5NDQ3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455094478", "bodyText": "This kind of string concatenations may be expensive if executed too often.", "author": "RaulGracia", "createdAt": "2020-07-15T14:27:22Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"IllegalArgumentException\", e));\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message), e);\n+            }\n+        }\n+\n+        if (retValue == null) {\n+            retValue = new ChunkStorageException(chunkName, message, e);\n+        }\n+\n+        return retValue;\n+    }\n+\n+    private String getObjectPath(String objectName) {\n+        return config.getPrefix() + objectName;", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE1MDk5NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455150995", "bodyText": "Not really. concatenation is not in loop or anything. This concat is unavoidable we need to add object name to prefix and then pass it to S3 client as parameter.\nI am sure this is the fastest way to concat two strings. String.format or other things like StringBuffer etc would have additional overhead that is not necessary.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T15:43:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA5NDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455116693", "bodyText": "Could it be possible that by some error we end up with a negative length that prevent this loop from finishing? Would a condition like length > 0 be safer in this case?", "author": "RaulGracia", "createdAt": "2020-07-15T14:54:59Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,305 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public ChunkStorageException convertExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Timer timer = new Timer();\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length != 0);", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTEzNDE0Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455134143", "bodyText": "+1.\nI'm always advocating for a broader condition in these cases for the same reason as Raul mentioned.\nIf you look below, you already use this pattern. Let's be consistent.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxOTQ4MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455319480", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:23:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147195", "bodyText": "Why would client be null? you set it in the constructor", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:37:41Z", "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();\n+        client = new S3JerseyClientWrapper(s3Config, s3Proxy);\n+        client.createBucket(bucketName);\n+        List<ObjectKey> keys = client.listObjects(bucketName).getObjects().stream()\n+                .map(object -> new ObjectKey(object.getKey()))\n+                .collect(Collectors.toList());\n+\n+        if (!keys.isEmpty()) {\n+            client.deleteObjects(new DeleteObjectsRequest(bucketName).withKeys(keys));\n+        }\n+    }\n+\n+    public void close() throws Exception {\n+        if (client != null) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5ODgwNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455198806", "bodyText": "in case close is called on half initialized object.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T17:02:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5OTk4NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455299984", "bodyText": "What is a half initialized object? Can you show me an example where only half of a constructor executes and then you get an object as a result?", "author": "andreipaduroiu", "createdAt": "2020-07-15T19:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxMTcwOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455311708", "bodyText": "line 56 above.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:08:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxODk1OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455318958", "bodyText": "You can not write following code\ntry {\n} catch (Exception e) {\n         if (client != null) {\n             client.destroy();\n         }\n         if (s3Proxy != null) {\n             s3Proxy.stop();\n         }\n         throw e;\n     }\n\n\nTask :bindings:compileTestJava\n/home/sachin/workspace/active5/pravega/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java:56: error: variable client might not have been initialized\nif (client != null) {\n^\n/home/sachin/workspace/active5/pravega/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java:59: error: variable s3Proxy might not have been initialized\nif (s3Proxy != null) {\n^\n2 errors\n\n\nTask :bindings:compileTestJava FAILED\n\n\nbut following works so keeping it that way.\n} catch (Exception e) {\n            close();\n            throw e;\n        }\n\n\n  public void close() throws Exception {\n        if (client != null) {\n            client.destroy();\n        }\n        if (s3Proxy != null) {\n            s3Proxy.stop();\n        }\n    }\n}", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:22:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147646", "bodyText": "There is a chance that something in the constructor fails. We want to put a try-catch block here to shut down the s3proxy if we fail before we exit the constructor.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:38:25Z", "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE3NDg2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455174866", "bodyText": "The close is called in  @after in test classes.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:23:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5OTU4NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455299584", "bodyText": "You will have nothing to call close on since this is the constructor. If the constructor throws, there will be no object to call close on, but the side effects will remain. It is your responsibility to clean up any such side effects in the constructor should it fail.", "author": "andreipaduroiu", "createdAt": "2020-07-15T19:45:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxMjc2Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455312762", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}], "type": "inlineReview"}, {"oid": "76093576b69ac136b985a396a397ca56089069f1", "url": "https://github.com/pravega/pravega/commit/76093576b69ac136b985a396a397ca56089069f1", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T17:44:09Z", "type": "commit"}, {"oid": "6b364af35c7dbda36d481b27a3d70501523c64c5", "url": "https://github.com/pravega/pravega/commit/6b364af35c7dbda36d481b27a3d70501523c64c5", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - make return value of doSetReadOnly as void.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T18:30:49Z", "type": "commit"}, {"oid": "d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "url": "https://github.com/pravega/pravega/commit/d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - After concat delete chunks properly in ChunkedSegmentStorage instead of ChunkStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T20:02:12Z", "type": "commit"}, {"oid": "52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "url": "https://github.com/pravega/pravega/commit/52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Better parameter comment for defrag.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T20:34:20Z", "type": "commit"}, {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63", "url": "https://github.com/pravega/pravega/commit/30225a38a4b6dde90ff3154ef14651097df8dd63", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-16T01:55:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNDc0Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455734747", "bodyText": "as for S3 storage, we should have a common method to perform Paths.get(config.getRoot(), chunkName)\nit can also be a security issue to let chunkName be anystring.\nwe should have some validation that at least blocks:\n\nnull chars\nrelative path sequences (\"..\", \"/\",\"\",\".\")\nanche chunkname cannot be empty\n\nwe cannot assume that the caller is a good guy, even if this is our own code", "author": "eolivelli", "createdAt": "2020-07-16T12:03:09Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxOTU1Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456619552", "bodyText": "fixed the empty name part.\nNow also checking that path is not directory, that should handle second case.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNDc0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455735577", "bodyText": "isn't Files.size(path)  enough ?\nwhy are we passing thru PosixFileAttributes ?", "author": "eolivelli", "createdAt": "2020-07-16T12:04:54Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjI3Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736276", "bodyText": "you also have this getFileSize\nhttps://github.com/pravega/pravega/pull/4699/files#diff-17a1fa6c3b4ce4b99b95fde90189ccc5R190", "author": "eolivelli", "createdAt": "2020-07-16T12:06:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTc0Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456609747", "bodyText": "This was from the time when I had isReadOnly as property of Chunk .\nFixed", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736106", "bodyText": "please also check that this is a RegularFile, not a directory or a link (for instance)", "author": "eolivelli", "createdAt": "2020-07-16T12:05:55Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNzA3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456207078", "bodyText": "Good check to add. Adding", "author": "sachin-j-joshi", "createdAt": "2020-07-17T04:08:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTgzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456609833", "bodyText": "Fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737315", "bodyText": "this is bad from a security perspective\nString.valueOf(sourcePath)\nyou can use sourcePath.toFile()", "author": "eolivelli", "createdAt": "2020-07-16T12:08:19Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzgwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737803", "bodyText": "why are you using RandomAccessFile ?", "author": "eolivelli", "createdAt": "2020-07-16T12:09:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNDQxMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456204413", "bodyText": "Good catch.  Fixing it.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T03:56:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455738714", "bodyText": "why are you opening the same file more times ?\nit is better to open the file for write only once.\nI suggest you also to:\n\ncreate a temporary file\nfill it\nrename new the file with ATOMIC_MOVE flag to the targetPath\nThis way we are protected from partial writes", "author": "eolivelli", "createdAt": "2020-07-16T12:10:53Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNTQwOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456205409", "bodyText": "Good catch that I was opening same file multiple times. Fixing it.\nthe target chunk could be multi GB file.\nAs for why partial writes are still okay because,\nunless the whole operation succeeds..\n\nchunk metadata is not updated So even if resultant chunk has more data we never consider it valid part of chunk.\nThe source chunks are not deleted.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T04:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDAxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610015", "bodyText": "Fixed for loop", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:35:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739085", "bodyText": "Maybe this throw new UnsupportedOperationException()  should be the default implementation in the base class", "author": "eolivelli", "createdAt": "2020-07-16T12:11:35Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {\n+                    while (length > 0) {\n+                        long bytesTransferred = targetChannel.transferFrom(sourceFile.getChannel(), offset, length);\n+                        offset += bytesTransferred;\n+                        length -= bytesTransferred;\n+                    }\n+                    targetChannel.force(true);\n+                    totalBytesConcated += length;\n+                    offset += length;\n+                }\n+\n+            }\n+            return totalBytesConcated;\n+        } catch (IOException e) {\n+            throw convertExeption(chunks[0].getName(), \"doConcat\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3OTczOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456179738", "bodyText": "Ok. Makes sense.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T02:14:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDExOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610119", "bodyText": "done", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:35:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTc2Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739767", "bodyText": "what happens in case of failure during the execution of this method ?\nwe are losing the contents of the file IMHO", "author": "eolivelli", "createdAt": "2020-07-16T12:12:56Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3OTI5MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456179290", "bodyText": "There is a very specific segment store fail over situation when actual chunk has more data than its length recorded in metadata. This can happen when zombie instance keeps writing to chunk even after it is no longer the owner of the segment. The new segment store instance will effectively \"seal\" that chunk by recording the most recent length and adding a fresh new chunk. In short data after recorded length is garbage that needs to be ignored anyway.\nSee https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2#segment-store-failover\nHDFS concat does not have a concept of concatenating a partial  file. So we need to remove the end part.\nEven if we fail after truncate it still leaves the valid data untouched.\nSegment concat is called in only two cases\n\nfor cases where transaction segment is merged into the main segment.\nde-fragmenting an existing segment by merging smaller chunks into bigger chunks.\n\nIn both the cases above there is no possibility of having garbage data/invalid data at the front of the chunks.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T02:12:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455740133", "bodyText": "what about org.apache.hadoop.hdfs.DistributedFileSystem.class.getName() ?\nthis way we will have a compile time guarantee that the name is good", "author": "eolivelli", "createdAt": "2020-07-16T12:13:34Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwODM5MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456508390", "bodyText": "This sounds sensible.\n@sachin-j-joshi since this this not new code (it's borrowed from the other HDFSStorage.java), may I suggest collecting all these HDFS improvement ideas (there's at least one other below) in a new GitHub issue and we can address them later?\nThis particular change is unrelated to PDP34 so I don't want to tackle it here.", "author": "andreipaduroiu", "createdAt": "2020-07-17T15:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDkzNA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610934", "bodyText": "Fixed\nconf.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:36:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741050", "bodyText": "do we have a way to pass additional configuration parameters to this client ?\nI mean, probably users would like to add additional configurations, in segment store configuration file\nhdfs.customconfig.xxxx=yyyyy", "author": "eolivelli", "createdAt": "2020-07-16T12:15:16Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // We do not want FileSystem to cache clients/instances based on target URI.\n+        // This allows us to close instances without affecting other clients/instances. This should not affect performance.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\n+        if (!this.config.isReplaceDataNodesOnFailure()) {\n+            // Default is DEFAULT, so we only set this if we want it disabled.\n+            conf.set(\"dfs.client.block.write.replace-datanode-on-failure.policy\", \"NEVER\");\n+        }\n+\n+        this.fileSystem = openFileSystem(conf);", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwNzI1NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456507254", "bodyText": "The purpose of this PR was not to tweak the HDFS client. We can take that challenge in a separate issue. We are setting all the configs that we believe we need, and define the \"pass-through\" values in our own config. Otherwise we'd have an unmanageable zoo of configurations, most of which will be beyond our control.", "author": "andreipaduroiu", "createdAt": "2020-07-17T15:15:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU0NDUzNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456544535", "bodyText": "I'll create a tracking issue for this.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T16:23:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU0NjY0MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456546640", "bodyText": "@andreipaduroiu agreed.\n@sachin-j-joshi probably there is not need to create a ticket if no user asks for it.", "author": "eolivelli", "createdAt": "2020-07-17T16:27:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTM5Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741392", "bodyText": "what happens in case of failure ? where is recovery handled ?", "author": "eolivelli", "createdAt": "2020-07-16T12:15:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -847,6 +857,11 @@ private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, St\n                     concatUsingAppend(concatArgs);\n                 }\n \n+                // Delete chunks.\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksToDelete.add(chunksToConcat.get(i).getName());", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3NTcxOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456175718", "bodyText": "The list of chunks to delete is returns to the caller in this case ChunkedSegmentStorage::concat\nChunkedSegmentStorage::concat calls ChunkedSegmentStorage::collectGarbage which then immediately deletes the garbage chunks. This is called only after the whole concat operation and the metadata commit succeeds.\nWhat happens when delete fails ? We add it a list of \"to delete list\" of chunks which will be deleted by a background task.\nThere is a pending issue to create background garbage collection task #4903", "author": "sachin-j-joshi", "createdAt": "2020-07-17T01:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTM5Mg=="}], "type": "inlineReview"}, {"oid": "76d4c5537de5353dcdeaf5dd97afdd5aab313344", "url": "https://github.com/pravega/pravega/commit/76d4c5537de5353dcdeaf5dd97afdd5aab313344", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-17T18:16:49Z", "type": "commit"}]}